

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>åˆ†å¸ƒè®­ç»ƒ &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" />
    <link rel="next" title="ms-swift" href="../../../../../ms-swift/index.html" />
    <link rel="prev" title="NPUè®­ç»ƒ" href="npu_training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_installation.html">NPUå®‰è£…åŠé…ç½®</a></li>
<li class="toctree-l2"><a class="reference internal" href="npu_training.html">NPUè®­ç»ƒ</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">åˆ†å¸ƒè®­ç»ƒ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nativeddp">NativeDDP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">å•æœºå¤šå¡</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">å¤šæœºå¤šå¡</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#deepspeed">DeepSpeed</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id12">å•æœºå¤šå¡</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">å¤šæœºå¤šå¡</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">DeepSpeed é…ç½®æ–‡ä»¶</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fsdp">FSDP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id25">å•æœºå¤šå¡</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id29">å¤šæœºå¤šå¡</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fsdp2">FSDP2</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ray">Ray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id33">å•æœºå¤šå¡</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id35">å¤šæœºå¤šå¡</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
      <li class="breadcrumb-item active">åˆ†å¸ƒè®­ç»ƒ</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/LLaMA-Factory/source/advanced/distributed.rst.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<span id="id2"></span><h1>åˆ†å¸ƒè®­ç»ƒ<a class="headerlink" href="#id1" title="Link to this heading">ïƒ</a></h1>
<p>LLaMA-Factory æ”¯æŒå•æœºå¤šå¡å’Œå¤šæœºå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒã€‚åŒæ—¶ä¹Ÿæ”¯æŒ <a class="reference internal" href="#nativeddp"><span class="std std-ref">DDP</span></a>ã€<a class="reference internal" href="#deepspeed-ref"><span class="std std-ref">DeepSpeed</span></a> å’Œ <a class="reference internal" href="#fsdp-ref"><span class="std std-ref">FSDP</span></a> ä¸‰ç§åˆ†å¸ƒå¼å¼•æ“ã€‚</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/notes/ddp.html">DDP</a> (DistributedDataParallel) é€šè¿‡å®ç°æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œå®ç°è®­ç»ƒåŠ é€Ÿã€‚
ä½¿ç”¨ DDP çš„ç¨‹åºéœ€è¦ç”Ÿæˆå¤šä¸ªè¿›ç¨‹å¹¶ä¸”ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ›å»ºä¸€ä¸ª DDP å®ä¾‹ï¼Œä»–ä»¬ä¹‹é—´é€šè¿‡ <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> åº“åŒæ­¥ã€‚</p>
<p><a class="reference external" href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">DeepSpeed</a> æ˜¯å¾®è½¯å¼€å‘çš„åˆ†å¸ƒå¼è®­ç»ƒå¼•æ“ï¼Œå¹¶æä¾›ZeROï¼ˆZero Redundancy Optimizerï¼‰ã€offloadã€Sparse Attentionã€1 bit Adamã€æµæ°´çº¿å¹¶è¡Œç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚
æ‚¨å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚ä¸è®¾å¤‡é€‰æ‹©ä½¿ç”¨ã€‚</p>
<p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">FSDP</a> é€šè¿‡å…¨åˆ‡ç‰‡æ•°æ®å¹¶è¡ŒæŠ€æœ¯ï¼ˆFully Sharded Data Parallelï¼‰æ¥å¤„ç†æ›´å¤šæ›´å¤§çš„æ¨¡å‹ã€‚åœ¨ DDP ä¸­ï¼Œæ¯å¼  GPU éƒ½å„è‡ªä¿ç•™äº†ä¸€ä»½å®Œæ•´çš„æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨å‚æ•°ã€‚è€Œ FSDP åˆ‡åˆ†äº†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ä¸ä¼˜åŒ–å™¨å‚æ•°ï¼Œä½¿å¾—æ¯å¼  GPU åªä¿ç•™è¿™äº›å‚æ•°çš„ä¸€éƒ¨åˆ†ã€‚
é™¤äº†å¹¶è¡ŒæŠ€æœ¯ä¹‹å¤–ï¼ŒFSDP è¿˜æ”¯æŒå°†æ¨¡å‹å‚æ•°å¸è½½è‡³CPUï¼Œä»è€Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚ã€‚</p>
<p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">FSDP2</a> åœ¨é›†æˆFSDP1åŸºç¡€åŠŸèƒ½çš„å‰æä¸‹ï¼Œæ‘’å¼ƒäº† FSDP1 å°†å‚æ•°å‹æ‰æ‹¼æ¥çš„åšæ³•ï¼Œè½¬è€ŒåŸºäº DTensor å®ç°é€å‚æ•°åˆ‡åˆ†ï¼Œè¿™ä¸€æ¶æ„å‡çº§åœ¨å®Œæ•´ä¿ç•™æ¨¡å‹åŸå§‹ç»“æ„çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—ä¸é€šä¿¡çš„é‡å æ•ˆç‡ï¼Œè¿›è€Œå¢å¼ºè®­ç»ƒæ€§èƒ½ã€‚</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>å¼•æ“</p></th>
<th class="head"><p>æ•°æ®åˆ‡åˆ†</p></th>
<th class="head"><p>æ¨¡å‹åˆ‡åˆ†</p></th>
<th class="head"><p>ä¼˜åŒ–å™¨åˆ‡åˆ†</p></th>
<th class="head"><p>å‚æ•°å¸è½½</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DDP</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>ä¸æ”¯æŒ</p></td>
<td><p>ä¸æ”¯æŒ</p></td>
<td><p>ä¸æ”¯æŒ</p></td>
</tr>
<tr class="row-odd"><td><p>DeepSpeed</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
</tr>
<tr class="row-even"><td><p>FSDP</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
</tr>
<tr class="row-odd"><td><p>FSDP2</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
<td><p>æ”¯æŒ</p></td>
</tr>
</tbody>
</table>
<section id="nativeddp">
<span id="id3"></span><h2>NativeDDP<a class="headerlink" href="#nativeddp" title="Link to this heading">ïƒ</a></h2>
<p>NativeDDP æ˜¯ PyTorch æä¾›çš„ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒæ–¹å¼ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š</p>
<span class="target" id="torchrun"></span><section id="id4">
<span id="id5"></span><h3>å•æœºå¤šå¡<a class="headerlink" href="#id4" title="Link to this heading">ïƒ</a></h3>
<section id="llamafactory-cli">
<h4>llamafactory-cli<a class="headerlink" href="#llamafactory-cli" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ llamafactory-cli å¯åŠ¨ NativeDDP å¼•æ“ã€‚</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>examples/train_full/llama3_full_sft_ds3.yaml
</pre></div>
</div>
<p>å¦‚æœ <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> æ²¡æœ‰æŒ‡å®šï¼Œåˆ™é»˜è®¤ä½¿ç”¨æ‰€æœ‰GPUã€‚å¦‚æœéœ€è¦æŒ‡å®šGPUï¼Œä¾‹å¦‚ç¬¬0ã€1ä¸ªGPUï¼Œå¯ä»¥ä½¿ç”¨ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>config/config1.yaml
</pre></div>
</div>
</section>
<section id="id6">
<h4>torchrun<a class="headerlink" href="#id6" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> æŒ‡ä»¤å¯åŠ¨ NativeDDP å¼•æ“è¿›è¡Œå•æœºå¤šå¡è®­ç»ƒã€‚ä¸‹é¢æä¾›ä¸€ä¸ªç¤ºä¾‹ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w">  </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc-per-node<span class="o">=</span><span class="m">8</span><span class="w">  </span>src/train.py<span class="w"> </span><span class="se">\</span>
--stage<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w">  </span><span class="se">\</span>
--do_train<span class="w"> </span><span class="se">\</span>
--dataset<span class="w"> </span>alpaca_en_demo<span class="w"> </span><span class="se">\</span>
--template<span class="w"> </span>llama3<span class="w"> </span><span class="se">\</span>
--finetuning_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
--output_dir<span class="w">  </span>saves/llama3-8b/lora/<span class="w"> </span><span class="se">\</span>
--overwrite_cache<span class="w"> </span><span class="se">\</span>
--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--gradient_accumulation_steps<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
--lr_scheduler_type<span class="w"> </span>cosine<span class="w"> </span><span class="se">\</span>
--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
--save_steps<span class="w"> </span><span class="m">500</span><span class="w"> </span><span class="se">\</span>
--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
--num_train_epochs<span class="w"> </span><span class="m">2</span>.0<span class="w"> </span><span class="se">\</span>
--plot_loss<span class="w"> </span><span class="se">\</span>
--bf16
</pre></div>
</div>
</section>
<section id="accelerate">
<h4>accelerate<a class="headerlink" href="#accelerate" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> æŒ‡ä»¤å¯åŠ¨è¿›è¡Œå•æœºå¤šå¡è®­ç»ƒã€‚</p>
<p>é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œæ ¹æ®éœ€æ±‚å›ç­”ä¸€ç³»åˆ—é—®é¢˜åç”Ÿæˆé…ç½®æ–‡ä»¶ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>config
</pre></div>
</div>
<p>ä¸‹é¢æä¾›ä¸€ä¸ªç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼š</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># accelerate_singleNode_config.yaml</span>
<span class="nt">compute_environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class="nt">debug</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">distributed_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MULTI_GPU</span>
<span class="nt">downcast_bf16</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;no&#39;</span>
<span class="nt">enable_cpu_affinity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">gpu_ids</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">all</span>
<span class="nt">machine_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">main_training_function</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="nt">mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fp16</span>
<span class="nt">num_machines</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">num_processes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="nt">rdzv_backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class="nt">same_network</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">tpu_env</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">tpu_use_cluster</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">tpu_use_sudo</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">use_cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹æŒ‡ä»¤å¼€å§‹è®­ç»ƒ:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>accelerate_singleNode_config.yaml<span class="w"> </span><span class="se">\</span>
src/train.py<span class="w"> </span>training_config.yaml
</pre></div>
</div>
</section>
</section>
<section id="id7">
<span id="id8"></span><h3>å¤šæœºå¤šå¡<a class="headerlink" href="#id7" title="Link to this heading">ïƒ</a></h3>
<section id="id9">
<h4>llamafactory-cli<a class="headerlink" href="#id9" title="Link to this heading">ïƒ</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="m">192</span>.168.0.1<span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">29500</span><span class="w"> </span><span class="se">\</span>
llamafactory-cli<span class="w"> </span>train<span class="w"> </span>examples/train_lora/llama3_lora_sft.yaml

<span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="m">192</span>.168.0.1<span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">29500</span><span class="w"> </span><span class="se">\</span>
llamafactory-cli<span class="w"> </span>train<span class="w"> </span>examples/train_lora/llama3_lora_sft.yaml
</pre></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 70.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>å˜é‡å</p></th>
<th class="head"><p>ä»‹ç»</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FORCE_TORCHRUN</p></td>
<td><p>æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨torchrun</p></td>
</tr>
<tr class="row-odd"><td><p>NNODES</p></td>
<td><p>èŠ‚ç‚¹æ•°é‡</p></td>
</tr>
<tr class="row-even"><td><p>NODE_RANK</p></td>
<td><p>å„ä¸ªèŠ‚ç‚¹çš„rankã€‚</p></td>
</tr>
<tr class="row-odd"><td><p>MASTER_ADDR</p></td>
<td><p>ä¸»èŠ‚ç‚¹çš„åœ°å€ã€‚</p></td>
</tr>
<tr class="row-even"><td><p>MASTER_PORT</p></td>
<td><p>ä¸»èŠ‚ç‚¹çš„ç«¯å£ã€‚</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id10">
<h4>torchrun<a class="headerlink" href="#id10" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> æŒ‡ä»¤å¯åŠ¨ NativeDDP å¼•æ“è¿›è¡Œå¤šæœºå¤šå¡è®­ç»ƒã€‚</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--master_port<span class="w"> </span><span class="m">29500</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--nnodes<span class="o">=</span><span class="m">2</span><span class="w"> </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="se">\</span>
--master_addr<span class="o">=</span><span class="m">192</span>.168.0.1<span class="w">  </span>train.py
torchrun<span class="w"> </span>--master_port<span class="w"> </span><span class="m">29500</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--nnodes<span class="o">=</span><span class="m">2</span><span class="w"> </span>--node_rank<span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="se">\</span>
--master_addr<span class="o">=</span><span class="m">192</span>.168.0.1<span class="w">  </span>train.py
</pre></div>
</div>
</section>
<section id="id11">
<h4>accelerate<a class="headerlink" href="#id11" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> æŒ‡ä»¤å¯åŠ¨è¿›è¡Œå¤šæœºå¤šå¡è®­ç»ƒã€‚</p>
<p>é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œæ ¹æ®éœ€æ±‚å›ç­”ä¸€ç³»åˆ—é—®é¢˜åç”Ÿæˆé…ç½®æ–‡ä»¶ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>config
</pre></div>
</div>
<p>ä¸‹é¢æä¾›ä¸€ä¸ªç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼š</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># accelerate_multiNode_config.yaml</span>
<span class="nt">compute_environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class="nt">debug</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">distributed_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MULTI_GPU</span>
<span class="nt">downcast_bf16</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;no&#39;</span>
<span class="nt">enable_cpu_affinity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">gpu_ids</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">all</span>
<span class="nt">machine_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">main_process_ip</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;192.168.0.1&#39;</span>
<span class="nt">main_process_port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">29500</span>
<span class="nt">main_training_function</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="nt">mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fp16</span>
<span class="nt">num_machines</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">num_processes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">rdzv_backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class="nt">same_network</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">tpu_env</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">tpu_use_cluster</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">tpu_use_sudo</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">use_cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹æŒ‡ä»¤å¼€å§‹è®­ç»ƒ:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>accelerate_multiNode_config.yaml<span class="w"> </span><span class="se">\</span>
train.py<span class="w"> </span>llm_config.yaml
</pre></div>
</div>
</section>
</section>
</section>
<section id="deepspeed">
<span id="deepspeed-ref"></span><h2>DeepSpeed<a class="headerlink" href="#deepspeed" title="Link to this heading">ïƒ</a></h2>
<p>DeepSpeed æ˜¯ç”±å¾®è½¯å¼€å‘çš„ä¸€ä¸ªå¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨æé«˜å¤§æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚åœ¨ä½¿ç”¨ DeepSpeed ä¹‹å‰ï¼Œæ‚¨éœ€è¦å…ˆä¼°è®¡è®­ç»ƒä»»åŠ¡çš„æ˜¾å­˜å¤§å°ï¼Œå†æ ¹æ®ä»»åŠ¡éœ€æ±‚ä¸èµ„æºæƒ…å†µé€‰æ‹©åˆé€‚çš„ ZeRO é˜¶æ®µã€‚</p>
<ul class="simple">
<li><p>ZeRO-1: ä»…åˆ’åˆ†ä¼˜åŒ–å™¨å‚æ•°ï¼Œæ¯ä¸ªGPUå„æœ‰ä¸€ä»½å®Œæ•´çš„æ¨¡å‹å‚æ•°ä¸æ¢¯åº¦ã€‚</p></li>
<li><p>ZeRO-2: åˆ’åˆ†ä¼˜åŒ–å™¨å‚æ•°ä¸æ¢¯åº¦ï¼Œæ¯ä¸ªGPUå„æœ‰ä¸€ä»½å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€‚</p></li>
<li><p>ZeRO-3: åˆ’åˆ†ä¼˜åŒ–å™¨å‚æ•°ã€æ¢¯åº¦ä¸æ¨¡å‹å‚æ•°ã€‚</p></li>
</ul>
<p>ç®€å•æ¥è¯´ï¼šä» ZeRO-1 åˆ° ZeRO-3ï¼Œé˜¶æ®µæ•°è¶Šé«˜ï¼Œæ˜¾å­˜éœ€æ±‚è¶Šå°ï¼Œä½†æ˜¯è®­ç»ƒé€Ÿåº¦ä¹Ÿä¾æ¬¡å˜æ…¢ã€‚æ­¤å¤–ï¼Œè®¾ç½® <code class="docutils literal notranslate"><span class="pre">offload_param=cpu</span></code> å‚æ•°ä¼šå¤§å¹…å‡å°æ˜¾å­˜éœ€æ±‚ï¼Œä½†ä¼šæå¤§åœ°ä½¿è®­ç»ƒé€Ÿåº¦å‡æ…¢ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ï¼Œ
åº”å½“ä½¿ç”¨ ZeRO-1ï¼Œå¹¶ä¸”ç¡®ä¿ <code class="docutils literal notranslate"><span class="pre">offload_param=none</span></code>ã€‚</p>
<p>LLaMA-Factoryæä¾›äº†ä½¿ç”¨ä¸åŒé˜¶æ®µçš„ DeepSpeed é…ç½®æ–‡ä»¶çš„ç¤ºä¾‹ã€‚åŒ…æ‹¬ï¼š</p>
<ul class="simple">
<li><p><a class="reference internal" href="#zero-0"><span class="std std-ref">ZeRO-0</span></a> (ä¸å¼€å¯)</p></li>
<li><p><a class="reference internal" href="#zero-2"><span class="std std-ref">ZeRO-2</span></a></p></li>
<li><p><a class="reference internal" href="#zero2o"><span class="std std-ref">ZeRO-2+offload</span></a></p></li>
<li><p><a class="reference internal" href="#zero-3"><span class="std std-ref">ZeRO-3</span></a></p></li>
<li><p><a class="reference internal" href="#zero3o"><span class="std std-ref">ZeRO-3+offload</span></a></p></li>
<li><p><a class="reference internal" href="#zero1-2-autotp"><span class="std std-ref">ZeRO-1/2+AutoTP</span></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p><a class="reference external" href="https://huggingface.co/docs/transformers/deepspeed/">https://huggingface.co/docs/transformers/deepspeed</a> æä¾›äº†æ›´ä¸ºè¯¦ç»†çš„ä»‹ç»ã€‚</p>
</div>
<section id="id12">
<span id="id13"></span><h3>å•æœºå¤šå¡<a class="headerlink" href="#id12" title="Link to this heading">ïƒ</a></h3>
<section id="id14">
<h4>llamafactory-cli<a class="headerlink" href="#id14" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ llamafactory-cli å¯åŠ¨ DeepSpeed å¼•æ“è¿›è¡Œå•æœºå¤šå¡è®­ç»ƒã€‚</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>examples/train_full/llama3_full_sft_ds3.yaml
</pre></div>
</div>
<p>ä¸ºäº†å¯åŠ¨ DeepSpeed å¼•æ“ï¼Œé…ç½®æ–‡ä»¶ä¸­ <code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> å‚æ•°æŒ‡å®šäº† DeepSpeed é…ç½®æ–‡ä»¶çš„è·¯å¾„:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">...</span>
<span class="nt">deepspeed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">examples/deepspeed/ds_z3_config.json</span>
<span class="nn">...</span>
</pre></div>
</div>
</section>
<section id="id15">
<h4>deepspeed<a class="headerlink" href="#id15" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> æŒ‡ä»¤å¯åŠ¨ DeepSpeed å¼•æ“è¿›è¡Œå•æœºå¤šå¡è®­ç»ƒã€‚</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deepspeed<span class="w"> </span>--include<span class="w"> </span>localhost:1<span class="w"> </span>your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</pre></div>
</div>
<p>ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deepspeed<span class="w"> </span>--num_gpus<span class="w"> </span><span class="m">8</span><span class="w"> </span>src/train.py<span class="w"> </span><span class="se">\</span>
--deepspeed<span class="w"> </span>examples/deepspeed/ds_z3_config.json<span class="w"> </span><span class="se">\</span>
--stage<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w">  </span><span class="se">\</span>
--do_train<span class="w"> </span><span class="se">\</span>
--dataset<span class="w"> </span>alpaca_en<span class="w"> </span><span class="se">\</span>
--template<span class="w"> </span>llama3<span class="w"> </span><span class="se">\</span>
--finetuning_type<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
--output_dir<span class="w">  </span>saves/llama3-8b/lora/full<span class="w"> </span><span class="se">\</span>
--overwrite_cache<span class="w"> </span><span class="se">\</span>
--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--gradient_accumulation_steps<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
--lr_scheduler_type<span class="w"> </span>cosine<span class="w"> </span><span class="se">\</span>
--logging_steps<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
--save_steps<span class="w"> </span><span class="m">500</span><span class="w"> </span><span class="se">\</span>
--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
--num_train_epochs<span class="w"> </span><span class="m">2</span>.0<span class="w"> </span><span class="se">\</span>
--plot_loss<span class="w"> </span><span class="se">\</span>
--bf16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> æŒ‡ä»¤å¯åŠ¨ DeepSpeed å¼•æ“æ—¶æ‚¨æ— æ³•ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> æŒ‡å®šGPUã€‚è€Œéœ€è¦ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deepspeed<span class="w"> </span>--include<span class="w"> </span>localhost:1<span class="w"> </span>your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--include</span> <span class="pre">localhost:1</span></code> è¡¨ç¤ºåªæ˜¯ç”¨æœ¬èŠ‚ç‚¹çš„gpu1ã€‚</p>
</div>
</section>
</section>
<section id="id16">
<span id="id17"></span><h3>å¤šæœºå¤šå¡<a class="headerlink" href="#id16" title="Link to this heading">ïƒ</a></h3>
<p>LLaMA-Factory æ”¯æŒä½¿ç”¨ DeepSpeed çš„å¤šæœºå¤šå¡è®­ç»ƒï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="m">192</span>.168.0.1<span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">29500</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>examples/train_lora/llama3_lora_sft_ds3.yaml
<span class="nv">FORCE_TORCHRUN</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="m">192</span>.168.0.1<span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">29500</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>examples/train_lora/llama3_lora_sft_ds3.yaml
</pre></div>
</div>
<section id="id18">
<h4>deepspeed<a class="headerlink" href="#id18" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> æŒ‡ä»¤æ¥å¯åŠ¨å¤šæœºå¤šå¡è®­ç»ƒã€‚</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deepspeed<span class="w"> </span>--num_gpus<span class="w"> </span><span class="m">8</span><span class="w"> </span>--num_nodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--hostfile<span class="w"> </span>hostfile<span class="w"> </span>--master_addr<span class="w"> </span>hostname1<span class="w"> </span>--master_port<span class="o">=</span><span class="m">9901</span><span class="w"> </span><span class="se">\</span>
your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<ul>
<li><dl>
<dt>å…³äºhostfile:</dt><dd><p>hostfileçš„æ¯ä¸€è¡ŒæŒ‡å®šä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯è¡Œçš„æ ¼å¼ä¸º <code class="docutils literal notranslate"><span class="pre">&lt;hostname&gt;</span> <span class="pre">slots=&lt;num_slots&gt;</span></code> ï¼Œ
å…¶ä¸­ <code class="docutils literal notranslate"><span class="pre">&lt;hostname&gt;</span></code> æ˜¯èŠ‚ç‚¹çš„ä¸»æœºåï¼Œ <code class="docutils literal notranslate"><span class="pre">&lt;num_slots&gt;</span></code> æ˜¯è¯¥èŠ‚ç‚¹ä¸Šçš„GPUæ•°é‡ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š
.. code-block:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">worker</span><span class="o">-</span><span class="mi">1</span> <span class="n">slots</span><span class="o">=</span><span class="mi">4</span>
<span class="n">worker</span><span class="o">-</span><span class="mi">2</span> <span class="n">slots</span><span class="o">=</span><span class="mi">4</span>
</pre></div>
</div>
<p>è¯·åœ¨ <a class="reference external" href="https://www.deepspeed.ai/getting-started/">https://www.deepspeed.ai/getting-started/</a> äº†è§£æ›´å¤šã€‚</p>
</dd>
</dl>
</li>
<li><p>å¦‚æœæ²¡æœ‰æŒ‡å®š <code class="docutils literal notranslate"><span class="pre">hostfile</span></code> å˜é‡, DeepSpeed ä¼šæœç´¢ <code class="docutils literal notranslate"><span class="pre">/job/hostfile</span></code> æ–‡ä»¶ã€‚å¦‚æœä»æœªæ‰¾åˆ°ï¼Œé‚£ä¹ˆ DeepSpeed ä¼šä½¿ç”¨æœ¬æœºä¸Šæ‰€æœ‰å¯ç”¨çš„GPUã€‚</p></li>
</ul>
</div>
</section>
<section id="id19">
<h4>accelerate<a class="headerlink" href="#id19" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> æŒ‡ä»¤å¯åŠ¨ DeepSpeed å¼•æ“ã€‚
é¦–å…ˆé€šè¿‡ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆ DeepSpeed é…ç½®æ–‡ä»¶ï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>config
</pre></div>
</div>
<p>ä¸‹é¢æä¾›ä¸€ä¸ªé…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># deepspeed_config.yaml</span>
<span class="nt">compute_environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class="nt">debug</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">deepspeed_config</span><span class="p">:</span>
<span class="w">    </span><span class="nt">deepspeed_multinode_launcher</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">standard</span>
<span class="w">    </span><span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">    </span><span class="nt">offload_optimizer_device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">none</span>
<span class="w">    </span><span class="nt">offload_param_device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">none</span>
<span class="w">    </span><span class="nt">zero3_init_flag</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">zero_stage</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="nt">distributed_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DEEPSPEED</span>
<span class="nt">downcast_bf16</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;no&#39;</span>
<span class="nt">enable_cpu_affinity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">machine_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">main_process_ip</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;192.168.0.1&#39;</span>
<span class="nt">main_process_port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">29500</span>
<span class="nt">main_training_function</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="nt">mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fp16</span>
<span class="nt">num_machines</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">num_processes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">rdzv_backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class="nt">same_network</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">tpu_env</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">tpu_use_cluster</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">tpu_use_sudo</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">use_cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>éšåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>deepspeed_config.yaml<span class="w"> </span><span class="se">\</span>
train.py<span class="w"> </span>llm_config.yaml
</pre></div>
</div>
</section>
</section>
<section id="id20">
<h3>DeepSpeed é…ç½®æ–‡ä»¶<a class="headerlink" href="#id20" title="Link to this heading">ïƒ</a></h3>
<section id="zero-0">
<span id="id21"></span><h4>ZeRO-0<a class="headerlink" href="#zero-0" title="Link to this heading">ïƒ</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### ds_z0_config.json</span>
<span class="p p-Indicator">{</span>
<span class="w">    </span><span class="s">&quot;train_batch_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;gradient_accumulation_steps&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;gradient_clipping&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;zero_allow_untested_optimizer&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;fp16&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;enabled&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;loss_scale&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">0</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;loss_scale_window&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">1000</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;initial_scale_power&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">16</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;hysteresis&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">2</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;min_loss_scale&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">1</span>
<span class="w">    </span><span class="p p-Indicator">},</span>
<span class="w">    </span><span class="s">&quot;bf16&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;enabled&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span>
<span class="w">    </span><span class="p p-Indicator">},</span>
<span class="w">    </span><span class="s">&quot;zero_optimization&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;stage&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">0</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;allgather_partitions&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;allgather_bucket_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">5e8</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;overlap_comm&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;reduce_scatter&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;reduce_bucket_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">5e8</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;contiguous_gradients&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;round_robin_gradients&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span>
<span class="w">    </span><span class="p p-Indicator">}</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
</section>
<section id="zero-2">
<span id="id22"></span><h4>ZeRO-2<a class="headerlink" href="#zero-2" title="Link to this heading">ïƒ</a></h4>
<p>åªéœ€åœ¨ ZeRO-0 çš„åŸºç¡€ä¸Šä¿®æ”¹ <code class="docutils literal notranslate"><span class="pre">zero_optimization</span></code> ä¸­çš„ <code class="docutils literal notranslate"><span class="pre">stage</span></code> å‚æ•°å³å¯ã€‚</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### ds_z2_config.json</span>
<span class="p p-Indicator">{</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="nv">&quot;zero_optimization&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;stage&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">2</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="p p-Indicator">}</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
</section>
<section id="zero-2-offload">
<span id="zero2o"></span><h4>ZeRO-2+offload<a class="headerlink" href="#zero-2-offload" title="Link to this heading">ïƒ</a></h4>
<p>åªéœ€åœ¨ ZeRO-0 çš„åŸºç¡€ä¸Šåœ¨ <code class="docutils literal notranslate"><span class="pre">zero_optimization</span></code> ä¸­æ·»åŠ  <code class="docutils literal notranslate"><span class="pre">offload_optimizer</span></code> å‚æ•°å³å¯ã€‚</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### ds_z2_offload_config.json</span>
<span class="p p-Indicator">{</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="nv">&quot;zero_optimization&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;stage&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">2</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;offload_optimizer&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;device&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;cpu&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;pin_memory&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span>
<span class="w">        </span><span class="p p-Indicator">},</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="p p-Indicator">}</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
</section>
<section id="zero-3">
<span id="id23"></span><h4>ZeRO-3<a class="headerlink" href="#zero-3" title="Link to this heading">ïƒ</a></h4>
<p>åªéœ€åœ¨ ZeRO-0 çš„åŸºç¡€ä¸Šä¿®æ”¹ <code class="docutils literal notranslate"><span class="pre">zero_optimization</span></code> ä¸­çš„å‚æ•°ã€‚</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### ds_z3_config.json</span>
<span class="p p-Indicator">{</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="nv">&quot;zero_optimization&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;stage&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">3</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;overlap_comm&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;contiguous_gradients&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;sub_group_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">1e9</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;reduce_bucket_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;stage3_param_persistence_threshold&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;auto&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;stage3_max_live_parameters&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">1e9</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;stage3_max_reuse_distance&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">1e9</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span>
<span class="w">    </span><span class="p p-Indicator">}</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
</section>
<section id="zero-3-offload">
<span id="zero3o"></span><h4>ZeRO-3+offload<a class="headerlink" href="#zero-3-offload" title="Link to this heading">ïƒ</a></h4>
<p>åªéœ€åœ¨ ZeRO-3 çš„åŸºç¡€ä¸Šæ·»åŠ  <code class="docutils literal notranslate"><span class="pre">zero_optimization</span></code> ä¸­çš„ <code class="docutils literal notranslate"><span class="pre">offload_optimizer</span></code> å’Œ <code class="docutils literal notranslate"><span class="pre">offload_param</span></code> å‚æ•°å³å¯ã€‚</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### ds_z3_offload_config.json</span>
<span class="p p-Indicator">{</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="nv">&quot;zero_optimization&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;stage&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">3</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;offload_optimizer&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;device&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;cpu&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;pin_memory&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span>
<span class="w">        </span><span class="p p-Indicator">},</span>
<span class="w">        </span><span class="s">&quot;offload_param&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;device&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;cpu&quot;</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="s">&quot;pin_memory&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">true</span>
<span class="w">        </span><span class="p p-Indicator">},</span>
<span class="w">    </span><span class="nv">...</span>
<span class="w">    </span><span class="p p-Indicator">}</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
</section>
<section id="zero-1-2-autotp">
<span id="zero1-2-autotp"></span><h4>ZeRO-1/2+AutoTP<a class="headerlink" href="#zero-1-2-autotp" title="Link to this heading">ïƒ</a></h4>
<p>TPæ˜¯ä¸€ç§é‡è¦çš„å¤§æ¨¡å‹è®­ç»ƒå†…å­˜ä¼˜åŒ–æŠ€æœ¯ã€‚ä»¥å¾€åœ¨ Hugging Face Trainer ä¸­ï¼Œæ¨¡å‹æ‰©å±•ä¸»è¦ä¾èµ– ZeRO/FSDP çš„åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼Œå…¶ä¸­ ZeRO3 è™½ç„¶èŠ‚çœæ˜¾å­˜ï¼Œä½†é€šä¿¡å¼€é”€è¾ƒå¤§ï¼›ZeRO1/2 é€šä¿¡å¼€é”€è¾ƒä½ï¼Œå´åœ¨è¶…å¤§æ¨¡å‹åœºæ™¯ä¸‹å—é™äºæ˜¾å­˜å®¹é‡ã€‚</p>
<p>ä¸ºæ­¤ï¼ŒDeepSpeed å¼•å…¥äº† <strong>åŸç”Ÿè‡ªåŠ¨å¼ é‡å¹¶è¡Œï¼ˆAutoTPï¼‰è®­ç»ƒ</strong>ï¼Œå¹¶æ­£å¼æ”¯æŒ Hugging Face Transformersã€‚è¯¥èƒ½åŠ›å¯ä¸ ZeRO ç»“åˆï¼Œåœ¨è®­ç»ƒé˜¶æ®µå®ç°æ›´ä¼˜çš„æ€§èƒ½ä¸å†…å­˜å¹³è¡¡ï¼Œå¸¦æ¥ä»¥ä¸‹ä¼˜åŠ¿ï¼š</p>
<ul class="simple">
<li><p>åœ¨æ¯” FSDP / ZeRO3 æ›´ä½é€šä¿¡å¼€é”€çš„æƒ…å†µä¸‹å®ç°å¤§æ¨¡å‹æ‰©å±•ï¼ˆå¦‚ AutoTP + ZeRO1 è¾¾åˆ°æ¥è¿‘ ZeRO3 çš„æ˜¾å­˜èŠ‚çœæ•ˆæœï¼‰</p></li>
<li><p>æ”¯æŒæ›´å¤§çš„ batch sizeï¼Œæé«˜è®­ç»ƒåå</p></li>
<li><p>æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ‹“å±•åº”ç”¨åœºæ™¯</p></li>
</ul>
<p>ç›®å‰ AutoTP è®­ç»ƒå·²æ”¯æŒ <strong>ZeRO1 å’Œ ZeRO2</strong>ã€‚è¯¥åŠŸèƒ½å·²åœ¨ <strong>DeepSpeed â‰¥ 0.16.4</strong> ä¸­æä¾›ã€‚</p>
<p>å…·ä½“çš„å‚æ•°é…ç½®å¦‚ä¸‹ï¼š</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### ds2_autotp.json</span>
<span class="p p-Indicator">{</span>
<span class="w">    </span><span class="s">&quot;ZeRO_optimization&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">{</span>
<span class="w">        </span><span class="s">&quot;stage&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">2</span><span class="p p-Indicator">,</span>
<span class="w">        </span><span class="nv">...</span>
<span class="w">    </span><span class="p p-Indicator">},</span>
<span class="w">    </span><span class="s">&quot;tensor_parallel&quot;</span><span class="p p-Indicator">:{</span>
<span class="w">        </span><span class="s">&quot;autotp_size&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">4</span>
<span class="w">    </span><span class="p p-Indicator">},</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
<p>å½“å‰æ¨¡å‹æ”¯æŒå—é™ï¼Œå…·ä½“æ”¯æŒåˆ—è¡¨è¯·æŸ¥çœ‹ <a class="reference external" href="https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/#supported-models">AutoTPæ”¯æŒæ¨¡å‹åˆ—è¡¨</a> ã€‚</p>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p><a class="reference external" href="https://www.deepspeed.ai/docs/config-json/">https://www.deepspeed.ai/docs/config-json/</a> æä¾›äº†å…³äºdeepspeedé…ç½®æ–‡ä»¶çš„æ›´è¯¦ç»†çš„ä»‹ç»ã€‚</p>
</div>
</section>
</section>
</section>
<section id="fsdp">
<span id="fsdp-ref"></span><h2>FSDP<a class="headerlink" href="#fsdp" title="Link to this heading">ïƒ</a></h2>
<p>PyTorch çš„å…¨åˆ‡ç‰‡æ•°æ®å¹¶è¡ŒæŠ€æœ¯ <a class="reference external" href="https://pytorch.org/docs/stable/fsdp.html">FSDP</a> ï¼ˆFully Sharded Data Parallelï¼‰èƒ½è®©æˆ‘ä»¬å¤„ç†æ›´å¤šæ›´å¤§çš„æ¨¡å‹ã€‚LLaMA-Factoryæ”¯æŒä½¿ç”¨ FSDP å¼•æ“è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚</p>
<p>FSDP çš„å‚æ•° <code class="docutils literal notranslate"><span class="pre">ShardingStrategy</span></code> çš„ä¸åŒå–å€¼å†³å®šäº†æ¨¡å‹çš„åˆ’åˆ†æ–¹å¼ï¼š</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">FULL_SHARD</span></code>: å°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€éƒ½åˆ‡åˆ†åˆ°ä¸åŒçš„GPUä¸Šï¼Œç±»ä¼¼ZeRO-3ã€‚</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SHARD_GRAD_OP</span></code>: å°†æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€åˆ‡åˆ†åˆ°ä¸åŒçš„GPUä¸Šï¼Œæ¯ä¸ªGPUä»å„è‡ªä¿ç•™ä¸€ä»½å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€‚ç±»ä¼¼ZeRO-2ã€‚</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NO_SHARD</span></code>: ä¸åˆ‡åˆ†ä»»ä½•å‚æ•°ã€‚ç±»ä¼¼ZeRO-0ã€‚</p></li>
</ul>
<section id="id25">
<span id="id26"></span><h3>å•æœºå¤šå¡<a class="headerlink" href="#id25" title="Link to this heading">ïƒ</a></h3>
<section id="id27">
<h4>llamafactory-cli<a class="headerlink" href="#id27" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨åªéœ€æ ¹æ®éœ€è¦ä¿®æ”¹ <code class="docutils literal notranslate"><span class="pre">examples/accelerate/fsdp_config.yaml</span></code> ä»¥åŠ <code class="docutils literal notranslate"><span class="pre">examples/extras/fsdp_qlora/llama3_lora_sft.yaml</span></code> ï¼Œæ–‡ä»¶ç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å¯åŠ¨ FSDP+QLoRA å¾®è°ƒï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>examples/extras/fsdp_qlora/train.sh
</pre></div>
</div>
</section>
<section id="id28">
<h4>accelerate<a class="headerlink" href="#id28" title="Link to this heading">ïƒ</a></h4>
<p>æ­¤å¤–ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ accelerate å¯åŠ¨ FSDP å¼•æ“ï¼Œ <strong>èŠ‚ç‚¹æ•°ä¸ GPU æ•°å¯ä»¥é€šè¿‡ num_machines å’Œ  num_processes æŒ‡å®š</strong>ã€‚å¯¹æ­¤ï¼ŒHuggingface æä¾›äº†ä¾¿æ·çš„é…ç½®åŠŸèƒ½ã€‚
åªéœ€è¿è¡Œï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>config
</pre></div>
</div>
<p>æ ¹æ®æç¤ºå›ç­”ä¸€ç³»åˆ—é—®é¢˜åï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”Ÿæˆ FSDP æ‰€éœ€çš„é…ç½®æ–‡ä»¶ã€‚</p>
<p>å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œé…ç½® <code class="docutils literal notranslate"><span class="pre">fsdp_config.yaml</span></code> ã€‚</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">### /examples/accelerate/fsdp_config.yaml</span>
<span class="nt">compute_environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class="nt">debug</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">distributed_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class="nt">downcast_bf16</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;no&#39;</span>
<span class="nt">fsdp_config</span><span class="p">:</span>
<span class="w">    </span><span class="nt">fsdp_auto_wrap_policy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class="w">    </span><span class="nt">fsdp_backward_prefetch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class="w">    </span><span class="nt">fsdp_forward_prefetch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">fsdp_cpu_ram_efficient_loading</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">fsdp_offload_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"> </span><span class="c1"># offload may affect training speed</span>
<span class="w">    </span><span class="nt">fsdp_sharding_strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FULL_SHARD</span>
<span class="w">    </span><span class="nt">fsdp_state_dict_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FULL_STATE_DICT</span>
<span class="w">    </span><span class="nt">fsdp_sync_module_states</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">fsdp_use_orig_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">machine_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">main_training_function</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="nt">mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fp16</span><span class="w"> </span><span class="c1"># or bf16</span>
<span class="nt">num_machines</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># the number of nodes</span>
<span class="nt">num_processes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># the number of GPUs in all nodes</span>
<span class="nt">rdzv_backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class="nt">same_network</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">tpu_env</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">tpu_use_cluster</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">tpu_use_sudo</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">use_cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<ul class="simple">
<li><p>è¯·ç¡®ä¿ <code class="docutils literal notranslate"><span class="pre">num_processes</span></code> å’Œå®é™…ä½¿ç”¨çš„æ€»GPUæ•°é‡ä¸€è‡´</p></li>
</ul>
</div>
<p>éšåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>fsdp_config.yaml<span class="w"> </span><span class="se">\</span>
src/train.py<span class="w"> </span>llm_config.yaml
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">è­¦å‘Š</p>
<p>ä¸è¦åœ¨ FSDP+QLoRA ä¸­ä½¿ç”¨ GPTQ/AWQ æ¨¡å‹</p>
</div>
</section>
</section>
<section id="id29">
<span id="id30"></span><h3>å¤šæœºå¤šå¡<a class="headerlink" href="#id29" title="Link to this heading">ïƒ</a></h3>
<section id="id31">
<h4>accelerate<a class="headerlink" href="#id31" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨å¯ä»¥é€šè¿‡ <cite>accelerate config</cite> æ ¹æ®æç¤ºå›ç­”ä¸€ç³»åˆ—é—®é¢˜åï¼Œç”Ÿæˆ å¤šæœº FSDP æ‰€éœ€çš„é…ç½®æ–‡ä»¶ã€‚</p>
<p>å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œé…ç½® fsdp_config.yaml ã€‚</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1">#examples/accelerate/fsdp_config_multiple_nodes.yaml</span>
<span class="nt">compute_environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class="nt">debug</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">distributed_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class="nt">downcast_bf16</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;no&#39;</span>
<span class="nt">fsdp_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">fsdp_auto_wrap_policy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class="w">  </span><span class="nt">fsdp_backward_prefetch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class="w">  </span><span class="nt">fsdp_forward_prefetch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">fsdp_cpu_ram_efficient_loading</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">fsdp_offload_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">fsdp_sharding_strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FULL_SHARD</span>
<span class="w">  </span><span class="nt">fsdp_state_dict_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">FULL_STATE_DICT</span>
<span class="w">  </span><span class="nt">fsdp_sync_module_states</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">fsdp_use_orig_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">machine_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">main_training_function</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="nt">mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bf16</span><span class="w">  </span><span class="c1"># or fp16</span>
<span class="nt">main_process_ip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">192.168.0.1</span>
<span class="nt">main_process_port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">29500</span>
<span class="nt">num_machines</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">num_processes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">rdzv_backend</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class="nt">same_network</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">tpu_env</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">tpu_use_cluster</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">tpu_use_sudo</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">use_cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>è¿™ä»½yamlæ–‡ä»¶é‡Œï¼Œæ‚¨ä¸»è¦éœ€è¦æ³¨æ„é…ç½®çš„å‚æ•°æ˜¯ä»¥ä¸‹å››ä¸ªï¼š</p>
<ul class="simple">
<li><p>num_machines: èŠ‚ç‚¹ï¼ˆæœºå™¨ï¼‰çš„æ•°é‡ã€‚</p></li>
<li><p>num_processes: æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„ GPU æ€»æ•°ï¼Œå³ num_machines * num_processes_per_machineã€‚</p></li>
<li><p>main_process_ip: ä¸»è¿›ç¨‹æ‰€åœ¨èŠ‚ç‚¹çš„ IP åœ°å€ï¼›è¯·ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨ç›¸åŒçš„ IPã€‚</p></li>
<li><p>main_process_port: ä¸»è¿›ç¨‹çš„ç«¯å£å·ï¼›è¯·ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨ç›¸åŒçš„ç«¯å£ã€‚</p></li>
<li><p>machine_rank: å½“å‰èŠ‚ç‚¹ï¼ˆæœºå™¨ï¼‰çš„ç¼–å·ï¼Œä» 0 å¼€å§‹ï¼›å¹¶ä¸”ä¸»èŠ‚ç‚¹ï¼ˆmain_process_ipï¼‰çš„ machine_rank å¿…é¡»ä¸º 0ã€‚</p></li>
</ul>
<p>å½“é…ç½®å®Œæˆåï¼Œåœ¨æ‰€æœ‰æœºå™¨ä¸Šè¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯å¯åŠ¨FSDPçš„å¤šæœºå¤šå¡è®­ç»ƒï¼š</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>fsdp_config_multiple_nodes.yaml<span class="w"> </span><span class="se">\</span>
train.py<span class="w"> </span>llm_config.yaml
</pre></div>
</div>
</section>
</section>
</section>
<section id="fsdp2">
<span id="fsdp2-ref"></span><h2>FSDP2<a class="headerlink" href="#fsdp2" title="Link to this heading">ïƒ</a></h2>
<p>å½“å‰LLamafactoryåŸºäºAccelerateé›†æˆä½¿ç”¨FSDP2çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œä¸FSDPçš„å·®å¼‚ä¸»è¦åœ¨äºå‚æ•°é…ç½®çš„ä¸åŒã€‚æ‚¨å¯ä»¥é‡‡ç”¨FSDPç›¸åŒçš„æ–¹å¼è¿è¡Œå•æœºå¤šå¡å’Œå¤šæœºå¤šå¡ã€‚æˆ‘ä»¬ä¹Ÿä¸ºæ‚¨æä¾›äº†é€šç”¨çš„å…¥å‚é…ç½®ï¼š</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#examples/accelerate/fsdp2_config.yaml</span>
compute_environment:<span class="w"> </span>LOCAL_MACHINE
debug:<span class="w"> </span><span class="nb">false</span>
distributed_type:<span class="w"> </span>FSDP
downcast_bf16:<span class="w"> </span><span class="s1">&#39;no&#39;</span>
fsdp_config:
<span class="w">  </span>fsdp_auto_wrap_policy:<span class="w"> </span>TRANSFORMER_BASED_WRAP
<span class="w">  </span>fsdp_cpu_ram_efficient_loading:<span class="w"> </span><span class="nb">true</span>
<span class="w">  </span>fsdp_offload_params:<span class="w"> </span><span class="nb">false</span>
<span class="w">  </span>fsdp_reshard_after_forward:<span class="w"> </span><span class="nb">true</span>
<span class="w">  </span>fsdp_state_dict_type:<span class="w"> </span>FULL_STATE_DICT
<span class="w">  </span>fsdp_version:<span class="w"> </span><span class="m">2</span>
machine_rank:<span class="w"> </span><span class="m">0</span>
main_training_function:<span class="w"> </span>main
mixed_precision:<span class="w"> </span>bf16<span class="w">  </span><span class="c1"># or fp16</span>
num_machines:<span class="w"> </span><span class="m">1</span><span class="w">  </span><span class="c1"># the number of nodes</span>
num_processes:<span class="w"> </span><span class="m">2</span><span class="w">  </span><span class="c1"># the number of GPUs in all nodes</span>
rdzv_backend:<span class="w"> </span>static
same_network:<span class="w"> </span><span class="nb">true</span>
tpu_env:<span class="w"> </span><span class="o">[]</span>
tpu_use_cluster:<span class="w"> </span><span class="nb">false</span>
tpu_use_sudo:<span class="w"> </span><span class="nb">false</span>
use_cpu:<span class="w"> </span><span class="nb">false</span>
</pre></div>
</div>
<p>æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¿«é€Ÿæ‹‰èµ·è®­ç»ƒè„šæœ¬ï¼š</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>fsdp2_config.yaml<span class="w"> </span><span class="se">\</span>
train.py<span class="w"> </span>llm_config.yaml
</pre></div>
</div>
<p>æ›´å¤šçš„å…¥å‚é…ç½®å·®å¼‚ï¼Œæ‚¨å¯ä»¥å‚è€ƒ <a class="reference external" href="chttps://huggingface.co/docs/accelerate/main/en/concept_guides/fsdp1_vs_fsdp2">Accelerate FSDP1 vs FSDP2</a> ã€‚</p>
</section>
<section id="ray">
<span id="ray-ref"></span><h2>Ray<a class="headerlink" href="#ray" title="Link to this heading">ïƒ</a></h2>
<p>å½“å‰ LlamaFactory è¿˜æ”¯æŒé€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ <code class="docutils literal notranslate"><span class="pre">USE_RAY=1</span></code> æ¥å¯ç”¨ Ray è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚æ‚¨å¯ä»¥å‚è€ƒ <a class="reference external" href="https://docs.ray.io/en/latest/">Rayå®˜æ–¹æ–‡æ¡£</a> äº†è§£æ›´å¤šä¿¡æ¯ã€‚</p>
<section id="id33">
<h3>å•æœºå¤šå¡<a class="headerlink" href="#id33" title="Link to this heading">ïƒ</a></h3>
<p>æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> å‚æ•°æ¥æŒ‡å®šä½¿ç”¨çš„GPUæ•°é‡ã€‚</p>
<section id="nativeddp-deepspeed">
<h4>NativeDDP, DeepSpeed<a class="headerlink" href="#nativeddp-deepspeed" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ llamafactory-cli æŒ‡ä»¤å¯åŠ¨ NativeDDP å¼•æ“åŠ DeepSpeed å¼•æ“ã€‚</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">USE_RAY</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>training_config.yaml
</pre></div>
</div>
</section>
<section id="id34">
<h4>FSDP<a class="headerlink" href="#id34" title="Link to this heading">ïƒ</a></h4>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ accelerate æŒ‡ä»¤å¯åŠ¨ FSDP å¼•æ“ã€‚ä½¿ç”¨ accelerate å¯åŠ¨æ—¶ï¼Œéœ€è¦è®¾ç½® fsdp_config.yaml æ–‡ä»¶ä¸­ <code class="docutils literal notranslate"><span class="pre">num_processes</span></code> å‚æ•°ä¸º 1ã€‚</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">USE_RAY</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>fsdp_config.yaml<span class="w"> </span><span class="se">\</span>
src/train.py<span class="w"> </span>training_config.yaml
</pre></div>
</div>
</section>
</section>
<section id="id35">
<h3>å¤šæœºå¤šå¡<a class="headerlink" href="#id35" title="Link to this heading">ïƒ</a></h3>
<p>ä½¿ç”¨ Ray è¿›è¡Œå¤šæœºå¤šå¡è®­ç»ƒæ—¶ï¼Œæ‚¨é¦–å…ˆéœ€è¦åˆ†åˆ«åœ¨å„ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚</p>
<p>ä¸»èŠ‚ç‚¹ï¼š</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ray<span class="w"> </span>start<span class="w"> </span>--head<span class="w"> </span>--port<span class="o">=</span><span class="m">6379</span>
</pre></div>
</div>
<p>ä»èŠ‚ç‚¹ï¼š</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ray<span class="w"> </span>start<span class="w"> </span>--address<span class="o">=</span><span class="s1">&#39;master node IP:6379&#39;</span>
</pre></div>
</div>
<p>ç„¶åï¼Œæ‚¨å¯ä»¥åœ¨ä¸»èŠ‚ç‚¹ä½¿ç”¨ä¸å•æœºç›¸åŒçš„æŒ‡ä»¤å¯åŠ¨è®­ç»ƒï¼Œæˆ–è€…ä½¿ç”¨ ray job submit å‘½ä»¤æäº¤è®­ç»ƒä»»åŠ¡ã€‚</p>
<section id="id36">
<h4>NativeDDP, DeepSpeed<a class="headerlink" href="#id36" title="Link to this heading">ïƒ</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">USE_RAY</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>training_config.yaml
</pre></div>
</div>
<p>ä½¿ç”¨ ray job submit å‘½ä»¤æäº¤è®­ç»ƒä»»åŠ¡ã€‚</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">RAY_API_SERVER_ADDRESS</span><span class="o">=</span><span class="s1">&#39;&#39;</span>http://dashboard-host:dashboard-port<span class="w"> </span><span class="se">\</span>
ray<span class="w"> </span>job<span class="w"> </span>submit<span class="w"> </span>--<span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>training_config.yaml
</pre></div>
</div>
</section>
<section id="id37">
<h4>FSDP<a class="headerlink" href="#id37" title="Link to this heading">ïƒ</a></h4>
<p>è¿™é‡ŒåŒæ ·éœ€è¦è®¾ç½® fsdp_config.yaml æ–‡ä»¶ä¸­ <code class="docutils literal notranslate"><span class="pre">num_processes</span></code> å‚æ•°ä¸º 1ï¼Œå¹¶ä¸”æ— éœ€è®¾ç½®å¤šæœºç›¸å…³å‚æ•°ã€‚</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">USE_RAY</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>fsdp_config.yaml<span class="w"> </span><span class="se">\</span>
src/train.py<span class="w"> </span>training_config.yaml
</pre></div>
</div>
<p>ä½¿ç”¨ ray job submit å‘½ä»¤æäº¤è®­ç»ƒä»»åŠ¡ã€‚</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">RAY_API_SERVER_ADDRESS</span><span class="o">=</span><span class="s1">&#39;&#39;</span>http://dashboard-host:dashboard-port<span class="w"> </span><span class="se">\</span>
ray<span class="w"> </span>job<span class="w"> </span>submit<span class="w"> </span>--<span class="w"> </span><span class="se">\</span>
<span class="nv">USE_RAY</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
--config_file<span class="w"> </span>fsdp_config.yaml<span class="w"> </span><span class="se">\</span>
src/train.py<span class="w"> </span>training_config.yaml
</pre></div>
</div>
<span class="target" id="id38"></span></section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="é¡µè„š">
        <a href="npu_training.html" class="btn btn-neutral float-left" title="NPUè®­ç»ƒ" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> ä¸Šä¸€é¡µ</a>
        <a href="../../../../../ms-swift/index.html" class="btn btn-neutral float-right" title="ms-swift" accesskey="n" rel="next">ä¸‹ä¸€é¡µ <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>