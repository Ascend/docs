

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Long-Sequence Training Using Ulysses &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Long-Sequence Training Using Ulysses</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/sources/_generated/sources/VeOmni/key_features/ulysses.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="long-sequence-training-using-ulysses">
<h1>Long-Sequence Training Using Ulysses<a class="headerlink" href="#long-sequence-training-using-ulysses" title="Link to this heading">ïƒ</a></h1>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#veomni-long-sequence-training-using-ulysses">VeOmni Long-Sequence Training Using Ulysses</a></p>
<ul>
<li><p><a class="reference external" href="#table-of-contents">Table of Contents</a></p></li>
<li><p><a class="reference external" href="#-overview">ğŸ“š Overview</a></p></li>
<li><p><a class="reference external" href="#-quick-start">ğŸš€ Quick Start</a></p></li>
<li><p><a class="reference external" href="#-dive-into-ulysses-sequence-parallelism">ğŸ” Dive into Ulysses Sequence Parallelism</a></p>
<ul>
<li><p><a class="reference external" href="#what-is-all_to_all">What is all_to_all?</a></p></li>
<li><p><a class="reference external" href="#deepspeed-ulysses">DeepSpeed-Ulysses</a></p></li>
<li><p><a class="reference external" href="#communication-analysis">Communication Analysis</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#%EF%B8%8F-core-api">âš™ï¸ Core API</a></p></li>
<li><p><a class="reference external" href="#%EF%B8%8F-support-ulysses-for-a-new-model">ğŸ› ï¸ Support Ulysses for a New Model</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="overview">
<h2>ğŸ“š Overview<a class="headerlink" href="#overview" title="Link to this heading">ïƒ</a></h2>
<p>In this tutorial, we introduce the implementation of DeepSpeed-Ulysses for efficient long-sequence training in VeOmni. The Ulysses method optimizes memory usage by splitting both the input tensor and intermediate activations along the sequence dimension. This innovative approach significantly enhances memory efficiency, enabling the training of models with longer sequence lengths.</p>
<p>Reference Paper: <a class="reference external" href="https://arxiv.org/abs/2309.14509">DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</a></p>
</section>
<section id="quick-start">
<h2>ğŸš€ Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading">ïƒ</a></h2>
<p>To enable Ulysses, users can specify the <code class="docutils literal notranslate"><span class="pre">ulysses_parallel_size</span></code> parameter in the configuration file or the launch command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>train.sh<span class="w"> </span>tasks/multimodal/omni/train_qwen2_5_vl.py<span class="w"> </span>configs/multimodal/qwen2_5_vl/qwen2_5_vl_fsdp1.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model.model_path<span class="w"> </span>YOUR_MODEL_PATH<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--data.train_path<span class="w"> </span>YOUR_DATA_PATH<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train.ulysses_parallel_size<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<p>Currently, we have supported Ulysses on the following models:</p>
<p>Language models:</p>
<ul class="simple">
<li><p>LlaMa</p></li>
<li><p>Qwen2.5</p></li>
</ul>
<p>Multimodal models:</p>
<ul class="simple">
<li><p>Qwen2-VL</p></li>
<li><p>Qwen2.5-VL</p></li>
</ul>
</section>
<section id="dive-into-ulysses-sequence-parallelism">
<h2>ğŸ” Dive into Ulysses Sequence Parallelism<a class="headerlink" href="#dive-into-ulysses-sequence-parallelism" title="Link to this heading">ïƒ</a></h2>
<p>Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. Ulysses use all-to-all collective communication operations to implement SP with attention.</p>
<section id="what-is-all-to-all">
<h3>What is all_to_all?<a class="headerlink" href="#what-is-all-to-all" title="Link to this heading">ïƒ</a></h3>
<p>Suppose we have P GPUs and a sequence whose shape is [S, H], where N denotes the sequence length and d represents the hidden size (head num * head dim). Each GPU initially holds the sequence's [S/P, H] partition. After performing an all_to_all communication, each GPU will get a head-splitting sequence whose shape is [S, H/P]. An illustration figure when P = 4 is as follows:</p>
<p><img alt="all_to_all communication" src="../../../../../_images/all_2_all.jpg" /></p>
</section>
<section id="deepspeed-ulysses">
<h3>DeepSpeed-Ulysses<a class="headerlink" href="#deepspeed-ulysses" title="Link to this heading">ïƒ</a></h3>
<p>We use the all_to_all based sequence parallelism which is proposed by DeepSpeed, named DeepSpeed-Ulysses.</p>
<p><img alt="DeepSpeed-Ulysses" src="../../../../../_images/ulysses.png" /></p>
<p>(Image source: <a class="reference external" href="https://arxiv.org/abs/2309.14509">DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</a>)</p>
<p>The figure above shows the overall architecture of DeepSpeed-Ulysses. It only introduces two extra all_to_all communications in the attention module while it does not modify other parts such as normalization and MLP. The input sequence is first evenly divided across the GPUs. The first all_to_all communication gathers the query, key, and value ([S/P, H]) along the sequence dimension and scatters the sequence in the head dimension ([S, H/P]). After the attention part, another all_to_all is performed to transfer the attention output ([S, H/P]) from head-sliced back to sequence-sliced ([S/P, H]). DeepSpeed-Ulysses is attention agnostic since it gathers the whole sequence dimension during attention computation. Thus, it can be easily used with FlashAttention. However, it is constrained by the number of attention heads since the sp size should be divided evenly by head_num. Note that DeepSpeed-Ulysses does not impact the memory consumed by the model states. To support large sequence-length training with a large language model, DeepSpeed-Ulysses can be integrated with ZeRO and FSDP.</p>
</section>
<section id="communication-analysis">
<h3>Communication Analysis<a class="headerlink" href="#communication-analysis" title="Link to this heading">ïƒ</a></h3>
<p>The communication volume transmitted per link for an all-to-all for aggregate message of size M over P GPUs can be estimated as $M(P-1)/P^2$. For a transformer model with hidden size H, the sequence length of S, and parallelism degree of P, and let $\mu = (P-1)/P$. DeepSpeed-Ulysses performs all-to-all for the QKV projections with an aggregate message size of $3SH$ before the attention computation, which introduces $3SH\mu/P$ communication volume; and another all-to-all for output context projection with a size Nh for each transformer layer, which introduces $SH\mu/P$ communication volume. Therefore, DeepSpeed sequence parallelism incurs an aggregate communication volume per link of $4SH\mu/P = 4M(P-1)/P^2$.</p>
</section>
</section>
<section id="core-api">
<h2>âš™ï¸ Core API<a class="headerlink" href="#core-api" title="Link to this heading">ïƒ</a></h2>
<ol class="simple">
<li><p>gather_seq_scatter_heads</p></li>
</ol>
<p>A method to do all-to-all before attention, make sure the sequence is full for attention score computation.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gather_seq_scatter_heads</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">seq_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">unpadded_dim_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">async_op</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>Args:</p>
<ul class="simple">
<li><p>x: tensor to be synced</p></li>
<li><p>seq_dim: sequence dim that will be gathered</p></li>
<li><p>head_dim: head dim that will be scattered. &quot;head&quot; is a concept from Multi-head Attention or Group Query Attention</p></li>
<li><p>unpadded_dim_size: the full sequence size before padding and sharding</p></li>
<li><p>async_op: if True, will return a torch._C._distributed_c10d.Work object, users can use this arg to do comm-compute overlap</p></li>
</ul>
<ol class="simple">
<li><p>gather_heads_scatter_seq</p></li>
</ol>
<p>A method to do all-to-all after attention, transforming the activation from head-split to sequence-split.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gather_heads_scatter_seq</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">seq_dim</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>Args:</p>
<ul class="simple">
<li><p>x: Tensor to be synced</p></li>
<li><p>head_dim: head dim that will be gathered</p></li>
<li><p>seq_dim: sequence dim that will be scattered</p></li>
</ul>
<ol class="simple">
<li><p>reduce_sequence_parallel_loss</p></li>
</ol>
<p>A method to reduce loss within the sequence parallel group, re-scale the loss according to the number of valid tokens.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reduce_sequence_parallel_loss</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_valid_tokens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</pre></div>
</div>
<p>Args:</p>
<ul class="simple">
<li><p>loss: loss tensor to be reduced</p></li>
<li><p>num_valid_tokens: the number of valid tokens in current rank</p></li>
</ul>
</section>
<section id="support-ulysses-for-a-new-model">
<h2>ğŸ› ï¸ Support Ulysses for a New Model<a class="headerlink" href="#support-ulysses-for-a-new-model" title="Link to this heading">ïƒ</a></h2>
<p>Typically, enabling Ulysses for a new model involves three key steps:</p>
<ol class="simple">
<li><p>Create a sequence parallel group based on the specified Ulysses parallel size.</p></li>
<li><p>Shard input sequences across the sequence parallel groups.</p></li>
<li><p>Modify the modelâ€™s attention and loss computation to support Ulysses.</p></li>
</ol>
<p>In VeOmni, the first two steps are automated. Users only need to specify the ulysses_parallel_size parameter, and VeOmni will handle the creation of sequence parallel groups and the sharding of input data. This allows users to focus solely on step 3â€”modifying the modelâ€™s architecture to implement Ulysses sequence parallelism.</p>
<p>To make this process easier, we provide an abstract pseudo-code example as a reference for implementing Ulysses in a new model:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">veomni.distributed.sequence_parallel</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">gather_seq_scatter_heads_qkv</span><span class="p">,</span>
    <span class="n">gather_heads_scatter_seq</span><span class="p">,</span>
    <span class="n">reduce_sequence_parallel_loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Step1: Create a sequence parallel group</span>
<span class="c1"># VeOmni will construct a sequence parallel group based on the parallel size</span>

<span class="c1"># Step2: Shard input sequences</span>
<span class="c1"># Suppose we get an input x of shape [batch_size, seq_len, dim]</span>
<span class="c1"># we first shard x among sequence parallel groups</span>
<span class="c1"># now x is of shape [batch_size, seq_len/n, dim] on each sp rank</span>

<span class="c1"># Step3 (part1): modify attention computation</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># [batch_size, seq_pad/n, dim]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">gather_seq_scatter_heads</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [batch_size, seq_len, dim/n]</span>
<span class="o">...</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">gather_heads_scatter_seq</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, seq_pad/n, dim]</span>

<span class="c1"># Step3 (part2): reduce loss after model forward</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">reduce_sequence_parallel_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">num_valid_tokens</span><span class="p">)</span>
<span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="async-ulysses-cp">
<h2>âš¡ Async Ulysses CP<a class="headerlink" href="#async-ulysses-cp" title="Link to this heading">ïƒ</a></h2>
<p>We also support <strong>Async Ulysses</strong> which further improves performance by overlapping communication and computation, reducing communication latency and improving hardware utilization.</p>
<section id="asynchronous-ulysses">
<h3>Asynchronous Ulysses<a class="headerlink" href="#asynchronous-ulysses" title="Link to this heading">ïƒ</a></h3>
<p><img alt="async_ulysses" src="../../../../../_images/async_ulysses.jpg" /></p>
<p>VeOmni extends the original Ulysses implementation with asynchronous communication capabilities, further improving performance by overlapping communication and computation.</p>
<section id="performance-benefits">
<h4>Performance Benefits<a class="headerlink" href="#performance-benefits" title="Link to this heading">ïƒ</a></h4>
<p>By overlapping communication and computation, Async Ulysses:</p>
<ul class="simple">
<li><p>Reduces idle time during communication operations</p></li>
<li><p>Lowers end-to-end training time</p></li>
<li><p>Maintains nearly the same memory efficiency as original Ulysses</p></li>
</ul>
</section>
<section id="enabling-async-ulysses">
<h4>Enabling Async Ulysses<a class="headerlink" href="#enabling-async-ulysses" title="Link to this heading">ïƒ</a></h4>
<p>To enable Async Ulysses, simply set the <code class="docutils literal notranslate"><span class="pre">async_enabled</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<p>Notice: Async Ulysses works when <code class="docutils literal notranslate"><span class="pre">ulysses_parallel_size</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>train.sh<span class="w"> </span>tasks/multimodal/omni/train_qwen_vl.py<span class="w"> </span>configs/multimodal/qwen3_vl/qwen3_vl_dense.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train.ulysses_parallel_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train.async_enabled<span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
</section>
</section>
<section id="api">
<h3>API<a class="headerlink" href="#api" title="Link to this heading">ïƒ</a></h3>
<ol class="simple">
<li><p>async_ulysses_qkv_projection</p></li>
</ol>
<p>An asynchronous method to perform QKV projection and all-to-all communication, overlapping computation and communication.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">async_ulysses_qkv_projection</span><span class="p">(</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">seq_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">head_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">q_weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">k_weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">k_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">v_weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_q_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_q_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_k_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_k_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">normalized_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">unpadded_dim_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</pre></div>
</div>
<p>Args:</p>
<ul class="simple">
<li><p>hidden_states: Input hidden states</p></li>
<li><p>seq_dimension: Sequence dimension</p></li>
<li><p>head_dimension: Head dimension</p></li>
<li><p>q_weight: Query projection weight</p></li>
<li><p>q_bias: Query projection bias</p></li>
<li><p>k_weight: Key projection weight</p></li>
<li><p>k_bias: Key projection bias</p></li>
<li><p>v_weight: Value projection weight</p></li>
<li><p>v_bias: Value projection bias</p></li>
<li><p>norm_type: Normalization type (&quot;rmsnorm&quot; or &quot;layernorm&quot;)</p></li>
<li><p>norm_q_weight: Query normalization weight</p></li>
<li><p>norm_q_bias: Query normalization bias</p></li>
<li><p>norm_k_weight: Key normalization weight</p></li>
<li><p>norm_k_bias: Key normalization bias</p></li>
<li><p>normalized_shape: Normalization shape</p></li>
<li><p>eps: Normalization epsilon</p></li>
<li><p>unpadded_dim_size: Unpadded dimension size</p></li>
<li><p>head_dim: Head dimension size</p></li>
<li><p>group: Process group (optional)</p></li>
</ul>
<ol class="simple">
<li><p>async_ulysses_output_projection</p></li>
</ol>
<p>An asynchronous method to perform output projection and all-to-all communication.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">async_ulysses_output_projection</span><span class="p">(</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">seq_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">head_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">proj_weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">proj_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">unpadded_dim_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</pre></div>
</div>
<p>Args:</p>
<ul class="simple">
<li><p>hidden_states: Input hidden states</p></li>
<li><p>seq_dimension: Sequence dimension</p></li>
<li><p>head_dimension: Head dimension</p></li>
<li><p>proj_weight: Projection weight</p></li>
<li><p>proj_bias: Projection bias</p></li>
<li><p>unpadded_dim_size: Unpadded dimension size</p></li>
<li><p>group: Process group (optional)</p></li>
</ul>
</section>
<section id="id1">
<h3>Enabling Async Ulysses<a class="headerlink" href="#id1" title="Link to this heading">ïƒ</a></h3>
<p>To enable Async Ulysses for an existing model, you need to:</p>
<ol class="simple">
<li><p>Check if Async Ulysses is supported for your model (currently supported for Qwen3VL Dense)</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">async_enabled=True</span></code> in your training configuration</p></li>
<li><p>Ensure you're using Flash Attention 2.0 and Ulysses Context Parallelism is <strong>enabled</strong></p></li>
<li><p>Verify that your hardware supports asynchronous operations</p></li>
</ol>
<p>Async Ulysses is currently available for the following models:</p>
<ul class="simple">
<li><p>Qwen3VL Dense</p></li>
</ul>
<p>Support for more models will be added in future releases.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>