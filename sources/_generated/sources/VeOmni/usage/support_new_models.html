

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Support New Models &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Support New Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/sources/_generated/sources/VeOmni/usage/support_new_models.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="support-new-models">
<h1>Support New Models<a class="headerlink" href="#support-new-models" title="Link to this heading">ÔÉÅ</a></h1>
<p><strong>Author</strong>: Juntian Liu</p>
<p><strong>TLDR:</strong> This tutorial demonstrates how to enable new models in VeOmni using a combination of FSDP, Expert Parallelism (EP), and Sequence Parallelism (SP/Ulysses), using Qwen3-VL MoE as a practical example. We'll modify the modeling code from HuggingFace directly to support distributed training at scale.</p>
<hr class="docutils" />
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">ÔÉÅ</a></h2>
<p>Enabling a new multimodal model in VeOmni requires careful integration of three parallelism strategies:</p>
<ul class="simple">
<li><p><strong>FSDP</strong>: Native PyTorch support for sharding the model's parameters, gradients, and optimizer states across multiple GPUs, enabling efficient training of large models that exceed single-GPU memory capacity.</p></li>
<li><p><strong>SP</strong>: Distributing input along the sequence dimension across GPUs</p></li>
<li><p><strong>EP</strong>: Sharding MoE experts across GPUs</p></li>
</ul>
<p>This guide uses Qwen3-VL MoE as a reference implementation, showing the specific code modifications needed for each parallelism type.</p>
</section>
<hr class="docutils" />
<section id="fsdp-support-native-integration">
<h2>1. FSDP Support - Native Integration<a class="headerlink" href="#fsdp-support-native-integration" title="Link to this heading">ÔÉÅ</a></h2>
<p>FSDP support in VeOmni is straightforward thanks to PyTorch's native FSDP implementation. Most models work out-of-the-box with minimal modifications.</p>
<section id="dummy-vit-forward-for-vlms">
<h3>Dummy ViT Forward for VLMs<a class="headerlink" href="#dummy-vit-forward-for-vlms" title="Link to this heading">ÔÉÅ</a></h3>
<p>This is <strong>only required for VLM (Vision-Language Models)</strong> and <strong>only for the ViT (Vision Transformer) component</strong>. The dummy forward is needed for <strong>both image and video inputs</strong> to prevent FSDP reduce-scatter hangs.</p>
<p>When using FSDP, if some ranks in the same FSDP group receive <code class="docutils literal notranslate"><span class="pre">None</span></code> for <code class="docutils literal notranslate"><span class="pre">pixel_values</span></code> (images) or <code class="docutils literal notranslate"><span class="pre">pixel_values_videos</span></code> (videos) while others receive valid inputs, the backward reduce-scatter operation will hang.</p>
<section id="reference-implementation-in-vision-model">
<h4>Reference Implementation in Vision Model<a class="headerlink" href="#reference-implementation-in-vision-model" title="Link to this heading">ÔÉÅ</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">dummy_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dummy forward to avoid FSDP reduce-scatter hang when some ranks get None pixel_values</span>
<span class="sd">    This is only needed for VLM&#39;s ViT component, for both image and video inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">fsdp_enabled</span><span class="p">:</span>
        <span class="n">sp_size</span> <span class="o">=</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_size</span>
        <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">grid_thw</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dummy_data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">:</span> <span class="n">pixel_values</span><span class="p">,</span> <span class="s2">&quot;grid_thw&quot;</span><span class="p">:</span> <span class="n">grid_thw</span><span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">dummy_data</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="add-into-the-main-forward-pass">
<h4>Add into the Main Forward Pass<a class="headerlink" href="#add-into-the-main-forward-pass" title="Link to this heading">ÔÉÅ</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For image inputs as the example</span>
<span class="k">if</span> <span class="n">pixel_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">image_embeds</span><span class="p">,</span> <span class="n">deepstack_image_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_image_features</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">image_grid_thw</span><span class="p">)</span>
    <span class="c1"># ...</span>
<span class="k">elif</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">fsdp_enabled</span><span class="p">:</span>
    <span class="c1"># Dummy ViT forward for image path</span>
    <span class="n">fake_embeds</span><span class="p">,</span> <span class="n">fake_deepstack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">visual</span><span class="o">.</span><span class="n">dummy_forward</span><span class="p">()</span>
    <span class="n">fake_embeds</span> <span class="o">=</span> <span class="n">fake_embeds</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.0</span>
    <span class="n">fake_embeds</span> <span class="o">=</span> <span class="n">fake_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">fake_embeds</span>
</pre></div>
</div>
<p>This ensures all ranks participate in collective operations without affecting the actual result (multiplying by 0.0 ensures no contribution to gradients).</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="sequence-parallelism-support">
<h2>2. Sequence Parallelism Support<a class="headerlink" href="#sequence-parallelism-support" title="Link to this heading">ÔÉÅ</a></h2>
<p>VeOmni automatically registers the wrapped FlashAttention from veomni/ops/attention.py, LOSS_MAPPING from veomni/ops/loss.py. You also need to handle sequence-dimension slicing for embeddings carefully and manage the tensor transformations between the ViT and language model components.</p>
<section id="language-model-part-simple-position-embedding-slicing">
<h3>2.1 Language Model Part - Simple Position Embedding Slicing<a class="headerlink" href="#language-model-part-simple-position-embedding-slicing" title="Link to this heading">ÔÉÅ</a></h3>
<p>For the language model, the modification is straightforward - just a few extra lines to slice position embeddings to match each rank's hidden states:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create complete position embeddings to be shared across the decoder layers</span>
<span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>

<span class="c1"># slice position embedding if using sp</span>
<span class="n">sp_group</span> <span class="o">=</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_group</span> <span class="k">if</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_enabled</span> <span class="k">else</span> <span class="kc">None</span>
<span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">slice_position_embedding</span><span class="p">(</span><span class="n">position_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sp_group</span><span class="o">=</span><span class="n">sp_group</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">slice_position_embedding</span></code> function handles distributing the position embeddings across ranks to match the sequence-sliced hidden states.</p>
</section>
<section id="vision-transformer-vit-part-padding-and-slicing">
<h3>2.2 Vision Transformer (ViT) Part - Padding and Slicing<a class="headerlink" href="#vision-transformer-vit-part-padding-and-slicing" title="Link to this heading">ÔÉÅ</a></h3>
<p>The ViT component needs to be handled carefully because of padding considerations and the need to process the unpadded and unsliced grid_thw</p>
<section id="key-challenge-padding-and-grid-handling">
<h4>Key Challenge: Padding and Grid Handling<a class="headerlink" href="#key-challenge-padding-and-grid-handling" title="Link to this heading">ÔÉÅ</a></h4>
<p>The vision hidden states are <strong>padded and sliced</strong> in the data collator, but <code class="docutils literal notranslate"><span class="pre">grid_thw</span></code> (temporal, height, width grid information) remains <strong>unpadded and unsliced</strong>. This creates a mismatch that must be handled carefully:</p>
<ol class="simple">
<li><p>Position embeddings are computed from the <strong>raw grid_thw</strong></p></li>
<li><p>Hidden states have been <strong>padded and sequence-sliced</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cu_seqlens</span></code> (cumulative sequence lengths) are also computed from <strong>raw grid_thw</strong></p></li>
</ol>
<p>We need to use <code class="docutils literal notranslate"><span class="pre">sp_pad_and_slice</span></code> to pad and slice position embeddings to match the padded hidden states.</p>
</section>
<section id="reference-implementation">
<h4>Reference Implementation<a class="headerlink" href="#reference-implementation" title="Link to this heading">ÔÉÅ</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute position embeddings from raw grid</span>
<span class="n">pos_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fast_pos_embed_interpolate</span><span class="p">(</span><span class="n">grid_thw</span><span class="p">)</span>

<span class="c1"># slice pos embedding if using sp so that the sharded hidden_states receive their corresponding position embeddings</span>
<span class="n">sp_group</span> <span class="o">=</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_group</span> <span class="k">if</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_enabled</span> <span class="k">else</span> <span class="kc">None</span>
<span class="k">if</span> <span class="n">sp_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># We need to do padding here because hidden_states was padded with pad_scale=4</span>
    <span class="n">pos_embeds</span> <span class="o">=</span> <span class="n">sp_pad_and_slice</span><span class="p">(</span><span class="n">pos_embeds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">pos_embeds</span>

<span class="c1"># Compute cumulative sequence lengths from raw grid</span>
<span class="n">cu_seqlens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">grid_thw</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">grid_thw</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">grid_thw</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span>
    <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">grid_thw</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">cu_seqlens</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">cu_seqlens</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Compute rotary position embeddings</span>
<span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rot_pos_emb</span><span class="p">(</span><span class="n">grid_thw</span><span class="p">)</span>

<span class="c1"># Get before-sliced full seq from cu_seqlens</span>
<span class="c1"># total_seq_len should equal to seq_len when not using SP</span>
<span class="n">total_seq_len</span> <span class="o">=</span> <span class="n">cu_seqlens</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">rotary_pos_emb</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">total_seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">rotary_pos_emb</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">position_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">cos</span><span class="p">(),</span> <span class="n">emb</span><span class="o">.</span><span class="n">sin</span><span class="p">())</span>

<span class="c1"># slice pos embedding if using sp to let sharded hidden_states get its corresponding pos embedding</span>
<span class="k">if</span> <span class="n">sp_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">position_embeddings</span>
    <span class="c1"># Apply same padding and slicing as hidden_states</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">sp_pad_and_slice</span><span class="p">(</span><span class="n">cos</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">sp_pad_and_slice</span><span class="p">(</span><span class="n">sin</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">position_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="why-pad-scale-4">
<h4>Why <code class="docutils literal notranslate"><span class="pre">pad_scale=4</span></code>?<a class="headerlink" href="#why-pad-scale-4" title="Link to this heading">ÔÉÅ</a></h4>
<p>Qwen3-VL performs a <strong>4-to-1 spatial merge</strong> at the end of the ViT. The data collator pads vision sequences to multiples of 4 to ensure this merge operation works correctly. Since <code class="docutils literal notranslate"><span class="pre">grid_thw</span></code> is unpadded but hidden states are padded with pad_scale=4, <code class="docutils literal notranslate"><span class="pre">sp_pad_and_slice</span></code> with <code class="docutils literal notranslate"><span class="pre">pad_scale=4</span></code> ensures position embeddings are padded and sliced to match.</p>
</section>
</section>
<section id="vit-lm-connection-part">
<h3>2.3 ViT-LM Connection Part<a class="headerlink" href="#vit-lm-connection-part" title="Link to this heading">ÔÉÅ</a></h3>
<p>Connecting the ViT to the LM requires carefully orchestrating gather_seq_scatter_heads (All2All) operations to merge the vision embeddings back into the full LM input; after ViT processing, this merge back into the original sequence proceeds through three sub-steps (using image fill-back as an example).</p>
<p><strong>Step 1: Process the Input Embeddings</strong></p>
<p>Apply the All2All operation to the sp-sliced <code class="docutils literal notranslate"><span class="pre">inputs_embeds</span></code> (including both vision placeholder and text):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_enabled</span><span class="p">:</span>
    <span class="c1"># Input:  (batch_size, seq_len // sp_size, hidden_size)</span>
    <span class="c1"># Output: (batch_size, seq_len, hidden_size // sp_size)</span>
    <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">gather_seq_scatter_heads</span><span class="p">(</span>
        <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_group</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><strong>Step 2.3b: Gather Complete Image Sequence Length</strong></p>
<p>Also use <code class="docutils literal notranslate"><span class="pre">gather_seq_scatter_heads</span></code> on ViT-processed image embeddings to also gather along the <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> dimension and scatter along the <code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> dimension:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_enabled</span><span class="p">:</span>
    <span class="c1"># (seq_len // sp_size, hidden_size) to  (seq_len, hidden_size // sp_size)</span>
    <span class="n">image_embeds</span> <span class="o">=</span> <span class="n">gather_seq_scatter_heads</span><span class="p">(</span>
        <span class="n">image_embeds</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_group</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><strong>Step 2.3b: Fill Back to Correct Positions</strong></p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">image_mask</span></code> marks the positions of image tokens. This mask is precomputed in process_sample and kept unsliced during data preprocessing to avoid unnecessary communication during training and to ensure correct handling under sequence parallelism.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">image_embeds</span></code> are ViT-processed and after All2All image features, with shape <code class="docutils literal notranslate"><span class="pre">(seq_length,</span> <span class="pre">hidden_size</span> <span class="pre">//</span> <span class="pre">sp_size)</span></code></p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">masked_scatter</span></code> to fill image features back into the corresponding positions in input embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">masked_scatter</span><span class="p">(</span><span class="n">image_mask</span><span class="p">,</span> <span class="n">image_embeds</span><span class="p">)</span>
</pre></div>
</div>
<p>Each rank only fills only a portion of <code class="docutils literal notranslate"><span class="pre">input_embeds</span></code>, but this portion is <strong>partitioned at hidden_size dimension</strong>, not at the sequence dimension.</p>
<p><strong>Final Transformation</strong></p>
<p>Finally, restore to normal SP partitioning before sending to the LM part:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_enabled</span><span class="p">:</span>
    <span class="c1"># Restore: (batch_size, seq_len, hidden_size // sp_size)</span>
    <span class="c1">#       -&gt; (batch_size, seq_len // sp_size, hidden_size)</span>
    <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">gather_heads_scatter_seq</span><span class="p">(</span>
        <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_group</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The same logic applies to video embeddings processing.</p>
</section>
<section id="special-case-deepstack-visual-embeddings">
<h3>2.4 Special Case: Deepstack Visual Embeddings<a class="headerlink" href="#special-case-deepstack-visual-embeddings" title="Link to this heading">ÔÉÅ</a></h3>
<p>Deepstack embeddings require a different approach: <strong>All-Gather followed by rank-specific slicing</strong>. This is necessary because we want to avoid multiple extra All2All operations later in <code class="docutils literal notranslate"><span class="pre">_deepstack_process</span></code>.</p>
<section id="why-this-approach">
<h4>Why This Approach?<a class="headerlink" href="#why-this-approach" title="Link to this heading">ÔÉÅ</a></h4>
<p>Instead of keeping deepstack embeddings distributed and requiring All2All communication in every LM deepstack layer, we</p>
<ol class="simple">
<li><p>All-Gather deepstack embeddings once after ViT</p></li>
<li><p>Slice them according to each rank's sequence partition using masks</p></li>
<li><p>Each rank gets its local deepstack embeddings and masks</p></li>
<li><p>In later LM parts, <strong>no extra communication is needed</strong></p></li>
</ol>
</section>
<section id="reference-sp-processing-for-image-deepstack">
<h4>Reference SP Processing for Image Deepstack<a class="headerlink" href="#reference-sp-processing-for-image-deepstack" title="Link to this heading">ÔÉÅ</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">pixel_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Do all_gather on deepstack_image_embeds</span>
    <span class="c1"># (seq_len // sp_size, hidden_size) -&gt; (seq_len, hidden_size)</span>
    <span class="n">deepstack_image_embeds</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">_Gather</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">sp_group</span><span class="p">,</span> <span class="n">embed</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">embed</span> <span class="ow">in</span> <span class="n">deepstack_image_embeds</span>
    <span class="p">]</span>

    <span class="c1"># Now use image_mask to select visual tokens for this rank&#39;s sequence slice</span>
    <span class="n">image_mask_1d</span> <span class="o">=</span> <span class="n">image_mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># (batch_size, seq_len)</span>

    <span class="c1"># Determine which sequence positions belong to this rank</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">image_mask_1d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">seq_per_rank</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">sp_size</span>
    <span class="n">rank_start</span> <span class="o">=</span> <span class="n">sp_rank</span> <span class="o">*</span> <span class="n">seq_per_rank</span>
    <span class="n">rank_end</span> <span class="o">=</span> <span class="n">rank_start</span> <span class="o">+</span> <span class="n">seq_per_rank</span>

    <span class="c1"># Get the mask for this rank&#39;s sequence slice and save it for later</span>
    <span class="n">rank_image_mask</span> <span class="o">=</span> <span class="n">image_mask_1d</span><span class="p">[:,</span> <span class="n">rank_start</span><span class="p">:</span><span class="n">rank_end</span><span class="p">]</span>  <span class="c1"># (batch_size, seq_len // sp_size)</span>

    <span class="c1"># Count how many visual tokens are before this rank&#39;s slice</span>
    <span class="n">before_rank_mask</span> <span class="o">=</span> <span class="n">image_mask_1d</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank_start</span><span class="p">]</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">before_rank_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Count how many visual tokens are in this rank&#39;s slice</span>
    <span class="n">num_visual_tokens</span> <span class="o">=</span> <span class="n">rank_image_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Slice the all-gathered deepstack embeddings</span>
    <span class="n">deepstack_image_embeds</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">embed</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">num_visual_tokens</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">embed</span> <span class="ow">in</span> <span class="n">deepstack_image_embeds</span>
    <span class="p">]</span>
</pre></div>
</div>
<p>The same logic applies to video deepstack embeddings. Performing this step once ensures that each rank has the exact deepstack embeddings required for its sequence partition, removing the need for further communication in the LM deepstack layers.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="expert-parallelism-ep-support">
<h2>3. Expert Parallelism (EP) Support<a class="headerlink" href="#expert-parallelism-ep-support" title="Link to this heading">ÔÉÅ</a></h2>
<p>EP requires two main components: define EP parallel plan and enable fused MoE forward implementation.</p>
<section id="define-parallel-plan">
<h3>3.1 Define Parallel Plan<a class="headerlink" href="#define-parallel-plan" title="Link to this heading">ÔÉÅ</a></h3>
<p>Create a <code class="docutils literal notranslate"><span class="pre">parallel_plan.py</span></code> file to specify which expert layers should be sharded:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed._tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Shard</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">....distributed.parallel_plan</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParallelPlan</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_parallel_plan</span><span class="p">():</span>
    <span class="n">ep_plan</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model.language_model.layers.*.mlp.experts.gate_up_proj&quot;</span><span class="p">:</span> <span class="n">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="s2">&quot;model.language_model.layers.*.mlp.experts.down_proj&quot;</span><span class="p">:</span> <span class="n">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">parallel_plan</span> <span class="o">=</span> <span class="n">ParallelPlan</span><span class="p">(</span>
        <span class="n">ep_plan</span><span class="o">=</span><span class="n">ep_plan</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">parallel_plan</span>
</pre></div>
</div>
</section>
<section id="enable-fused-moe-forward">
<h3>3.2 Enable Fused MoE Forward<a class="headerlink" href="#enable-fused-moe-forward" title="Link to this heading">ÔÉÅ</a></h3>
<p>Add the fused MoE MLP implementation to improve performance and enable EP support.</p>
<section id="special-handling-for-qwen3-vl-moe">
<h4>Special Handling for Qwen3-VL MoE<a class="headerlink" href="#special-handling-for-qwen3-vl-moe" title="Link to this heading">ÔÉÅ</a></h4>
<p><strong>Important</strong>: Qwen3-VL MoE <strong>combines gate and up projections into a single <code class="docutils literal notranslate"><span class="pre">gate_up_proj</span></code> tensor</strong>. We need additional handling to match our <code class="docutils literal notranslate"><span class="pre">fused_moe_forward</span></code> interface, which expects separate gate and up projections.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">moe_implementation</span> <span class="o">==</span> <span class="s2">&quot;eager&quot;</span><span class="p">:</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">get_parallel_state</span><span class="p">()</span><span class="o">.</span><span class="n">ep_enabled</span><span class="p">,</span> <span class="s2">&quot;_moe_implementation=&#39;eager&#39; does not support EP&quot;</span>
    <span class="n">next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">expert_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">router_indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span><span class="p">)</span>
    <span class="c1"># ... standard implementation</span>

<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">moe_implementation</span> <span class="o">==</span> <span class="s2">&quot;fused&quot;</span><span class="p">:</span>


    <span class="c1"># Qwen3-VL MoE combines gate and up into gate_up_proj</span>
    <span class="c1"># Split gate_up_proj into gate_proj and up_proj to match fused_moe_forward interface</span>
    <span class="c1"># Current: gate_up_proj shape is (num_experts, hidden_size, 2 * expert_dim)</span>
    <span class="n">gate_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_up_proj</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_dim</span><span class="p">]</span>
    <span class="n">up_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_up_proj</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_dim</span> <span class="p">:]</span>

    <span class="c1"># Transpose weights to match expected shape for fused_moe_forward</span>
    <span class="n">gate_proj_t</span> <span class="o">=</span> <span class="n">gate_proj</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">up_proj_t</span> <span class="o">=</span> <span class="n">up_proj</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">down_proj_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

    <span class="n">next_states</span> <span class="o">=</span> <span class="n">fused_moe_forward</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">num_experts</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span><span class="p">,</span>
        <span class="n">routing_weights</span><span class="o">=</span><span class="n">routing_weights_topk</span><span class="p">,</span>
        <span class="n">selected_experts</span><span class="o">=</span><span class="n">router_indices</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">fc1_1_weight</span><span class="o">=</span><span class="n">gate_proj_t</span><span class="p">,</span>  <span class="c1"># Separated gate projection</span>
        <span class="n">fc1_2_weight</span><span class="o">=</span><span class="n">up_proj_t</span><span class="p">,</span>    <span class="c1"># Separated up projection</span>
        <span class="n">fc2_weight</span><span class="o">=</span><span class="n">down_proj_t</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># ...</span>
</pre></div>
</div>
<p>The key modifications are:</p>
<ol class="simple">
<li><p><strong>Split</strong> <code class="docutils literal notranslate"><span class="pre">gate_up_proj</span></code> into separate <code class="docutils literal notranslate"><span class="pre">gate_proj</span></code> and <code class="docutils literal notranslate"><span class="pre">up_proj</span></code></p></li>
<li><p><strong>Transpose</strong> all weight matrices to match <code class="docutils literal notranslate"><span class="pre">fused_moe_forward</span></code> expected shapes</p></li>
<li><p>Pass separated projections to <code class="docutils literal notranslate"><span class="pre">fused_moe_forward</span></code></p></li>
</ol>
</section>
</section>
</section>
<hr class="docutils" />
<section id="remove-cpu-gpu-synchronization-for-better-performance">
<h2>4. Remove CPU-GPU Synchronization for Better Performance<a class="headerlink" href="#remove-cpu-gpu-synchronization-for-better-performance" title="Link to this heading">ÔÉÅ</a></h2>
<p>CPU-GPU synchronization in attention layers can create significant performance bottlenecks in some cases. We eliminate this issue by precomputing the attention parameters before entering the blocks.</p>
<section id="vision-attention-optimization">
<h3>4.1 Vision Attention Optimization<a class="headerlink" href="#vision-attention-optimization" title="Link to this heading">ÔÉÅ</a></h3>
<p>Compute max_seqlen to perform the CPU-GPU synchronization once, then pass the value to be shared for all layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate max_seqlen from cu_seqlens here to avoid per layer CPU-GPU sync</span>
<span class="n">max_seqlen</span> <span class="o">=</span> <span class="p">(</span><span class="n">cu_seqlens</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">cu_seqlens</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">for</span> <span class="n">layer_num</span><span class="p">,</span> <span class="n">blk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">cu_seqlens</span><span class="o">=</span><span class="n">cu_seqlens</span><span class="p">,</span>
        <span class="n">rotary_pos_emb</span><span class="o">=</span><span class="n">rotary_pos_emb</span><span class="p">,</span>
        <span class="n">max_seqlen</span><span class="o">=</span><span class="n">max_seqlen</span><span class="p">,</span>  <span class="c1"># Pre-computed, no per-layer sync needed</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="language-model-attention-optimization">
<h3>4.2 Language Model Attention Optimization<a class="headerlink" href="#language-model-attention-optimization" title="Link to this heading">ÔÉÅ</a></h3>
<p>Pre-compute FlashAttention kwargs from 3D position IDs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pre-compute flash_attn_kwargs in the data_collator to avoid per-layer CPU-GPU sync</span>
<span class="c1"># Store in kwargs before pass to language model</span>

<span class="c1"># Pop these keys before the ViT parts to prevent the ViT attention from using them</span>
<span class="n">flash_attn_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cu_seq_lens_q&quot;</span><span class="p">,</span> <span class="s2">&quot;cu_seq_lens_k&quot;</span><span class="p">,</span> <span class="s2">&quot;max_length_q&quot;</span><span class="p">,</span> <span class="s2">&quot;max_length_k&quot;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">flash_attn_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

<span class="c1"># Later, restore these kwargs before calling language_model</span>
<span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">flash_attn_kwargs</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
    <span class="c1"># ... other args</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="register-your-model">
<h2>5. Register Your Model<a class="headerlink" href="#register-your-model" title="Link to this heading">ÔÉÅ</a></h2>
<p>Finally, register your model class so VeOmni can automatically match it with the config.</p>
<section id="in-veomni-models-transformers-init-py">
<h3>In <code class="docutils literal notranslate"><span class="pre">veomni/models/transformers/__init__.py</span></code><a class="headerlink" href="#in-veomni-models-transformers-init-py" title="Link to this heading">ÔÉÅ</a></h3>
<p>Add your model to the lists:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">.qwen3_vl_moe</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">qwen3_vl_moe</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;qwen3_vl_moe&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>(support-new-models#in-your-model-file)=</p>
</section>
<section id="in-your-model-file">
<h3>In your model file<a class="headerlink" href="#in-your-model-file" title="Link to this heading">ÔÉÅ</a></h3>
<p>Export the model class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Register the customized model in __init__.py so that VeOmni uses our custom modeling/config/processor code instead of the Hugging Face version</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">...loader</span><span class="w"> </span><span class="kn">import</span> <span class="n">MODEL_CONFIG_REGISTRY</span><span class="p">,</span> <span class="n">MODELING_REGISTRY</span><span class="p">,</span> <span class="n">MODEL_PROCESSOR_REGISTRY</span>

<span class="nd">@MODEL_CONFIG_REGISTRY</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;qwen3_vl_moe&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">register_qwen3_vl_moe_config</span><span class="p">():</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">.configuration_qwen3_vl_moe</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen3VLMoeConfig</span>
    <span class="k">return</span> <span class="n">Qwen3VLMoeConfig</span>

<span class="nd">@MODELING_REGISTRY</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;qwen3_vl_moe&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">register_qwen3_vl_moe_modeling</span><span class="p">(</span><span class="n">architecture</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">modeling_qwen3_vl_moe</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">.modeling_qwen3_vl_moe</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen3VLMoeForCausalLM</span><span class="p">,</span> <span class="n">Qwen3VLMoeForSequenceClassification</span><span class="p">,</span> <span class="n">Qwen3VLMoeForTokenClassification</span>
    <span class="k">if</span> <span class="s2">&quot;ForCausalLM&quot;</span> <span class="ow">in</span> <span class="n">architecture</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Qwen3VLMoeForCausalLM</span>
    <span class="k">elif</span> <span class="s2">&quot;ForSequenceClassification&quot;</span> <span class="ow">in</span> <span class="n">architecture</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Qwen3VLMoeForSequenceClassification</span>
    <span class="k">elif</span> <span class="s2">&quot;ForTokenClassification&quot;</span> <span class="ow">in</span> <span class="n">architecture</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Qwen3VLMoeForTokenClassification</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># None</span>
        <span class="k">return</span> <span class="n">Qwen3VLMoeForCausalLM</span>

<span class="nd">@MODEL_PROCESSOR_REGISTRY</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Qwen3VLMoeProcessor&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">register_qwen3_vl_moe_processor</span><span class="p">():</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">processing_qwen3_vl_moe</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">.processing_qwen3_vl_moe</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen3VLMoeProcessor</span>
    <span class="k">return</span> <span class="n">Qwen3VLMoeProcessor</span>
</pre></div>
</div>
</section>
<section id="additional-helper-functions">
<h3>Additional Helper Functions<a class="headerlink" href="#additional-helper-functions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Add helper functions for position ID computation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the get_position_id_func to be used in data_transform</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_position_id_func</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">fake_model</span> <span class="o">=</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">get_position_id</span><span class="p">,</span> <span class="n">Qwen3VLMoeModel</span><span class="o">.</span><span class="n">get_rope_index</span><span class="p">,</span> <span class="n">fake_model</span><span class="p">)</span>
</pre></div>
</div>
<p>And wrap the position ID function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wrapped Qwen3VLMoeModel.get_rope_index to use in process_sample for obtaining position_ids in advance in process_sample</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_position_id</span><span class="p">(</span><span class="n">main_func</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># must be a global func for multiprocessing serialize</span>
    <span class="n">position_ids</span><span class="p">,</span> <span class="n">rope_deltas</span> <span class="o">=</span> <span class="n">main_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">position_ids</span><span class="p">,</span> <span class="s2">&quot;rope_deltas&quot;</span><span class="p">:</span> <span class="n">rope_deltas</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">ÔÉÅ</a></h2>
<p>Enabling a new model in VeOmni with full parallelism support involves:</p>
<ol class="simple">
<li><p><strong>FSDP</strong>: Add dummy forward paths to ViT components (for VLMs only) to handle both image and video inputs and prevent collective operation hangs</p></li>
<li><p><strong>Sequence Parallelism</strong>:</p>
<ul class="simple">
<li><p>LM: Simple position embedding slicing with <code class="docutils literal notranslate"><span class="pre">slice_position_embedding</span></code></p></li>
<li><p>ViT: Careful padding and slicing with <code class="docutils literal notranslate"><span class="pre">sp_pad_and_slice</span></code> to handle data collator padding</p></li>
<li><p>ViT-LM connection: Multi-step gather/scatter operations to merge vision features</p></li>
<li><p>Deepstack: All-Gather once and slice per-rank to eliminate later communication</p></li>
</ul>
</li>
<li><p><strong>Expert Parallelism</strong>: Define parallel plans, implement fused MoE forward, handle combined weight tensors (like Qwen3-VL's <code class="docutils literal notranslate"><span class="pre">gate_up_proj</span></code>)</p></li>
<li><p><strong>Performance</strong>: Pre-compute attention parameters to eliminate CPU-GPU synchronization</p></li>
<li><p><strong>Registration</strong>: Export model classes and helper functions</p></li>
</ol>
<p>By following these patterns from the Qwen3-VL MoE implementation, you can adapt most Hugging Face models to work efficiently with VeOmni‚Äôs distributed training infrastructure</p>
</section>
<section id="acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Link to this heading">ÔÉÅ</a></h2>
<p>Thanks to ByteDance Seed and AML team: Qianli Ma, Zhelun shi, Yifan Pi, Tianle Zhong and Xiao Yu</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>