

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>常见问题整理 &mdash; 昇腾开源  文档</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            昇腾开源
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">🏁 开始使用</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">快速安装昇腾环境</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🏗️  基础设施与框架</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🧠 训练与微调框架</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🚀 推理与服务</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🎨 多模态、应用与评测</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">昇腾开源</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">常见问题整理</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source/Instruction/Frequently-asked-questions.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>常见问题整理<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<blockquote>
<div><p>[!WARNING]
该文档待更新到ms-swift4.0</p>
</div></blockquote>
<p>下面是swift使用过程中遇到的一些常见问题。</p>
<section id="id2">
<h2>训练<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<section id="q1-swift">
<h3>Q1: Swift微调支持的模型和数据集有哪些？<a class="headerlink" href="#q1-swift" title="Link to this heading"></a></h3>
<p>详见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Supported-models-and-datasets.html">支持的模型和数据集</a>。</p>
</section>
<section id="q2">
<h3>Q2: 使用自定义数据集训练时支持的数据格式有哪些？<a class="headerlink" href="#q2" title="Link to this heading"></a></h3>
<p>自定义数据集格式见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html">自定义数据集</a>。</p>
</section>
<section id="q3-dataset-info-json">
<h3>Q3: 自定义数据集dataset_info.json格式，如何通过这种方式使用自定义数据集？<a class="headerlink" href="#q3-dataset-info-json" title="Link to this heading"></a></h3>
<p>dataset_info.json格式见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html">自定义数据集</a>。命令行，<code class="docutils literal notranslate"><span class="pre">--custom_dataset_info</span> <span class="pre">xxx.json</span></code>，<code class="docutils literal notranslate"><span class="pre">--dataset</span> <span class="pre">&lt;dataset_id_or_path&gt;</span></code>。</p>
</section>
<section id="q4">
<h3>Q4: 如何在界面训练使用自定义数据集？<a class="headerlink" href="#q4" title="Link to this heading"></a></h3>
<p>界面训练使用自定义数据集与命令行一致，参考文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html">自定义数据集</a>。</p>
</section>
<section id="q5-jsonl-index-00000-query-11111-response-22222-source-qqq">
<h3>Q5: 数据集jsonl文件里的一行能不能写成这样？{&quot;index&quot;: &quot;00000&quot;, &quot;query&quot;: &quot;11111&quot;, &quot;response&quot;: &quot;22222&quot;, 'source':'qqq'}<a class="headerlink" href="#q5-jsonl-index-00000-query-11111-response-22222-source-qqq" title="Link to this heading"></a></h3>
<p>可以有额外字段的，这些字段默认不会被使用。详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id4">命令行参数remove_unused_columns</a>。</p>
</section>
<section id="q6">
<h3>Q6: 命令行参数在哪个文档中查看？<a class="headerlink" href="#q6" title="Link to this heading"></a></h3>
<p>详见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q7">
<h3>Q7: 离线环境训练需要配置的参数有哪些？<a class="headerlink" href="#q7" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">--model</span> <span class="pre">本地路径</span></code>，<code class="docutils literal notranslate"><span class="pre">--check_model</span> <span class="pre">false</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q8-model-type">
<h3>Q8: model_type在哪儿查看？<a class="headerlink" href="#q8-model-type" title="Link to this heading"></a></h3>
<p>查看文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Supported-models-and-datasets.html">支持的模型和数据集</a>。</p>
</section>
<section id="q9-gguf">
<h3>Q9: 模型训练完能直接转gguf格式吗？<a class="headerlink" href="#q9-gguf" title="Link to this heading"></a></h3>
<p>目前只支持导出ModelFile，详见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q10-swift-sft">
<h3>Q10: swift支持预训练吗，我看只有sft？<a class="headerlink" href="#q10-swift-sft" title="Link to this heading"></a></h3>
<p>支持，命令行<code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">pt</span></code>，<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/pretrain">预训练例子</a>，数据集格式见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html">自定义数据集</a>。</p>
</section>
<section id="q11-lora-lora">
<h3>Q11: 想问一下用lora微调的模型，如果想断点续训的话，是应该先把它合成一整个模型吗，还是可以不合起来，直接通过路径来指定原模型和lora块<a class="headerlink" href="#q11-lora-lora" title="Link to this heading"></a></h3>
<p>不合并，<code class="docutils literal notranslate"><span class="pre">--resume_from_checkpoint output/xxx/vx-xxx/checkpoint-xxx</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q12">
<h3>Q12: 我想控制一下从网上下载下来的原始模型权重的位置，怎么才能做到把原始的模型放在指定的文件夹里呢？<a class="headerlink" href="#q12" title="Link to this heading"></a></h3>
<p>可以配置环境变量<code class="docutils literal notranslate"><span class="pre">MODELSCOPE_CACHE=your_path</span></code>将原始的模型存到指定路径；如果用sdk下载，通过<code class="docutils literal notranslate"><span class="pre">cache_dir=&quot;本地地址&quot;</span></code>；也可以使用<code class="docutils literal notranslate"><span class="pre">modelscope</span> <span class="pre">download</span></code>命令行工具或<code class="docutils literal notranslate"><span class="pre">git</span></code>下载，详见modelscope文档<a class="reference external" href="https://modelscope.cn/docs/models/download">模型下载</a>。训练时<code class="docutils literal notranslate"><span class="pre">--model</span></code>配置本地路径即可。如果需要在离线环境训练，配置<code class="docutils literal notranslate"><span class="pre">--check_model</span> <span class="pre">false</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q14-qwen-2-vl">
<h3>Q14: 有微调qwen-2-vl的完整的教程和命令行吗？<a class="headerlink" href="#q14-qwen-2-vl" title="Link to this heading"></a></h3>
<p>参考多模态模型训练的<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal">例子</a>。</p>
</section>
<section id="q15-trick-llmneftune">
<h3>Q15: 多模态大模型微调有什么支持的trick吗，类似llm的neftune?<a class="headerlink" href="#q15-trick-llmneftune" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">piassa/olora/dora</span></code>这些<code class="docutils literal notranslate"><span class="pre">lora</span></code>的变种或者<code class="docutils literal notranslate"><span class="pre">fourierft</span></code>都可以尝试。参考<code class="docutils literal notranslate"><span class="pre">sft</span></code>参数里面的各种trick，有一些不一定在多模态上适用。</p>
</section>
<section id="q16-evalaccckptacc">
<h3>Q16: 训练过程中eval得到的acc和对应保存的ckpt去重新推理一遍计算得到的acc不是一致的<a class="headerlink" href="#q16-evalaccckptacc" title="Link to this heading"></a></h3>
<p>训练时候的eval_acc和推理时候的acc 计算方式不一样的。<code class="docutils literal notranslate"><span class="pre">acc_strategy</span></code>: 默认为<code class="docutils literal notranslate"><span class="pre">'token'</span></code>, 可选择的值包括: <code class="docutils literal notranslate"><span class="pre">'token'</span></code>, <code class="docutils literal notranslate"><span class="pre">'seq'</span></code>.</p>
</section>
<section id="q17-swift">
<h3>Q17: 魔搭官方镜像与swift环境<a class="headerlink" href="#q17-swift" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code>命令启动容器即可，如：<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--gpus</span> <span class="pre">all</span> <span class="pre">-p</span> <span class="pre">8000:8000</span> <span class="pre">-it</span> <span class="pre">-d</span> <span class="pre">--name</span> <span class="pre">ms</span> <span class="pre">modelscope-registry.cn-beijing.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-1.26.0-LLM</span> <span class="pre">/bin/bash</span></code>，启动容器后拉最新代码安装swift。另外，针对大模型训练场景，提供了<code class="docutils literal notranslate"><span class="pre">ms-swift</span></code>镜像，额外增加了<code class="docutils literal notranslate"><span class="pre">Megatron-SWIFT</span></code>的依赖，如：<code class="docutils literal notranslate"><span class="pre">modelscope-registry.cn-beijing.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-vllm0.8.5.post1-modelscope1.26.0-swift3.4.1.post1</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/GetStarted/SWIFT-installation.html">swift安装文档</a>。</p>
</section>
<section id="q18">
<h3>Q18: 多机多卡训练命令行<a class="headerlink" href="#q18" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node">多机多卡例子</a></p>
</section>
<section id="q19-template">
<h3>Q19: 如何选择template?<a class="headerlink" href="#q19-template" title="Link to this heading"></a></h3>
<p>见<a class="reference external" href="https://github.com/modelscope/ms-swift/issues/1813">issue</a>。</p>
</section>
<section id="q20-torchrunswift-sft">
<h3>Q20: 多卡训练torchrun和swift sft如何使用？<a class="headerlink" href="#q20-torchrunswift-sft" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">sft</span></code>走的就是<code class="docutils literal notranslate"><span class="pre">torchrun</span></code>。</p>
</section>
<section id="q21-sft-tokenize">
<h3>Q21: 有个问题，因为我的sft数据集太大了，然后每次tokenize都需要很久，有解决方案吗？<a class="headerlink" href="#q21-sft-tokenize" title="Link to this heading"></a></h3>
<p>使用<code class="docutils literal notranslate"><span class="pre">lazy_tokenize</span></code>或流式读取<code class="docutils literal notranslate"><span class="pre">streaming</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q22-shuffle">
<h3>Q22: 训练时，如果两个数据集直接追加一起放在训练集中，模型在训练的时候内部会有shuffle的流程吗？还是按顺序取数据去训练？<a class="headerlink" href="#q22-shuffle" title="Link to this heading"></a></h3>
<p>命令行参数<code class="docutils literal notranslate"><span class="pre">dataset_shuffle</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数文档</a>。</p>
</section>
<section id="q23-deepspeed">
<h3>Q23: 如果模型两张卡，数据不开并行，deepspeed就会出现报错，怎么处理呢？<a class="headerlink" href="#q23-deepspeed" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> 和 <code class="docutils literal notranslate"><span class="pre">device_map</span></code>是不兼容的，两个只能选1个。</p>
</section>
<section id="q24-vlm">
<h3>Q24: vlm模型训练如何减少显存使用？<a class="headerlink" href="#q24-vlm" title="Link to this heading"></a></h3>
<p>配置<code class="docutils literal notranslate"><span class="pre">--freeze_vit</span> <span class="pre">true</span></code>，以及限制最大像素的参数<code class="docutils literal notranslate"><span class="pre">--max_pixels</span></code>。</p>
</section>
<section id="q25-model-type-sftspecial-tokenschat-template">
<h3>Q25: 没有适配model_type的模型，sft时可以自定义special_tokens和chat_template吗？<a class="headerlink" href="#q25-model-type-sftspecial-tokenschat-template" title="Link to this heading"></a></h3>
<p>可以。参考<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/BestPractices/MLLM-Registration.html">注册多模态模型最佳实践</a>。</p>
</section>
<section id="q26-pythondpoqwen2-vl">
<h3>Q26: 可以在python脚本里面用DPO去训练qwen2-vl吗？<a class="headerlink" href="#q26-pythondpoqwen2-vl" title="Link to this heading"></a></h3>
<p>可以。从<code class="docutils literal notranslate"><span class="pre">swift.pipelines</span></code>中导入<code class="docutils literal notranslate"><span class="pre">rlhf_main</span></code> 和<code class="docutils literal notranslate"><span class="pre">RLHFArguments</span></code>。</p>
</section>
<section id="q27-mllm-vqa">
<h3>Q27: 请问训练MLLM时，可否先进行纯文本的预训练，然后接入VQA数据集进行微调呢？<a class="headerlink" href="#q27-mllm-vqa" title="Link to this heading"></a></h3>
<p>可以。也可以混着训练。</p>
</section>
<section id="q28-qwen2sftdpo-v100-nan">
<h3>Q28: 基于qwen2的sft模型进行dpo训练，v100的机器，训练时都是Nan呢？<a class="headerlink" href="#q28-qwen2sftdpo-v100-nan" title="Link to this heading"></a></h3>
<p>V100机器要用fp32训练qwen2。</p>
</section>
<section id="q29-swift">
<h3>Q29: 想问一下，swift，能支持蒸馏吗？<a class="headerlink" href="#q29-swift" title="Link to this heading"></a></h3>
<p>参考这个<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/sampler/distill/distill.sh">例子</a>。</p>
</section>
<section id="q30-checkpoint">
<h3>Q30: 当前训练完默认保存多少个checkpoint？<a class="headerlink" href="#q30-checkpoint" title="Link to this heading"></a></h3>
<p>默认保存所有的checkpoint，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数 save_total_limit</a>。</p>
</section>
<section id="q31-grounding">
<h3>Q31: Grounding任务中通用数据格式支持一个类别有多个实例吗？<a class="headerlink" href="#q31-grounding" title="Link to this heading"></a></h3>
<p>目前均支持了一个物体对应多个bbox，参考文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html#grounding">自定义数据集</a>。</p>
</section>
<section id="q32-numpy-object">
<h3>Q32: 这个错误为什么会出现在这，numpy.object找不到在哪？<a class="headerlink" href="#q32-numpy-object" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">numpy==1.26.3</span></code>，尝试一下。</p>
</section>
<section id="q33-swift">
<h3>Q33: swift框架能支持序列并行了吗？<a class="headerlink" href="#q33-swift" title="Link to this heading"></a></h3>
<p>支持。参考这里的<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel">例子</a>。</p>
</section>
<section id="q34-v100qwen2-1-5b-loss-0-0-acc-0-0-grad-norm-nan">
<h3>Q34: 用v100微调qwen2-1.5B时，loss': 0.0, 'acc': 0.0, 'grad_norm': nan，是什么问题呢?<a class="headerlink" href="#q34-v100qwen2-1-5b-loss-0-0-acc-0-0-grad-norm-nan" title="Link to this heading"></a></h3>
<p>尝试用fp32。</p>
</section>
<section id="q35-gptq">
<h3>Q35: gptq量化模型，能全参数微调吗？<a class="headerlink" href="#q35-gptq" title="Link to this heading"></a></h3>
<p>不能。gptq模型的int型参数无法参与求导，只能附着lora等额外结构参与更新。</p>
</section>
<section id="q36-qlora-glm4-chat">
<h3>Q36: 请问如果想用qlora的方式微调的话应该如何设置参数呢?glm4-chat<a class="headerlink" href="#q36-qlora-glm4-chat" title="Link to this heading"></a></h3>
<p>参考qlora<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora">例子</a>。</p>
</section>
<section id="q37-swift">
<h3>Q37: 请教一个问题，我应该如何在swift框架下扩充我的词表呢？<a class="headerlink" href="#q37-swift" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id3">命令行参数new_special_tokens</a>。</p>
</section>
<section id="q38-huggingface">
<h3>Q38: 同名的模型是可以直接使用huggingface上的吗？<a class="headerlink" href="#q38-huggingface" title="Link to this heading"></a></h3>
<p>设置环境变量<code class="docutils literal notranslate"><span class="pre">USE_HF=1</span></code>。</p>
</section>
<section id="q39-qwen2-vl-2b">
<h3>Q39: 请问Qwen2-VL-2B能进行增量预训练吗？有指导文件吗?有图文,也有纯文本的。<a class="headerlink" href="#q39-qwen2-vl-2b" title="Link to this heading"></a></h3>
<p>支持，按<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html">自定义数据集文档</a>中的格式组织数据就行。</p>
</section>
<section id="q40-frame-rate-minicpmv">
<h3>Q40: 请问下用视频做训练的时候，如何在参数中控制抽帧率，设了frame_rate设不起, minicpmv<a class="headerlink" href="#q40-frame-rate-minicpmv" title="Link to this heading"></a></h3>
<p>设置环境变量<code class="docutils literal notranslate"><span class="pre">MAX_NUM_FRAMES</span></code>。</p>
</section>
<section id="q41-swift">
<h3>Q41: swift在训练的时候，可以把验证集的推理结果保存下来吗？<a class="headerlink" href="#q41-swift" title="Link to this heading"></a></h3>
<p>训练结束后，运行swift infer，会保存。</p>
</section>
<section id="q42-fulldpo-checkpoint-1">
<h3>Q42: 我全量full参数dpo，为何保存的checkpoint 比原本模型文件要大呢?整整大了1倍<a class="headerlink" href="#q42-fulldpo-checkpoint-1" title="Link to this heading"></a></h3>
<p>用V100微调，存的是fp32类型。</p>
</section>
<section id="q43-swiftllm-deepspeed-zero3">
<h3>Q43: 多机训练速度缓慢，在使用swift框架进行LLM训练时，发现采用deepspeed zero3训练会出现严重的速度下降问题<a class="headerlink" href="#q43-swiftllm-deepspeed-zero3" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://github.com/modelscope/ms-swift/issues/1825">issue</a>。</p>
</section>
<section id="q44-swiftqwen2-vl-sftvit-llm-finetune">
<h3>Q44: swift现在是支持qwen2-vl多阶段预训练的吗？我看官方的最佳实践里的sft好像都是vit+llm一起训的，不知道支不支持单独finetune<a class="headerlink" href="#q44-swiftqwen2-vl-sftvit-llm-finetune" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">--freeze_vit</span></code>，<code class="docutils literal notranslate"><span class="pre">--freeze_aligner</span></code>，<code class="docutils literal notranslate"><span class="pre">--freeze_llm</span></code>这几个参数可以控制，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#tuner">命令行参数文档</a>。</p>
</section>
<section id="q45-qwen2-vl">
<h3>Q45: qwen2-vl是不是不支持混合纯文本数据?<a class="headerlink" href="#q45-qwen2-vl" title="Link to this heading"></a></h3>
<p>支持图文和纯文本。</p>
</section>
<section id="q46-loss">
<h3>Q46: 微调的时候可以绘制不同数据集的loss曲线吗？<a class="headerlink" href="#q46-loss" title="Link to this heading"></a></h3>
<p>支持channel loss，参考<code class="docutils literal notranslate"><span class="pre">--enable_channel_loss</span></code>。</p>
</section>
<section id="q47">
<h3>Q47: 模型训练后，回复重复了很多内容<a class="headerlink" href="#q47" title="Link to this heading"></a></h3>
<p>参考<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Pre-training-and-Fine-tuning.html">预训练与微调</a>。如果训练过程中出现重复的情况，请多训练几个epoch, 清洗数据, 全参数训练, 采用RLHF的方式缓解。</p>
</section>
<section id="q48-swiftprompt-tuningprefix-tuning">
<h3>Q48: 想问一下swift目前支持prompt tuning或者prefix tuning吗？<a class="headerlink" href="#q48-swiftprompt-tuningprefix-tuning" title="Link to this heading"></a></h3>
<p>不支持，这两个方法知识遗忘比较严重，目前不推荐使用。</p>
</section>
<section id="q49-a10">
<h3>Q49: 两张A10训练报错如下：<a class="headerlink" href="#q49-a10" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[rank0]: torch.distributed.DistBackendError: NCCL error in:../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970， unhandled system error (run with NCCL_DEBUG=INFO for details),NCCL version 2.20.5
[rank0]:ncclSystemError: System call (e.g. socket,malloc) or external library call failed or device error.
</pre></div>
</div>
<p>请检查共享内存是否太小，nccl需要共享内存。</p>
</section>
<section id="q50-ddp">
<h3>Q50: 请问在采用DDP微调训练的过程中，冻结某些层时导致的某些参数未参与梯度回传问题怎么解决？<a class="headerlink" href="#q50-ddp" title="Link to this heading"></a></h3>
<p>配置参数<code class="docutils literal notranslate"><span class="pre">--ddp_find_unused_parameters</span> <span class="pre">true</span></code>。</p>
</section>
<section id="q51-swift">
<h3>Q51: swift有没有数据集质检工具？<a class="headerlink" href="#q51-swift" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://github.com/modelscope/data-juicer">data-juicer</a>。</p>
</section>
<section id="q52-web">
<h3>Q52: web端在哪启动模型并行?只找到了数据并行的勾选项，没找到模型并行在哪。<a class="headerlink" href="#q52-web" title="Link to this heading"></a></h3>
<p>指定可见显卡就可以。</p>
</section>
<section id="q53-dataset">
<h3>Q53: 设置--dataset的话，怎么让数据集下载到固定位置，我在命令行参数没找到，下次如果再次读取的话如何可以从下载的地方读取<a class="headerlink" href="#q53-dataset" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">dataset_path</span></code>支持文件夹，一般是<code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span></code>下载下来的数据集文件夹。详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Customization/Custom-dataset.html#dataset-info-json">自定义数据集文档</a>。</p>
</section>
<section id="q54-streaming-true-num-train-epochsmax-stepsnum-train-epochs">
<h3>Q54: --streaming true，我设置num_train_epochs会报错让我设置max_steps。不可以只设置num_train_epochs吗？<a class="headerlink" href="#q54-streaming-true-num-train-epochsmax-stepsnum-train-epochs" title="Link to this heading"></a></h3>
<p>详见<code class="docutils literal notranslate"><span class="pre">streaming</span></code>参数说明，<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id4">命令行参数文档</a>。</p>
</section>
<section id="q55-tools-tools">
<h3>Q55: 好奇tools为啥是&quot;[]&quot;，不是直接支持[]呢，能否帮忙解答一下，这个tools为啥是&quot;[]&quot;这种格式呢，不是直接使用[]呢，有些不理解<a class="headerlink" href="#q55-tools-tools" title="Link to this heading"></a></h3>
<p>这是因为datasets的底层pyarrow对于类型管控比较严格。我们官方的grounding数据集的objects部分也是因为这个原因要用str，要不pyarrow就会报错：你每行的类型不一致。</p>
</section>
<section id="q56-check-dataset-strategy-discard">
<h3>Q56: 这个参数不能用吗？check_dataset_strategy==discard<a class="headerlink" href="#q56-check-dataset-strategy-discard" title="Link to this heading"></a></h3>
<p>swift3.0没这个参数了，用<code class="docutils literal notranslate"><span class="pre">strict</span></code>参数。</p>
</section>
<section id="q57-sft">
<h3>Q57: 运行sft命令出现报错如下：<a class="headerlink" href="#q57-sft" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: Expected to mark a variable ready only once.This error is caused by one of the following reasons: 1) Use of a module parameter outsid forward function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph( ) as round if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple oint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph( ) as a workaround if dule graph does not change over iterations.
</pre></div>
</div>
<p>加一下这个参数，<code class="docutils literal notranslate"><span class="pre">--gradient_checkpointing_kwargs</span> <span class="pre">'{&quot;use_reentrant&quot;:</span> <span class="pre">false}'</span></code>。或者升级ms-swift，默认设置了这个参数。</p>
</section>
<section id="q58-attributeerror-trainerstate-object-has-no-attribute-last-model-checkpoint">
<h3>Q58: 有遇到过这个问题嘛？AttributeError:’TrainerState’ object has no attribute ’last_model_checkpoint’<a class="headerlink" href="#q58-attributeerror-trainerstate-object-has-no-attribute-last-model-checkpoint" title="Link to this heading"></a></h3>
<p>数据集太少了，增加一些。数据数量不足一个step导致的报错。</p>
</section>
<section id="q59-custompreprocessorpreprocess">
<h3>Q59: 我看到custompreprocessor里面可以定义preprocess，这个是在训练开始前全部会处理好，还是一边训练一边加载的啊<a class="headerlink" href="#q59-custompreprocessorpreprocess" title="Link to this heading"></a></h3>
<p>如果设置了参数<code class="docutils literal notranslate"><span class="pre">--streaming</span> <span class="pre">true</span></code>，就是一边训练一边加载。默认是全部处理完然后训练。</p>
</section>
<section id="q60-internvl2-5-freeze-parametersvision-model-mlp1-freeze-parameters-freeze-vit-freeze-aligner-freeze-llmfalse-trainable-parameters-mlp1-mlp1train-parastrain-mlp1">
<h3>Q60: 全参数训练internvl2_5，为啥里面的 freeze parameters默认就有vision_model 和 mlp1？我看命令行参数的文档里面freeze parameters默认为[],命令中显示设置 freeze vit， freeze aligner， freeze llm都为False，又会打印出来trainable parameters：[‘mlp1’] 也不知道是指只有mlp1可以train 还是 所有的paras都可以train 只是mlp1打印一下<a class="headerlink" href="#q60-internvl2-5-freeze-parametersvision-model-mlp1-freeze-parameters-freeze-vit-freeze-aligner-freeze-llmfalse-trainable-parameters-mlp1-mlp1train-parastrain-mlp1" title="Link to this heading"></a></h3>
<p>先freeze parameters再active parameters。<code class="docutils literal notranslate"><span class="pre">freeze</span> <span class="pre">vit/freeze</span> <span class="pre">aligner/freeze</span> <span class="pre">llm</span></code>这三个参数会对freeze parameters 和trainable parameters进行调整.因为有些模型的<code class="docutils literal notranslate"><span class="pre">vit</span></code>中包含<code class="docutils literal notranslate"><span class="pre">aligner</span></code>，所以会将<code class="docutils literal notranslate"><span class="pre">aligner</span></code>单独加入trainable_parameters。</p>
</section>
<section id="q61-swiftllamapro">
<h3>Q61: 请问swift中的llamapro对多模态做适配了吗？<a class="headerlink" href="#q61-swiftllamapro" title="Link to this heading"></a></h3>
<p>支持的。</p>
</section>
<section id="q62-2-xmax-pixels-3-x-max-pixel-12000-9000-2-xresacle-imageinternvl">
<h3>Q62: 我发现2.x支持MAX_PIXELS，3.x文档里有个--max_pixel参数是一个意思吗，他的处理逻辑是啥样的？我用12000*9000的图片，2.x设置resacle_image训练internvl还是会崩<a class="headerlink" href="#q62-2-xmax-pixels-3-x-max-pixel-12000-9000-2-xresacle-imageinternvl" title="Link to this heading"></a></h3>
<p>环境变量的参数是对应模型的参数，<code class="docutils literal notranslate"><span class="pre">MAX_PIXELS</span></code>只支持qwen2vl的，internvl有自己的环境变量参数，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id18">特定模型参数</a>。</p>
</section>
<section id="q63-qwen-basechat">
<h3>Q63: 从qwen base模型微调成chat模型有没有实践文档，有什么要特别配置的吗?<a class="headerlink" href="#q63-qwen-basechat" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">sft</span></code>，没有其他需要特别配置的，参考<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/base_to_chat">例子</a>。</p>
</section>
<section id="q64-sequence-parallel">
<h3>Q64: sequence parallel例子在哪呀？<a class="headerlink" href="#q64-sequence-parallel" title="Link to this heading"></a></h3>
<p>看这个例子<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel">sequence_parallel</a>。</p>
</section>
<section id="q65-swift">
<h3>Q65: swift能支持训练自己定义的模型结构吗？<a class="headerlink" href="#q65-swift" title="Link to this heading"></a></h3>
<p>可以的，只需要自定义<code class="docutils literal notranslate"><span class="pre">get_model_tokenizer_xxx</span></code>函数就好了，返回<code class="docutils literal notranslate"><span class="pre">model</span></code>和<code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>。</p>
</section>
<section id="q66-name-or-path-mnt-workspace-model-qwen2-5-14b-instruct-longlora-llamalonglora">
<h3>Q66: 我用&quot;name_or_path&quot;: &quot;/mnt/workspace/model/Qwen2.5-14B-Instruct&quot;跑longlora 发现出现了报错，不会是只有个llama系列可以使用longlora吧<a class="headerlink" href="#q66-name-or-path-mnt-workspace-model-qwen2-5-14b-instruct-longlora-llamalonglora" title="Link to this heading"></a></h3>
<p>只有llama系列能用<code class="docutils literal notranslate"><span class="pre">longlora</span></code>。</p>
</section>
<section id="q67-swiftspecial-token">
<h3>Q67: 想问下swift怎么加入自己的special token？<a class="headerlink" href="#q67-swiftspecial-token" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id3">命令行参数文档new_special_tokens</a>。</p>
</section>
<section id="q68-freeze-parameters-ratio-0-7-llm30-30">
<h3>Q68: --freeze_parameters_ratio这个参数，如果设定为0.7，是不是说明训练的时候只更新llm的30%的参数？是随机更新30%吗，这个参数更新的机制是什么呀？<a class="headerlink" href="#q68-freeze-parameters-ratio-0-7-llm30-30" title="Link to this heading"></a></h3>
<p>从下往上freeze。</p>
</section>
<section id="q69-map">
<h3>Q69: map过程为啥这么慢，这是正常的吗？<a class="headerlink" href="#q69-map" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Map: 4%|██ | 9000/203823 [02:18&lt;50:34, 64.19 examples/s]
</pre></div>
</div>
<p>设置参数<code class="docutils literal notranslate"><span class="pre">--dataset_num_proc</span></code>可以开多进程。</p>
</section>
<section id="q70">
<h3>Q70: 请问数据集如何能够删除重新下载，感觉数据集出了点问题<a class="headerlink" href="#q70" title="Link to this heading"></a></h3>
<p>设置参数<code class="docutils literal notranslate"><span class="pre">--download_mode</span></code>。</p>
</section>
<section id="q71-safetensors-rust-safetensorerror-error-while-deserializing-header-headertoolarge">
<h3>Q71: 请问这个问题如何解决？safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge<a class="headerlink" href="#q71-safetensors-rust-safetensorerror-error-while-deserializing-header-headertoolarge" title="Link to this heading"></a></h3>
<p>磁盘空间不足了，模型没有保存完整。</p>
</section>
<section id="q72-swift3-0get-default-template-type">
<h3>Q72: swift3.0不支持get_default_template_type是吗？<a class="headerlink" href="#q72-swift3-0get-default-template-type" title="Link to this heading"></a></h3>
<p>请查看<code class="docutils literal notranslate"><span class="pre">model.model_meta.template</span></code>，信息都存在<code class="docutils literal notranslate"><span class="pre">model.model_meta和model.model_info</span></code>。</p>
</section>
<section id="q73-left-padding">
<h3>Q73: 请问默认模型训练都是left padding是吧?<a class="headerlink" href="#q73-left-padding" title="Link to this heading"></a></h3>
<p>训练可以选择使用左padding还是右padding。默认是右padding, <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">infer</span></code>都是左padding。</p>
</section>
<section id="q74-grounding">
<h3>Q74: 请问下现在支持grounding任务了吗<a class="headerlink" href="#q74-grounding" title="Link to this heading"></a></h3>
<p>examples下有<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/grounding.sh">例子</a>。</p>
</section>
<section id="q75-ms-swift-llm-emb">
<h3>Q75: 请问现在ms-swift支持对比学习，从而训练llm_emb吗?<a class="headerlink" href="#q75-ms-swift-llm-emb" title="Link to this heading"></a></h3>
<p>支持，<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding">例子</a>。</p>
</section>
<section id="q76-pefttrl-grposwift">
<h3>Q76: 话说直接从peft和trl库，手搓微调和grpo代码和swift官方在同参数下进行训练，效果差异大吗？<a class="headerlink" href="#q76-pefttrl-grposwift" title="Link to this heading"></a></h3>
<p>区别不大，额外支持了多模态。</p>
</section>
<section id="q77-swift-minicpmo-assert-media-type-in-image-video">
<h3>Q77: swift 目前不支持 minicpmo 使用音频模态输入的训练吗？会报错： assert media_type in {'image', 'video'}<a class="headerlink" href="#q77-swift-minicpmo-assert-media-type-in-image-video" title="Link to this heading"></a></h3>
<p>目前不支持音频。</p>
</section>
<section id="q78-swiftdeepseek-r1-671b">
<h3>Q78: swift可以微调deepseek R1 671B吗？<a class="headerlink" href="#q78-swiftdeepseek-r1-671b" title="Link to this heading"></a></h3>
<p>可以，template是接入了的，不过过程会比较麻烦，要先fp8转bf16。</p>
</section>
<section id="q79-swift-git-clone">
<h3>Q79: 最新的swift框架不是通过这个命令来指定模型的位置的么？这是我已经下载好的模型位置，不知道为什么还要下载，还下不下来，提示报错git clone<a class="headerlink" href="#q79-swift-git-clone" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--model<span class="w"> </span>/mnt/workspace/.cache/modelscope/hub/deepseek-ai/deepseek-vl2/<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>有些需要clone repo，然后通过<code class="docutils literal notranslate"><span class="pre">local_repo_path</span></code>指定。</p>
</section>
<section id="q80-swiftgrpo">
<h3>Q80: swift现在支持多模态的grpo吗？<a class="headerlink" href="#q80-swiftgrpo" title="Link to this heading"></a></h3>
<p>支持。</p>
</section>
<section id="q81-grporeward">
<h3>Q81: grpo的reward函数支持自己定义么?<a class="headerlink" href="#q81-grporeward" title="Link to this heading"></a></h3>
<p>支持，参考<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/plugin">examples/train/grpo/plugin</a>。</p>
</section>
<section id="q82-torch-dtype-float16-bf16-lib-python3-12-site-packages-torch-amp-grad-scaler-py-line-260-in-unscale-grads-raise-valueerror-attempting-to-unscale-fp16-gradients-valueerror-attempting-to-unscale-fp16-gradients">
<h3>Q82: 请问为什么 --torch_dtype float16 （卡不能使用bf16）会出现报错：lib/python3.12/site-packages/torch/amp/grad_scaler.py&quot;, line 260, in <em>unscale_grads</em> raise ValueError(&quot;Attempting to unscale FP16 gradients.&quot;) ValueError: Attempting to unscale FP16 gradients.<a class="headerlink" href="#q82-torch-dtype-float16-bf16-lib-python3-12-site-packages-torch-amp-grad-scaler-py-line-260-in-unscale-grads-raise-valueerror-attempting-to-unscale-fp16-gradients-valueerror-attempting-to-unscale-fp16-gradients" title="Link to this heading"></a></h3>
<p>全参数，不能fp16训练的。</p>
</section>
<section id="q83-swiftreward-qwen2-5-7b-ppogrporewardlora">
<h3>Q83: 请教一个问题。我用swift训练了一个reward模型（基线是qwen2.5-7b），然后用在ppo或者grpo中加载会报错。reward模型是lora训练的。<a class="headerlink" href="#q83-swiftreward-qwen2-5-7b-ppogrporewardlora" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--rlhf_type<span class="w"> </span>ppo<span class="w"> </span><span class="se">\</span>
--model<span class="w"> </span>Qwen/Qwen2.5-14B-Instruct<span class="w"> </span><span class="se">\</span>
--reward_model<span class="w"> </span>/mnt/workspace/output/rm/model<span class="w"> </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-zh#20000&#39;</span><span class="w"> </span>--torch_dtype<span class="w"> </span>float32<span class="w"> </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--learning_rate<span class="w"> </span>1e-5<span class="w"> </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span>--lora_alpha<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
--gradient_accumulation_steps<span class="w"> </span><span class="m">16</span><span class="w"> </span>--eval_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span>--save_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>lora训练的reward model需要merge一下。</p>
</section>
<section id="q84-deepseek-vl2-transformers-4-42-4-42peft">
<h3>Q84: 各位大佬，请问要微调deepseek_vl2，transformers用什么什么版本？官方文档说&lt;4.42，但是4.42及以下也报错。peft版本也要降低吗？<a class="headerlink" href="#q84-deepseek-vl2-transformers-4-42-4-42peft" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">peft==0.11.*</span></code>。</p>
</section>
<section id="q85-generate-train-split-30-swift-2-xlazy-tokenize">
<h3>Q85: 请问generate train split太慢了有没有什么好办法呀（大概有30多个数据集，总数据量百万左右）。之前swift 2.x好像没有这么慢。lazy tokenize 已经开了<a class="headerlink" href="#q85-generate-train-split-30-swift-2-xlazy-tokenize" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--dataset_num_proc</span> <span class="pre">16</span></code>。</p>
</section>
<section id="q86-qwen2-5vl-visual-encoderlorallm">
<h3>Q86: 请问下微调qwen2.5vl的时候，我想使用全参数微调visual encoder同时使用LoRA微调LLM，怎么实现呢？<a class="headerlink" href="#q86-qwen2-5vl-visual-encoderlorallm" title="Link to this heading"></a></h3>
<p>参考这里<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal/lora_llm_full_vit">例子</a>。</p>
</section>
<section id="q87-swift">
<h3>Q87: 问一下，swift怎么使用自定义的损失函数？<a class="headerlink" href="#q87-swift" title="Link to this heading"></a></h3>
<p>plugin中加就可以了。</p>
</section>
<section id="q88-moe">
<h3>Q88: 请问下MoE的参数有哪些，参数表里关键字搜索不到？专家数量，专家路由这些参数怎么设置？<a class="headerlink" href="#q88-moe" title="Link to this heading"></a></h3>
<p>直接用config.json中的参数。</p>
</section>
<section id="q89-grpolmdeploy-lmdeployengineload-weights">
<h3>Q89: grpo训练中使用lmdeploy会报相关函数不存在的问题，想请教下具体问题，在lmdeployengine类里面确实没找到load_weights这个函数<a class="headerlink" href="#q89-grpolmdeploy-lmdeployengineload-weights" title="Link to this heading"></a></h3>
<p>只在turbomind引擎下支持。</p>
</section>
<section id="q90-moonlight-16b-a3b-instruct-ms-swift">
<h3>Q90: Moonlight-16B-A3B-Instruct, 我在微调这个模型的时候报错怎么办?ms-swift好像不支持这个模型进行微调<a class="headerlink" href="#q90-moonlight-16b-a3b-instruct-ms-swift" title="Link to this heading"></a></h3>
<p>因为是模型文件中禁止了训练, 参考deepseek_vl2的解决方案，你搜搜issue。</p>
</section>
<section id="q91-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16">
<h3>Q91: 训练时出了这个错应该咋解决？RuntimeError: “triu_tril_cuda_template“ not implemented for ‘BFloat16'<a class="headerlink" href="#q91-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">01</span>,2,3,4,5,6,7<span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Internlm3-8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span>train.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed<span class="w"> </span>zero3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<p>升级torch。</p>
</section>
<section id="q92-grpo-lossgrad-norm0">
<h3>Q92: grpo训练，loss和grad_norm全是0，正常的吗？<a class="headerlink" href="#q92-grpo-lossgrad-norm0" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;:    0.0.    &#39;grad norm&#39;:0.0,    &#39;learning_rate&#39;:9e-08,    &#39;memory(GiB)&#39;:88.1，    &#39;train_speed(iter/s)&#39;:0.009252，    &#39;completion_length&#39;:    150.00000763，    &#39;response_clip ratio&#39;: 0.0,    &#39;rewards/Format&#39;:1.0,    &#39;reward
: 1.0,    &#39;reward std&#39;:0.0，    &#39;kl&#39;: 0.0, &#39;clip_ratio&#39;: 0.0,    &#39;epoch&#39;: 0.0， &#39;qlobal step/max steps&#39;:&#39;1/1052&#39;，    &#39;percentage&#39;:&#39;0.10%    &#39;elapsed time&#39;:    &#39;36s    &#39;remaining time&#39;: &#39;10h 43m 54s&#39;}
{&#39;loss&#39;: 0.0，&#39;grad_norm&#39;:0.0，&#39;learning_rate&#39;: 1.8e-07,&#39;memory(GiB)&#39;:94.15，&#39;train_speed(iter/s)&#39;:0.014782，&#39;completion_length&#39;: 133.25000763，&#39;response_clip_ratio&#39;: 0.0，&#39;rewards/Format&#39;: 1.0, &#39;rewa rd&#39;: 1.0，&#39;reward_std&#39;: 0.0, &#39;kl&#39;: 0.0，&#39;clip_ratio&#39;: 0.0,&#39;epoch&#39;: 0.0, &#39;global_step/max_steps&#39;: &#39;2/1052&#39;，&#39;percentage&#39;: &#39;0.19%&#39;, &#39;elapsed_time&#39;: &#39;1m 3s&#39;， &#39;remaining_time&#39;: &#39;9h 19m 49s&#39;}
{&#39;loss&#39;: 0.0， &#39;qrad norm&#39;: 0.0, &#39;learning rate&#39;: 2.7e-07,&#39;memory(GiB)&#39;: 94.15，&#39;train_speed(iter/s)&#39;: 0.018695，&#39;completion_length&#39;: 123.08333969，，&#39;response_clip_ratio&#39;: 0.0，&#39;rewards/Format&#39;: 1.0, &#39;rewa rd&#39;: 1.0， &#39;reward_ std&#39;: 0.0,&#39;kl&#39;: 0.0,&#39;clip_ratio&#39;: 0.0， &#39;epoch&#39;: 0.0， &#39;global_step/max_steps&#39;: &#39;3/1052&#39;，&#39;percentage&#39;: &#39;0.29%，&#39;elapsed_time&#39;: &#39;1m 29s&#39;，&#39;remaining_time&#39;: &#39;8h 39m 34s&#39;}
</pre></div>
</div>
<p>训练过程中loss接近0是正常情况，参考<a class="reference external" href="https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851">issue</a>。</p>
</section>
<section id="q93-grpo-accuracy-orm">
<h3>Q93: 请教一下这个grpo的内置奖励函数，从哪里可以传入accuracy_orm<a class="headerlink" href="#q93-grpo-accuracy-orm" title="Link to this heading"></a></h3>
<p>目前是直接改代码。</p>
</section>
<section id="q94-solution-solution">
<h3>Q94: 我看这奖励函数有solution参数，是要从数据集里面传过来吗？就是我数据集必须有solution这项？<a class="headerlink" href="#q94-solution-solution" title="Link to this heading"></a></h3>
<p>是的，针对math问题，不然不好算accuracy。</p>
</section>
<section id="q95-token-acc">
<h3>Q95: 训练为什么没有token_acc？<a class="headerlink" href="#q95-token-acc" title="Link to this heading"></a></h3>
<p>有些模型<code class="docutils literal notranslate"><span class="pre">logits</span></code>和<code class="docutils literal notranslate"><span class="pre">labels</span></code>数量对不上，就不算的。</p>
</section>
<section id="q96-ovis2-lora-tuner-type-lora">
<h3>Q96: 微调Ovis2 使用lora参数不起作用？加不加--tuner_type lora \，好像都是全参数微调？显存没变化。<a class="headerlink" href="#q96-ovis2-lora-tuner-type-lora" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">--max_length</span></code>限制一下，这个模型有点特殊，需要padding到max_length。</p>
</section>
<section id="q97-qwen2-5-valueerror-the-model-did-not-return-a-loss-from-the-inputs-only-the-following-keys-logits-for-reference-the-inputs-it-received-are-input-ids-attention-mask">
<h3>Q97: 请问下用qwen2.5跑一个分类任务，抱下面的错误，是哪里配置的有问题呢？ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.<a class="headerlink" href="#q97-qwen2-5-valueerror-the-model-did-not-return-a-loss-from-the-inputs-only-the-following-keys-logits-for-reference-the-inputs-it-received-are-input-ids-attention-mask" title="Link to this heading"></a></h3>
<p>数据集是这样的：{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;xxxxx&quot;}, {&quot;label&quot;: 1}]}
<code class="docutils literal notranslate"><span class="pre">label</span></code>写在<code class="docutils literal notranslate"><span class="pre">message</span></code>同级。</p>
</section>
<section id="q98-vllmengine-engine-engine">
<h3>Q98: 启动了VllmEngine，要如何退出呀？就是调用了engine，模型就被载入显存准备工作。但是我推理完想要engine释放显存。下次调用时，再加载。而不是一直占用<a class="headerlink" href="#q98-vllmengine-engine-engine" title="Link to this heading"></a></h3>
<p>sleep mode啊，支持的。<code class="docutils literal notranslate"><span class="pre">engine.sleep(level=1)/engine.wake_up()</span></code>，构造的时候加一个<code class="docutils literal notranslate"><span class="pre">enable_sleep_mode=True</span></code>。</p>
</section>
<section id="q99-streaming-trainer-sampler-random">
<h3>Q99: 求问，streaming模式下，trainer_sampler_random是不是就没有作用了呢？<a class="headerlink" href="#q99-streaming-trainer-sampler-random" title="Link to this heading"></a></h3>
<p>streaming是不随机的。</p>
</section>
<section id="q100-grpovllm-vllmtrust-rwmote-code">
<h3>Q100: 请问grpo使用vllm进行推理，vllm可以设置trust_rwmote_code吗？<a class="headerlink" href="#q100-grpovllm-vllmtrust-rwmote-code" title="Link to this heading"></a></h3>
<p>默认就是true的。</p>
</section>
<section id="q101-pretrain-streamingpacking-max-steps-epochsbssteps">
<h3>Q101: 请教一下，pretrain阶段数据集比较大，用了streaming流式和packing打包数据，这时候需要设置 max_steps，有没有参数或者命令可以根据epochs、bs等参数计算打包后的总的steps吗？<a class="headerlink" href="#q101-pretrain-streamingpacking-max-steps-epochsbssteps" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--max_steps</span></code>或<code class="docutils literal notranslate"><span class="pre">--max_epochs</span></code>，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id4">命令行参数文档</a>streaming参数说明。</p>
</section>
<section id="q102-unsloth-assert-type-target-modules-in-list-tuple-target-modules-all-linear">
<h3>Q102: unsloth训练，报错：assert(type(target modules) in (list,tuple,))。配置的参数是--target modules all-linear<a class="headerlink" href="#q102-unsloth-assert-type-target-modules-in-list-tuple-target-modules-all-linear" title="Link to this heading"></a></h3>
<p>别用<code class="docutils literal notranslate"><span class="pre">all-linear</span></code>，改为具体的模块列表，比如<code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">q</span> <span class="pre">k</span> <span class="pre">v</span></code>。</p>
</section>
<section id="q103-swift">
<h3>Q103: Swift现在支持多标签分类么？<a class="headerlink" href="#q103-swift" title="Link to this heading"></a></h3>
<p>支持的。自定义数据集文档有格式，然后在命令行参数文档中搜索一下<code class="docutils literal notranslate"><span class="pre">problem_type</span></code>，改一下，其他和回归是一样的。</p>
</section>
<section id="q104-packingflash-attn">
<h3>Q104: 请问packing中flash_attn是分开处理的还是合并处理的？<a class="headerlink" href="#q104-packingflash-attn" title="Link to this heading"></a></h3>
<p>一定需要flash_attn，不然是有误差，attention_mask会出问题。</p>
</section>
<section id="q105-qwen2-5-omni-freeze-vit-false">
<h3>Q105: 请问对于qwen2.5-omni来说--freeze_vit false意味这视觉编码器和音频编码器都打开了，有什么办法可以只打开音频编码器不打开视觉编码器吗？<a class="headerlink" href="#q105-qwen2-5-omni-freeze-vit-false" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">--target_regex</span></code>写一下。</p>
</section>
<section id="q106-swift">
<h3>Q106: 请问现在swift的强化学习那几种训练方法支持序列并行么？<a class="headerlink" href="#q106-swift" title="Link to this heading"></a></h3>
<p>支持pt, sft, dpo and grpo。</p>
</section>
<section id="q107-lora-sfttokenizer-json">
<h3>Q107: 使用 lora sft之后是不会储存tokenizer.json吗<a class="headerlink" href="#q107-lora-sfttokenizer-json" title="Link to this heading"></a></h3>
<p>lora不会存储，merge后才会把这些文件迁移过来，因为lora目录需要配合原模型使用。</p>
</section>
<section id="q108-grpo-reward-model-reward-funcs">
<h3>Q108: GRPO 的reward_model 和 reward_funcs可以一起用吗？<a class="headerlink" href="#q108-grpo-reward-model-reward-funcs" title="Link to this heading"></a></h3>
<p>可以。</p>
</section>
<section id="q109-grpokl">
<h3>Q109: 想请教一下，在进行GRPO时不打算引入KL项，有相关的参数可以调整吗？<a class="headerlink" href="#q109-grpokl" title="Link to this heading"></a></h3>
<p>命令行参数搜一下beta。</p>
</section>
<section id="q110-grpo-orm-kwargsmessages-assistantcontent">
<h3>Q110: 请教一个问题，做grpo的时候，如何在orm的自定义奖励函数中获取原始标签呢？我打印了kwargs的messages字段，里面的每一项的assistant的content的值已经被替换成生成的结果了<a class="headerlink" href="#q110-grpo-orm-kwargsmessages-assistantcontent" title="Link to this heading"></a></h3>
<p>放到另外的列里。</p>
</section>
<section id="q111-num-iterations-1-clip-dapo-clip-higher-verl-micro-batch-policy-model-clip-ms-swift-mini-batch">
<h3>Q111: 默认只用 num_iterations=1 的话，clip 就失去作用了吧？dapo 的 clip higher 也没用。我看 veRL 有个 micro batch 可以设置单轮小批次更新 policy model 来使得 clip 项生效，ms-swift 的 mini batch 看源码貌似只是做了梯度累加？<a class="headerlink" href="#q111-num-iterations-1-clip-dapo-clip-higher-verl-micro-batch-policy-model-clip-ms-swift-mini-batch" title="Link to this heading"></a></h3>
<p>是的，需要num_iterations&gt;1。</p>
</section>
<section id="q112-qwen2-5-omni-talker">
<h3>Q112: 请问qwen2.5-omni的训练支持全参训练吗，是否支持talker的训练？<a class="headerlink" href="#q112-qwen2-5-omni-talker" title="Link to this heading"></a></h3>
<p>目前不支持talker训练，只有thinker。</p>
</section>
<section id="q113-sequence-parallelliger-kernel">
<h3>Q113: 请问，sequence parallel是否可以和liger kernel同时启用呀？<a class="headerlink" href="#q113-sequence-parallelliger-kernel" title="Link to this heading"></a></h3>
<p>可以。</p>
</section>
<section id="q114-ppormpolicy">
<h3>Q114: 请问ppo训练rm和policy有什么要求呢？<a class="headerlink" href="#q114-ppormpolicy" title="Link to this heading"></a></h3>
<p>现在ppo还只支持rm和policy是同一系列的模型(tokenizer/template)。</p>
</section>
<section id="q115-llama3-18b-3-2-1b-llama-3-1">
<h3>Q115: 还想问一下，由于llama3.1没有小于8B的模型，因此我想用3.2 1B的的来微调，那么还能用Llama-3.1这个奖励模型吗？<a class="headerlink" href="#q115-llama3-18b-3-2-1b-llama-3-1" title="Link to this heading"></a></h3>
<p>要求是<code class="docutils literal notranslate"><span class="pre">template</span></code>和<code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>要一样， 3.1 和 3.2 应该问题不大。</p>
</section>
<section id="q116-swiftmappiing">
<h3>Q116: 请问swift是否能缓存一份mappiing之后的数据？方便排查训练数据的问题<a class="headerlink" href="#q116-swiftmappiing" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--load_from_cache_file</span> <span class="pre">false</span></code>。</p>
</section>
<section id="q117-warning-none-of-the-inputs-have-requires-grad-true">
<h3>Q117: 全参数训练为啥会有warning: none of the inputs have requires_grad=True?<a class="headerlink" href="#q117-warning-none-of-the-inputs-have-requires-grad-true" title="Link to this heading"></a></h3>
<p>如果vit没有训练，那有这个warning是正常的，如果训练了，则不应该抛出。</p>
</section>
<section id="q118-qwen2-5vl-ulyssessdpa">
<h3>Q118: 现在qwen2.5vl ulysses支持sdpa吗？<a class="headerlink" href="#q118-qwen2-5vl-ulyssessdpa" title="Link to this heading"></a></h3>
<p>vl模型的目前仅支持flash-attn，纯文本两种都支持。</p>
</section>
<section id="q119-videos">
<h3>Q119: 请问这图片列表形式的videos现在支持了吗？格式如下<a class="headerlink" href="#q119-videos" title="Link to this heading"></a></h3>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;video&gt;是一只狮子在跑步&quot;</span><span class="p">}],</span><span class="w"> </span><span class="nt">&quot;videos&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[[</span><span class="s2">&quot;1.jpg&quot;</span><span class="p">,</span><span class="s2">&quot;2.jpg&quot;</span><span class="p">]]}</span>
</pre></div>
</div>
<p>支持了，使用文件目录的方式。</p>
</section>
<section id="q120-grposave-stepsstepglobal-step-global-step18-wandbstep628">
<h3>Q120: 请教一个问题，grpo脚本中的save_steps指的是step还是global step？目前本地训练显示的global step是18， wandb上显示的step是628。<a class="headerlink" href="#q120-grposave-stepsstepglobal-step-global-step18-wandbstep628" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">global_step</span></code>，本地tqdm显示的。</p>
</section>
<section id="q121-use-logits-to-keep">
<h3>Q121: use_logits_to_keep 现在多模态大模型上可以用吗？<a class="headerlink" href="#q121-use-logits-to-keep" title="Link to this heading"></a></h3>
<p>如果多模态token的展开在模型的forward内会报错。</p>
</section>
<section id="q122-50step100step">
<h3>Q122: 请问一下为什么训练到会有好几次显存大幅度增加，已经50step或者100step<a class="headerlink" href="#q122-50step100step" title="Link to this heading"></a></h3>
<p>设置环境变量<code class="docutils literal notranslate"><span class="pre">PYTORCH_CUDA_ALLOC_CONF</span></code>，具体查看torch文档。</p>
</section>
<section id="q123-packing-cache">
<h3>Q123: 请问packing_cache这个参数设置，多机训练，我设置了文件夹地址后还是会报错，这有啥特殊要求吗?<a class="headerlink" href="#q123-packing-cache" title="Link to this heading"></a></h3>
<p>需要设置为共享的磁盘路径。</p>
</section>
<section id="q124-qwen3thinkingthinking">
<h3>Q124: Qwen3非thinking模式和thinking模式，数据集和参数设置有什么不同吗？<a class="headerlink" href="#q124-qwen3thinkingthinking" title="Link to this heading"></a></h3>
<p>查看这个<a class="reference external" href="https://github.com/modelscope/ms-swift/issues/4030">issue</a>。</p>
</section>
<section id="q125-megatron-swift">
<h3>Q125: 请问megatron-swift如何配置断点续训？<a class="headerlink" href="#q125-megatron-swift" title="Link to this heading"></a></h3>
<p>配置<code class="docutils literal notranslate"><span class="pre">--mcore_model</span></code>加载checkpoint，另外根据需要配置这几个参数，<code class="docutils literal notranslate"><span class="pre">--finetune</span></code>，<code class="docutils literal notranslate"><span class="pre">--no_load_optim</span></code>，<code class="docutils literal notranslate"><span class="pre">--no_load_rng</span></code>。如果是lora断点续训，配置<code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span></code>，其他同全参数训练，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Megatron-SWIFT/Command-line-parameters.html">megatron-swift命令行参数文档</a>。</p>
</section>
<section id="q126-kimi-vl-a3b-instruct">
<h3>Q126: 有没有人在复现Kimi-VL-A3B-Instruct的时候出现了如下的报错？<a class="headerlink" href="#q126-kimi-vl-a3b-instruct" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[rank7]:    File &quot;/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruet/modeling_kimi_v1.py&quot;, line 946, in forward
[rank7]:      assert not self.training
[rank7]:
[rank7]:    AssertionError
</pre></div>
</div>
<p>需要将<code class="docutils literal notranslate"><span class="pre">config.json</span></code>中的<code class="docutils literal notranslate"><span class="pre">topk_method</span></code>改成<code class="docutils literal notranslate"><span class="pre">greedy</span></code>。</p>
</section>
<section id="q127-qwenvl2-5-max-pixels-swift">
<h3>Q127: 想问下训练qwenvl2.5的时候，怎么保证设定max_pixels后检测框的坐标是对的？这部分swift是怎么处理的？<a class="headerlink" href="#q127-qwenvl2-5-max-pixels-swift" title="Link to this heading"></a></h3>
<p>会保存预处理前和后的，然后对bbox进行调整，不过推理没有这样的调整，需要提前手动处理图片。</p>
</section>
<section id="q128-datasetsolution-preprocess-datasetmessagesimages-solution">
<h3>Q128: 问一个问题，我想在dataset中返回solution 字段，所以我写了preprocess，但是dataset只返回messages和images，没返回solution这该如何修改呢？<a class="headerlink" href="#q128-datasetsolution-preprocess-datasetmessagesimages-solution" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--remove_unused_columns</span> <span class="pre">false</span></code>。</p>
</section>
<section id="q129-lora-peft0-11-0-peft">
<h3>Q129: 请问下，lora参数合并报错，目前peft是0.11.0，这个是因为peft版本需要升级吗<a class="headerlink" href="#q129-lora-peft0-11-0-peft" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>File &quot;/opt/conda/lib/python3.9/site-packages/peft/config.py&quot;, line 118, in from_peft_type
  return config_cls(**kwargs)
TypeError: __init__() got an unexpected keyword argument &#39;corda_config&#39;
</pre></div>
</div>
<p>训练和合并的peft版本不一致导致的。</p>
</section>
<section id="q130-dpo">
<h3>Q130: 请问现在支持多轮的DPO吗？<a class="headerlink" href="#q130-dpo" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q131-action-preprocessor">
<h3>Q131: 想问下，我之前运行过一次注册数据集了，后面修改了action_preprocessor的代码，但是数据还是和原来的一样，像是忽略了我的修改？<a class="headerlink" href="#q131-action-preprocessor" title="Link to this heading"></a></h3>
<p>设置一下<code class="docutils literal notranslate"><span class="pre">--load_from_cache_file</span> <span class="pre">false</span></code>。</p>
</section>
<section id="q132-sequence-parallel-sizeloss-lossdebug-sequence-parallel-size-sft">
<h3>Q132: 请问sequence_parallel_size和自定义loss冲突吗？我在自定义loss里面打印了一些debug信息，但是发现开启了sequence_parallel_size 过后就没有信息被打印出来了，也没有报错，sft训练是正常的，担心是自动调了什么别的库。<a class="headerlink" href="#q132-sequence-parallel-sizeloss-lossdebug-sequence-parallel-size-sft" title="Link to this heading"></a></h3>
<p>冲突的，sequence parallel在自己的代码中定制了loss，可以自己改下<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/trainers/sequence_parallel/ulysses.py">这里</a>。</p>
</section>
<section id="q133-swift-pt-image-image-token">
<h3>Q133: swift pt这种训练方式，如果传入了<code class="docutils literal notranslate"><span class="pre">&lt;image&gt;</span></code>，<code class="docutils literal notranslate"><span class="pre">&lt;image&gt;</span></code>是不是没有被屏蔽，每个token也参与损失计算？<a class="headerlink" href="#q133-swift-pt-image-image-token" title="Link to this heading"></a></h3>
<p>不算损失的，可以在命令行日志中找一下打印的labels看看。</p>
</section>
<section id="q134-packingpytorch-allocator-cache-flushes-since-last-step-oom">
<h3>Q134: 想问一个问题，多模态packing预训练每次pytorch allocator cache flushes since last step后，显存使用好像就会增长一点，步数多了容易oom<a class="headerlink" href="#q134-packingpytorch-allocator-cache-flushes-since-last-step-oom" title="Link to this heading"></a></h3>
<p>加个环境变量<code class="docutils literal notranslate"><span class="pre">PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True'</span></code>。</p>
</section>
<section id="q135-focal-loss-loss">
<h3>Q135: 如何在训练时使用focal loss？当前支持的loss种类哪里有？<a class="headerlink" href="#q135-focal-loss-loss" title="Link to this heading"></a></h3>
<p>可以在这里添加新的<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss/mapping.py">loss</a>。</p>
</section>
<section id="q136-rolloutpipeline-parallel-size-trlvllmword-size">
<h3>Q136: rollout设置了pipeline parallel size，貌似trl和vllm里获取不到word size这个值。<a class="headerlink" href="#q136-rolloutpipeline-parallel-size-trlvllmword-size" title="Link to this heading"></a></h3>
<p>rollout应该是不兼容pipeline parallel。</p>
</section>
<section id="q137-qwen2audiosftpacking">
<h3>Q137: 请问qwen2audio的sft支持packing吗？<a class="headerlink" href="#q137-qwen2audiosftpacking" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q138-gspotop-entropy-quantile-importance-sampling-level-sequence-x-token">
<h3>Q138: 请问gspo训练支持传入参数top_entropy_quantile吗？传入了--importance_sampling_level sequence后，还能实现对熵分布前x%的token的优化吗？<a class="headerlink" href="#q138-gspotop-entropy-quantile-importance-sampling-level-sequence-x-token" title="Link to this heading"></a></h3>
<p>支持，顺序是先正常计算loss（受importance_sampling_level影响），再根据top_entropy_quantile mask掉loss。</p>
</section>
<section id="q139-gkdstudent-modelteacher-modelmodel-type-densemoe">
<h3>Q139: gkd训练student model和teacher model的model_type需要一致吗，一个dense一个moe可以吗?<a class="headerlink" href="#q139-gkdstudent-modelteacher-modelmodel-type-densemoe" title="Link to this heading"></a></h3>
<p>可以的，只需要词表一样，不过带moe就会比较慢。</p>
</section>
<section id="q140-flash-attentionattention-implemation-none">
<h3>Q140: 请问在不支持flash attention的设备上attention implemation默认是什么呢？文档中默认是none<a class="headerlink" href="#q140-flash-attentionattention-implemation-none" title="Link to this heading"></a></h3>
<p>默认使用sdpa。</p>
</section>
<section id="q141-freeze-parameters-ratio">
<h3>Q141: freeze_parameters_ratio参数是从注意力层开始算的吗？<a class="headerlink" href="#q141-freeze-parameters-ratio" title="Link to this heading"></a></h3>
<p>从embedding开始算，可以结合trainable_parameters设置。</p>
</section>
<section id="q142-lora-adapterdpogrpo">
<h3>Q142: 请问，lora微调后，不支持在adapter上继续做dpo或者grpo训练吗？<a class="headerlink" href="#q142-lora-adapterdpogrpo" title="Link to this heading"></a></h3>
<p>支持了, 命令行参数文档中搜索<code class="docutils literal notranslate"><span class="pre">--adapters</span></code>。</p>
</section>
<section id="q143">
<h3>Q143: 多模态数据集希望在加载数据之后做动态数据增强，例如，给输入数据随机添加噪声。请问要自定义动态数据增强，需要继承或重写哪些部分呢？<a class="headerlink" href="#q143" title="Link to this heading"></a></h3>
<p>在template中修改encode方法。</p>
</section>
<section id="q144-ppo">
<h3>Q144: ppo训练支持梯度裁剪吗？<a class="headerlink" href="#q144-ppo" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q145-liger-kernelpadding-freegrpo">
<h3>Q145: 请问liger kernel和padding free没法在grpo阶段一起开吗？<a class="headerlink" href="#q145-liger-kernelpadding-freegrpo" title="Link to this heading"></a></h3>
<p>是的。如果一起开，需要改liger grpo loss的实现，在liger kernel库中，不方便改。</p>
</section>
<section id="q146-swift-sft-lora-trainable-parameters-loralanguage-model-score-head">
<h3>Q146: 请问，swift sft的命令行参数里面，使用lora训练和--trainable_parameters参数是兼容的吗？就是用lora训练language model，然后同时在最后一层加个score head一起训练。<a class="headerlink" href="#q146-swift-sft-lora-trainable-parameters-loralanguage-model-score-head" title="Link to this heading"></a></h3>
<p>不兼容，用modules_to_save。</p>
</section>
<section id="q147">
<h3>Q147: 多机多卡训练，只有主节点有日志，是正常的吗？<a class="headerlink" href="#q147" title="Link to this heading"></a></h3>
<p>正常的。</p>
</section>
<section id="q148-swiftlearning-rate">
<h3>Q148: swift能够支持设置最小的learning rate吗，感觉最后减到太小了<a class="headerlink" href="#q148-swiftlearning-rate" title="Link to this heading"></a></h3>
<p>可以设置，<code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span> <span class="pre">cosine_with_min_lr</span> <span class="pre">--lr_scheduler_kwargs</span> <span class="pre">'{&quot;min_lr&quot;:</span> <span class="pre">1e-6}'</span></code>。</p>
</section>
<section id="q149-split-dataset-ratio">
<h3>Q149: 设置了split_dataset_ratio，但是一直到训练结束也没有验证集的验证过程，是哪里没配置好吗？<a class="headerlink" href="#q149-split-dataset-ratio" title="Link to this heading"></a></h3>
<p>流式读取streaming不划分验证集，设置一下val_dataset。</p>
</section>
<section id="q150-grpochannel-loss">
<h3>Q150: grpo支持channel_loss吗<a class="headerlink" href="#q150-grpochannel-loss" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q151-grpotask-id-task">
<h3>Q151: 请问grpo有什么办法透传task_id吗？想区别训练集的不同task。<a class="headerlink" href="#q151-grpotask-id-task" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/GRPO/DeveloperGuide/multi_task.html">多任务训练</a>。</p>
</section>
<section id="q152-yamlgrposft">
<h3>Q152: 目前支持用yaml文件配置grpo和sft吗？<a class="headerlink" href="#q152-yamlgrposft" title="Link to this heading"></a></h3>
<p>都支持的，该配置是在main.py中直接处理成命令行。</p>
</section>
<section id="q153-swift">
<h3>Q153: swift支持多节点的分布式训练吗？<a class="headerlink" href="#q153-swift" title="Link to this heading"></a></h3>
<p>参考这里的<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node">例子</a>。</p>
</section>
<section id="q154-finetune-vlm-ms-swift">
<h3>Q154: 几个任务一起finetune vlm，不同任务视频采样规则不一致，ms swift是否支持？在哪里配置？<a class="headerlink" href="#q154-finetune-vlm-ms-swift" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数文档</a>看下<code class="docutils literal notranslate"><span class="pre">interleave_prob</span></code>。</p>
</section>
<section id="q155-gkdtokenizer">
<h3>Q155: 请教gkd现在支持教师和学生模型tokenizer不一样吗？<a class="headerlink" href="#q155-gkdtokenizer" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q156-use-liger-kernellog-entropy">
<h3>Q156: 请问现在是不支持use_liger_kernel和log_entropy一起用吗？<a class="headerlink" href="#q156-use-liger-kernellog-entropy" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q157-grpo">
<h3>Q157: 训练grpo的时候，进行观察的时候，怎么没有熵这个曲线呀<a class="headerlink" href="#q157-grpo" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--log_entropy</span> <span class="pre">true</span></code>，算entropy会有额外的一点开销，所以没有默认记录。</p>
</section>
<section id="q158-swifttensor">
<h3>Q158: swift里把图像读进来转换成tensor的位置在哪里？<a class="headerlink" href="#q158-swifttensor" title="Link to this heading"></a></h3>
<p>Template的_encode方法。</p>
</section>
<section id="q159-questionanswer-columns-question-query-answer-response-answerresponse">
<h3>Q159: 问一下，原始数据集包含question和answer两列，命令行做映射之后--columns {&quot;question&quot;: &quot;query&quot;, &quot;answer&quot;: &quot;response&quot;}，奖励函数无论是用answer还是response都会报错没有列。怎么样才能把数据集的列透传进去呢？<a class="headerlink" href="#q159-questionanswer-columns-question-query-answer-response-answerresponse" title="Link to this heading"></a></h3>
<p>这些是保留列，换个列名。</p>
</section>
<section id="q160-ms-swiftqwen3-30b-a3bmoelora-aux-loss-aux-loss-coef1">
<h3>Q160: 想问下，ms-swift对qwen3-30b-a3b的moe模型lora微调，aux-loss基本没变化，即使设置aux-loss-coef为1也没变化。<a class="headerlink" href="#q160-ms-swiftqwen3-30b-a3bmoelora-aux-loss-aux-loss-coef1" title="Link to this heading"></a></h3>
<p>all-router也加到target_modules。</p>
</section>
<section id="q161-epochcheckpoint">
<h3>Q161: 下面的脚本，可以按epoch保存checkpoint吗？<a class="headerlink" href="#q161-epochcheckpoint" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>megatron<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mcore_model<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$MODEL_PATH</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$DATA_PATH</span><span class="s2">&quot;</span><span class="w">  </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_alpha<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--sequence_parallel<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--micro_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--global_batch_size<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_granularity<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_method<span class="w"> </span>uniform<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_num_layers<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--attention_backend<span class="w"> </span>flash<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--sequence_parallel<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cross_entropy_loss_fusion<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_warmup_fraction<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--min_lr<span class="w"> </span>1e-5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$OUTPUT_PATH</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--split_dataset_ratio<span class="w"> </span><span class="m">0</span>.02<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">25</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--finetune<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_load_rng<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_load_optim<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_optim<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_rng<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_num_proc<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_author<span class="w"> </span>swift<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name<span class="w"> </span>swift-robot
</pre></div>
</div>
<p>还没支持按epoch存储。</p>
</section>
<section id="q162-apex">
<h3>Q162: 请问下，遇到这个报错，怎么处理？安装了apex也不行<a class="headerlink" href="#q162-apex" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: ColumnParallelLinear was called with gradient_accumulation_fusion set to True but the custom CUDA extension fused_weight_gradient_mlp_cuda module is not found. To use gradient_accumulation_fusion you must install APEX with --cpp_ext and --cuda_ext. For example: pip install --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext .&quot; Note that the extension requires CUDA&gt;=11. Otherwise, you must turn off gradient accumulation fusion.
</pre></div>
</div>
<p>设置一下<code class="docutils literal notranslate"><span class="pre">--gradient_accumulation_fusion</span> <span class="pre">false</span></code>。</p>
</section>
<section id="q163-moelora-target-modulesall-linear">
<h3>Q163: moe的lora训练，target_modules参数设置了all-linear，是包括了路由器模块吗？<a class="headerlink" href="#q163-moelora-target-modulesall-linear" title="Link to this heading"></a></h3>
<p>看gate是否是nn.Linear实现，如果是nn.Parameter就不训练，详见命令行参数<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#tuner">target_parameters</a>。</p>
</section>
<section id="q164-grpocolocateuse-async-engine">
<h3>Q164: grpo训练colocate模式不支持use_async_engine吗？<a class="headerlink" href="#q164-grpocolocateuse-async-engine" title="Link to this heading"></a></h3>
<p>不支持。</p>
</section>
<section id="q165-qloramerge">
<h3>Q165: qlora训练后的模型可以merge吗？<a class="headerlink" href="#q165-qloramerge" title="Link to this heading"></a></h3>
<p>参考<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora">qlora例子</a>。</p>
</section>
</section>
<section id="id3">
<h2>推理<a class="headerlink" href="#id3" title="Link to this heading"></a></h2>
<section id="id4">
<h3>Q1:swift推理有文档吗？<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<p>swift支持python脚本、命令行、ui界面推理，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Inference-and-deployment.html">推理和部署</a>。</p>
</section>
<section id="id5">
<h3>Q2: 训练后的模型如何使用数据集推理？<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p>参数<code class="docutils literal notranslate"><span class="pre">--load_data_args</span> <span class="pre">true</span></code>或<code class="docutils literal notranslate"><span class="pre">--val_dataset</span> <span class="pre">&lt;your-val-dataset&gt;</span></code>，见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q3-swift">
<h3>Q3: swift推理的时候可以指定下载好的模型吗？<a class="headerlink" href="#q3-swift" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">--model</span></code>配置本地路径即可，详见<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q4-label">
<h3>Q4: 我想在一个没有label的数据集上推理，怎么做呢？我看文档里面的数据集格式都是训练集<a class="headerlink" href="#q4-label" title="Link to this heading"></a></h3>
<p>配置参数<code class="docutils literal notranslate"><span class="pre">--val_dataset</span> <span class="pre">&lt;your-val-dataset&gt;</span></code>。</p>
</section>
<section id="q5-valueerror-input-length-of-input-ids-is-35-but-max-length-is-set-to-20">
<h3>Q5: 遇到报错ValueError: Input length of input_ids is 35, but <code class="docutils literal notranslate"><span class="pre">max_length</span></code> is set to 20.如何解决？<a class="headerlink" href="#q5-valueerror-input-length-of-input-ids-is-35-but-max-length-is-set-to-20" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>raise ValueError(
ValueError: Input length of input_ids is 35, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
</pre></div>
</div>
<p>设置<code class="docutils literal notranslate"><span class="pre">model.generation_config.max_new_tokens</span></code>。</p>
</section>
<section id="q6-qwen2-vl">
<h3>Q6: qwen2-vl推理（训练）爆显存<a class="headerlink" href="#q6-qwen2-vl" title="Link to this heading"></a></h3>
<p>设置命令行参数<code class="docutils literal notranslate"><span class="pre">--max_pixels</span> <span class="pre">xxx</span></code>、环境变量<code class="docutils literal notranslate"><span class="pre">MAX_PIXELS=xxx</span></code>、或特定模型参数<code class="docutils literal notranslate"><span class="pre">--model_kwargs</span> <span class="pre">'{&quot;max_pixels&quot;:</span> <span class="pre">xxx}'</span></code>，其中环境变量仅对文档中对应的模型生效，详见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id18">特定模型参数</a>。</p>
</section>
<section id="q7-v100-python-https-swift2x-readthedocs-io-zh-cn-latest-multi-modal-qwen2-vl-e6-9c-80-e4-bd-b3-e5-ae-9e-e8-b7-b5-html-cuda-visible-devices-0-1-2-3-swift-infer-model-type-qwen2-vl-7b-instruct-runtimeerror-probability-tensor-contains-either-inf-nan-or-element-0">
<h3>Q7: v100显卡，在python虚拟环境中，参考https://swift2x.readthedocs.io/zh-cn/latest/Multi-Modal/qwen2-vl%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html 完成环境准备，在测试推理命令：CUDA_VISIBLE_DEVICES=0,1,2,3 swift infer --model_type qwen2-vl-7b-instruct 时报错：RuntimeError: probability tensor contains either <code class="docutils literal notranslate"><span class="pre">inf</span></code>, <code class="docutils literal notranslate"><span class="pre">nan</span></code> or element &lt; 0<a class="headerlink" href="#q7-v100-python-https-swift2x-readthedocs-io-zh-cn-latest-multi-modal-qwen2-vl-e6-9c-80-e4-bd-b3-e5-ae-9e-e8-b7-b5-html-cuda-visible-devices-0-1-2-3-swift-infer-model-type-qwen2-vl-7b-instruct-runtimeerror-probability-tensor-contains-either-inf-nan-or-element-0" title="Link to this heading"></a></h3>
<p>尝试用A10或者3090机器推理。</p>
</section>
<section id="q8-cuda-visible-devices-0-swift-infer-ckpt-dir-output-glm4v-9b-chat-vx-xxx-checkpoint-xxx-merged-load-data-args-true">
<h3>Q8: 运行下面命令，预测之后的结果在哪里？CUDA_VISIBLE_DEVICES=0 swift infer --ckpt_dir output/glm4v-9b-chat/vx-xxx/checkpoint-xxx-merged --load_data_args true<a class="headerlink" href="#q8-cuda-visible-devices-0-swift-infer-ckpt-dir-output-glm4v-9b-chat-vx-xxx-checkpoint-xxx-merged-load-data-args-true" title="Link to this heading"></a></h3>
<p>日志中会打印路径。</p>
</section>
<section id="q9-swift-inferlogprobs">
<h3>Q9: 现在最新的swift版本，infer命令能通过logprobs参数输出概率值吗？<a class="headerlink" href="#q9-swift-inferlogprobs" title="Link to this heading"></a></h3>
<p>可以输出logprobs，命令行推理设置<code class="docutils literal notranslate"><span class="pre">--logprobs</span> <span class="pre">true</span></code>，python脚本推理设置<code class="docutils literal notranslate"><span class="pre">request_config</span> <span class="pre">=</span> <span class="pre">RequestConfig(...,</span> <span class="pre">logprobs=True,</span> <span class="pre">top_logprobs=2)</span></code>，参考<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/tests/infer/test_logprobs.py">test_logprobs.py</a>。</p>
</section>
<section id="q10-vllm-assert-factor-in-rope-scaling">
<h3>Q10: vllm会报错，assert factor in rope_scaling<a class="headerlink" href="#q10-vllm-assert-factor-in-rope-scaling" title="Link to this heading"></a></h3>
<p>详见qwen2-vl <a class="reference external" href="https://github.com/QwenLM/Qwen2.5-VL/issues/96">issue#96</a>。</p>
</section>
<section id="q11-vllm">
<h3>Q11: vllm作为推理后端的话，模型必须合并以后才能调用吗？<a class="headerlink" href="#q11-vllm" title="Link to this heading"></a></h3>
<p>可以不合并，详见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">命令行参数</a>。</p>
</section>
<section id="q12-python-cpu">
<h3>Q12: 请问在使用python脚本推理时，如何使用cpu?<a class="headerlink" href="#q12-python-cpu" title="Link to this heading"></a></h3>
<p>设置环境变量，<code class="docutils literal notranslate"><span class="pre">os.environ['CUDA_VISIBLE_DEVICES']</span> <span class="pre">=</span> <span class="pre">'-1'</span></code>。</p>
</section>
<section id="q13-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16">
<h3>Q13: 有人遇到过这个问题吗?RuntimeError: &quot;triu_tril_cuda_template&quot; not implemented for'BFloat16'<a class="headerlink" href="#q13-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16" title="Link to this heading"></a></h3>
<p>升级torch,这个版本的torch没实现这个算子。</p>
</section>
<section id="q14-qwen2-audio">
<h3>Q14: qwen2-audio支持流式推理吗？<a class="headerlink" href="#q14-qwen2-audio" title="Link to this heading"></a></h3>
<p>支持，详见<a class="reference external" href="https://github.com/modelscope/ms-swift/issues/1653">issue</a></p>
</section>
<section id="q15-inference-client-do-sample">
<h3>Q15: inference client推理多模态，do_sample在哪里设置？<a class="headerlink" href="#q15-inference-client-do-sample" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">temperature=0</span></code>。</p>
</section>
<section id="q16-ms-swift">
<h3>Q16: ms-swift支持大模型批处理不？<a class="headerlink" href="#q16-ms-swift" title="Link to this heading"></a></h3>
<p>支持的。详见<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo.py">demo</a>。</p>
</section>
<section id="q17-ms-swift">
<h3>Q17: ms-swift量化模型的时候，显示内存不足，可以在量化的时候少占用一些资源吗，慢一点没关系。<a class="headerlink" href="#q17-ms-swift" title="Link to this heading"></a></h3>
<p>尝试设置<code class="docutils literal notranslate"><span class="pre">--device_map</span> <span class="pre">cpu</span></code>。</p>
</section>
<section id="q18-swift">
<h3>Q18: swift支持对多模态模型量化吗？<a class="headerlink" href="#q18-swift" title="Link to this heading"></a></h3>
<p>支持。</p>
</section>
<section id="q19-gptq">
<h3>Q19: 使用GPTQ报错如下，请问是啥原因？<a class="headerlink" href="#q19-gptq" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>if llm_config[&#39;architectures&#39;][0] == &#39;LlamaForCausalLM&#39;:
KeyError: &#39;architectures&#39;
</pre></div>
</div>
<p>尝试transformers==4.44.*版本。</p>
</section>
<section id="q20-swift-infer">
<h3>Q20: swift infer如何将评估的结果保存到指定文件呢 每次都不知道保存到哪里了<a class="headerlink" href="#q20-swift-infer" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--result_path</span> <span class="pre">your_path</span></code>，详见<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/arguments/infer_args.py">InferArguments</a>。</p>
</section>
<section id="q21-awqyi-vl-6b">
<h3>Q21: AWQ量化yi-vl-6b出错如下：<a class="headerlink" href="#q21-awqyi-vl-6b" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: swift.llm.utils.model.get_model_tokenizer_with_flash_attn() got multiple values for keyword argument &#39;automodel_class&#39;.
</pre></div>
</div>
<p>请使用gptq量化。</p>
</section>
<section id="q22-swift-exportqwen2-5-72bgptq-int4-max-model-length-32768-128-factorization-could-not-be-completed-because-the-input-is-not-positive-definite-the-leading-minor-of-order-18145-is-not-pisitive-definite">
<h3>Q22: 想问一下用swift export对qwen2.5 72B模型进行gptq int4量化，max model length=32768用的是默认值，给的校准数据集有128个样本，但是量化的时候报错了，报错日志是：factorization could not be completed because the input is not positive-definite(the leading minor of order 18145 is not pisitive-definite)。是什么原因？<a class="headerlink" href="#q22-swift-exportqwen2-5-72bgptq-int4-max-model-length-32768-128-factorization-could-not-be-completed-because-the-input-is-not-positive-definite-the-leading-minor-of-order-18145-is-not-pisitive-definite" title="Link to this heading"></a></h3>
<p>海森矩阵不正定的问题，试试其他的数据集。</p>
</section>
<section id="q23-sft">
<h3>Q23: 请问批量推理是只能自己编写代码运行吗？不可以按照 sft 那样填脚本参数码<a class="headerlink" href="#q23-sft" title="Link to this heading"></a></h3>
<p>可以，<code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span> <span class="pre">--val_dataset</span> <span class="pre">xxx</span> <span class="pre">--max_batch_size</span> <span class="pre">16</span> <span class="pre">...</span> </code></p>
</section>
<section id="q24-swift-app-temperature">
<h3>Q24: 问一下，swift app推理时，temperature默认是多少的？<a class="headerlink" href="#q24-swift-app-temperature" title="Link to this heading"></a></h3>
<p>默认从generation_config.json中读取。</p>
</section>
<section id="q25">
<h3>Q25: 请问，导出和量化的时候可以多卡吗？<a class="headerlink" href="#q25" title="Link to this heading"></a></h3>
<p>加载模型可以多卡，量化是单卡。</p>
</section>
<section id="q26-swift-exporttemplate-type-template-type-swift-export-template-type-template">
<h3>Q26: swift export的时候传入自定义的template_type,是不是就可以永久改掉template_type了？如果swift export --template_type 自定义,是不是就可以把模型对应的template改掉<a class="headerlink" href="#q26-swift-exporttemplate-type-template-type-swift-export-template-type-template" title="Link to this heading"></a></h3>
<p>不会被修改,swift中的template是定义在swift内部的,不是以jinja方式保存的。</p>
</section>
<section id="q27-awqqwen2vl-typeerror-qwen2vlforconditionalgeneration-init-got-an-unexpected-keyword-argument-use-cache">
<h3>Q27: awq量化Qwen2VL报错：TypeError: Qwen2VLForConditionalGeneration.<strong>init</strong>() got an unexpected keyword argument 'use_cache'<a class="headerlink" href="#q27-awqqwen2vl-typeerror-qwen2vlforconditionalgeneration-init-got-an-unexpected-keyword-argument-use-cache" title="Link to this heading"></a></h3>
<p>用<code class="docutils literal notranslate"><span class="pre">gptq</span></code>量化。</p>
</section>
<section id="q28-ddp-infermax-batch-size-batch-sizebatch-size">
<h3>Q28: ddp 推理，infer里面的这个max_batch_size，是指每张卡的batch_size还是总的batch_size<a class="headerlink" href="#q28-ddp-infermax-batch-size-batch-sizebatch-size" title="Link to this heading"></a></h3>
<p>每张卡。</p>
</section>
<section id="q29-swift-inferencemessages-query-responseanswerprompt-answer-inference">
<h3>Q29: 请问swift.inference现在支持messages格式的输入吗？现在看到好像只能用query格式，得到response。数据answer里面已经包含了部分prompt，希望补全answer，应该怎么修改inference<a class="headerlink" href="#q29-swift-inferencemessages-query-responseanswerprompt-answer-inference" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&lt;system&gt;&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;query1&gt;&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;answer1, &quot;}]}
</pre></div>
</div>
<p>用swift3是可以的，参考<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_agent.py">examples/infer/demo_agent</a>。</p>
</section>
<section id="q30-swift-infer-result-path">
<h3>Q30: 请问swift infer的时候，如何让结果实时写入result_path，而不是最后一次性写入呢？<a class="headerlink" href="#q30-swift-infer-result-path" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
--ckpt_dir<span class="w"> </span>model_dir<span class="w"> </span><span class="se">\</span>
--streaming<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
--val_dataset<span class="w"> </span>dataset.jsonl<span class="w"> </span><span class="se">\</span>
--result_path<span class="w"> </span>result.jsonl
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--stream</span> <span class="pre">true</span></code>，这样可以一条条写，不过是非batch推理的。</p>
</section>
<section id="q31-swift-merge-loraollamaapi">
<h3>Q31: 我在swift训练推理的时候是有效果的，但是用merge_lora后再通过ollama的api开接口的时候效果就没了<a class="headerlink" href="#q31-swift-merge-loraollamaapi" title="Link to this heading"></a></h3>
<p>试试transformers加载，swift的template是对齐transformers的。</p>
</section>
<section id="q32">
<h3>Q32: 模型推理的时候如果需要在特定前缀下继续推理的话是设置哪个参数？<a class="headerlink" href="#q32" title="Link to this heading"></a></h3>
<p>参数<code class="docutils literal notranslate"><span class="pre">--response_prefix</span></code>。</p>
</section>
<section id="q33">
<h3>Q33: 一直报这个错怎么改呀？<a class="headerlink" href="#q33" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>File &quot;/mnt/workspace/swift/swift/1lm/dataset/preprocessor/core. py&quot;, line 69, in _check_messages raise
ValueError(f&#39;assistant_message; {assistant_message}&#39;)
ValueError: assistant_message: {&#39;role&#39; :&#39;assistant&#39;, &#39;content&#39;: &#39;&#39;}
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span>swift<span class="w"> </span>sft<span class="w"> </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-7B-Instruct<span class="w"> </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span>--dataset<span class="w"> </span>/mnt/workspace/data.json<span class="w"> </span>--deepspeed<span class="w"> </span>zero2<span class="w"> </span>--max_length<span class="w"> </span><span class="m">16384</span>
</pre></div>
</div>
<p>数据集assistant字段为空，如果是推理，把这个空字符串删掉，因为这个会导致训练时nan，会做检查。</p>
</section>
<section id="q34-importerror-cannot-import-name-shard-checkpoint-from-transformers-modeling-utils-usr-local-lib-python3-10-dist-packages-transformers-modeling-utilspy">
<h3>Q34: 推理报错，ImportError: cannot import name 'shard_checkpoint' from 'transformers.modeling_utils' (/usr/local/lib/python3.10/dist-packages/transformers/modeling_utilspy）<a class="headerlink" href="#q34-importerror-cannot-import-name-shard-checkpoint-from-transformers-modeling-utils-usr-local-lib-python3-10-dist-packages-transformers-modeling-utilspy" title="Link to this heading"></a></h3>
<p>尝试卸载autoawq。</p>
</section>
<section id="q35-swift-sample-batch-forsample">
<h3>Q35: swift sample的时候，好像不支持batch？好像是for循环一个个例子sample，有点慢<a class="headerlink" href="#q35-swift-sample-batch-forsample" title="Link to this heading"></a></h3>
<p>有一个<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/rft/rft.py">脚本</a>，可以用多进程对数据集拆分采样。</p>
</section>
<section id="q36-swiftembedding">
<h3>Q36: 请问swift支持embedding模型的推理吗？<a class="headerlink" href="#q36-swiftembedding" title="Link to this heading"></a></h3>
<p>参考这里的<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_embedding.py">例子</a>。</p>
</section>
<section id="q37-swift-oom-oom">
<h3>Q37: swift框架推理支持模型或者张量并行么？训练不会oom，推理时候报oom了<a class="headerlink" href="#q37-swift-oom-oom" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>/path/to/checkpoint-xxx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--merge_lora<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>vllm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_data_args<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_gpu_memory_utilization<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_max_model_len<span class="w"> </span><span class="m">32768</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">15536</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_limit_mm_per_prompt<span class="w"> </span><span class="s1">&#39;{&quot;image&quot;: 8, &quot;video&quot;: 2}&#39;</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:368 &#39;invalid argument&#39;
</pre></div>
</div>
<p>加一下<code class="docutils literal notranslate"><span class="pre">--disable_custom_all_reduce</span> <span class="pre">true</span></code>。</p>
</section>
<section id="q38-ddp">
<h3>Q38: 请问流式推理支持ddp吗？<a class="headerlink" href="#q38-ddp" title="Link to this heading"></a></h3>
<p>流式不支持ddp。</p>
</section>
<section id="q39-ovis2-2b">
<h3>Q39: 在Ovis2-2B推理的过程中出现的问题<a class="headerlink" href="#q39-ovis2-2b" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[rank1]: safetensors_rust.SafetensorError: Error while deserializing header:MetadataIncompleteBuffer
Downloading Model from https://www.modelscope.cn to directory:/mnt/workspace/.cache/modelscope/hub/models/AIDC-AI/Ovis2-2B
</pre></div>
</div>
<p>模型权重损坏了。</p>
</section>
<section id="q40-grpo-temprature-0-logits-self-temperature-logitsnan-temperature1-topk1-topp1">
<h3>Q40: 问一个问题，请问在grpo训练中如果做贪心搜索那么如何设置呢？文档中有说推理时可通过temprature=0设置为贪心搜索但是训练如果这样设置，计算logits / self.temperature，logits会出现很多nan，是不是将temperature设置为1，topk设置为1，topp也设置为1，模型推理就是贪心搜索的结果呢？<a class="headerlink" href="#q40-grpo-temprature-0-logits-self-temperature-logitsnan-temperature1-topk1-topp1" title="Link to this heading"></a></h3>
<p>topk设置成1就可以了。</p>
</section>
<section id="q41-swift-gptq-awq-fp8activationweight">
<h3>Q41: 请问下，swift做量化时，gptq/awq/fp8三种方法分别是针对activation和weight中的哪个做的量化呢？<a class="headerlink" href="#q41-swift-gptq-awq-fp8activationweight" title="Link to this heading"></a></h3>
<p>只有权重。</p>
</section>
<section id="q42-ms-swifttransformers-enginevllm-engine">
<h3>Q42: 请问用ms-swift推理时transformers engine和vllm engine，推理结果差了很多，这个是什么原因呢？<a class="headerlink" href="#q42-ms-swifttransformers-enginevllm-engine" title="Link to this heading"></a></h3>
<p>看看参数有没有对齐。此外，VllmEngine和TransformersEngine是有差异的，TransformersEngine和transformers推理是对齐的。</p>
</section>
<section id="q43-swiftqwen2audio">
<h3>Q43: 请问用swift做qwen2audio的推理，推理结果出现混乱，可能是啥原因呢？<a class="headerlink" href="#q43-swiftqwen2audio" title="Link to this heading"></a></h3>
<p>使用transformers4.48。</p>
</section>
<section id="q44-max-batch-sizebatch-size">
<h3>Q44: 多卡推理，--max_batch_size是所有卡加起来的batch_size吗？<a class="headerlink" href="#q44-max-batch-sizebatch-size" title="Link to this heading"></a></h3>
<p>不是，是每张卡的。</p>
</section>
<section id="q45-image">
<h3>Q45: 推理只能传图片路径吗？有没有办法传Image对象？<a class="headerlink" href="#q45-image" title="Link to this heading"></a></h3>
<p>可以传<code class="docutils literal notranslate"><span class="pre">PIL.Image</span></code>。</p>
</section>
<section id="q46-swiftmodelscope">
<h3>Q46: swift训练的模型不能通过modelscope加载，怎么解决的？<a class="headerlink" href="#q46-swiftmodelscope" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://github.com/modelscope/ms-swift/issues/5440">issue#5440</a>，transformers4.55.2训练的LoRA不能使用小于4.52的版本加载了。</p>
</section>
<section id="q47-last-hidden-state">
<h3>Q47: 有没有示例脚本支持大模型推理的时候输出last_hidden_state？<a class="headerlink" href="#q47-last-hidden-state" title="Link to this heading"></a></h3>
<p>没有，可以参考grpo trainer的<code class="docutils literal notranslate"><span class="pre">_get_last_hidden_state</span></code>方法。</p>
</section>
<section id="q48-init-weights-lora-gaadaper">
<h3>Q48: 请问，用--init_weights lora-ga微调完的adaper，推理的时候会报这种错误，如何解决？<a class="headerlink" href="#q48-init-weights-lora-gaadaper" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>raise ValueError(f&quot;Unknown initialization {init_lora_weights=}&quot;) ValueError: Unknown initialization init_lora_weights=&#39;lora-ga&#39;
</pre></div>
</div>
<p>在你的checkpoint文件夹里面有一个叫converted的文件夹，用那个。</p>
</section>
<section id="q49-swift-infer">
<h3>Q49: 使用swift infer命令进行推理，支持多机推理吗？<a class="headerlink" href="#q49-swift-infer" title="Link to this heading"></a></h3>
<p>如果单节点放得下模型，外面封装k8s就行。如果单节点放不下那就不支持。</p>
</section>
<section id="q50-acc-rouge">
<h3>Q50: 推理时如何计算acc/rouge等指标？<a class="headerlink" href="#q50-acc-rouge" title="Link to this heading"></a></h3>
<p>参考<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html#id14">推理参数metric</a>。</p>
</section>
<section id="q51-system-prompt-system-system">
<h3>Q51: 如何将system_prompt置空？我删除了--system参数，但是它会给我加上默认的system。<a class="headerlink" href="#q51-system-prompt-system-system" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--system</span> <span class="pre">''</span></code>。</p>
</section>
<section id="q52-extra">
<h3>Q52: 推理数据里面有一个extra字段，推理结果没法保存这个字段，应该如何设置才能保存额外的字段呢？<a class="headerlink" href="#q52-extra" title="Link to this heading"></a></h3>
<p>设置<code class="docutils literal notranslate"><span class="pre">--remove_unused_columns</span> <span class="pre">false</span></code>。</p>
</section>
</section>
<section id="id6">
<h2>部署<a class="headerlink" href="#id6" title="Link to this heading"></a></h2>
<section id="q1">
<h3>Q1: 如何部署训练后的模型？<a class="headerlink" href="#q1" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">deploy</span> <span class="pre">--adapters</span> <span class="pre">xxx</span></code>，见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Inference-and-deployment.html">推理和部署</a>。</p>
</section>
<section id="q2-vllm">
<h3>Q2: 如何使用vllm部署进行多卡部署？<a class="headerlink" href="#q2-vllm" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy">例子</a>。</p>
</section>
<section id="q3-vllm">
<h3>Q3: 请问用vllm部署的时候，客户端怎么传入图片？<a class="headerlink" href="#q3-vllm" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client/mllm">客户端例子</a>。</p>
</section>
<section id="q4-qwen2-7b-openaiapiclient-completions-create-client-chat-completions-create-qwen2-7b-instruct-q5-k-m-ggufclient-chat-completions-create">
<h3>Q4: 有个问题想问一下，qwen2-7b部署后使用客户端时，调用openai的api要使用client.completions.create，不能使用client.chat.completions.create，但是使用qwen2-7b-instruct-q5_k_m.gguf的时候可以使用client.chat.completions.create，这是为什么呀？<a class="headerlink" href="#q4-qwen2-7b-openaiapiclient-completions-create-client-chat-completions-create-qwen2-7b-instruct-q5-k-m-ggufclient-chat-completions-create" title="Link to this heading"></a></h3>
<p>base模型可以用client.chat.completions.create的，不过这个是兼容行为。</p>
</section>
<section id="q5-swift-deploy-ctrl-c-python">
<h3>Q5: 使用两张卡用swift deploy启动服务端后，用Ctrl+C退出后，会一直有一个python进程，一直占用一张卡的显存，这是正常现象吗？<a class="headerlink" href="#q5-swift-deploy-ctrl-c-python" title="Link to this heading"></a></h3>
<p>需要kill 一下, 这是vllm的问题。</p>
</section>
<section id="q6-lmdeployvllm">
<h3>Q6: 在哪查看模型是否支持lmdeploy或vllm加速？<a class="headerlink" href="#q6-lmdeployvllm" title="Link to this heading"></a></h3>
<p>vllm和lmdeploy分别有自己的模型支持范围，请查看各自官方文档来确定是否可用。</p>
</section>
<section id="q7-2-5-7b-instruct-vllm-fp16">
<h3>Q7: 通义千问2.5-数学-7B-Instruct，会偶尔这样一直返回乱码，是什么问题呢？用vllm部署，fp16。<a class="headerlink" href="#q7-2-5-7b-instruct-vllm-fp16" title="Link to this heading"></a></h3>
<p>尝试bf16。</p>
</section>
<section id="q8-swift">
<h3>Q8: swift推理服务启动后，交互进行设置的温度之类的配置，如何设置呢？<a class="headerlink" href="#q8-swift" title="Link to this heading"></a></h3>
<p>推理只能启动前设置。部署可以在启动时设置默认，之后在客户端继续设置，覆盖默认。</p>
</section>
<section id="q9-qwen2vl-vllm-base64-curl">
<h3>Q9: 在本地部署qwen2vl模型，推理后端使用vllm，本地视频怎么传入呢？可以使用 base64 传进去吗？curl调用如何加载视频呢？<a class="headerlink" href="#q9-qwen2vl-vllm-base64-curl" title="Link to this heading"></a></h3>
<p>base64，详见<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client/mllm">mllm客户端例子</a></p>
</section>
<section id="q10-qwen2-vl-vllm">
<h3>Q10: qwen2-vl部署时报错如下，是vllm的版本不对么？<a class="headerlink" href="#q10-qwen2-vl-vllm" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Unrecognized keys in `rope_scaling`for &#39;rope_type&#39;=&#39;default&#39;: {&#39;mrope_section&#39;} Unrecognized keys in `rope_scaling`for &#39;rope_type&#39;=&#39;default&#39;: {&#39;mrope_section&#39;}
</pre></div>
</div>
<p>详见<a class="reference external" href="https://github.com/QwenLM/Qwen2.5-VL/issues/209">issue</a>。</p>
</section>
<section id="q11-swift-deploy-token-logprobs-true-null">
<h3>Q11: 我用swift deploy做推理的时候，想让他输出token的概率，我加了logprobs True，但是它输出null，这个是什么原因呢？<a class="headerlink" href="#q11-swift-deploy-token-logprobs-true-null" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">RAY_memory_monitor_refresh_ms</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>nohup<span class="w"> </span>swift<span class="w"> </span>deploy<span class="w"> </span>--ckpt_dir<span class="w"> </span>/mnt/workspace/checkpoint_600<span class="w"> </span>--infer_backend<span class="w"> </span>vllm<span class="w"> </span>--logprobs<span class="w"> </span>True<span class="w"> </span>--load_data_args<span class="w"> </span><span class="nb">false</span><span class="w"> </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>需要客户端传参数，<code class="docutils literal notranslate"><span class="pre">request_config</span> <span class="pre">=</span> <span class="pre">RequestConfig(...,</span> <span class="pre">logprobs=True,</span> <span class="pre">top_logprobs=2)</span></code>。</p>
</section>
<section id="q12-swift3-0-url">
<h3>Q12: swift3.0 部署推理，可以设置请求的超时时间么？如果图片url非法，会等在那里<a class="headerlink" href="#q12-swift3-0-url" title="Link to this heading"></a></h3>
<p>设置环境变量<code class="docutils literal notranslate"><span class="pre">SWIFT_TIMEOUT</span></code>。或者<code class="docutils literal notranslate"><span class="pre">InferClient</span></code>中可以传参数。</p>
</section>
<section id="q13-swift-streamtrue-streamtrue">
<h3>Q13: swift部署的模型怎么没法流式生成啊？服务端的stream设为True了，客户端的stream也设为True了，但它就是没法流式生成<a class="headerlink" href="#q13-swift-streamtrue-streamtrue" title="Link to this heading"></a></h3>
<p>客户端控制的，查看<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client">examples/deploy/client</a>。</p>
</section>
<section id="q14-swift-pil-image">
<h3>Q14: swift部署好多模态模型之后，客户端传PIL.Image，有示例没?<a class="headerlink" href="#q14-swift-pil-image" title="Link to this heading"></a></h3>
<p>看这个<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/deploy/client/mllm/openai_client.py">client例子</a>。</p>
</section>
<section id="q15-deploy">
<h3>Q15: 请问 deploy部署时候，设置什么参数可以实现一次输出，输出多个结果呢？<a class="headerlink" href="#q15-deploy" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">RequestConfig</span></code>参数<code class="docutils literal notranslate"><span class="pre">n</span></code>。</p>
</section>
<section id="q16-swift-deploy-infer-backend-vllm-vllm-vllm-serve-10">
<h3>Q16: 比使用 swift deploy 部署，指定参数为 --infer_backend vllm，直接使用 vllm 部署：vllm serve ，效果差了接近10个点，有人知道什么原因不？<a class="headerlink" href="#q16-swift-deploy-infer-backend-vllm-vllm-vllm-serve-10" title="Link to this heading"></a></h3>
<p>估计是template没对上。</p>
</section>
<section id="q17-qwem3">
<h3>Q17: 部署命令怎么关闭qwem3的深度思考模式？<a class="headerlink" href="#q17-qwem3" title="Link to this heading"></a></h3>
<p>查看这个<a class="reference external" href="https://github.com/modelscope/ms-swift/issues/4030">issue</a>。</p>
</section>
<section id="q18-ms-swiftvllm-vllm-swift">
<h3>Q18: 请问，我用ms-swift的vllm部署推理，比原生vllm要慢很多，这个是swift框架的问题嘛？<a class="headerlink" href="#q18-ms-swiftvllm-vllm-swift" title="Link to this heading"></a></h3>
<p>main分支应该默认使用V1 engine了，加一个<code class="docutils literal notranslate"><span class="pre">VLLM_USE_V1=1</span></code>试试，还有是图像分辨率，要对齐一下。</p>
</section>
<section id="q19-swiftvllm">
<h3>Q19: 如果swift部署用vllm加速，能分别指定不同卡上使用显存的比例吗？<a class="headerlink" href="#q19-swiftvllm" title="Link to this heading"></a></h3>
<p>不支持异构。</p>
</section>
<section id="q20-vllm-model-language-model-embed-tokens-weight">
<h3>Q20: 模型保存后，好像无法被vllm读取，会报错没有“model.language_model.embed_tokens.weight”，有解法吗？<a class="headerlink" href="#q20-vllm-model-language-model-embed-tokens-weight" title="Link to this heading"></a></h3>
<p>训练前后的transformers版本需要一致。</p>
</section>
<section id="q21-systemsystem-promptsystem-prompttemplatesystem-prompt">
<h3>Q21: 通过--system参数指定system prompt与数据集中每个数据前加system prompt以及template的system prompt是不是有一个就行？这些方式对模型来说，是不是一样的？<a class="headerlink" href="#q21-systemsystem-promptsystem-prompttemplatesystem-prompt" title="Link to this heading"></a></h3>
<p>system优先级：数据集中的&gt;命令行的&gt;template中默认的。</p>
</section>
<section id="q22-swift-transformers-engine">
<h3>Q22: swift transformers engine部署模型后，推理无法并行，数据也没办法分配到其他显卡上，用的全是第一张卡。<a class="headerlink" href="#q22-swift-transformers-engine" title="Link to this heading"></a></h3>
<p>尝试swift infer，deploy不支持DDP。</p>
</section>
<section id="q23-swift-deploy-thinking-extra-body">
<h3>Q23: swift deploy部署的模型，怎么在客户端禁止thinking？我在请求的时候加了extra body也不行。<a class="headerlink" href="#q23-swift-deploy-thinking-extra-body" title="Link to this heading"></a></h3>
<p>现在只能在swift deploy启动的时候禁止thinking。</p>
</section>
</section>
<section id="id7">
<h2>评测<a class="headerlink" href="#id7" title="Link to this heading"></a></h2>
<section id="id8">
<h3>Q1: swift支持的评测集有哪些？<a class="headerlink" href="#id8" title="Link to this heading"></a></h3>
<p>纯文本评测：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;obqa&#39;, &#39;cmb&#39;, &#39;AX_b&#39;, &#39;siqa&#39;, &#39;nq&#39;, &#39;mbpp&#39;, &#39;winogrande&#39;, &#39;mmlu&#39;, &#39;BoolQ&#39;, &#39;cluewsc&#39;, &#39;ocnli&#39;, &#39;lambada&#39;,
&#39;CMRC&#39;, &#39;ceval&#39;, &#39;csl&#39;, &#39;cmnli&#39;, &#39;bbh&#39;, &#39;ReCoRD&#39;, &#39;math&#39;, &#39;humaneval&#39;, &#39;eprstmt&#39;, &#39;WSC&#39;, &#39;storycloze&#39;,
&#39;MultiRC&#39;, &#39;RTE&#39;, &#39;chid&#39;, &#39;gsm8k&#39;, &#39;AX_g&#39;, &#39;bustm&#39;, &#39;afqmc&#39;, &#39;piqa&#39;, &#39;lcsts&#39;, &#39;strategyqa&#39;, &#39;Xsum&#39;, &#39;agieval&#39;,
&#39;ocnli_fc&#39;, &#39;C3&#39;, &#39;tnews&#39;, &#39;race&#39;, &#39;triviaqa&#39;, &#39;CB&#39;, &#39;WiC&#39;, &#39;hellaswag&#39;, &#39;summedits&#39;, &#39;GaokaoBench&#39;,
&#39;ARC_e&#39;, &#39;COPA&#39;, &#39;ARC_c&#39;, &#39;DRCD&#39;
</pre></div>
</div>
<p>多模态评测：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;COCO_VAL&#39;, &#39;MME&#39;, &#39;HallusionBench&#39;, &#39;POPE&#39;, &#39;MMBench_DEV_EN&#39;, &#39;MMBench_TEST_EN&#39;, &#39;MMBench_DEV_CN&#39;, &#39;MMBench_TEST_CN&#39;,
&#39;MMBench&#39;, &#39;MMBench_CN&#39;, &#39;MMBench_DEV_EN_V11&#39;, &#39;MMBench_TEST_EN_V11&#39;, &#39;MMBench_DEV_CN_V11&#39;,
&#39;MMBench_TEST_CN_V11&#39;, &#39;MMBench_V11&#39;, &#39;MMBench_CN_V11&#39;, &#39;SEEDBench_IMG&#39;, &#39;SEEDBench2&#39;,
&#39;SEEDBench2_Plus&#39;, &#39;ScienceQA_VAL&#39;, &#39;ScienceQA_TEST&#39;, &#39;MMT-Bench_ALL_MI&#39;, &#39;MMT-Bench_ALL&#39;,
&#39;MMT-Bench_VAL_MI&#39;, &#39;MMT-Bench_VAL&#39;, &#39;AesBench_VAL&#39;, &#39;AesBench_TEST&#39;, &#39;CCBench&#39;, &#39;AI2D_TEST&#39;, &#39;MMStar&#39;,
&#39;RealWorldQA&#39;, &#39;MLLMGuard_DS&#39;, &#39;BLINK&#39;, &#39;OCRVQA_TEST&#39;, &#39;OCRVQA_TESTCORE&#39;, &#39;TextVQA_VAL&#39;, &#39;DocVQA_VAL&#39;,
&#39;DocVQA_TEST&#39;, &#39;InfoVQA_VAL&#39;, &#39;InfoVQA_TEST&#39;, &#39;ChartQA_TEST&#39;, &#39;MathVision&#39;, &#39;MathVision_MINI&#39;,
&#39;MMMU_DEV_VAL&#39;, &#39;MMMU_TEST&#39;, &#39;OCRBench&#39;, &#39;MathVista_MINI&#39;, &#39;LLaVABench&#39;, &#39;MMVet&#39;, &#39;MTVQA_TEST&#39;,
&#39;MMLongBench_DOC&#39;, &#39;VCR_EN_EASY_500&#39;, &#39;VCR_EN_EASY_100&#39;, &#39;VCR_EN_EASY_ALL&#39;, &#39;VCR_EN_HARD_500&#39;,
&#39;VCR_EN_HARD_100&#39;, &#39;VCR_EN_HARD_ALL&#39;, &#39;VCR_ZH_EASY_500&#39;, &#39;VCR_ZH_EASY_100&#39;, &#39;VCR_ZH_EASY_ALL&#39;,
&#39;VCR_ZH_HARD_500&#39;, &#39;VCR_ZH_HARD_100&#39;, &#39;VCR_ZH_HARD_ALL&#39;, &#39;MMDU&#39;, &#39;MMBench-Video&#39;, &#39;Video-MME&#39;
</pre></div>
</div>
<p>详见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Evaluation.html">评测</a>。</p>
</section>
<section id="id9">
<h3>Q2: 如何使用自定义评测集？<a class="headerlink" href="#id9" title="Link to this heading"></a></h3>
<p>纯文本、多模态自定义评测集必须和某个官方评测集数据格式（pattern）保持一致，见文档<a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Evaluation.html">评测</a>。</p>
</section>
<section id="q3-python3-11-mmengine">
<h3>Q3: python3.11环境，评测时mmengine报错<a class="headerlink" href="#q3-python3-11-mmengine" title="Link to this heading"></a></h3>
<p>尝试python3.10环境。或先安装全量依赖： <code class="docutils literal notranslate"><span class="pre">pip3</span> <span class="pre">install</span> <span class="pre">evalscope[all]</span></code>，再打patch： <code class="docutils literal notranslate"><span class="pre">pip3</span> <span class="pre">install https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/package/evalscope-0.5.3.post1-py3-none-any.whl</span></code>。</p>
</section>
<section id="q4-swift-eval">
<h3>Q4: 官方支持的评测数据集手动下载后，swift eval能配置本地路径评测吗？<a class="headerlink" href="#q4-swift-eval" title="Link to this heading"></a></h3>
<p>先下载评测数据集<a class="reference external" href="https://modelscope.cn/datasets/swift/evalscope_resource/files">eval.zip</a>，解压后将里面的内容放到 <code class="docutils literal notranslate"><span class="pre">~/.cache/modelscope/media_resources/evalscope/data</span></code>文件夹下；再执行swift eval命令就可以使用本地数据。</p>
</section>
<section id="q5-bug">
<h3>Q5: 自定义评测是不是有bug，把标准例子改成英文，一直都跑不通？<a class="headerlink" href="#q5-bug" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>swift<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--model_type<span class="w"> </span><span class="s1">&#39;qwen2_5-1_5b-instruct&#39;</span><span class="w"> </span>--eval_dataset<span class="w"> </span>no<span class="w"> </span>--custom_eval_config<span class="w"> </span><span class="s1">&#39;/mnt/workspace/test_data/config_eval.json&#39;</span>
</pre></div>
</div>
<p>这是依赖了nltk的包，然后nltk的tokenizer需要下载一个punkt_tab的zip文件，国内有些环境下载不太稳定或者直接失败。已尝试改了代码做兜底，规避这个问题；参考<a class="reference external" href="https://github.com/nltk/nltk/issues/3293">issue</a>。</p>
</section>
<section id="q6-eval-vllm">
<h3>Q6: eval微调后的模型，总是会在固定的百分比停掉，但是vllm服务看着一直是有在正常运行的。模型越大，断开的越早。<a class="headerlink" href="#q6-eval-vllm" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">SWIFT_TIMEOUT</span></code>环境变量设置为-1。</p>
</section>
<section id="q7-evalscope">
<h3>Q7: evalscope 支持多模型对比吗？<a class="headerlink" href="#q7-evalscope" title="Link to this heading"></a></h3>
<p>详见<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html">文档</a>。</p>
</section>
<section id="q8">
<h3>Q8: 多模态数据集有没有自定义评估？<a class="headerlink" href="#q8" title="Link to this heading"></a></h3>
<p>多模态自定义评估可以参考<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/index.html">文档</a>。</p>
</section>
<section id="q9-ms-swiftqps-tokens-s">
<h3>Q9: ms-swift有方法测试qps，延迟，tokens/s吗？<a class="headerlink" href="#q9-ms-swiftqps-tokens-s" title="Link to this heading"></a></h3>
<p>可以尝试使用evalscope的<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html">模型推理性能压测</a>。</p>
</section>
<section id="q10-mmlu">
<h3>Q10: 评估的时候可不可以控制数据集条数？评估一个mmlu需要一个多小时，也太慢了。<a class="headerlink" href="#q10-mmlu" title="Link to this heading"></a></h3>
<p>配置参数<code class="docutils literal notranslate"><span class="pre">--eval_limit</span></code>，这里的<code class="docutils literal notranslate"><span class="pre">--eval_limit</span></code>是控制了每个subset的条数，比如mmlu有50多个subset，每个limit10条，那就是500多条。</p>
</section>
<section id="q11">
<h3>Q11: 想请问一下，评测时不是相当于让模型输出一次回答然后检查答案对不对吗，有没有办法可以记录或看到每次完整的回答呢？<a class="headerlink" href="#q11" title="Link to this heading"></a></h3>
<p>ceval这种多选题的评测是通过计算每个选项的logits来得到的，没有输出回答内容；想得到回答内容的话，可以部署模型服务指定api url来评测，这样是通过解析模型输出来评测的，详见<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#api">文档</a>，后面这两种可以做成可选项。</p>
</section>
<section id="q12-evalscope-prompt-txt">
<h3>Q12: 我想用evalscope压测一下我的模型，想采用prompt.txt文件的形式，这个文件内容的格式应该是什么样子的呀？<a class="headerlink" href="#q12-evalscope-prompt-txt" title="Link to this heading"></a></h3>
<p>配置line_by_line，详见<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/parameters.html#id5">文档</a>。</p>
</section>
<section id="q13-evalscope-perf-parallelnumber">
<h3>Q13: 使用evalscope perf进行模型推理性能压测，parallel和number这两个参数怎样使用呢？<a class="headerlink" href="#q13-evalscope-perf-parallelnumber" title="Link to this heading"></a></h3>
<p>number是请求的总数量，parallel是并发数量。</p>
</section>
<section id="q14-swift-eval-1024token-max-new-tokens-5000">
<h3>Q14: 问一下评估swift eval里，模型最多生成1024token就结束了，这个如何修改？设置--max_new_tokens 5000，看起来没起作用<a class="headerlink" href="#q14-swift-eval-1024token-max-new-tokens-5000" title="Link to this heading"></a></h3>
<p>swift里面这个参数还没透出，可以使用evalscope来运行，model里面配置max_tokens参考<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/vlmevalkit_backend.html#id6">文档</a>。</p>
</section>
<section id="q15-evalscopedeepseek-r1-benchmark-aimemath-500">
<h3>Q15: 请问evalscope现在支持deepseek-r1 的相关benchmark吗？AIME、MATH-500这样<a class="headerlink" href="#q15-evalscopedeepseek-r1-benchmark-aimemath-500" title="Link to this heading"></a></h3>
<p>支持的，这里有<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/best_practice/deepseek_r1_distill.html">最佳实践</a>。</p>
</section>
<section id="q16-evalscopegpqa-valueerror-buildingconfig-gpqa-extended-not-found-available-default">
<h3>Q16: 想问一下evalscope测评gpqa使用本地路径报错： ValueError: BuildingConfig 'gpqa_extended' not found. Available: ['default']<a class="headerlink" href="#q16-evalscopegpqa-valueerror-buildingconfig-gpqa-extended-not-found-available-default" title="Link to this heading"></a></h3>
<p>参数配置如下：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>--datasets<span class="w"> </span>gpqa<span class="w"> </span>--dataset-args<span class="w"> </span><span class="s1">&#39;{&quot;gpqa&quot;: {&quot;local_path&quot;: &quot;/mnt/workspace/gpqa&quot;} }&#39;</span>
</pre></div>
</div>
<p>数据集如果要下载到本地使用，建议从modelscope上克隆仓库再指定路径。</p>
</section>
<section id="q17-evalscopearc">
<h3>Q17: 用evalscope评测arc数据集的时候，报这个错误，这是什么原因呢，用的是加载本地数据路径方式<a class="headerlink" href="#q17-evalscopearc" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>KeyError: &#39;RequestId&#39;
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--datasets<span class="w"> </span>arc<span class="w"> </span>--dataset-args<span class="w"> </span><span class="s1">&#39;{&quot;arc&quot;: {&quot;local_path&quot;: &quot;/mnt/workspace/arc&quot;}}&#39;</span>
</pre></div>
</div>
<p>参考<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#id10">文档</a>，arc数据集本身需要通过py脚本来下载数据，直接clone仓库不行。</p>
</section>
<section id="q18-opencompass">
<h3>Q18: 请教一下，想使用opencompass的后端评测，如何从本地加载下载好的数据集？<a class="headerlink" href="#q18-opencompass" title="Link to this heading"></a></h3>
<p>opencompass后端不支持设置<code class="docutils literal notranslate"><span class="pre">data_args</span></code>。</p>
</section>
<section id="q19-swift-eval-eval-backend-opencompass">
<h3>Q19: swift eval 来评估模型，--eval_backend OpenCompass不支持自定义数据集吗？<a class="headerlink" href="#q19-swift-eval-eval-backend-opencompass" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ValueError: eval_dataset: /mnt/workspace/data.jsonl is not supported.
eval_backend: OpenCompass supported datasets: [&#39;C3&#39;, &#39;summedits&#39;, &#39;WiC&#39;, &#39;csl&#39;, &#39;lambada&#39;, &#39;mbpp&#39;, &#39;hellaswag&#39;, &#39;ARC_e&#39;, &#39;math&#39;, &#39;nq&#39;, &#39;race&#39;, &#39;MultiRC&#39;, &#39;cmb&#39;, &#39;ceval&#39;, &#39;GaokaoBench&#39;, &#39;mmlu&#39;, &#39;winogrande&#39;, &#39;tnews&#39;, &#39;triviaqa&#39;, &#39;CB&#39;, &#39;cluewsc&#39;, &#39;humaneval&#39;, &#39;AX_g&#39;, &#39;DRCD&#39;, &#39;RTE&#39;, &#39;ocnli_fc&#39;, &#39;gsm8k&#39;, &#39;obqa&#39;, &#39;ReCoRD&#39;, &#39;Xsum&#39;, &#39;ocnli&#39;, &#39;WSC&#39;, &#39;siqa&#39;, &#39;agieval&#39;, &#39;piqa&#39;, &#39;cmnli&#39;, &#39;cmmlu&#39;, &#39;eprstmt&#39;, &#39;storycloze&#39;, &#39;AX_b&#39;, &#39;afqmc&#39;, &#39;strategyqa&#39;, &#39;bustm&#39;, &#39;BoolQ&#39;, &#39;COPA&#39;, &#39;ARC_c&#39;, &#39;PMMEval&#39;, &#39;chid&#39;, &#39;CMRC&#39;, &#39;lcsts&#39;]
</pre></div>
</div>
<p>opencompass不支持自定义数据集，用native可以自定义模式。</p>
</section>
<section id="q20-a100evalscoperagas-10">
<h3>Q20: 我在本地用单张A100运行模型来做evalscope官方文档里的<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html">RAGAS评测任务</a>时，跑文档中的两个样例花费了10分钟的时间，请问这是正常的么？有没有什么办法可以优化运行速度。<a class="headerlink" href="#q20-a100evalscoperagas-10" title="Link to this heading"></a></h3>
<p>rag评测本身确实比较耗资源，使用本地critic llm确实会慢一些，处理不了batch请求，建议用vllm这样的框架来拉起任务。</p>
</section>
<section id="q21-evalscoperag-api">
<h3>Q21: 用evalscope评测RAG，但是嵌入式模型我也想用 API 方式调用，支持吗？我看文档上没有写<a class="headerlink" href="#q21-evalscoperag-api" title="Link to this heading"></a></h3>
<p>目前embedding模型还没支持API调用，后续会支持。</p>
</section>
<section id="q22-evalscpoe-evalscope-answer-xxx-answer">
<h3>Q22: 使用evalscpoe测试本地训练后的模型，测试数据输出是很简单的，但是训练模型的时候数据构造的是推理的方式，这样测试结果就比较低，请问evalscope怎么仅仅使用模型输出里<answer>xxx</answer>里的数据测试？<a class="headerlink" href="#q22-evalscpoe-evalscope-answer-xxx-answer" title="Link to this heading"></a></h3>
<p>dataset-args中设置 {&quot;filters&quot;: {&quot;remove_until&quot;: &quot;</think>&quot;}} ，参考这个<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html#id3">文档</a>。设置这个参数，计算指标的时候会去掉<code class="docutils literal notranslate"><span class="pre">&lt;think&gt;</span></code>。</p>
</section>
<section id="q23-evalscope-opencompass">
<h3>Q23: evalscope原生是可以生成报告的 其他后端如opencompass是不支持生成报告可视化是吗？<a class="headerlink" href="#q23-evalscope-opencompass" title="Link to this heading"></a></h3>
<p>目前只支持native的可视化，其他后端还不支持。</p>
</section>
<section id="q24-ifeval">
<h3>Q24: 请问一下评测ifeval报这个错是什么原因？<a class="headerlink" href="#q24-ifeval" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[Errno 20] Not a directory: &#39;/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab&#39;
</pre></div>
</div>
<p>解压这个文件，<code class="docutils literal notranslate"><span class="pre">unzip</span> <span class="pre">/path/to/nltk_data/tokenizers/punkt_tab.zip</span></code>。</p>
</section>
<section id="q25-eval-backend-opencompass">
<h3>Q25: 请问评测时eval_backend='OpenCompass'，怎么指定离线数据集路径？<a class="headerlink" href="#q25-eval-backend-opencompass" title="Link to this heading"></a></h3>
<p>查看<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/opencompass_backend.html#id3">数据准备教程</a>，下载数据集并解压。不用指定<code class="docutils literal notranslate"><span class="pre">dataset-args</span></code>，将数据集文件夹（即data文件夹）放置在当前工作路径下即可。</p>
</section>
<section id="q26-evalscope">
<h3>Q26: 用evalscope报这个错是什么原因<a class="headerlink" href="#q26-evalscope" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>unzip: cannot find or open /root/nltk_data/tokenizers/punkt_tab.zip, /root/nltk_data/tokenizers/punkt_tab.zip.zip or /root/nltk_data/tokenizers/punkt_tab.zip.ZIP
</pre></div>
</div>
<p>这是在下载nltk的依赖，手动下载<a class="reference external" href="https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/open_data/nltk_data/punkt_tab.zip">punkt_tab.zip</a>，解压到<code class="docutils literal notranslate"><span class="pre">~/nltk_data/tokenizers</span></code>下面。</p>
</section>
<section id="q27">
<h3>Q27: 为啥纯文本没问题，测多模态我们指定路径了，但他还是检测不到数据集，会去下载？<a class="headerlink" href="#q27" title="Link to this heading"></a></h3>
<p>vlmevalkit流程跟native不一样，会自己下载数据放到<code class="docutils literal notranslate"><span class="pre">~/LMUData/</span></code>下面。</p>
</section>
<section id="q28-evalscopescore">
<h3>Q28: 请教下，evalscope的score是如何计算的，这部分有文档说明吗？<a class="headerlink" href="#q28-evalscopescore" title="Link to this heading"></a></h3>
<p>请参考这个<a class="reference external" href="https://github.com/modelscope/evalscope/issues/610">issue</a>。</p>
</section>
<section id="q29-swift-evalbenchmark-llmjudge">
<h3>Q29: 请问一下swift eval做benchmark评测的时候，是否可以指定llm作为judge, 参数应该怎么传进去？<a class="headerlink" href="#q29-swift-evalbenchmark-llmjudge" title="Link to this heading"></a></h3>
<p>支持，使用swift得从<code class="docutils literal notranslate"><span class="pre">extra_eval_args</span></code>去传递<code class="docutils literal notranslate"><span class="pre">judge-model-args</span></code>参数，包括<code class="docutils literal notranslate"><span class="pre">api_key，api_url，model_id</span></code>，整体是一个json字符串。</p>
</section>
<section id="q30-responsecot-vlmevalkit">
<h3>Q30: 请问，思考模型怎么在response中去掉CoT再进行评测？vlmevalkit后端<a class="headerlink" href="#q30-responsecot-vlmevalkit" title="Link to this heading"></a></h3>
<p>这个后处理还不支持。</p>
</section>
<section id="q31-embedding">
<h3>Q31: 请问embedding评测，第一次运行会去官方下载数据集。可以下载后指定目录吗？<a class="headerlink" href="#q31-embedding" title="Link to this heading"></a></h3>
<p>目前embedding评测，只有自定义的数据集才能指定路径。</p>
</section>
<section id="id10">
<h3>Q32: 请问怎么自定义评测指标？<a class="headerlink" href="#id10" title="Link to this heading"></a></h3>
<p>是想在自定义的数据集上使用自定义的评测指标吗？目前还没有plugin的方式，需自行修改evalscope/benchmarks/general_qa/general_qa_adapter.py 里面的match方法。</p>
</section>
<section id="q33-mmvetjudge-model-judge-model">
<h3>Q33: mmvet评测需要配置judge model，按这个<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/vlmevalkit_backend.html#id12">文档</a>配置judge model，裁判模型的名称一定要是这三个里面的吗？<a class="headerlink" href="#q33-mmvetjudge-model-judge-model" title="Link to this heading"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
File &quot;/usr/local/lib/python3.10/dist-packages/vlmeval/run.py&quot;, line 484, in run_task eval_results = dataset.evaluate(result_file,**judge_kwargs )
File &quot;/usr/local/lib/python3.10/dist-packages/vlmeval/dataset/image_mcq.py&quot;, line 244, in evaluate assert model in [&#39;chatgpt-0125&#39;， &#39;exact_matching&#39;，&#39;gpt-4-0125&#39;] AssertionError
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">task_cfg_dict</span> <span class="o">=</span> <span class="n">TaskConfig</span><span class="p">(</span>
    <span class="n">work_dir</span><span class="o">=</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
    <span class="n">eval_backend</span><span class="o">=</span><span class="s1">&#39;VLMEvalKit&#39;</span><span class="p">,</span>
    <span class="n">eval_config</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;MMVet&#39;</span><span class="p">,</span> <span class="s1">&#39;DocVQA_VAL&#39;</span><span class="p">,</span> <span class="s1">&#39;MMBench_DEV_EN&#39;</span><span class="p">],</span>
        <span class="s1">&#39;limit&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;all&#39;</span><span class="p">,</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s1">&#39;api_base&#39;</span><span class="p">:</span> <span class="s1">&#39;http://127.0.0.1:8001/v1/chat/completions&#39;</span><span class="p">,</span>
            <span class="s1">&#39;key&#39;</span><span class="p">:</span> <span class="s1">&#39;EMPTY&#39;</span><span class="p">,</span>
            <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;CustomAPIModel&#39;</span><span class="p">,</span>
            <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;Qwen2.5-VL-72B-Instruct-AWQ&#39;</span><span class="p">,</span>
            <span class="s1">&#39;img_size&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;video_llm&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s1">&#39;max_tokens&#39;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,}</span>
            <span class="p">],</span>
        <span class="s1">&#39;reuse&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;nproc&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
        <span class="s1">&#39;judge&#39;</span><span class="p">:</span> <span class="s1">&#39;exact_matching&#39;</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>对于多选题问答，是有这个要求，不然的话就指定exact_matching。试试MMBench跟MMVet的评测分开，不使用judge model。</p>
</section>
<section id="q34-eval">
<h3>Q34: 请问在执行eval的时候出现了多卡显存分配不均是什么原因？<a class="headerlink" href="#q34-eval" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">8</span>
<span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5,6,7<span class="se">\ </span><span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">802816</span><span class="se">\ </span>swift<span class="w"> </span>eval<span class="se">\</span>
--model<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$MODEL_PATH</span><span class="s2">” \$EXTRA_ARGS \</span>
<span class="s2">--eval_backend Native \ --infer_backend transformers\ --device_map auto \</span>
<span class="s2">--eval_limit&quot;</span><span class="nv">$EVAL_LIMIT</span><span class="s2">&quot;\ --eval_dataset general_qa\</span>
<span class="s2">--dataset_args &quot;</span><span class="o">{</span><span class="se">\&quot;</span>general_qa<span class="se">\&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="se">\&quot;</span>local_path<span class="se">\&quot;</span>:<span class="w"> </span><span class="se">\&quot;</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span><span class="se">\&quot;</span>,<span class="w"> </span><span class="se">\&quot;</span>subset_list<span class="se">\&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="se">\&quot;</span><span class="si">${</span><span class="nv">SUBSET_NAME</span><span class="si">}</span><span class="se">\&quot;</span><span class="o">]}}</span><span class="s2">&quot; \ --host 127.0.0.1\&gt; &quot;</span><span class="nv">$LOG_FILE</span><span class="s2">&quot; 2&gt;&amp;1</span>
</pre></div>
</div>
<p>swift eval不支持ddp方式启动。</p>
</section>
<section id="q35-evalscope-input-token">
<h3>Q35: 请问，使用evalscope评测 如何控制input token为固定长度？<a class="headerlink" href="#q35-evalscope-input-token" title="Link to this heading"></a></h3>
<p>控制长度只支持random数据集，参考<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#random">文档</a>。</p>
</section>
<section id="q36-evalscope-app-outputs">
<h3>Q36: evalscope app找不到报告是怎么回事，outputs目录下明明有对应的记录<a class="headerlink" href="#q36-evalscope-app-outputs" title="Link to this heading"></a></h3>
<p>可能是推理性能压测，evalscope perf可视化参考<a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#id5">文档</a>。</p>
</section>
<section id="q37-swiftquery">
<h3>Q37: 请问哪里可以看到swift评测的时候送入的query除了问题之外还有哪些额外的字段呢？<a class="headerlink" href="#q37-swiftquery" title="Link to this heading"></a></h3>
<p>最简单的方法是看输出的reviews文件中的input字段，是输入给模型的内容转换后的markdown格式。如果用backend是opencompass的话没有这些，需要用native backend。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024, Ascend。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>