

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>命令行参数 &mdash; 昇腾开源  文档</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            昇腾开源
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">🏁 开始使用</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">快速安装昇腾环境</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🏗️  基础设施与框架</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🧠 训练与微调框架</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🚀 推理与服务</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">🎨 多模态、应用与评测</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">昇腾开源</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">命令行参数</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source/Megatron-SWIFT/Command-line-parameters.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>命令行参数<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<section id="megatron">
<h2>Megatron参数<a class="headerlink" href="#megatron" title="Link to this heading"></a></h2>
<p><strong>训练参数</strong>:</p>
<ul class="simple">
<li><p>🔥micro_batch_size: 每个device的批次大小，默认为1。</p></li>
<li><p>🔥global_batch_size: 总批次大小，等价于<code class="docutils literal notranslate"><span class="pre">micro_batch_size*数据并行大小*梯度累加步数</span></code>。默认为16。</p>
<ul>
<li><p>其中，<code class="docutils literal notranslate"><span class="pre">数据并行大小</span> <span class="pre">(DP)</span> <span class="pre">=</span> <span class="pre">总GPU数</span> <span class="pre">/</span> <span class="pre">(TP</span> <span class="pre">×</span> <span class="pre">PP</span> <span class="pre">×</span> <span class="pre">CP)</span></code>。</p></li>
</ul>
</li>
<li><p>🔥recompute_granularity: 重新计算激活的粒度，可选项为'full', 'selective' and 'none'。其中full代表重新计算整个transformer layer，selective代表只计算transformer layer中的核心注意力部分。通常'selective'是推荐的。默认为'selective'。</p>
<ul>
<li><p>当你设置为'selective'时，你可以通过指定<code class="docutils literal notranslate"><span class="pre">--recompute_modules</span></code>来选择对哪些部分进行重新计算。</p></li>
</ul>
</li>
<li><p>🔥recompute_method: 该参数需将recompute_granularity设置为'full'才生效，可选项为'uniform', 'block'。默认为None。</p></li>
<li><p>🔥recompute_num_layers: 该参数需将recompute_granularity设置为'full'才生效，默认为None。若<code class="docutils literal notranslate"><span class="pre">recompute_method</span></code>设置为uniform，该参数含义为每个均匀划分的重新计算单元的transformer layers数量。例如你可以指定为<code class="docutils literal notranslate"><span class="pre">--recompute_granularity</span> <span class="pre">full</span> <span class="pre">--recompute_method</span> <span class="pre">uniform</span> <span class="pre">--recompute_num_layers</span> <span class="pre">4</span></code>。recompute_num_layers越大，显存占用越小，计算成本越大。注意：当前进程中的模型层数需能被<code class="docutils literal notranslate"><span class="pre">recompute_num_layers</span></code>整除。默认为None。</p></li>
<li><p>🔥recompute_modules: 选项包括&quot;core_attn&quot;, &quot;moe_act&quot;, &quot;layernorm&quot;, &quot;mla_up_proj&quot;, &quot;mlp&quot;, &quot;moe&quot;，默认值为<code class="docutils literal notranslate"><span class="pre">[&quot;core_attn&quot;]</span></code>。该参数在<code class="docutils literal notranslate"><span class="pre">--recompute_granularity</span> <span class="pre">selective</span></code>时生效。例如在MoE训练时，你可以通过指定<code class="docutils literal notranslate"><span class="pre">--recompute_granularity</span> <span class="pre">selective</span> <span class="pre">--recompute_modules</span> <span class="pre">core_attn</span> <span class="pre">moe</span></code>降低显存。其中&quot;core_attn&quot;、&quot;mlp&quot; 和 &quot;moe&quot; 使用常规检查点，&quot;moe_act&quot;、&quot;layernorm&quot; 和 &quot;mla_up_proj&quot; 使用输出丢弃检查点。</p>
<ul>
<li><p>&quot;core_attn&quot;: 重新计算 Transformer 层中的核心注意力部分。</p></li>
<li><p>&quot;mlp&quot;: 重新计算密集的 MLP 层。</p></li>
<li><p>&quot;moe&quot;: 重新计算 MoE 层。</p></li>
<li><p>&quot;moe_act&quot;: 重新计算 MoE 中的 MLP 激活函数部分。</p></li>
<li><p>&quot;layernorm&quot;: 重新计算 input_layernorm 和 pre_mlp_layernorm。</p></li>
<li><p>&quot;mla_up_proj&quot;: 重新计算 MLA 上投影和 RoPE 应用部分。</p></li>
</ul>
</li>
<li><p>🔥train_iters: 训练的总迭代次数，默认为None。</p>
<ul>
<li><p>提示：你可以通过设置<code class="docutils literal notranslate"><span class="pre">--num_train_epochs</span></code>来设置训练的epochs数。在使用非流式数据集时，会自动根据数据集数量计算<code class="docutils literal notranslate"><span class="pre">train_iters</span></code>（兼容packing）。</p></li>
</ul>
</li>
<li><p>🔥num_train_epochs: 指定训练的epochs数。当使用非流式数据集时，该参数会为你自动计算train_iters而不需要手动传入<code class="docutils literal notranslate"><span class="pre">train_iters</span></code>。当使用流式数据集时，该参数会在训练到<code class="docutils literal notranslate"><span class="pre">num_train_epochs</span></code>时强制退出训练，并对权重进行验证和保存。默认为None。</p></li>
<li><p>masked_softmax_fusion: 默认为True。用于开启query_key_value的scaling, masking, and softmax融合。</p></li>
<li><p>bias_dropout_fusion: 默认为True。用于开启bias和dropout的融合。</p></li>
<li><p>bias_activation_fusion: 如果为True，则在可能的情况下融合偏置加法和激活函数。默认为True。</p></li>
<li><p>apply_rope_fusion: 默认为False。用于开启rope融合。该参数为megatron-core参数透传。注意：并不是所有情况都支持rope融合，例如：MLA、mrope等不支持。</p></li>
<li><p>gradient_accumulation_fusion: 默认为True。用于开启梯度累加融合。</p></li>
<li><p>🔥cross_entropy_loss_fusion: 启动交叉熵损失计算融合。默认为True。</p></li>
<li><p>cross_entropy_fusion_impl: 交叉熵损失融合的实现。可选为'native'和'te'。默认为'native'。</p></li>
<li><p>calculate_per_token_loss: 根据全局批次中的非填充token数量来对交叉熵损失进行缩放。默认为None，<code class="docutils literal notranslate"><span class="pre">task_type</span></code>为'causal_lm'且为预训练/微调时，默认为True，否则默认为False。</p></li>
<li><p>🔥attention_backend: 使用的注意力后端 (flash、fused、unfused、local、auto)。默认为 flash。</p>
<ul>
<li><p>如果安装'flash_attention_3'，<code class="docutils literal notranslate"><span class="pre">--attention_backend</span> <span class="pre">flash</span></code>则优先使用fa3。训练脚本参考<a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/flash_attention_3">这里</a>。多模态模型的vit部分要使用flash_attention_3，请设置<code class="docutils literal notranslate"><span class="pre">--attn_impl</span> <span class="pre">flash_attention_3</span></code>。</p></li>
<li><p>有些模型可能不支持flash，你需要手动设置<code class="docutils literal notranslate"><span class="pre">--attention_backend</span> <span class="pre">unfused/fused</span> <span class="pre">--padding_free</span> <span class="pre">false</span></code>，例如：Llama4、GPT-OSS。</p></li>
</ul>
</li>
<li><p>optimizer: 优化器类型，可选为'adam'、'sgd'。默认为adam。</p>
<ul>
<li><p>注意：此'adam'为'adamw'，参考<a class="reference external" href="https://github.com/NVIDIA/TransformerEngine/blob/d8f1e68f7c414f3e7985a8b41de4443b2f819af3/transformer_engine/pytorch/optimizers/fused_adam.py#L69-L70">这里</a>。</p></li>
</ul>
</li>
<li><p>🔥optimizer_cpu_offload: 将优化器状态卸载到 CPU，例如设置：<code class="docutils literal notranslate"><span class="pre">--use_precision_aware_optimizer</span> <span class="pre">true</span> <span class="pre">--optimizer_cpu_offload</span> <span class="pre">true</span> <span class="pre">--optimizer_offload_fraction</span> <span class="pre">0.7</span></code>。默认为False。</p>
<ul>
<li><p>该参数可以显著降低显存占用（但增加内存占用）。若global_batch_size较大，则对训练速度的影响不大。</p></li>
</ul>
</li>
<li><p>🔥optimizer_offload_fraction: 卸载到 CPU 的优化器状态所占比例。默认为1.。</p></li>
<li><p>use_precision_aware_optimizer: 使用 TransformerEngine 中的精度感知优化器，该优化器允许将主参数和优化器状态设置为较低精度，例如 fp16 和 fp8。</p></li>
<li><p>main_grads_dtype: 启用 use_precision_aware_optimizer 时主梯度的 dtype。可选为'fp32', 'bf16'。默认为'fp32'。</p></li>
<li><p>main_params_dtype: 启用 use_precision_aware_optimizer 时主参数的 dtype。可选为'fp32', 'fp16'。默认为'fp32'。</p></li>
<li><p>exp_avg_dtype: 启用 use_precision_aware_optimizer 时，adam 优化器中 exp_avg（即一阶矩）的 dtype。该 dtype 用于在训练过程中将优化器状态存储在内存中，但不会影响内核计算时的精度。可选为'fp32', 'fp16', 'bf16', 'fp8'。默认为'fp32'。</p></li>
<li><p>exp_avg_sq_dtype: 启用 use_precision_aware_optimizer 时，adam 优化器中 exp_avg_sq（即二阶矩）的 dtype。该 dtype 用于在训练过程中将优化器状态存储在内存中，但不会影响内核计算的精度。可选为'fp32', 'fp16', 'bf16', 'fp8'。默认为'fp32'。</p></li>
<li><p>manual_gc: 禁用默认垃圾回收器，手动触发垃圾回收。默认为False。</p></li>
<li><p>manual_gc_steps: 手动触发垃圾回收的间隔（steps）。默认为0。</p></li>
<li><p>manual_gc_eval: 当使用手动垃圾回收时（<code class="docutils literal notranslate"><span class="pre">--manual_gc</span> <span class="pre">true</span></code>），在每次评估运行的开始和结束时禁用垃圾回收。默认为True。</p></li>
</ul>
<p><strong>数据参数</strong>:</p>
<ul class="simple">
<li><p>seed: python、numpy、pytorch和cuda的随机种子，默认为42。</p></li>
<li><p>dataset_shuffle: 是否对dataset进行随机操作。默认为True。</p>
<ul>
<li><p>注意：<strong>Megatron-SWIFT的随机包括两个部分</strong>：数据集的随机，由<code class="docutils literal notranslate"><span class="pre">dataset_shuffle</span></code>控制；train_dataloader中的随机，由<code class="docutils literal notranslate"><span class="pre">train_dataloader_shuffle</span></code>控制。</p></li>
</ul>
</li>
<li><p>train_dataloader_shuffle: 是否对train_dataloader使用随机，默认为True。val_dataset不进行随机操作。</p></li>
<li><p>🔥dataloader_num_workers: dataloader的workers数量，默认为4。</p>
<ul>
<li><p>注意：若设置<code class="docutils literal notranslate"><span class="pre">--streaming</span> <span class="pre">true</span></code>，则设置为1。</p></li>
</ul>
</li>
<li><p>dataloader_pin_memory: 默认为True。</p></li>
<li><p>dataloader_persistent_workers: 默认为True。</p></li>
<li><p>dataloader_prefetch_factor: 默认为2。</p></li>
<li><p>data_sharding: 当<code class="docutils literal notranslate"><span class="pre">--train_dataloader_shuffle</span> <span class="pre">true</span></code>时对 train_dataloader 生效，默认为False。该参数控制数据集随机的范围。若设置为True，则先对数据集进行分片，然后对每个分片进行随机处理（略节约内存）；若设置为False，则先对数据集进行随机，再进行分片（更好的随机效果）。</p></li>
<li><p>🔥group_by_length: 是否在训练数据集中将长度大致相同的样本分组在一起（有随机因素），以最小化填充并确保各节点与进程的负载均衡以提高效率。默认为False。具体算法参考<code class="docutils literal notranslate"><span class="pre">transformers.trainer_pt_utils.get_length_grouped_indices</span></code>。</p></li>
<li><p>te_rng_tracker: 使用 Transformer Engine 版本的随机数生成器。默认为False。</p></li>
<li><p>data_parallel_random_init: 在数据并行的各个 rank 之间启用不同的随机初始化。默认为False。</p></li>
<li><p>padding_free: 将一个batch中的数据进行展平而避免数据padding，从而降低显存占用并加快训练。默认为True。</p>
<ul>
<li><p>若要自定义attention_mask，你可以设置<code class="docutils literal notranslate"><span class="pre">--padding_free</span> <span class="pre">false</span></code>。</p></li>
<li><p>注意：<strong>Megatron-SWIFT训练特性优先支持padding_free格式</strong>，若非特殊情况，请勿修改该值。</p></li>
</ul>
</li>
<li><p>mlp_padding_free: 默认为False。用于padding_free设置为false时，对mlp进行padding_free优化。这可以在自定义attention_mask的同时，提升训练速度和减少显存占用。</p></li>
</ul>
<p><strong>学习率参数</strong>:</p>
<ul class="simple">
<li><p>lr_warmup_init: 学习率warmup的初始值。学习率调度器从这个值开始进行预热。默认为0。</p></li>
<li><p>🔥lr: 初始学习率，最终会根据学习率预热策略和衰减策略决定每个迭代的学习率。默认为None，<strong>全参数训练默认为1e-5，LoRA训练默认为1e-4</strong>。</p></li>
<li><p>lr_decay_style: 学习率衰减策略，默认为'cosine'。通常设置为'constant', 'linear', 'cosine', 'inverse-square-root', 'WSD'。</p></li>
<li><p>🔥lr_decay_iters: 学习率衰减的迭代次数。默认为None，则设置为<code class="docutils literal notranslate"><span class="pre">--train_iters</span></code>。</p></li>
<li><p>lr_warmup_iters: 线性学习率预热的迭代次数，默认为0。</p></li>
<li><p>🔥lr_warmup_fraction: 线性学习率预热阶段所占比例，默认为None。</p></li>
<li><p>🔥min_lr: 学习率的最小值，将低于该阈值的学习率裁剪为该值，默认为0。</p></li>
<li><p>lr_wsd_decay_style: WSD 退火阶段的衰减方式。默认为'exponential'。</p></li>
<li><p>lr_wsd_decay_iters: 学习率衰减的迭代次数。默认为 None。</p></li>
</ul>
<p><strong>正则化参数</strong>:</p>
<ul class="simple">
<li><p>🔥weight_decay: 默认为0.1。</p></li>
<li><p>weight_decay_incr_style: 权重衰减的递增函数。可选为'constant', 'linear', 'cosine'。默认为'constant'。</p></li>
<li><p>start_weight_decay: L2 正则化的初始权重衰减系数。</p></li>
<li><p>end_weight_decay: 训练结束时 L2 正则化的权重衰减系数。</p></li>
<li><p>🔥clip_grad: l2梯度裁剪，默认为1.0。</p>
<ul>
<li><p>日志中打印的grad_norm为未裁剪前的值。</p></li>
</ul>
</li>
<li><p>adam_beta1: 默认0.9。</p></li>
<li><p>adam_beta2: 默认0.95。</p></li>
<li><p>adam_eps: 默认1e-8。</p></li>
<li><p>sgd_momentum: 设置<code class="docutils literal notranslate"><span class="pre">--optimizer</span> <span class="pre">sgd</span></code>时生效，默认为0.9。</p></li>
</ul>
<p><strong>checkpoint参数</strong>:</p>
<ul class="simple">
<li><p>🔥output_dir: checkpoint的输出目录，默认None。在训练中，若未设置该参数，则默认为<code class="docutils literal notranslate"><span class="pre">f'megatron_output/{model_suffix}'</span></code>，例如<code class="docutils literal notranslate"><span class="pre">'megatron_output/Qwen2.5-7B-Instruct'</span></code>。</p>
<ul>
<li><p>注意：<strong>若在多机训练时，请确保每个节点的保存路径指向相同位置</strong>，否则你需要在训练后手动集中这些权重。</p></li>
</ul>
</li>
<li><p>🔥save_steps: checkpoint保存的间隔（steps），默认为500。</p>
<ul>
<li><p>注意：训练结束时一定会保存权重。</p></li>
</ul>
</li>
<li><p>🔥no_save_optim: 不保存optimizer，默认为False。在全参数训练时，可以显著降低存储时间。</p></li>
<li><p>🔥no_save_rng: 不保存rng，默认为False。</p></li>
<li><p>🔥mcore_model: 加载的checkpoint目录（mcore存储格式），默认None。对于断点续训的介绍，请查看<code class="docutils literal notranslate"><span class="pre">--finetune</span> <span class="pre">false</span></code>参数的介绍。</p>
<ul>
<li><p>megatron-swift推荐直接加载和存储safetensors权重，参考<a class="reference internal" href="Mcore-Bridge.html"><span class="doc">mcore-bridge文档</span></a>。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--model</span></code>与<code class="docutils literal notranslate"><span class="pre">--mcore_model</span></code>的区别：<code class="docutils literal notranslate"><span class="pre">--model/--adapters/--ref_model/--ref_adapters</span></code>后加safetensors权重目录，<code class="docutils literal notranslate"><span class="pre">--mcore_model/--mcore_adapter/--mcore_ref_model/--mcore_ref_adapter</span></code>后加mcore权重目录。<code class="docutils literal notranslate"><span class="pre">--model/--adapters</span></code>不支持加载断点续训状态，因此若设置<code class="docutils literal notranslate"><span class="pre">--no_save_optim</span> <span class="pre">false</span></code>，将额外存储mcore权重格式用于断点续训，你需要使用<code class="docutils literal notranslate"><span class="pre">--mcore_model/--mcore_adapter</span></code>来加载断点续训的状态。</p></li>
</ul>
</li>
<li><p>🔥no_load_optim: 不载入optimizer，默认为False。</p>
<ul>
<li><p>注意：断点续训时，设置<code class="docutils literal notranslate"><span class="pre">--no_load_optim</span> <span class="pre">false</span></code>读取优化器状态通常比<code class="docutils literal notranslate"><span class="pre">--no_load_optim</span> <span class="pre">true</span></code>不读取优化器状态消耗更大的显存资源。</p></li>
</ul>
</li>
<li><p>🔥no_load_rng: 不载入rng，默认为False。</p></li>
<li><p>🔥finetune: 将模型加载并微调。<strong>不加载检查点的优化器和随机种子状态，并将迭代数设置为0</strong>。默认为True。</p>
<ul>
<li><p>注意：<strong>断点续训</strong>你需要设置<code class="docutils literal notranslate"><span class="pre">--mcore_model</span></code>（lora训练需要额外设置<code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span></code>），若设置<code class="docutils literal notranslate"><span class="pre">--finetune</span> <span class="pre">true</span></code>，将不加载优化器状态和随机种子状态并将迭代数设置为0，不会进行数据集跳过；若设置<code class="docutils literal notranslate"><span class="pre">--finetune</span> <span class="pre">false</span></code>，将读取迭代数并跳过之前训练的数据集数量，优化器状态和随机种子状态的读取通过<code class="docutils literal notranslate"><span class="pre">--no_load_optim</span></code>和<code class="docutils literal notranslate"><span class="pre">--no_load_rng</span></code>控制。</p></li>
<li><p>流式数据集<code class="docutils literal notranslate"><span class="pre">--streaming</span></code>，暂不支持跳过数据集。</p></li>
</ul>
</li>
<li><p>perform_initialization: 对权重进行初始化，默认为False。</p></li>
<li><p>use_cpu_initialization: 在cpu上初始化权重，默认为False。在进行HF和MCore权重转换时会被使用。通常不需要修改该值。</p></li>
<li><p>🔥async_save: 使用异步检查点保存。目前仅适用于<code class="docutils literal notranslate"><span class="pre">torch_dist</span></code>分布式检查点格式。默认为False。</p></li>
<li><p>🔥save_total_limit: 最多保存的checkpoint数，会将过期的checkpoint进行删除。默认为None，保存所有的checkpoint。该参数需设置为<code class="docutils literal notranslate"><span class="pre">&gt;=2</span></code>的数，若设置为2，则保存best checkpoint和last checkpoint。该参数暂不兼容<code class="docutils literal notranslate"><span class="pre">async_save</span></code>。</p></li>
<li><p>metric_for_best_model: 默认为None，GRPO默认为'reward'，其他情况默认为'loss'。</p></li>
<li><p>greater_is_better: 默认为None，即当<code class="docutils literal notranslate"><span class="pre">metric_for_best_model</span></code>含'loss'时，设置为False，否则设置为True。</p></li>
<li><p>use_persistent_ckpt_worker: 为异步保存启动持久化检查点工作进程。默认为False。</p></li>
<li><p>dist_ckpt_save_pre_mcore_014: 使用 Megatron-Core 0.14 之前的格式存储。默认为False。</p></li>
<li><p>dist_ckpt_optim_fully_reshardable: 使优化器分布式检查点完全可重分片（TP/PP/EP/DP），而不是仅支持普通的DP重分片。默认为False。</p></li>
<li><p>distrib_optim_fully_reshardable_mem_efficient: 在分布式优化器检查点保存和加载过程中，通过使用Gloo（而非NCCL），并仅使用单个rank进行保存，以尽可能减少内存使用。仅在遇到主机或设备内存问题时启用，仅在设置了<code class="docutils literal notranslate"><span class="pre">--dist-ckpt-optim-fully-reshardable</span></code>标志时生效。默认为False。</p></li>
</ul>
<p><strong>分布式参数</strong>:
并行技术的选择请参考<a class="reference external" href="Quick-start.md#%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7">训练技巧文档</a>。</p>
<ul class="simple">
<li><p>ddp_backend: 分布式后端，可选为'nccl', 'gloo'。默认为nccl。</p></li>
<li><p>ddp_timeout: 默认为18000000，单位为秒。</p></li>
<li><p>🔥use_distributed_optimizer: 使用分布式优化器（即zero1）。默认为True。</p></li>
<li><p>🔥tensor_model_parallel_size: tp数，默认为1。</p></li>
<li><p>🔥pipeline_model_parallel_size: pp数，默认为1。</p></li>
<li><p>🔥decoder_first_pipeline_num_layers: decoder第一个流水线阶段所包含的Transformer层数。默认为 None，表示将Transformer层数平均分配到所有流水线阶段。</p>
<ul>
<li><p>该参数通常用于<strong>Transformer层数无法被PP整除</strong>，或者多模态模型第0个pp阶段显存占用过高的情况。</p></li>
</ul>
</li>
<li><p>🔥decoder_last_pipeline_num_layers: decoder最后一个流水线阶段所包含的Transformer层数。默认为 None，表示将Transformer层数平均分配到所有流水线阶段。</p></li>
<li><p>account_for_embedding_in_pipeline_split: 如果设置为 True，在流水线并行的划分和放置策略中，输入 embedding 层会被视为一个标准的 Transformer 层来处理。默认为False。</p></li>
<li><p>account_for_loss_in_pipeline_split: 如果设置为 True，在流水线并行的划分和放置策略中，loss 层会被视为一个标准的 Transformer 层来处理。默认为False。</p></li>
<li><p>overlap_p2p_comm: 在 1F1B 中将流水线并行通信与前向和反向块重叠，默认为True。</p></li>
<li><p>align_param_gather: 设置为True，所有 PP 阶段将同时启动参数全收集（all-gather）操作。否则，每个 PP 阶段将根据需要独立启动。默认为True。</p></li>
<li><p>🔥sequence_parallel: 启动序列并行优化，该参数需要设置<code class="docutils literal notranslate"><span class="pre">tensor_model_parallel_size</span></code>才生效。默认为False。</p></li>
<li><p>🔥context_parallel_size: cp数，默认为1。</p></li>
<li><p>tp_comm_overlap: 启用张量并行通信与GEMM（通用矩阵乘法）内核的重叠（降低通信耗时）。默认为False。</p></li>
<li><p>🔥overlap_grad_reduce: 启用DDP中grad reduce操作的重叠（降低DP通信耗时）。默认为False。</p></li>
<li><p>🔥overlap_param_gather: 启用分布式优化器中参数all-gather的重叠（降低DP通信耗时）。默认为False。</p></li>
<li><p>virtual_pipeline_model_parallel_size: 每个流水线并行 rank 的虚拟流水线阶段数量。默认为None。vpp并行，用于减少pp并行的计算空泡，提高GPU利用率，但会略微提高通信量。</p></li>
<li><p>microbatch_group_size_per_vp_stage: 每个虚拟流水线阶段处理的连续微批次数量。默认为None，等于pipeline_model_parallel_size。</p></li>
<li><p>🔥pipeline_model_parallel_layout: 一个描述自定义流水线（pp/vpp）模型并行布局的字符串。例如：&quot;E|(t|)*3,m|m||L&quot;。其中 E、L、t、m 分别表示嵌入层（embedding）、损失层（loss）、Transformer 解码器层和 MTP 层。阶段之间用 &quot;|&quot; 分隔。重复的阶段或层可以通过乘法表示。逗号仅用于提升可读性（无实际语法作用）。默认值为 None，表示不使用此参数设置布局。</p>
<ul>
<li><p>该参数通常在异构GPU集群上使用。</p></li>
</ul>
</li>
<li><p>🔥expert_model_parallel_size: 专家并行数，默认为1。</p></li>
<li><p>🔥expert_tensor_parallel_size: 专家TP并行度。默认值为1。</p></li>
</ul>
<p><strong>日志参数</strong>:</p>
<ul class="simple">
<li><p>report_to: 启用的日志后端。默认为<code class="docutils literal notranslate"><span class="pre">['tensorboard']</span></code>。可选项为'tensorboard', 'wandb'和'swanlab'。'wandb'和'swanlab'登陆可以使用<code class="docutils literal notranslate"><span class="pre">WANDB_API_KEY</span></code>、<code class="docutils literal notranslate"><span class="pre">SWANLAB_API_KEY</span></code>环境变量。</p></li>
<li><p>🔥logging_steps: 日志记录的间隔（steps），默认为5。</p></li>
<li><p>tensorboard_dir: tensorboard日志写入的目录。默认None，即存储在<code class="docutils literal notranslate"><span class="pre">f'{save}/runs'</span></code>目录下。</p></li>
<li><p>tensorboard_queue_size: 用于暂存事件和摘要的 TensorBoard 队列大小；当队列中待处理的事件和摘要数量达到该大小时，下一次调用 &quot;add&quot; 相关方法会触发将数据刷新写入磁盘。默认为50。</p></li>
<li><p>wandb_project: wandb项目名称，默认为'megatron-swift'。</p></li>
<li><p>wandb_exp_name: wandb 实验名称。默认为<code class="docutils literal notranslate"><span class="pre">--output_dir</span></code>的值。</p></li>
<li><p>swanlab_project: swanlab项目名称，默认为'megatron-swift'。</p></li>
<li><p>swanlab_exp_name: swanlab 实验名称。默认为<code class="docutils literal notranslate"><span class="pre">--output_dir</span></code>的值。</p></li>
</ul>
<p><strong>评估参数</strong>:</p>
<ul class="simple">
<li><p>🔥eval_iters: 评估的迭代次数，默认为<code class="docutils literal notranslate"><span class="pre">-1</span></code>，根据验证数据集的数量设置合适的值。<strong>若验证集数量少于global_batch_size，则不进行评估</strong>。若使用流式数据集，该值需要手动设置。</p></li>
<li><p>🔥eval_steps: 评估的间隔（steps），即每训练多少steps进行评估。默认为None，即设置为<code class="docutils literal notranslate"><span class="pre">save_steps</span></code>。</p></li>
</ul>
<p><strong>fp8参数</strong>:</p>
<ul class="simple">
<li><p>fp8_format: 用于前向和反向传播中FP8张量的FP8格式方案。可选为'e4m3'，'hybrid'。默认为None。</p></li>
<li><p>fp8_recipe: 用于前向和反向传播中 FP8 张量的 FP8 算法方案。可选为'tensorwise', 'delayed', 'mxfp8', 'blockwise'。默认为'delayed'。其中blockwise fp8需要 cuda129 以上版本。</p></li>
<li><p>fp8_amax_history_len: 每个张量记录 amax 历史的步数。默认为1024。</p></li>
<li><p>fp8_amax_compute_algo: 用于根据历史记录计算 amax 的算法。可选为'most_recent', 'max'。默认为'max'。</p></li>
<li><p>fp8_param_gather: 保持计算参数为 fp8（不使用任何其他中间数据类型），并在 fp8 格式下执行参数的 all-gather 操作。默认为False。</p>
<ul>
<li><p>提示：若想导出FP8权重格式，设置为True；否则设置为False。</p></li>
</ul>
</li>
</ul>
<p><strong>混合精度参数</strong>:</p>
<ul class="simple">
<li><p>fp16: fp16模式。默认为None，会根据模型的torch_dtype进行设置，即torch_dtype为float16或者float32则fp16设置为True。torch_dtype默认读取config.json。</p></li>
<li><p>bf16: bf16模式。默认为None，会根据模型的torch_dtype进行设置，即torch_dtype为bfloat16则bf16设置为True。</p></li>
<li><p>apply_query_key_layer_scaling: 将<code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">*</span> <span class="pre">K^T</span></code> 缩放为 <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">层数</span></code>（例如：第layer_num层则除以layer_num）。这对fp16训练很有帮助。默认为None，即若使用<code class="docutils literal notranslate"><span class="pre">--fp16</span></code>，则设置为True。</p></li>
<li><p>🔥attention_softmax_in_fp32: 在attention_mask和softmax中使用fp32进行计算。默认为True。</p></li>
<li><p>accumulate_allreduce_grads_in_fp32: 在 fp32 精度下进行梯度累积和全规约操作。如果开启<code class="docutils literal notranslate"><span class="pre">--bf16</span></code>且<code class="docutils literal notranslate"><span class="pre">main_params_dtype</span></code>为'fp32'，则设置为True。否则默认设置为False。</p></li>
</ul>
<p><strong>MoE参数</strong>:</p>
<ul class="simple">
<li><p>moe_router_load_balancing_type: 确定路由器的负载均衡策略。可选项为&quot;aux_loss&quot;、&quot;seq_aux_loss&quot;、&quot;global_aux_loss&quot;、&quot;sinkhorn&quot;、&quot;none&quot;。其中, &quot;global_aux_loss&quot;需要&quot;megatron-core&gt;=0.15&quot;。默认值为 None。从config.json中读取。</p></li>
<li><p>🔥moe_router_dtype: 用于路由计算和专家输出加权平均的数据类型。可选为'none', 'fp32'、'fp64'，这增强了数值稳定性，尤其是在专家数量较多时。与<code class="docutils literal notranslate"><span class="pre">moe_permute_fusion</span></code>一起使用时，性能影响可以忽略不计。默认为'fp32'。'none'代表不改变数据类型。</p></li>
<li><p>moe_token_dispatcher_type: 要使用的token分发器类型。可选选项包括 'allgather'、'alltoall'、'flex'和'alltoall_seq'。默认值为'alltoall'。</p></li>
<li><p>moe_enable_deepep: 启用 DeepEP 以实现 MoE 模型中的高效 token 调度和合并。仅在通过设置 <code class="docutils literal notranslate"><span class="pre">--moe_token_dispatcher_type</span> <span class="pre">flex</span></code> 使用弹性 token 调度器时有效。</p></li>
<li><p>🔥moe_grouped_gemm: 当每个rank包含多个专家时，通过在多个流中启动多个本地 GEMM 内核，利用 TransformerEngine中的GroupedLinear提高利用率和性能。默认为True。</p></li>
<li><p>🔥moe_permute_fusion: 在令牌分发过程中融合令牌重排操作。默认为False。</p></li>
<li><p>🔥moe_aux_loss_coeff: 默认为0，不使用aux_loss。<strong>通常情况下，该值设置的越大，训练效果越差，但MoE负载越均衡</strong>，请根据实验效果，选择合适的值。</p></li>
<li><p>moe_z_loss_coeff: z-loss 的缩放系数。默认为None。</p></li>
<li><p>🔥moe_shared_expert_overlap: 启用共享专家计算与调度器通信之间的重叠。如果不启用此选项，共享专家将在路由专家之后执行。仅在设置了<code class="docutils literal notranslate"><span class="pre">moe_shared_expert_intermediate_size</span></code>时有效。默认为False。</p></li>
<li><p>🔥moe_expert_capacity_factor: 每个专家的容量因子，None表示不会丢弃任何token。默认为None。通过设置 <code class="docutils literal notranslate"><span class="pre">--moe_expert_capacity_factor</span></code>，超出专家容量的 token 会基于其被选中的概率被丢弃。可以<strong>令训练负载均匀，提升训练速度</strong>（例如设置为1或2）。</p></li>
<li><p>moe_pad_expert_input_to_capacity: 对每个专家（expert）的输入进行填充，使其长度与专家容量（expert capacity length）对齐，默认为False。该操作仅在设置了 <code class="docutils literal notranslate"><span class="pre">--moe_expert_capacity_factor</span></code> 参数后才生效。</p></li>
<li><p>moe_token_drop_policy: 可选为'probs', 'position'。默认为'probs'。</p></li>
</ul>
<p><strong>MTP参数</strong></p>
<ul class="simple">
<li><p>mtp_num_layers: 多token预测（MTP）层的数量。MTP将每个位置的预测范围扩展到多个未来token。此MTP实现使用D个顺序模块依次预测D个额外的token。默认为None。（需要&quot;megatron-core&gt;=0.14&quot;）</p>
<ul>
<li><p>注意：mtp_num_layers的值，将不自动从config.json获取，需手动设置。你可以参考config.json中的<code class="docutils literal notranslate"><span class="pre">num_nextn_predict_layers</span></code>字段填写该值。使用mcore-bridge时，将优先从safetensors文件中加载MTP权重，若无法找到，则进行随机初始化。（若要使用blockwise fp8 + mtp，请使用mcore&gt;=0.15）</p></li>
</ul>
</li>
<li><p>mtp_loss_scaling_factor: 多token预测（MTP）损失的缩放因子。我们计算所有深度上MTP损失的平均值，然后乘以该缩放因子得到总体MTP损失，它将作为一个额外的训练目标。默认为0.1。</p></li>
</ul>
<p><strong>Tuner参数</strong>:</p>
<ul class="simple">
<li><p>tuner_type: 可选为'lora'和'full'。默认为'full'。（<strong>在ms-swift3.x中参数名为<code class="docutils literal notranslate"><span class="pre">train_type</span></code></strong>）</p></li>
<li><p>🔥freeze_llm: 该参数只对多模态模型生效，可用于全参数训练和LoRA训练，但会产生不同的效果。若是全参数训练，将freeze_llm设置为True会将LLM部分权重进行冻结；若是LoRA训练且<code class="docutils literal notranslate"><span class="pre">target_modules</span></code>设置为'all-linear'，将freeze_llm设置为True将会取消在LLM部分添加LoRA模块。该参数默认为False。</p></li>
<li><p>🔥freeze_vit: 该参数只对多模态模型生效，可用于全参数训练和LoRA训练，但会产生不同的效果。若是全参数训练，将freeze_vit设置为True会将vit部分权重进行冻结；若是LoRA训练且<code class="docutils literal notranslate"><span class="pre">target_modules</span></code>设置为'all-linear'，将freeze_vit设置为True将会取消在vit部分添加LoRA模块。该参数默认为True。</p>
<ul>
<li><p>注意：<strong>这里的vit不仅限于vision_tower, 也包括audio_tower</strong>。若是Omni模型，若你只希望对vision_tower加LoRA，而不希望对audio_tower加LoRA，你可以修改<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/a5d4c0a2ce0658cef8332d6c0fa619a52afa26ff/swift/llm/model/model_arch.py#L544-L554">这里的代码</a>。</p></li>
</ul>
</li>
<li><p>🔥freeze_aligner: 该参数只对多模态模型生效，可用于全参数训练和LoRA训练，但会产生不同的效果。若是全参数训练，将freeze_aligner设置为True会将aligner（也称为projector）部分权重进行冻结；若是LoRA训练且<code class="docutils literal notranslate"><span class="pre">target_modules</span></code>设置为'all-linear'，将freeze_aligner设置为True将会取消在aligner部分添加LoRA模块。该参数默认为True。</p></li>
</ul>
<p>全参数训练：</p>
<ul class="simple">
<li><p>freeze_parameters: 需要被冻结参数的前缀，默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。</p></li>
<li><p>freeze_parameters_regex: 需要被冻结参数的正则表达式，默认为None。</p></li>
<li><p>freeze_parameters_ratio: 从下往上冻结的参数比例，默认为0。可设置为1将所有参数冻结，结合<code class="docutils literal notranslate"><span class="pre">trainable_parameters</span></code>设置可训练参数。除了设置为0/1，该参数不兼容pp并行。</p></li>
<li><p>trainable_parameters: 额外可训练参数的前缀，默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。</p></li>
<li><p>trainable_parameters_regex: 匹配额外可训练参数的正则表达式，默认为None。</p></li>
</ul>
<p>lora训练：</p>
<ul class="simple">
<li><p>mcore_adapter: 加载adapter的权重路径，用于lora断点续训，默认为None。lora断点续训方式与全参数一致，请关注<code class="docutils literal notranslate"><span class="pre">--finetune</span></code>参数的含义。</p></li>
<li><p>🔥target_modules: 指定lora模块的后缀，例如：你可以设置为<code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">linear_qkv</span> <span class="pre">linear_proj</span></code>。默认为<code class="docutils literal notranslate"><span class="pre">['all-linear']</span></code>，代表将所有的linear设置为target_modules。</p>
<ul>
<li><p>注意：在LLM和多模态LLM中，'all-linear'的行为有所不同。若是LLM则自动寻找除lm_head外的linear并附加tuner；<strong>若是多模态LLM，则默认只在LLM上附加tuner，该行为可以被<code class="docutils literal notranslate"><span class="pre">freeze_llm</span></code>、<code class="docutils literal notranslate"><span class="pre">freeze_vit</span></code>、<code class="docutils literal notranslate"><span class="pre">freeze_aligner</span></code>控制</strong>。</p></li>
<li><p>注意：若需要将所有的router设置为target_modules, 你可以额外设置<code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">all-router</span> <span class="pre">...</span></code>，例如：<code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">all-router</span> <span class="pre">all-linear</span></code>。</p></li>
<li><p>transformers和Megatron的Linear层后缀名称不同，在Megatron中，<code class="docutils literal notranslate"><span class="pre">linear_proj</span></code>代表<code class="docutils literal notranslate"><span class="pre">o_proj</span></code>，<code class="docutils literal notranslate"><span class="pre">linear_qkv</span></code>代表<code class="docutils literal notranslate"><span class="pre">q_proj,</span> <span class="pre">k_proj,</span> <span class="pre">v_proj</span></code>的拼接，<code class="docutils literal notranslate"><span class="pre">linear_fc1</span></code>代表<code class="docutils literal notranslate"><span class="pre">gate_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">up_proj</span></code>的拼接，<code class="docutils literal notranslate"><span class="pre">linear_fc2</span></code>代表<code class="docutils literal notranslate"><span class="pre">down_proj</span></code>。</p></li>
</ul>
</li>
<li><p>🔥target_regex: 指定lora模块的regex表达式，默认为<code class="docutils literal notranslate"><span class="pre">None</span></code>。如果该值传入，则target_modules参数失效。</p></li>
<li><p>🔥modules_to_save: 在已附加tuner后，额外指定一部分原模型模块参与训练和存储。默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。例如设置为<code class="docutils literal notranslate"><span class="pre">--modules_to_save</span> <span class="pre">word_embeddings</span> <span class="pre">output_layer</span></code>，在LoRA训练中解开<code class="docutils literal notranslate"><span class="pre">word_embeddings</span></code>和<code class="docutils literal notranslate"><span class="pre">output_layer</span></code>层进行训练，这两部分的权重信息最终会进行保存。</p></li>
<li><p>🔥lora_rank: 默认为<code class="docutils literal notranslate"><span class="pre">8</span></code>。</p></li>
<li><p>🔥lora_alpha: 默认为<code class="docutils literal notranslate"><span class="pre">32</span></code>。</p></li>
<li><p>lora_dropout: 默认为<code class="docutils literal notranslate"><span class="pre">0.05</span></code>。</p></li>
<li><p>lora_bias: 默认为<code class="docutils literal notranslate"><span class="pre">'none'</span></code>，可以选择的值: 'none'、'all'。如果你要将bias全都设置为可训练，你可以设置为<code class="docutils literal notranslate"><span class="pre">'all'</span></code>。</p></li>
<li><p>use_rslora: 默认为<code class="docutils literal notranslate"><span class="pre">False</span></code>，是否使用<code class="docutils literal notranslate"><span class="pre">RS-LoRA</span></code>。</p></li>
</ul>
<p><strong>Mcore-Bridge参数</strong></p>
<ul class="simple">
<li><p>model: safetensors权重的model_id或者model_path。默认为None。</p></li>
<li><p>model_type: 模型类型。介绍参考<a class="reference internal" href="../Instruction/Command-line-parameters.html"><span class="doc">ms-swift命令行参数文档</span></a>。</p></li>
<li><p>🔥save_safetensors: 默认为True，是否直接保存成safetensors权重。若设置了<code class="docutils literal notranslate"><span class="pre">--no_save_optim</span> <span class="pre">false</span></code>则额外mcore格式权重和优化器权重（也保存在output_dir中）。断点续训时使用<code class="docutils literal notranslate"><span class="pre">--mcore_model/--mcore_adapter/--no_load_optim/--no_load_rng</span></code>参数加载mcore格式权重。</p></li>
<li><p>adapters: safetensors格式的LoRA增量权重的adapter_id或者adapter_path。默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。</p></li>
<li><p>ref_model: ref_model safetensors权重的model_id或者model_path。采用grpo、dpo、kto算法且使用全参数训练时需要传入。默认为None，设置为<code class="docutils literal notranslate"><span class="pre">--model</span></code>。</p></li>
<li><p>ref_adapters: ref_adapters safetensors权重的adapter_id或者adapter_path的列表（目前只支持长度为1），默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。</p></li>
<li><p>use_hf: 控制模型下载、数据集下载、模型推送使用ModelScope还是HuggingFace。默认为False，使用ModelScope。</p></li>
<li><p>hub_token: hub token. modelscope的hub token可以查看<a class="reference external" href="https://modelscope.cn/my/myaccesstoken">这里</a>。默认为None。</p></li>
<li><p>merge_lora: 是否存储合并后的权重。默认为None，若<code class="docutils literal notranslate"><span class="pre">save_safetensors</span></code>设置为True，该参数默认值为<code class="docutils literal notranslate"><span class="pre">True</span></code>，否则为False。即默认情况下，存储为safetensors格式时会合并LoRA；存储为torch_dist格式时，不会合并LoRA。</p></li>
<li><p>max_shard_size: safetensors格式存储文件最大大小，默认'5GB'。</p></li>
<li><p>🔥offload_bridge: Megatron导出的用于vLLM更新HF格式权重使用CPU主存存放，以降低 GPU 显存占用。默认为 False。（在GRPO/GKD算法中生效）</p></li>
</ul>
<p><strong>多模态参数</strong>:</p>
<ul class="simple">
<li><p>vit_gradient_checkpointing: 多模态模型训练时，是否对vit部分开启gradient_checkpointing。默认为True。（<strong>Megatron-SWIFT的vit实现使用transformers实现</strong>）</p></li>
<li><p>attn_impl: 多模态模型训练时，设置vit部分的attn_impl实现。默认为'flash_attn'。</p></li>
<li><p>vit_lr: 当训练多模态大模型时，该参数指定vit的学习率，默认为None，等于learning_rate。通常与<code class="docutils literal notranslate"><span class="pre">--freeze_vit</span></code>、<code class="docutils literal notranslate"><span class="pre">--freeze_aligner</span></code>参数结合使用。</p>
<ul>
<li><p>提示：在日志中打印的&quot;learning rate&quot;为llm的学习率。</p></li>
</ul>
</li>
<li><p>aligner_lr: 当训练多模态大模型时，该参数指定aligner的学习率，默认为None，等于learning_rate。</p></li>
<li><p>gradient_checkpointing_kwargs: 传入<code class="docutils literal notranslate"><span class="pre">torch.utils.checkpoint</span></code>中的参数。例如设置为<code class="docutils literal notranslate"><span class="pre">--gradient_checkpointing_kwargs</span> <span class="pre">'{&quot;use_reentrant&quot;:</span> <span class="pre">false}'</span></code>。默认为None。该参数只对<code class="docutils literal notranslate"><span class="pre">vit_gradient_checkpointing</span></code>生效。</p></li>
</ul>
<p><strong>其他参数</strong>:</p>
<ul class="simple">
<li><p>check_model: 检查本地模型文件有损坏或修改并给出提示，默认为True。<strong>如果是断网环境，请设置为False</strong>。</p></li>
<li><p>rope_scaling: rope_scaling相关参数，默认为None。格式参考<a class="reference external" href="https://modelscope.cn/models/LLM-Research/Meta-Llama-3.1-8B-Instruct/file/view/master?fileName=config.json&amp;status=1">llama3.1 config.json</a>，传入json字符串。</p>
<ul>
<li><p><strong>目前rope_scaling模块使用transformers实现，支持transformers支持的所有rope_scaling。</strong></p></li>
</ul>
</li>
<li><p>apply_wd_to_qk_layernorm: 用于Qwen3-Next/Qwen3.5全参数训练，对 qk layernorm 应用权重衰减。默认为False。</p></li>
<li><p>enable_dft_loss: 是否在SFT训练中使用<a class="reference external" href="https://arxiv.org/abs/2508.05629">DFT</a> (Dynamic Fine-Tuning) loss，默认为False。</p></li>
<li><p>enable_channel_loss: 启用channel loss，默认为<code class="docutils literal notranslate"><span class="pre">False</span></code>。你需要在数据集中准备&quot;channel&quot;字段，ms-swift会根据该字段分组统计loss（若未准备&quot;channel&quot;字段，则归为默认<code class="docutils literal notranslate"><span class="pre">None</span></code> channel）。数据集格式参考<a class="reference external" href="../Customization/Custom-dataset.md#channel-loss">channel loss</a>。channel loss兼容packing/padding_free/loss_scale等技术。</p></li>
<li><p>🔥task_type: 默认为'causal_lm'。可选为'causal_lm'、'seq_cls'、'embedding'和'generative_reranker'。</p></li>
<li><p>num_labels: 分类模型（即<code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">seq_cls</span></code>）需要指定该参数。代表标签数量，默认为None。</p></li>
<li><p>problem_type: 分类模型（即<code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">seq_cls</span></code>）需要指定该参数。可选为'regression', 'single_label_classification', 'multi_label_classification'。默认为None，若模型为 reward_model 或 num_labels 为1，该参数为'regression'，其他情况，该参数为'single_label_classification'。</p></li>
<li><p>🔥save_strategy: 保存策略，可选项为'steps'和'epoch'。默认为'steps'。当设置为'epoch'时，会根据数据集大小自动计算<code class="docutils literal notranslate"><span class="pre">save_steps</span></code>和<code class="docutils literal notranslate"><span class="pre">eval_steps</span></code>以实现每个epoch保存一次，用户传入的<code class="docutils literal notranslate"><span class="pre">save_steps</span></code>和<code class="docutils literal notranslate"><span class="pre">eval_steps</span></code>参数值将被忽略。</p></li>
<li><p>callbacks: 自定义trainer callback，默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。</p></li>
</ul>
</section>
<section id="id2">
<h2>训练参数<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<p>Megatron训练参数继承自Megatron参数和基本参数（<strong>与ms-swift共用dataset、template等参数，也支持ms-swift中的特定模型参数</strong>）。基本参数的内容可以参考<a class="reference external" href="../Instruction/Command-line-parameters.md#%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0">这里</a>。此外还包括以下参数：</p>
<ul class="simple">
<li><p>add_version: 在<code class="docutils literal notranslate"><span class="pre">save</span></code>上额外增加目录<code class="docutils literal notranslate"><span class="pre">'&lt;版本号&gt;-&lt;时间戳&gt;'</span></code>防止权重覆盖，默认为True。</p></li>
<li><p>🔥create_checkpoint_symlink: 额外创建checkpoint软链接，方便书写自动化训练脚本。best_model和last_model的软链接路径分别为f'{output_dir}/best'和f'{output_dir}/last'。</p></li>
<li><p>🔥packing: 使用<code class="docutils literal notranslate"><span class="pre">padding_free</span></code>的方式将不同长度的数据样本打包成<strong>近似</strong>统一长度的样本（packing能保证不对完整的序列进行切分），实现训练时各节点与进程的负载均衡（避免长文本拖慢短文本的训练速度），从而提高GPU利用率，保持显存占用稳定。当使用 <code class="docutils literal notranslate"><span class="pre">--attention_backend</span> <span class="pre">flash</span></code> 时，可确保packed样本内的不同序列之间相互独立，互不可见（除Qwen3-Next，因为含有linear-attention）。该参数默认为<code class="docutils literal notranslate"><span class="pre">False</span></code>。Megatron-SWIFT的所有训练任务都支持该参数。注意：<strong>packing会导致数据集样本数减少，请自行调节梯度累加数和学习率</strong>。</p></li>
<li><p>packing_length: packing的长度。默认为None，设置为max_length。</p></li>
<li><p>packing_num_proc: packing的进程数，默认为1。需要注意的是，不同的<code class="docutils literal notranslate"><span class="pre">packing_num_proc</span></code>，最终形成的packed数据集是不同的。（该参数在流式packing时不生效）。通常不需要修改该值，packing速度远快于tokenize速度。</p></li>
<li><p>streaming: 流式读取并处理数据集，默认False。（流式数据集的随机并不彻底，可能导致loss波动剧烈。）</p>
<ul>
<li><p>注意：因为流式数据集无法获得其长度，因此需要设置<code class="docutils literal notranslate"><span class="pre">--train_iters</span></code>参数。设置<code class="docutils literal notranslate"><span class="pre">num_train_epochs</span></code>参数确保训练到对应epochs时退出训练，并对权重进行验证和保存。</p></li>
<li><p>注意：流式数据集可以跳过预处理等待，将预处理时间与训练时间重叠。流式数据集的预处理只在rank0上进行，并通过数据分发的方式同步到其他进程，<strong>其通常效率不如非流式数据集采用的数据分片读取方式</strong>。当训练的world_size较大时，预处理和数据分发将成为训练瓶颈。</p></li>
</ul>
</li>
<li><p>lazy_tokenize: 是否使用lazy_tokenize。若该参数设置为False，则在训练之前对所有的数据集样本进行tokenize（多模态模型则包括从磁盘中读取图片）。该参数默认为None，在LLM训练中默认为False，而MLLM训练默认为True，节约内存。</p></li>
<li><p>new_special_tokens: 需要新增的特殊tokens。默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。例子参考<a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/megatron/lora/new_special_tokens.sh">这里</a>。</p>
<ul>
<li><p>注意：你也可以传入以<code class="docutils literal notranslate"><span class="pre">.txt</span></code>结尾的文件路径，每行为一个special token。</p></li>
</ul>
</li>
</ul>
</section>
<section id="rlhf">
<h2>RLHF参数<a class="headerlink" href="#rlhf" title="Link to this heading"></a></h2>
<p>除了继承训练参数外，还支持以下参数：</p>
<ul class="simple">
<li><p>🔥rlhf_type: 默认为'dpo'。目前可选择为'dpo'、'grpo'、'kto'、'rm'和'gkd'。</p></li>
<li><p>loss_scale: 覆盖<a class="reference internal" href="../Instruction/Command-line-parameters.html"><span class="doc">基本参数</span></a>中的loss_scale。默认为'last_round'。</p></li>
<li><p>calculate_per_token_loss: 覆盖Megatron参数，默认为False。</p></li>
</ul>
<section id="dpo">
<h3>DPO参数<a class="headerlink" href="#dpo" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>mcore_ref_model: ref_model的加载路径。采用DPO/GRPO/KTO算法且使用全参数训练时需要传入。默认为None，即设置为<code class="docutils literal notranslate"><span class="pre">mcore_model</span></code>。</p></li>
<li><p>mcore_ref_adapter: 加载ref_adapter的权重路径，默认为None。若你要使用SFT产生的LoRA权重进行DPO，，请在训练时设置<code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span> <span class="pre">sft_ckpt</span> <span class="pre">--mcore_ref_adapter</span> <span class="pre">sft_ckpt</span> <span class="pre">--finetune</span> <span class="pre">true</span></code>。若是此场景的断点续训，则设置<code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span> <span class="pre">rlhf_ckpt</span> <span class="pre">--mcore_ref_adapter</span> <span class="pre">sft_ckpt</span> <span class="pre">--finetune</span> <span class="pre">false</span></code>。</p></li>
<li><p>beta: 含义与<a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#trl.DPOConfig">TRL</a>相同。控制与参考模型偏差程度的参数。beta值越高，表示与参考模型的偏差越小。对于 IPO 损失函数 (loss_type=&quot;ipo&quot;)，beta是<a class="reference external" href="https://huggingface.co/papers/2310.12036">论文</a>中所指的正则化参数。默认为0.1。</p></li>
<li><p>🔥rpo_alpha: 来自<a class="reference external" href="https://huggingface.co/papers/2404.19733">RPO 论文</a>中的参数，用于控制损失函数中NLL项的权重（即SFT损失），<code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">dpo_loss</span> <span class="pre">+</span> <span class="pre">rpo_alpha</span> <span class="pre">*</span> <span class="pre">sft_loss</span></code>，论文中推荐设置为<code class="docutils literal notranslate"><span class="pre">1.</span></code>。默认为<code class="docutils literal notranslate"><span class="pre">None</span></code>，即默认不引入sft_loss。</p></li>
<li><p>reference_free: 是否忽略提供的参考模型，并隐式地使用一个对所有响应赋予相等概率的参考模型。默认为False。</p></li>
<li><p>label_smoothing: 默认为0.。</p></li>
<li><p>f_divergence_type: 默认为<code class="docutils literal notranslate"><span class="pre">reverse_kl</span></code>。可选值参考<a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer">TRL文档</a>。</p></li>
<li><p>loss_type: 默认为'sigmoid'。可选值参考<a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions">TRL文档</a>。</p></li>
</ul>
</section>
<section id="kto">
<h3>KTO参数<a class="headerlink" href="#kto" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>mcore_ref_model: 含义同DPO。</p></li>
<li><p>mcore_ref_adapter: 含义同DPO。</p></li>
<li><p>beta: 控制与 ref_model 偏离程度的参数。较高的 beta 表示与 ref_model 偏离更小。默认为<code class="docutils literal notranslate"><span class="pre">0.1</span></code>。</p></li>
<li><p>loss_type: 默认为'kto'。可选值参考<a class="reference external" href="https://huggingface.co/docs/trl/main/en/kto_trainer#trl.KTOConfig.loss_type">TRL文档</a>。</p></li>
<li><p>desirable_weight: 抵消 desirable 和 undesirable 数量不均衡的影响，对 desirable 损失按该系数进行加权，默认为<code class="docutils literal notranslate"><span class="pre">1.</span></code>。</p></li>
<li><p>undesirable_weight: 抵消 desirable 和 undesirable 数量不均衡的影响，对 undesirable 损失按该系数进行加权，默认为<code class="docutils literal notranslate"><span class="pre">1.</span></code>。</p></li>
</ul>
</section>
<section id="rm">
<h3>RM参数<a class="headerlink" href="#rm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>center_rewards_coefficient: 用于激励奖励模型输出均值为零的奖励的系数，具体查看这篇<a class="reference external" href="https://huggingface.co/papers/2312.09244">论文</a>。推荐值：0.01。</p></li>
</ul>
</section>
<section id="grpo">
<h3>GRPO参数<a class="headerlink" href="#grpo" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>mcore_ref_model: 含义同DPO。</p></li>
<li><p>mcore_ref_adapter: 含义同DPO。</p></li>
<li><p>beta: KL正则系数，默认为0.04，设置为0时不加载ref model。</p></li>
<li><p>micro_batch_size: 每个device的批次大小，默认为1。</p></li>
<li><p>global_batch_size: 总批次大小，等价于<code class="docutils literal notranslate"><span class="pre">micro_batch_size*数据并行大小*梯度累加步数</span></code>。默认为16。</p></li>
<li><p>steps_per_generation: 每轮生成的优化步数，即采样批量大小相对global_batch_size的倍数，默认为1。</p></li>
<li><p>generation_batch_size: 采样批量大小，需要是global_batch_size的倍数，默认等于<code class="docutils literal notranslate"><span class="pre">global_batch_size*steps_per_generation</span></code>。</p></li>
<li><p>num_generations: 每个prompt采样的数量，论文中的G值，默认为8。</p></li>
<li><p>num_generations_eval: 评估阶段每个prompt采样的数量。允许在评估时使用较少的生成数量以节省计算资源。如果为 None，则使用 num_generations 的值。默认为 None。</p></li>
<li><p>reward_funcs: GRPO算法奖励函数，可选项为<code class="docutils literal notranslate"><span class="pre">accuracy</span></code>、<code class="docutils literal notranslate"><span class="pre">format</span></code>、<code class="docutils literal notranslate"><span class="pre">cosine</span></code>、<code class="docutils literal notranslate"><span class="pre">repetition</span></code>和<code class="docutils literal notranslate"><span class="pre">soft_overlong</span></code>，见swift/rewards/orm.py。你也可以在plugin中自定义自己的奖励函数。默认为<code class="docutils literal notranslate"><span class="pre">[]</span></code>。</p></li>
<li><p>reward_weights: 每个奖励函数的权重。必须与奖励函数和奖励模型的总数量匹配。默认为 None，即所有奖励的权重都相等，为<code class="docutils literal notranslate"><span class="pre">1.0</span></code>。</p>
<ul>
<li><p>提示：如果GRPO训练中包含<code class="docutils literal notranslate"><span class="pre">--reward_model</span></code>，则其加在奖励函数的最后位置。</p></li>
</ul>
</li>
<li><p>truncation_strategy: 对输入长度超过 <code class="docutils literal notranslate"><span class="pre">max_length</span></code>的处理方式，支持<code class="docutils literal notranslate"><span class="pre">delete</span></code>和<code class="docutils literal notranslate"><span class="pre">left</span></code>，代表删除、左侧裁剪，默认为<code class="docutils literal notranslate"><span class="pre">left</span></code>。注意对于多模态模型，左裁剪可能会裁剪掉多模态token导致模型前向报错shape mismatch。使用<code class="docutils literal notranslate"><span class="pre">delete</span></code>方式，对于超长数据和编码失败的样例会在原数据集中重采样其他数据作为补充。</p></li>
<li><p>loss_type: loss 归一化的类型，可选项为['grpo', 'bnpo', 'dr_grpo'], 默认为'grpo', 具体查看该<a class="reference external" href="https://github.com/huggingface/trl/pull/3256#discussion_r2033213348">pr</a>。</p></li>
<li><p>log_completions: 是否记录训练中的模型生成内容，默认为False。</p></li>
<li><p>vllm_mode: vLLM 集成模式，可选项为 <code class="docutils literal notranslate"><span class="pre">server</span></code> 和 <code class="docutils literal notranslate"><span class="pre">colocate</span></code>。server 模式使用 <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">rollout</span></code> 拉起的 vLLM 服务器进行采样，colocate 模式在程序内部署 vLLM。使用server端时，</p></li>
<li><p>vllm_mode server 参数</p>
<ul>
<li><p>vllm_server_host: vLLM server host地址，默认为None。</p></li>
<li><p>vllm_server_port: vLLM server 服务端口，默认为8000。</p></li>
<li><p>vllm_server_base_url: vLLM server的Base URL(比如 http://local_host:8000), 默认为None。设置后，忽略host和port设置。</p></li>
<li><p>vllm_server_timeout: 连接vLLM server的超时时间，默认为 240s。</p></li>
<li><p>vllm_server_pass_dataset: 透传额外的数据集信息到vLLM server，用于多轮训练。</p></li>
<li><p>async_generate: 异步rollout以提高训练速度，注意开启时采样会使用上一轮更新的模型进行采样，不支持多轮场景。默认<code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>SWIFT_UPDATE_WEIGHTS_BUCKET_SIZE: 环境变量，用于控制权重同步时的传输桶大小（bucket size），适用于 Server Mode 下的全参数训练，单位为 MB，默认值为 512 MB。</p></li>
</ul>
</li>
<li><p>vllm_mode colocate 参数（更多参数支持参考<a class="reference external" href="#vLLM%E5%8F%82%E6%95%B0">vLLM参数</a>。）</p>
<ul>
<li><p>vllm_gpu_memory_utilization: vllm透传参数，默认为0.9。</p></li>
<li><p>vllm_max_model_len: vllm透传参数，默认为None。</p></li>
<li><p>vllm_enforce_eager: vllm透传参数，默认为False。</p></li>
<li><p>vllm_limit_mm_per_prompt: vllm透传参数，默认为None。</p></li>
<li><p>vllm_enable_prefix_caching: vllm透传参数，默认为True。</p></li>
<li><p>vllm_tensor_parallel_size: tp并行数，默认为<code class="docutils literal notranslate"><span class="pre">1</span></code>。</p></li>
<li><p>vllm_enable_lora: 支持vLLM Engine 加载 LoRA adapter，默认为False。用于加速LoRA训练的权重同步，具体参考<a class="reference external" href="../Instruction/GRPO/GetStarted/GRPO.md#%E6%9D%83%E9%87%8D%E5%90%8C%E6%AD%A5%E5%8A%A0%E9%80%9F">文档</a>。</p></li>
<li><p>sleep_level: 训练时释放 vLLM 显存，可选项为[0, 1, 2], 默认为0，不释放。</p></li>
<li><p>offload_optimizer: 是否在vLLM推理时offload optimizer参数，默认为False。</p></li>
<li><p>offload_model: 是否在vLLM推理时 offload 模型，默认为False。</p></li>
</ul>
</li>
<li><p>num_iterations: 每条数据的更新次数，<a class="reference external" href="https://arxiv.org/abs/2402.03300">GRPO论文</a>中的 $\mu$ 值，默认为1。</p></li>
<li><p>epsilon: clip 系数，默认为0.2。</p></li>
<li><p>epsilon_high: upper clip 系数，默认为None，设置后与epsilon共同构成[epsilon, epsilon_high]裁剪范围。</p></li>
<li><p>dynamic_sample: 筛除group内奖励标准差为0的数据，额外采样新数据，默认为False。</p></li>
<li><p>max_resample_times: dynamic_sample设置下限制重采样次数，默认3次。</p></li>
<li><p>overlong_filter: 跳过超长截断的样本，不参与loss计算，默认为False。</p></li>
<li><p>delta: <a class="reference external" href="https://huggingface.co/papers/2505.07291">INTELLECT-2 tech report</a>中双侧 GRPO 上界裁剪值。若设置，建议大于 1 + epsilon。默认为None。</p></li>
<li><p>importance_sampling_level: 控制重要性采样比计算，可选项为 <code class="docutils literal notranslate"><span class="pre">token</span></code> 和 <code class="docutils literal notranslate"><span class="pre">sequence</span></code>，<code class="docutils literal notranslate"><span class="pre">token</span></code> 模式下保留原始的每个 token 的对数概率比，<code class="docutils literal notranslate"><span class="pre">sequence</span></code> 模式下则会对序列中所有有效 token 的对数概率比进行平均。<a class="reference external" href="https://arxiv.org/abs/2507.18071">GSPO论文</a>中使用sequence级别计算来稳定训练，默认为<code class="docutils literal notranslate"><span class="pre">token</span></code>。</p></li>
<li><p>scale_rewards: 指定奖励的缩放策略。可选值包括 <code class="docutils literal notranslate"><span class="pre">group</span></code>（按组内标准差缩放）、<code class="docutils literal notranslate"><span class="pre">batch</span></code>（按整个批次的标准差缩放）、<code class="docutils literal notranslate"><span class="pre">none</span></code>（不进行缩放）、<code class="docutils literal notranslate"><span class="pre">gdpo</span></code>（对每个奖励函数分别进行组内归一化后加权聚合，参考 <a class="reference external" href="https://arxiv.org/abs/2601.05242">GDPO 论文</a>）。在 ms-swift &lt; 3.10 版本中，该参数为布尔类型，<code class="docutils literal notranslate"><span class="pre">true</span></code> 对应 <code class="docutils literal notranslate"><span class="pre">group</span></code>，<code class="docutils literal notranslate"><span class="pre">false</span></code> 对应 <code class="docutils literal notranslate"><span class="pre">none</span></code>。默认值与 <code class="docutils literal notranslate"><span class="pre">advantage_estimator</span></code> 绑定：<code class="docutils literal notranslate"><span class="pre">grpo</span></code> 对应 <code class="docutils literal notranslate"><span class="pre">group</span></code>，<code class="docutils literal notranslate"><span class="pre">rloo</span></code> 对应 <code class="docutils literal notranslate"><span class="pre">none</span></code>，<code class="docutils literal notranslate"><span class="pre">reinforce_plus_plus</span></code> 对应 <code class="docutils literal notranslate"><span class="pre">batch</span></code>。</p>
<ul>
<li><p>注意：<code class="docutils literal notranslate"><span class="pre">gdpo</span></code> 模式不支持 <code class="docutils literal notranslate"><span class="pre">kl_in_reward=True</span></code>，若同时设置会自动将 <code class="docutils literal notranslate"><span class="pre">kl_in_reward</span></code> 设为 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p></li>
<li><p>GDPO 适用于多奖励优化场景：当使用多个奖励函数时，GDPO 会对每个奖励函数分别在组内进行标准化（减均值、除标准差），然后使用 <code class="docutils literal notranslate"><span class="pre">reward_weights</span></code> 进行加权求和，最后再进行批次级别的标准化。这种方式可以更好地保留各个奖励的相对差异，避免不同奖励组合坍塌成相同的 advantage 值。</p></li>
</ul>
</li>
<li><p>rollout_importance_sampling_mode: 训推不一致校正模式，可选项为 <code class="docutils literal notranslate"><span class="pre">token_truncate</span></code>、<code class="docutils literal notranslate"><span class="pre">token_mask</span></code>、<code class="docutils literal notranslate"><span class="pre">sequence_truncate</span></code>、<code class="docutils literal notranslate"><span class="pre">sequence_mask</span></code>。默认为None，不启用校正。具体参考<a class="reference internal" href="../Instruction/GRPO/AdvancedResearch/training_inference_mismatch.html"><span class="doc">文档</span></a>。</p></li>
<li><p>rollout_importance_sampling_threshold: 重要性采样权重的阈值，用于截断或屏蔽极端权重。默认为2.0。</p></li>
<li><p>log_rollout_offpolicy_metrics: 当 <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code> 未设置时，是否记录训推不一致诊断指标（KL、PPL、χ²等）。当设置了 <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code> 时，指标会自动记录。默认为False。</p></li>
<li><p>off_policy_sequence_mask_delta: Off-Policy Sequence Masking 阈值，来自 DeepSeek-V3.2 论文。当设置此值时，会计算每个序列的 <code class="docutils literal notranslate"><span class="pre">mean(old_policy_logps</span> <span class="pre">-</span> <span class="pre">policy_logps)</span></code>，若该值大于阈值且该序列的优势为负，则 mask 掉该序列不参与损失计算。默认为None，不启用。具体参考<a class="reference external" href="../Instruction/GRPO/AdvancedResearch/training_inference_mismatch.md#off-policy-sequence-masking">文档</a>。</p></li>
</ul>
<p>内置奖励函数参数参考<a class="reference external" href="../Instruction/Command-line-parameters.md#%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0">文档</a></p>
</section>
<section id="gkd">
<h3>GKD参数<a class="headerlink" href="#gkd" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>teacher_model: 教师模型的路径或模型 ID，必需参数。</p></li>
<li><p>teacher_model_type: 教师模型类型，默认为None，自动检测。</p></li>
<li><p>teacher_model_revision: 教师模型版本，默认为None。</p></li>
<li><p>beta: JSD 散度插值系数。0.0 代表 Forward KL，0.5 代表对称 JSD，1.0 代表 Reverse KL。默认为0.5。</p></li>
<li><p>lmbda: On-Policy 学习触发概率。0.0 代表纯 Off-Policy，1.0 代表纯 On-Policy。默认为0.5。</p></li>
<li><p>seq_kd: 是否使用教师生成的响应（Sequential KD），当前暂不支持。默认为False。</p></li>
<li><p>temperature: 用于采样和损失计算的温度参数。默认为0.9。</p></li>
<li><p>offload_teacher_model: 是否将教师模型卸载到 CPU 以节省 GPU 显存。默认为False。</p></li>
<li><p>sft_alpha: SFT 损失的混合系数，<code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">jsd_loss</span> <span class="pre">+</span> <span class="pre">sft_alpha</span> <span class="pre">*</span> <span class="pre">sft_loss</span></code>。当使用数据集响应（Off-Policy）时生效。默认为0。</p></li>
<li><p>max_completion_length: 生成时的最大 token 数。默认为512。</p></li>
<li><p>vllm_mode: 同 GRPO 参数，用于 On-Policy 生成。colocate 模式下在程序内部署 vLLM。</p>
<ul>
<li><p>注意：On-Policy 生成需要启用 vLLM（<code class="docutils literal notranslate"><span class="pre">--use_vllm</span> <span class="pre">true</span> <span class="pre">--vllm_mode</span> <span class="pre">colocate/server</span></code>）。</p></li>
<li><p>当 <code class="docutils literal notranslate"><span class="pre">lmbda</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> 但未启用 vLLM 时，将自动回退到 Off-Policy 模式。</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id3">
<h2>导出参数<a class="headerlink" href="#id3" title="Link to this heading"></a></h2>
<p>这里介绍<code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code>的参数，若要使用<code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code>导出命令，请参考<a class="reference external" href="../Instruction/Command-line-parameters.md#%E5%AF%BC%E5%87%BA%E5%8F%82%E6%95%B0">ms-swift命令行参数文档</a>。<code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code>相比<code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code>，支持分布式和多机导出。Megatron导出参数继承自Megatron参数和基本参数。</p>
<ul class="simple">
<li><p>🔥to_mcore: HF格式权重转成Megatron格式。默认为False。</p></li>
<li><p>🔥to_hf: Megatron格式权重转成HF格式。默认为False。</p></li>
<li><p>🔥merge_lora: 默认为None，若<code class="docutils literal notranslate"><span class="pre">to_hf</span></code>设置为True，该参数默认值为<code class="docutils literal notranslate"><span class="pre">True</span></code>，否则为False。即默认情况下，存储为safetensors格式时会合并LoRA；存储为torch_dist格式时，不会合并LoRA。合并后的权重存储在<code class="docutils literal notranslate"><span class="pre">--save</span></code>目录下。</p>
<ul>
<li><p>注意：transformers 5.0对Moe的模型组织结构进行了重构，该结构不支持Moe LoRA的推理，可能造成推理异常。<strong>建议对Moe模型进行Merge LoRA</strong>（vLLM不受影响）。</p></li>
<li><p>注意：由于transformers和Megatron模型专家结构并不一定一致（例如transformers的Qwen3-VL-Moe的专家部分并不是Linear实现，而是Parameters），因此部分模型无法转换（若Qwen3-VL-Moe只设置linear_proj和linear_qkv训练LoRA也支持转换）。但大多数的模型支持LoRA转换，例如：Qwen3-Moe，Qwen3-Omni-Moe，GLM4.5-V等。</p></li>
</ul>
</li>
<li><p>🔥test_convert_precision: 测试HF和Megatron格式权重转换的精度误差。默认为False。</p></li>
<li><p>test_convert_dtype: 转换精度测试使用的dtype，默认为'float32'。</p></li>
<li><p>exist_ok: 如果<code class="docutils literal notranslate"><span class="pre">args.save</span></code>存在，不抛出异常，进行覆盖。默认为False。</p></li>
<li><p>device_map: 设置<code class="docutils literal notranslate"><span class="pre">--test_convert_precision</span> <span class="pre">true</span></code>时生效，控制HF模型的加载位置，默认为'auto'。你可以设置为'cpu'节约显存资源。</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024, Ascend。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>