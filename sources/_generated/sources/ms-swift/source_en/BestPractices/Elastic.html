

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Elastic &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Elastic</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/BestPractices/Elastic.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="elastic">
<h1>Elastic<a class="headerlink" href="#elastic" title="Link to this heading">ÔÉÅ</a></h1>
<section id="installing-dependencies">
<h2>Installing Dependencies<a class="headerlink" href="#installing-dependencies" title="Link to this heading">ÔÉÅ</a></h2>
<p>Deploy a K8S cluster and deploy <a class="reference external" href="https://github.com/intelligent-machine-learning/dlrover">DLRover</a> in the cluster, and install the required packages using <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">dlrover</span> <span class="pre">&amp;&amp;</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">tornado</span> <span class="pre">&amp;&amp;</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">kubernetes</span> <span class="pre">&amp;&amp;</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">ms-swift</span></code></p>
<p>Other dependencies and versions verified through repeated testing in the training image:
deepspeed 0.16.5 (refer to this <a class="reference external" href="https://github.com/deepspeedai/DeepSpeed/pull/7585/files">PR</a> to fix issues related to universal checkpoint)
pytorch 2.6.0</p>
</section>
<section id="how-to-start">
<h2>How to Start<a class="headerlink" href="#how-to-start" title="Link to this heading">ÔÉÅ</a></h2>
<p>Enable elastic training by adding the <code class="docutils literal notranslate"><span class="pre">deepspeed_elastic</span></code> callback (optionally <code class="docutils literal notranslate"><span class="pre">graceful_exit</span></code>) in <code class="docutils literal notranslate"><span class="pre">--callbacks</span></code>, and configure DeepSpeed elasticity settings.</p>
<p>The command format is dlrover-run + DLrover command parameters + Swift startup command + Swift parameters.dlrover-run behaves like torchrun for most arguments, except for its custom parameters.</p>
<p>The dlrover-run arguments are as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">dlrover</span><span class="o">-</span><span class="n">run</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">nnodes</span> <span class="n">NNODES</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">nproc</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">node</span> <span class="n">NPROC_PER_NODE</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">rdzv</span><span class="o">-</span><span class="n">backend</span> <span class="n">RDZV_BACKEND</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">rdzv</span><span class="o">-</span><span class="n">endpoint</span> <span class="n">RDZV_ENDPOINT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">rdzv</span><span class="o">-</span><span class="nb">id</span> <span class="n">RDZV_ID</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">rdzv</span><span class="o">-</span><span class="n">conf</span> <span class="n">RDZV_CONF</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">standalone</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="nb">max</span><span class="o">-</span><span class="n">restarts</span> <span class="n">MAX_RESTARTS</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">monitor</span><span class="o">-</span><span class="n">interval</span> <span class="n">MONITOR_INTERVAL</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">start</span><span class="o">-</span><span class="n">method</span> <span class="p">{</span><span class="n">spawn</span><span class="p">,</span><span class="n">fork</span><span class="p">,</span><span class="n">forkserver</span><span class="p">}]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">role</span> <span class="n">ROLE</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">m</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">python</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">run</span><span class="o">-</span><span class="n">path</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">log</span><span class="o">-</span><span class="nb">dir</span> <span class="n">LOG_DIR</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">r</span> <span class="n">REDIRECTS</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">-</span><span class="n">t</span> <span class="n">TEE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">local</span><span class="o">-</span><span class="n">ranks</span><span class="o">-</span><span class="nb">filter</span> <span class="n">LOCAL_RANKS_FILTER</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">node</span><span class="o">-</span><span class="n">rank</span> <span class="n">NODE_RANK</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">master</span><span class="o">-</span><span class="n">addr</span> <span class="n">MASTER_ADDR</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">master</span><span class="o">-</span><span class="n">port</span> <span class="n">MASTER_PORT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">local</span><span class="o">-</span><span class="n">addr</span> <span class="n">LOCAL_ADDR</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">logs</span><span class="o">-</span><span class="n">specs</span> <span class="n">LOGS_SPECS</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">precheck</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">}]</span> <span class="p">[</span><span class="o">--</span><span class="n">node_unit</span> <span class="n">NODE_UNIT</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">auto_config</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">auto_tunning</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">exclude</span><span class="o">-</span><span class="n">straggler</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">save_at_breakpoint</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">accelerator</span> <span class="p">{</span><span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gpu</span><span class="p">,</span><span class="n">ascend</span><span class="o">-</span><span class="n">npu</span><span class="p">}]</span> <span class="p">[</span><span class="o">--</span><span class="n">training_port</span> <span class="n">TRAINING_PORT</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">switchbox</span><span class="o">-</span><span class="n">check</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">box</span><span class="o">-</span><span class="n">pairs</span> <span class="n">PAIR</span> <span class="p">[</span><span class="n">PAIR</span> <span class="o">...</span><span class="p">]]</span> <span class="p">[</span><span class="o">--</span><span class="nb">min</span><span class="o">-</span><span class="n">bandwidth</span> <span class="n">MIN_BANDWIDTH</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="nb">min</span><span class="o">-</span><span class="n">channels</span> <span class="n">MIN_CHANNELS</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">numa</span><span class="o">-</span><span class="n">affinity</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">network</span><span class="o">-</span><span class="n">check</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">comm</span><span class="o">-</span><span class="n">perf</span><span class="o">-</span><span class="n">test</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">ucp_device_type</span> <span class="n">UCP_DEVICE_TYPE</span><span class="p">]</span>
                   <span class="n">training_script</span>
</pre></div>
</div>
<p>In elastic training, the parameters  you may pay attention to focus on are:</p>
<p>--nnodes NNODES
Number of nodes, or the range of nodes in the form &lt;minimum_nodes&gt;:&lt;maximum_nodes&gt;.</p>
<p>--nproc-per-node NPROC_PER_NODE
Number of processes per node.</p>
<p>Example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">model</span><span class="o">=</span>your<span class="w"> </span>model<span class="w"> </span>path
<span class="nv">dataset</span><span class="o">=</span>your<span class="w"> </span>dataset
<span class="nv">output</span><span class="o">=</span><span class="w"> </span>your<span class="w"> </span>output<span class="w"> </span>dir
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="c1"># Set according to the actual GPU usage</span>
<span class="nv">deepspeed_config_or_type</span><span class="o">=</span>deepspeed<span class="w"> </span><span class="nb">type</span><span class="w"> </span>or<span class="w"> </span>configuration<span class="w"> </span>file<span class="w"> </span>path,<span class="w"> </span>e.g.,<span class="w"> </span>zero1<span class="w"> </span>or<span class="w"> </span>/xxx/ms-swift/swift/llm/ds_config/zero1.json

dlrover-run<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">1</span>:<span class="nv">$NODE_NUM</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="se">\</span>
/opt/conda/lib/python3.10/site-packages/swift/cli/sft.py<span class="w"> </span>--model<span class="w"> </span><span class="nv">$model</span><span class="w"> </span><span class="se">\</span>
--model_type<span class="w"> </span>qwen3<span class="w"> </span><span class="se">\</span>
--train_type<span class="w"> </span>lora<span class="w">  </span><span class="se">\</span>
--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
--dataset<span class="w"> </span><span class="nv">$dataset</span><span class="w"> </span><span class="se">\</span>
--num_train_epochs<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--learning_rate<span class="w"> </span>5e-7<span class="w"> </span><span class="se">\</span>
--gradient_accumulation_steps<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
--eval_steps<span class="w"> </span><span class="m">500</span><span class="w"> </span><span class="se">\</span>
--save_steps<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
--save_total_limit<span class="w"> </span><span class="m">20</span><span class="w"> </span><span class="se">\</span>
--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--output_dir<span class="w"> </span><span class="nv">$output</span><span class="w"> </span><span class="se">\</span>
--warmup_ratio<span class="w"> </span><span class="m">0</span>.01<span class="w"> </span><span class="se">\</span>
--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
--system<span class="w"> </span>You<span class="se">\ </span>are<span class="se">\ </span>a<span class="se">\ </span>helpful<span class="se">\ </span>assistant.<span class="w"> </span><span class="se">\</span>
--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
--lora_alpha<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
--dataset_num_proc<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--use_flash_ckpt<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
--callbacks<span class="w"> </span>deepspeed_elastic<span class="w"> </span>graceful_exit<span class="w"> </span><span class="se">\</span>
--deepspeed<span class="w"> </span><span class="nv">$deepspeed_config_or_type</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">ÔÉÅ</a></h2>
<p>By default, the zero1 configuration is as follows:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;bf16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;allgather_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;allgather_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_scatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;steps_per_print&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;elasticity&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;ignore_non_elastic_batch_info&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;max_train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;micro_batch_sizes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">          </span><span class="mi">4</span><span class="p">,</span>
<span class="w">          </span><span class="mi">2</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;min_gpus&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;max_gpus&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_time&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span>
<span class="w">      </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If users need custom configurations, they can specify the path to the custom zero1.json file in the deepspeed_config_or_type parameter. The elasticity-related configuration is as follows:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="err">...</span>

<span class="w">  </span><span class="nt">&quot;elasticity&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;ignore_non_elastic_batch_info&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;max_train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;micro_batch_sizes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">          </span><span class="mi">4</span><span class="p">,</span>
<span class="w">          </span><span class="mi">2</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;min_gpus&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;max_gpus&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_time&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span>
<span class="w">      </span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>ignore_non_elastic_batch_infoÔºöIndicates that the batch size configurations outside the elasticity settings will be ignored. During training, the batch size and related parameters will be dynamically adjusted based on the number of training processes.
Calculation principleÔºö
¬†global-training-batch-size = micro-batch-size * gradient-accumulation-steps * world-size</p></li>
<li><p>max_train_batch_sizeÔºö Maximum batch size</p></li>
<li><p>micro_batch_sizesÔºöList of allowed per-GPU micro-batch sizes under elasticity; candidates for train_micro_batch_size_per_gpu.</p></li>
<li><p>min_gpusÔºöMinimum number of GPUs.</p></li>
<li><p>max_gpusÔºöMaximum number of GPUs.
For more details, see: <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/#elastic-training-config-v01-and-v02">Deepspeed</a></p></li>
</ul>
</section>
<section id="starting-training">
<h2>Starting Training<a class="headerlink" href="#starting-training" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">elastic.iml.github.io/v1alpha1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ElasticJob</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deepspeed-elastic-swift</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dlrover</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">distributionStrategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">AllreduceStrategy</span>
<span class="w">  </span><span class="nt">optimizeMode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">single-job</span>
<span class="w">  </span><span class="nt">replicaSpecs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">worker</span><span class="p">:</span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># This should match the maximum value of --nnodes NNODES in the startup command</span>
<span class="w">      </span><span class="nt">template</span><span class="p">:</span>
<span class="w">        </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">          </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Never</span>
<span class="w">          </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">              </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="c1">#„ÄêTraining image, needs to have deepspeed, dlrover, and swift installed„Äë</span>
<span class="w">              </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span>
<span class="w">              </span><span class="nt">command</span><span class="p">:</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/bin/bash</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-c</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sh start.sh</span><span class="w"> </span><span class="c1"># Startup script</span>
<span class="w">              </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">                </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">                  </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;8&#39;</span>
<span class="w">                  </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16Gi</span>
<span class="w">                  </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="w">              </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/model</span>
<span class="w">                  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">volume-model</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/dev/shm</span>
<span class="w">                  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">volume-shm</span>
<span class="w">          </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Never</span>
<span class="w">          </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">hostPath</span><span class="p">:</span>
<span class="w">                </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/model</span>
<span class="w">                </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Directory</span>
<span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">volume-model</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">emptyDir</span><span class="p">:</span>
<span class="w">                </span><span class="nt">medium</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Memory</span>
<span class="w">                </span><span class="nt">sizeLimit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200Gi</span>
<span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">volume-shm</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>