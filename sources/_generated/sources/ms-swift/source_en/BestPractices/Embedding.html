

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Embedding Training &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Embedding Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/BestPractices/Embedding.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="embedding-training">
<h1>Embedding Training<a class="headerlink" href="#embedding-training" title="Link to this heading">ÔÉÅ</a></h1>
<p>SWIFT has already supported the training of embedding models, including both pure text and multimodal types. Currently supported models include:</p>
<ol class="simple">
<li><p>modernbert embedding model</p>
<ul class="simple">
<li><p><a class="reference external" href="https://modelscope.cn/models/iic/gte-modernbert-base">ModelScope</a> <a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base">Hugging Face</a></p></li>
</ul>
</li>
<li><p>gte embedding models</p>
<ul class="simple">
<li><p>1.5B: <a class="reference external" href="https://www.modelscope.cn/models/iic/gte_Qwen2-1.5B-instruct">ModelScope</a> <a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct">Hugging Face</a></p></li>
<li><p>7B: <a class="reference external" href="https://www.modelscope.cn/models/iic/gte_Qwen2-7B-instruct">ModelScope</a> <a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct">Hugging Face</a></p></li>
</ul>
</li>
<li><p>gme embedding models</p>
<ul class="simple">
<li><p>2B: <a class="reference external" href="https://www.modelscope.cn/models/iic/gme-Qwen2-VL-2B-Instruct">ModelScope</a> <a class="reference external" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">Hugging Face</a></p></li>
<li><p>7B: <a class="reference external" href="https://www.modelscope.cn/models/iic/gme-Qwen2-VL-7B-Instruct">ModelScope</a> <a class="reference external" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-7B-Instruct">Hugging Face</a></p></li>
</ul>
</li>
<li><p>qwen3-embedding models</p>
<ul class="simple">
<li><p>0.6B: <a class="reference external" href="https://www.modelscope.cn/models/Qwen/Qwen3-Embedding-0.6B">ModelScope</a> <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B">Hugging Face</a></p></li>
<li><p>4B: <a class="reference external" href="https://www.modelscope.cn/models/Qwen/Qwen3-Embedding-4B">ModelScope</a> <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-Embedding-4B">Hugging Face</a></p></li>
<li><p>8B: <a class="reference external" href="https://www.modelscope.cn/models/Qwen/Qwen3-Embedding-8B">ModelScope</a> <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-Embedding-8B">Hugging Face</a></p></li>
</ul>
</li>
<li><p>qwen3-vl-embedding models</p>
<ul class="simple">
<li><p>2B: <a class="reference external" href="https://www.modelscope.cn/models/Qwen/Qwen3-VL-Embedding-2B">ModelScope</a> <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-VL-Embedding-2B">Hugging Face</a></p></li>
<li><p>8B: <a class="reference external" href="https://www.modelscope.cn/models/Qwen/Qwen3-VL-Embedding-8B">ModelScope</a> <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B">Hugging Face</a></p></li>
</ul>
</li>
</ol>
<p>Developers can integrate their own models by ensuring the model forward output satisfies:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&quot;last_hidden_state&quot;: some-embedding-tensor}
</pre></div>
</div>
<p>The return value should be a JSON with a <code class="docutils literal notranslate"><span class="pre">last_hidden_state</span></code> key, where the value is an embedding tensor. For the input part, you can use our already supported templates. Users can also specify the</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">   </span>--task_type<span class="w"> </span>embedding
</pre></div>
</div>
<p>parameter to convert any other model into an embedding model for training.</p>
<p>It should be noted that the embedding models currently supported by SWIFT are all based on pure text or multimodal LLMs, and CLIP-type model training is not currently supported.</p>
<p>Additionally, all embedding models supported by SWIFT have normalization added at the end of the model forward pass. If you add new models yourself, please remember to include a normalization layer.</p>
<section id="loss">
<h2>Loss<a class="headerlink" href="#loss" title="Link to this heading">ÔÉÅ</a></h2>
<p>The Embedding models supported by SWIFT currently can use the following loss functions:</p>
<ul class="simple">
<li><p><strong>cosine_similarity</strong>: Cosine similarity loss, which calculates the similarity between two embeddings and fits based on the label value. It is effectively an MSE loss.</p></li>
<li><p><strong>contrastive</strong>: Contrastive learning loss with adjustable margin. Labels are only supported as 0 and 1.</p></li>
<li><p><strong>online_contrastive</strong>: Contrastive loss considering hard negatives and hard positives. Labels are only supported as 0 and 1.</p></li>
<li><p><strong>infonce</strong>: Computes pairwise cosine similarities between different rows within the same batch, maximizing similarity within rows and minimizing similarity between different rows. No labels are required.</p></li>
</ul>
<p>The source code for the loss functions can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss/mapping.py">here</a>.</p>
</section>
<section id="dataset-format">
<h2>Dataset Format<a class="headerlink" href="#dataset-format" title="Link to this heading">ÔÉÅ</a></h2>
<blockquote>
<div><p><strong>Note:</strong></p>
<ol class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">&lt;image&gt;</span></code> tag can appear anywhere inside <code class="docutils literal notranslate"><span class="pre">messages</span></code>/<code class="docutils literal notranslate"><span class="pre">positive_messages</span></code>/<code class="docutils literal notranslate"><span class="pre">negative_messages</span></code>. Each group has its own image fields: <code class="docutils literal notranslate"><span class="pre">images</span></code>/<code class="docutils literal notranslate"><span class="pre">positive_images</span></code>/<code class="docutils literal notranslate"><span class="pre">negative_images</span></code> to provide paths or URLs.</p></li>
<li><p>There is no longer any cross-field ordering requirement. Alignment rules:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">images</span></code> length equals the number of <code class="docutils literal notranslate"><span class="pre">&lt;image&gt;</span></code> tags in <code class="docutils literal notranslate"><span class="pre">messages</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">positive_images</span></code> and <code class="docutils literal notranslate"><span class="pre">negative_images</span></code> are both list-of-list. Their outer lengths equal the lengths of <code class="docutils literal notranslate"><span class="pre">positive_messages</span></code> and <code class="docutils literal notranslate"><span class="pre">negative_messages</span></code> respectively. For each outer item, the inner list length equals the number of <code class="docutils literal notranslate"><span class="pre">&lt;image&gt;</span></code> tags in that message sequence.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">messages</span></code> is the anchor sample; <code class="docutils literal notranslate"><span class="pre">positive_messages</span></code> and <code class="docutils literal notranslate"><span class="pre">negative_messages</span></code> are each a list of messages (hence one more <code class="docutils literal notranslate"><span class="pre">[]</span></code>). Accordingly, <code class="docutils literal notranslate"><span class="pre">positive_images</span></code>/<code class="docutils literal notranslate"><span class="pre">negative_images</span></code> are also list-of-list and align item-by-item.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;video&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;audio&gt;</span></code> are supported as well. Follow the same rules via <code class="docutils literal notranslate"><span class="pre">videos</span></code>/<code class="docutils literal notranslate"><span class="pre">positive_videos</span></code>/<code class="docutils literal notranslate"><span class="pre">negative_videos</span></code> and <code class="docutils literal notranslate"><span class="pre">audios</span></code>/<code class="docutils literal notranslate"><span class="pre">positive_audios</span></code>/<code class="docutils literal notranslate"><span class="pre">negative_audios</span></code>.</p></li>
<li><p>Current constraint: the outer length of <code class="docutils literal notranslate"><span class="pre">positive_messages</span></code> must be 1 (i.e., provide exactly one positive). Accordingly, the outer length of <code class="docutils literal notranslate"><span class="pre">positive_images</span></code> must also be 1.</p></li>
</ol>
</div></blockquote>
<section id="format-for-cosine-similarity-loss">
<h3>Format for Cosine Similarity Loss<a class="headerlink" href="#format-for-cosine-similarity-loss" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span># LLM
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence1&quot;}], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence2&quot;}]], &quot;label&quot;: 0.8}
# MLLM
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;&quot;}], &quot;images&quot;: [&quot;/some/images1.jpg&quot;], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence&quot;}]], &quot;positive_images&quot;: [[&quot;/some/images2.jpg&quot;]], &quot;label&quot;: 0.7}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence1&quot;}], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence2&quot;}]], &quot;positive_images&quot;: [[&quot;/some/images.jpg&quot;]], &quot;label&quot;: 0.7}
</pre></div>
</div>
<p>The eval metrics are the Pearson and Spearman's Rank Correlation Coefficient of the embeddings' euclidean distance/dot production and so on, totally 8 values.</p>
</section>
<section id="format-for-contrastive-online-contrastive-loss">
<h3>Format for Contrastive/Online Contrastive Loss<a class="headerlink" href="#format-for-contrastive-online-contrastive-loss" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span># LLM
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence1&quot;}], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence2&quot;}]], &quot;label&quot;: 1}
# MLLM
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;&quot;}], &quot;images&quot;: [&quot;/some/images1.jpg&quot;], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence&quot;}]], &quot;positive_images&quot;: [[&quot;/some/images2.jpg&quot;]], &quot;label&quot;: 1}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence1&quot;}], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence2&quot;}]], &quot;positive_images&quot;: [[&quot;/some/images.jpg&quot;]], &quot;label&quot;: 0}
</pre></div>
</div>
</section>
<section id="format-for-infonce">
<h3>Format for InfoNCE<a class="headerlink" href="#format-for-infonce" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span># LLM
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence1&quot;}], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence2&quot;}]]}
# MLLM
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;&quot;}], &quot;images&quot;: [&quot;/some/images.jpg&quot;], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;sentence&quot;}]]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence1&quot;}], &quot;images&quot;: [&quot;/some/images.jpg&quot;], &quot;positive_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence2&quot;}]], &quot;positive_images&quot;: [[&quot;/some/positive_images.jpg&quot;]], &quot;negative_messages&quot;: [[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;&lt;image&gt;sentence3&quot;}], [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;sentence4&quot;}]], &quot;negative_images&quot;: [[&quot;/some/negative_images1.jpg&quot;, &quot;/some/negative_images2.jpg&quot;], [&quot;/some/negative_images3.jpg&quot;]]}
</pre></div>
</div>
<p>InfoNCE loss supports the following environment variables:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_TEMPERATURE</span></code>: The temperature parameter. If not set, the default value is 0.1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_USE_BATCH</span></code>: Use <code class="docutils literal notranslate"><span class="pre">negative_messages</span></code> within the sample (hard negatives) or use other samples in the batch as in-batch negatives. The default is <code class="docutils literal notranslate"><span class="pre">True</span></code>, which means using in-batch negatives.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_HARD_NEGATIVES</span></code>: The number of hard negatives. If not set, all provided <code class="docutils literal notranslate"><span class="pre">negative_messages</span></code> will be used. Since the lengths may vary, a for loop will be used to compute the loss (slower). If set to a specific number, missing items will be randomly sampled, and excess items will be truncated to the first <code class="docutils literal notranslate"><span class="pre">INFONCE_HARD_NEGATIVES</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_MASK_FAKE_NEGATIVE</span></code>: Masks out fake negatives. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. When enabled, it checks <code class="docutils literal notranslate"><span class="pre">positive_similarity</span> <span class="pre">+</span> <span class="pre">INFONCE_FAKE_NEG_MARGIN</span></code>; any sample with similarity larger than this threshold will have its similarity set to <code class="docutils literal notranslate"><span class="pre">-inf</span></code> to prevent positive leakage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_FAKE_NEG_MARGIN</span></code>: Margin used by the fake-negative mask. Default: <code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_INCLUDE_QQ</span></code>: Include the q‚Äìq block (similarities among queries) in the denominator as additional negatives. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFONCE_INCLUDE_DD</span></code>: Include the d‚Äìd block (similarities of the positive doc to all in-batch docs) in the denominator as additional negatives. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ol>
<blockquote>
<div><p>You can also make the number of hard negatives equal across samples in the dataset, which avoids the for-loop computation and speeds up training even if <code class="docutils literal notranslate"><span class="pre">INFONCE_HARD_NEGATIVES</span></code> is not set.</p>
<p><code class="docutils literal notranslate"><span class="pre">negative_messages</span></code> can be omitted. In this case, keep <code class="docutils literal notranslate"><span class="pre">INFONCE_USE_BATCH=True</span></code> to use in-batch negatives (other samples in the batch) as negatives.</p>
</div></blockquote>
<p>The evaluation of InfoNCE loss includes the following metrics:</p>
<ul class="simple">
<li><p>mean_neg: The average of all hard negatives</p></li>
<li><p>mean_pos: The average of all positives</p></li>
<li><p>margin: The average of (positive - max hard negative)</p></li>
</ul>
</section>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">ÔÉÅ</a></h2>
<p>Training scripts provided by ms-swift:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/qwen3">Qwen3-Embedding/Qwen3-VL-Embedding model</a></p></li>
<li><p><a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/train_gme.sh">GME model</a></p></li>
</ul>
</section>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Link to this heading">ÔÉÅ</a></h2>
<p>SWIFT has supported the deployment of GME, GTE, Qwen3-Embedding models, please check <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/deploy/embedding/client.py">here</a>.</p>
<ul class="simple">
<li><p>For inference scripts, please refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_embedding.py">here</a>.</p></li>
</ul>
<p>You can also use the original model's code for inference:</p>
<p>https://www.modelscope.cn/models/iic/gte_Qwen2-7B-instruct</p>
<p>https://www.modelscope.cn/models/iic/gme-Qwen2-VL-7B-Instruct</p>
<p>If you've used other models to train embedding from scratch (for example, the original <code class="docutils literal notranslate"><span class="pre">qwen2-vl</span></code> model + <code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">embedding</span></code>), you can also use gme's inference code, but please note:</p>
<p>https://www.modelscope.cn/models/iic/gme-Qwen2-VL-7B-Instruct/file/view/master/gme_inference.py?status=1#L111</p>
<p>Please modify the template here to match the model's own template to ensure the final embeddings align correctly. It's particularly important to note that the template for the gme model is different from the chatml template for the <code class="docutils literal notranslate"><span class="pre">qwen2-vl</span></code> or <code class="docutils literal notranslate"><span class="pre">qwen2.5-vl</span></code> series. In its inference code, the ending character is <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> rather than <code class="docutils literal notranslate"><span class="pre">&lt;|im_end|&gt;</span></code>.</p>
</section>
<section id="advanced">
<h2>Advanced<a class="headerlink" href="#advanced" title="Link to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Qwen3-Embedding Custom Instruction:</p>
<ul>
<li><p>By default, there is no instruction; the input prompt is: <code class="docutils literal notranslate"><span class="pre">{Query}&lt;|endoftext|&gt;</span></code>.</p></li>
<li><p>You can add an instruction via the system message, changing the prompt to: <code class="docutils literal notranslate"><span class="pre">{Instruction}</span> <span class="pre">{Query}&lt;|endoftext|&gt;</span></code>.</p></li>
<li><p>Example:</p></li>
</ul>
</li>
</ul>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span>{&quot;messages&quot;: [
  {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Answer in English and list key points briefly.&quot;},
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Introduce Qwen3-Embedding&quot;}
]}
</pre></div>
</div>
<blockquote>
<div><p>Note: The Qwen3-Embedding template prepends the system content to the first user message and uses <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> as the ending token.</p>
</div></blockquote>
<section id="before-after-examples">
<h3>Before/After Examples<a class="headerlink" href="#before-after-examples" title="Link to this heading">ÔÉÅ</a></h3>
<ul>
<li><p>Without Instruction:</p>
<p>Input data (messages):</p>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span>{&quot;messages&quot;: [
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is Qwen3-Embedding?&quot;}
]}
</pre></div>
</div>
<p>After template conversion (actual prompt sent to the model):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>What is Qwen3-Embedding?&lt;|endoftext|&gt;
</pre></div>
</div>
</li>
<li><p>With Instruction:</p>
<p>Input data (messages with system):</p>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span>{&quot;messages&quot;: [
  {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Answer in English and list key points briefly.&quot;},
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is Qwen3-Embedding?&quot;}
]}
</pre></div>
</div>
<p>After template conversion (actual prompt sent to the model):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Answer in English and list key points briefly. What is Qwen3-Embedding?&lt;|endoftext|&gt;
</pre></div>
</div>
</li>
<li><p>Positive/Negative behave the same:</p>
<p>If a system message is provided within a positive/negative sequence, it is prepended to that sequence‚Äôs first user content; if no system is provided, nothing is prepended.</p>
<p>Input (one positive with system, one negative without):</p>
<div class="highlight-json lines notranslate"><div class="highlight"><pre><span></span>{
  &quot;messages&quot;: [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Anchor&quot;}
  ],
  &quot;positive_messages&quot;: [[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Instruction&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Positive&quot;}
  ]],
  &quot;negative_messages&quot;: [[
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Negative&quot;}
  ]]
}
</pre></div>
</div>
<p>After template conversion (actual prompts):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Anchor&lt;|endoftext|&gt;
Instruction Positive&lt;|endoftext|&gt;
Negative&lt;|endoftext|&gt;
</pre></div>
</div>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>