

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Complete Multimodal GRPO Experiment Workflow &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Complete Multimodal GRPO Experiment Workflow</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/BestPractices/GRPO-Multi-Modal-Training.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="complete-multimodal-grpo-experiment-workflow">
<h1>Complete Multimodal GRPO Experiment Workflow<a class="headerlink" href="#complete-multimodal-grpo-experiment-workflow" title="Link to this heading">ÔÉÅ</a></h1>
<p>This document explains how to use SWIFT GRPO for training multimodal models and tasks. The goal is to train on multiple multimodal tasks to improve task accuracy. Task definitions, training parameters, etc., refer to <a class="reference external" href="https://github.com/Deep-Agent/R1-V.git">R1-V</a> and <a class="reference external" href="https://github.com/EvolvingLMMs-Lab/open-r1-multimodal.git">open-r1-multimodal</a>.</p>
<hr class="docutils" />
<section id="clevrcount-task">
<h2><strong>ClevrCount Task</strong><a class="headerlink" href="#clevrcount-task" title="Link to this heading">ÔÉÅ</a></h2>
<section id="task-and-dataset-definition">
<h3><strong>Task and Dataset Definition</strong><a class="headerlink" href="#task-and-dataset-definition" title="Link to this heading">ÔÉÅ</a></h3>
<p>This task is based on the <code class="docutils literal notranslate"><span class="pre">clevr_cogen_a_train</span></code> dataset. The model's goal is to output the number of objects in the image. Therefore, we define the dataset as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ClevrPreprocessor</span><span class="p">(</span><span class="n">ResponsePreprocessor</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;query&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2"> Output the thinking process in &lt;think&gt; &lt;/think&gt; and</span>
<span class="s2"> final answer (number) in &lt;answer&gt; &lt;/answer&gt; tags.&quot;&quot;&quot;</span>
        <span class="n">row</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;query&#39;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>


<span class="n">register_dataset</span><span class="p">(</span>
    <span class="n">DatasetMeta</span><span class="p">(</span>
        <span class="n">ms_dataset_id</span><span class="o">=</span><span class="s1">&#39;AI-ModelScope/clevr_cogen_a_train&#39;</span><span class="p">,</span>
        <span class="n">subsets</span><span class="o">=</span><span class="p">[</span>
            <span class="n">SubsetDataset</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">,</span>
                <span class="n">subset</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">,</span>
                <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span>
            <span class="p">),</span>
        <span class="p">],</span>
        <span class="n">preprocess_func</span><span class="o">=</span><span class="n">ClevrPreprocessor</span><span class="p">(),</span>
        <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;qa&#39;</span><span class="p">,</span> <span class="s1">&#39;math&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The purpose of redefining the dataset preprocessor here is to modify the query. A sample dataset entry is as follows, including <code class="docutils literal notranslate"><span class="pre">messages</span></code>, <code class="docutils literal notranslate"><span class="pre">images</span></code>, and <code class="docutils literal notranslate"><span class="pre">solution</span></code> fields. The <code class="docutils literal notranslate"><span class="pre">solution</span></code> is used in the reward function, while <code class="docutils literal notranslate"><span class="pre">messages</span></code> and <code class="docutils literal notranslate"><span class="pre">images</span></code> serve as model input.</p>
<ul class="simple">
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">{'role':</span> <span class="pre">'assistant',</span> <span class="pre">'content':</span> <span class="pre">'&lt;answer&gt;</span> <span class="pre">3</span> <span class="pre">&lt;/answer&gt;'}</span></code> will be removed in GRPOTrainer and can be ignored. The 'solution' field will be passed directly into the ORM. When creating a custom dataset, the 'images' field should be organized as <code class="docutils literal notranslate"><span class="pre">[&quot;image_path1&quot;,</span> <span class="pre">&quot;image_path2&quot;]</span></code>.</p></li>
</ul>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;images&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;image_path1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;image_path2&quot;</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;How many items are there in the image? Output the thinking process in &lt;think&gt; &lt;/think&gt; and\n final answer (number) in &lt;answer&gt; &lt;/answer&gt; tags.&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;solution&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;answer&gt; 3 &lt;/answer&gt;&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="reward-function-definition">
<h2><strong>Reward Function Definition</strong><a class="headerlink" href="#reward-function-definition" title="Link to this heading">ÔÉÅ</a></h2>
<p>This task uses two reward functions: one is the format reward function mentioned in <code class="docutils literal notranslate"><span class="pre">Deepseek-R1</span></code>, and the other is the accuracy reward function for ClevrCount. The former is built into SWIFT and can be used directly with <code class="docutils literal notranslate"><span class="pre">--reward_funcs</span> <span class="pre">format</span></code>. The latter needs to be custom-defined. Here, we use the <code class="docutils literal notranslate"><span class="pre">external_plugin</span></code> method to define the accuracy reward function by placing the code in <code class="docutils literal notranslate"><span class="pre">swift/examples/train/grpo/plugin/plugin.py</span></code>.</p>
<p>The reward function's input includes <code class="docutils literal notranslate"><span class="pre">completions</span></code> and <code class="docutils literal notranslate"><span class="pre">solution</span></code> fields, representing the model-generated text and ground truth, respectively. Each is a list, allowing the computation of multiple completions simultaneously. Note that the <code class="docutils literal notranslate"><span class="pre">solution</span></code> field is passed through directly from the dataset definition. If there are task changes, corresponding modifications can be made to the dataset and reward function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiModalAccuracyORM</span><span class="p">(</span><span class="n">ORM</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">completions</span><span class="p">,</span> <span class="n">solution</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reward function that checks if the completion is correct.</span>
<span class="sd">        Args:</span>
<span class="sd">            completions (list[str]): Generated outputs</span>
<span class="sd">            solution (list[str]): Ground Truths.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[float]: Reward scores</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">math_verify</span><span class="w"> </span><span class="kn">import</span> <span class="n">parse</span><span class="p">,</span> <span class="n">verify</span>
        <span class="k">for</span> <span class="n">content</span><span class="p">,</span> <span class="n">sol</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">completions</span><span class="p">,</span> <span class="n">solution</span><span class="p">):</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="c1"># Try symbolic verification first</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">answer</span> <span class="o">=</span> <span class="n">parse</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">float</span><span class="p">(</span><span class="n">verify</span><span class="p">(</span><span class="n">answer</span><span class="p">,</span> <span class="n">parse</span><span class="p">(</span><span class="n">sol</span><span class="p">)))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">reward</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>  <span class="c1"># Continue to next verification method if this fails</span>

            <span class="c1"># If symbolic verification failed, try string matching</span>
            <span class="k">if</span> <span class="n">reward</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Extract answer from solution if it has think/answer tags</span>
                    <span class="n">sol_match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&lt;answer&gt;(.*?)&lt;/answer&gt;&#39;</span><span class="p">,</span> <span class="n">sol</span><span class="p">)</span>
                    <span class="n">ground_truth</span> <span class="o">=</span> <span class="n">sol_match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">if</span> <span class="n">sol_match</span> <span class="k">else</span> <span class="n">sol</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

                    <span class="c1"># Extract answer from content if it has think/answer tags</span>
                    <span class="n">content_match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&lt;answer&gt;(.*?)&lt;/answer&gt;&#39;</span><span class="p">,</span> <span class="n">content</span><span class="p">)</span>
                    <span class="n">student_answer</span> <span class="o">=</span> <span class="n">content_match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">if</span> <span class="n">content_match</span> <span class="k">else</span> <span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

                    <span class="c1"># Compare the extracted answers</span>
                    <span class="k">if</span> <span class="n">student_answer</span> <span class="o">==</span> <span class="n">ground_truth</span><span class="p">:</span>
                        <span class="n">reward</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="k">pass</span>  <span class="c1"># Keep reward as 0.0 if both methods fail</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rewards</span>
<span class="n">orms</span><span class="p">[</span><span class="s1">&#39;external_r1v_acc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">MultiModalAccuracyORM</span>
</pre></div>
</div>
<hr class="docutils" />
<section id="grpo-training-experiment-log">
<h3><strong>GRPO Training Experiment Log</strong><a class="headerlink" href="#grpo-training-experiment-log" title="Link to this heading">ÔÉÅ</a></h3>
<section id="training-parameters">
<h4><strong>Training Parameters</strong><a class="headerlink" href="#training-parameters" title="Link to this heading">ÔÉÅ</a></h4>
<p>We selected <code class="docutils literal notranslate"><span class="pre">Qwen2.5-VL-3B-Instruct</span></code> as the base model for training. The main reason for choosing the <code class="docutils literal notranslate"><span class="pre">Instruct</span></code> model over the base model is to rapidly achieve format rewards. Experiments were conducted on 8 GPUs. SWIFT GRPO training supports multi-GPU deployment to accelerate rollouts. If you encounter deployment errors for <code class="docutils literal notranslate"><span class="pre">qwen2.5-vl</span></code> on <code class="docutils literal notranslate"><span class="pre">vllm</span></code>, refer to <a class="reference external" href="https://github.com/vllm-project/vllm/issues/13285">this issue</a>.</p>
<p>Since the task is simple, we set <code class="docutils literal notranslate"><span class="pre">max_completion_length</span></code> to 1024 and selected <code class="docutils literal notranslate"><span class="pre">external_r1v_acc</span></code> and <code class="docutils literal notranslate"><span class="pre">format</span></code> as reward functions. The learning rate and beta are set to <code class="docutils literal notranslate"><span class="pre">1e-6</span></code> and <code class="docutils literal notranslate"><span class="pre">0.001</span></code>, respectively. Other configurations are as follows. The settings for <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">num_generations</span></code> can be referenced from <a class="reference internal" href="GRPO.html"><span class="doc">GRPO Full Workflow</span></a>.</p>
<p>launch external vLLM server using following script</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">6</span>,7<span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rollout<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-3B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_data_parallel_size<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">WANDB_API_KEY</span><span class="o">=</span>your_wandb_api_key<span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5<span class="w"> </span><span class="se">\</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">6</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rlhf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--rlhf_type<span class="w"> </span>grpo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-3B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--external_plugins<span class="w"> </span>examples/train/grpo/plugin/plugin.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_funcs<span class="w"> </span>external_r1v_acc<span class="w"> </span>format<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_vllm<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_mode<span class="w"> </span>server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_server_host<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_server_port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/clevr_cogen_a_train&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_from_cache_file<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_completion_length<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-6<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_strategy<span class="w"> </span><span class="s1">&#39;steps&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_strategy<span class="w"> </span><span class="s1">&#39;steps&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_steps<span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/GRPO_CLEVR_COUNTDOWN<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.01<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_generations<span class="w"> </span><span class="m">24</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--system<span class="w"> </span><span class="s1">&#39;examples/train/grpo/prompt.txt&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed<span class="w"> </span>zero3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log_completions<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_iterations<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--async_generate<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="m">0</span>.001<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
</section>
<section id="experimental-observations">
<h4><strong>Experimental Observations</strong><a class="headerlink" href="#experimental-observations" title="Link to this heading">ÔÉÅ</a></h4>
<p><a class="reference external" href="../../resources/grpo_clevr_count.png">image.png</a></p>
<ul class="simple">
<li><p>Given the simplicity of the dataset and task, the model converged after 500 epochs. Key observations:</p>
<ol class="simple">
<li><p>The custom <code class="docutils literal notranslate"><span class="pre">ClevrORM</span></code> reward steadily increased, proving the model learned how to complete the task. The task success rate climbed from an initial 0.4 to nearly 1.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">Format</span> <span class="pre">Reward</span></code> remained stable at 1, likely due to the consistent query format across all dataset samples.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">reward_std</span></code> stabilized below 0.1.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">completion</span> <span class="pre">length</span></code> eventually stabilized between 60-80 tokens, with the model learning a fixed output pattern for item-by-item counting.</p></li>
</ol>
</li>
</ul>
<hr class="docutils" />
<p>For additional tasks like Geometric QA and Open R1 Multimodal datasets, refer to their respective sections in the full experiment documentation.</p>
</section>
</section>
</section>
<section id="geometric-qa-task">
<h2><strong>Geometric QA Task</strong><a class="headerlink" href="#geometric-qa-task" title="Link to this heading">ÔÉÅ</a></h2>
<section id="id1">
<h3><strong>Task and Dataset Definition</strong><a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h3>
<p>This task is a Geometric QA task, where the task description is: given a geometric figure, answer mathematical questions related to the figure. The original data comes from <a class="reference external" href="https://arxiv.org/pdf/2312.11370">this paper</a>, and <a class="reference external" href="https://github.com/Deep-Agent/R1-V.git">R1-V</a> has preprocessed the data into a <code class="docutils literal notranslate"><span class="pre">problem-solution</span></code> format while retaining the images in the <code class="docutils literal notranslate"><span class="pre">image</span></code> field. Therefore, we do not need to redefine the dataset and can directly use <code class="docutils literal notranslate"><span class="pre">--dataset</span> <span class="pre">AI-ModelScope/GEOQA_R1V_Train_8K</span></code>.</p>
</section>
<hr class="docutils" />
<section id="reward-function">
<h3><strong>Reward Function</strong><a class="headerlink" href="#reward-function" title="Link to this heading">ÔÉÅ</a></h3>
<p>As this is also a mathematical problem, and the answers are already processed into final results, we directly use the previously defined <code class="docutils literal notranslate"><span class="pre">MultiModalAccuracyORM</span></code> reward function.</p>
</section>
<hr class="docutils" />
<section id="id2">
<h3><strong>GRPO Training Experiment Log</strong><a class="headerlink" href="#id2" title="Link to this heading">ÔÉÅ</a></h3>
<section id="id3">
<h4><strong>Training Parameters</strong><a class="headerlink" href="#id3" title="Link to this heading">ÔÉÅ</a></h4>
<p>The selected model and most hyperparameters are similar to the previous experiment, with two main differences:</p>
<ol class="simple">
<li><p><strong>SWIFT now supports the <code class="docutils literal notranslate"><span class="pre">--num_iteration</span></code> parameter</strong>, allowing multiple updates during a single rollout. We set it to 2.</p></li>
<li><p>During the experiment, we found that training might become unstable in mathematical problems, causing the model to collapse. This is characterized by a sharp drop in all rewards, a rapid increase in loss, <code class="docutils literal notranslate"><span class="pre">grad_norm</span></code>, and KL divergence, with no subsequent recovery. To prevent this, we set <code class="docutils literal notranslate"><span class="pre">--max_grad_norm</span> <span class="pre">0.5</span></code> to ensure stable training. Note that this instability can have some randomness.</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">WANDB_API_KEY</span><span class="o">=</span>your_wandb_api_key<span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5<span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">401408</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">6</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rlhf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--rlhf_type<span class="w"> </span>grpo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-3B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--external_plugins<span class="w"> </span>examples/train/grpo/plugin/plugin.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_funcs<span class="w"> </span>external_r1v_acc<span class="w"> </span>format<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_vllm<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_mode<span class="w"> </span>server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_server_host<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_server_port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/GEOQA_R1V_Train_8K&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_from_cache_file<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_completion_length<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-6<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_strategy<span class="w"> </span><span class="s1">&#39;steps&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_strategy<span class="w"> </span><span class="s1">&#39;steps&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_steps<span class="w"> </span><span class="m">400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/GRPO_GEOQA<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_generations<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--repetition_penalty<span class="w"> </span><span class="m">1</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--system<span class="w"> </span><span class="s1">&#39;examples/train/grpo/prompt.txt&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed<span class="w"> </span>zero3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log_completions<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_iterations<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--async_generate<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="m">0</span>.001<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_grad_norm<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id4">
<h4><strong>Experimental Observations</strong><a class="headerlink" href="#id4" title="Link to this heading">ÔÉÅ</a></h4>
<p><a class="reference external" href="../../resources/grpo_geoqa.png">image.png</a></p>
<p>The training curve is shown above.</p>
<ol class="simple">
<li><p>Compared to the <strong>Count task</strong>, the <strong>Geometric QA task</strong> is significantly more challenging. As a result, the <strong>Accuracy Reward</strong> converges much more slowly and has larger oscillations.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">completion_length</span></code> exhibits a clear trend, eventually stabilizing at around 250 tokens.</p></li>
<li><p>The reward converges, with the standard deviation stabilizing at around 0.2 after a total of 1300 steps.</p></li>
</ol>
<p>Below is an example of the model's completion after training:</p>
<div class="highlight-plaintext notranslate"><div class="highlight"><pre><span></span>User:
&quot;If AB is the tangent to circle O with the tangent point A, and BO intersects circle O at point C, and point D is located on circle O, what is the measure of angle ADC if the degree of angle ABO is 32.0?&quot;
Assistant:
&quot;&lt;think&gt;\nTo find the measure of angle ADC, we first need to understand the properties of the tangent and the circle. Since AB is a tangent to the circle at point A, the line segment OA (which is perpendicular to the tangent AB) is perpendicular to AB. This means that angle OAB is 90 degrees.\n\nGiven that angle ABO is 32 degrees, we can find angle BAO by subtracting angle ABO from 90 degrees:\n‚à†BAO = 90¬∞ - ‚à†ABO = 90¬∞ - 32¬∞ = 58¬∞\n\nSince angle BAO is an angle in the minor arc AD, the angle ADC, which is an angle in the minor arc AD, is half the angle BAO. Therefore, angle ADC is:\n‚à†ADC = 1/2 √ó ‚à†BAO = 1/2 √ó 58¬∞ = 29¬∞\n\nSo, the measure of angle ADC is 29 degrees.\n&lt;/think&gt;\n&lt;answer&gt;\nThe measure of angle ADC is 29 degrees.\n&lt;/answer&gt;&quot;
</pre></div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="multimodal-open-r1-dataset-experiment">
<h2><strong>Multimodal Open R1 Dataset Experiment</strong><a class="headerlink" href="#multimodal-open-r1-dataset-experiment" title="Link to this heading">ÔÉÅ</a></h2>
<section id="id5">
<h3><strong>Task and Dataset Definition</strong><a class="headerlink" href="#id5" title="Link to this heading">ÔÉÅ</a></h3>
<p>This task is based on the experiments in <a class="reference external" href="https://github.com/EvolvingLMMs-Lab/open-r1-multimodal.git">open-r1-multimodal</a>, using the dataset: <a class="reference external" href="https://www.modelscope.cn/datasets/lmms-lab/multimodal-open-r1-8k-verified">lmms-lab/multimodal-open-r1-8k-verified</a>. This dataset focuses on multimodal mathematical reasoning tasks, with data generated by GPT4o based on the <code class="docutils literal notranslate"><span class="pre">Math360K</span></code> and <code class="docutils literal notranslate"><span class="pre">Geo170K</span></code> datasets. It includes reasoning paths and verifiable answers. The dataset already contains <code class="docutils literal notranslate"><span class="pre">image</span></code>, <code class="docutils literal notranslate"><span class="pre">problem</span></code>, and <code class="docutils literal notranslate"><span class="pre">solution</span></code> fields, so no additional prompt modifications are required, and there is no need to redefine the dataset.</p>
</section>
<hr class="docutils" />
<section id="id6">
<h3><strong>Reward Function</strong><a class="headerlink" href="#id6" title="Link to this heading">ÔÉÅ</a></h3>
<p>We directly use the previously defined <code class="docutils literal notranslate"><span class="pre">MultiModalAccuracyORM</span></code> reward function.</p>
</section>
<hr class="docutils" />
<section id="id7">
<h3><strong>GRPO Training Experiment Log</strong><a class="headerlink" href="#id7" title="Link to this heading">ÔÉÅ</a></h3>
<section id="id8">
<h4><strong>Training Parameters</strong><a class="headerlink" href="#id8" title="Link to this heading">ÔÉÅ</a></h4>
<p>The selected model and most hyperparameters are similar to the previous experiment. Due to an <strong>OOM (Out of Memory) issue</strong>, we set <code class="docutils literal notranslate"><span class="pre">MAX_PIXELS=262144</span></code> to reduce memory usage.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">WANDB_API_KEY</span><span class="o">=</span>your_wandb_api_key<span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5<span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">262144</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">29600</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">6</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rlhf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--rlhf_type<span class="w"> </span>grpo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-3B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--external_plugins<span class="w"> </span>examples/train/grpo/plugin/plugin.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_funcs<span class="w"> </span>external_r1v_acc<span class="w"> </span>format<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_vllm<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_mode<span class="w"> </span>server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_server_host<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_server_port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s1">&#39;lmms-lab/multimodal-open-r1-8k-verified&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_from_cache_file<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_completion_length<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-6<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_strategy<span class="w"> </span><span class="s1">&#39;steps&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_strategy<span class="w"> </span><span class="s1">&#39;steps&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_steps<span class="w"> </span><span class="m">400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/GRPO_GEOQA<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_generations<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--repetition_penalty<span class="w"> </span><span class="m">1</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--system<span class="w"> </span><span class="s1">&#39;examples/train/grpo/prompt.txt&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed<span class="w"> </span>zero3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log_completions<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_iterations<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--async_generate<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="m">0</span>.001<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_grad_norm<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id9">
<h4><strong>Experimental Observations</strong><a class="headerlink" href="#id9" title="Link to this heading">ÔÉÅ</a></h4>
<p><a class="reference external" href="../../resources/grpo_openr1_multimodal.png">image.png</a></p>
<p>The training curve is shown above.</p>
<ul class="simple">
<li><p>With a total of 8k data samples, the training reached <strong>1268 steps</strong>, with the <strong>Accuracy Reward</strong> converging around 0.5.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">completion_length</span></code> stabilized around 200 tokens.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">reward_std</span></code> eventually oscillated around 0.2.</p></li>
</ul>
<p>Below is an example of the model's completion after training:</p>
<div class="highlight-plaintext notranslate"><div class="highlight"><pre><span></span>User:
&quot;Based on the map, which state falls into the lowest percentage range of lots, and what is that percentage range?&quot;
Assistant:
&quot;&lt;think&gt;\nThe image provided shows a color-coded map of the USA indicating different ranges of lot percentages across various states. According to the legend at the bottom, the lowest percentage range (6.0% - 6.5%) is represented by white. In the image, Alabama (AL) is shaded in white, which corresponds to the 6.0% - 6.5% category. Therefore, based on the map, the state that falls into the lowest percentage range of lots is Alabama, with the percentage range of 6.0% - 6.5%.\nTherefore, the answer is 6.0% - 6.5%.\n&lt;/think&gt;\n&lt;answer&gt;Alabama&lt;/answer&gt;&quot;
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>