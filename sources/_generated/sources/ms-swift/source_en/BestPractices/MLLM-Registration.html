

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Best Practices for Registering Multimodal Models &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Best Practices for Registering Multimodal Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/BestPractices/MLLM-Registration.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="best-practices-for-registering-multimodal-models">
<h1>Best Practices for Registering Multimodal Models<a class="headerlink" href="#best-practices-for-registering-multimodal-models" title="Link to this heading">ÔÉÅ</a></h1>
<p>This document introduces how to register a multimodal model in ms-swift and successfully perform inference and training. Using Qwen2.5-Omni as an example, we will register a new model_type and template <code class="docutils literal notranslate"><span class="pre">my_qwen2_5_omni</span></code>, supporting training with text, images, videos, and audio. Since Qwen2.5-Omni is already registered in ms-swift, we can use our custom components by explicitly specifying the model_type and template.</p>
<section id="environment-setup">
<h2>Environment Setup<a class="headerlink" href="#environment-setup" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Avoid future incompatibilities with documentation</span>
pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;ms-swift&gt;=4.0&quot;</span>

pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;transformers==4.57.*&quot;</span><span class="w"> </span><span class="s2">&quot;qwen_omni_utils==0.0.8&quot;</span>
</pre></div>
</div>
</section>
<section id="model-registration">
<h2>Model Registration<a class="headerlink" href="#model-registration" title="Link to this heading">ÔÉÅ</a></h2>
<p>First, we need to register the model to obtain the model and processor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PretrainedConfig</span><span class="p">,</span> <span class="n">PreTrainedModel</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">swift.model</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="n">ModelGroup</span><span class="p">,</span> <span class="n">ModelMeta</span><span class="p">,</span> <span class="n">MultiModelKeys</span><span class="p">,</span> <span class="n">get_model_processor</span><span class="p">,</span> <span class="n">register_model</span><span class="p">,</span>
                         <span class="n">register_model_arch</span><span class="p">,</span> <span class="n">ModelLoader</span><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.model.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">patch_qwen_vl_utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.model.patcher</span><span class="w"> </span><span class="kn">import</span> <span class="n">patch_get_input_embeddings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.model.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">use_submodel_func</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_env_args</span><span class="p">,</span> <span class="n">Processor</span>

<span class="n">register_model_arch</span><span class="p">(</span>
    <span class="n">MultiModelKeys</span><span class="p">(</span>
        <span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span>
        <span class="c1"># `freeze_llm`, `freeze_vit`, `freeze_aligner` behavior is determined by the values below.</span>
        <span class="c1"># For example: full parameter training, if `freeze_vit=True`, it will freeze parameters of model layers prefixed with `thinker.audio_tower` and `thinker.visual`.</span>
        <span class="c1"># LoRA training, if `freeze_vit=False`, it will additionally add LoRA to Linear layers prefixed with `thinker.audio_tower` and `thinker.visual`.</span>
        <span class="n">language_model</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;thinker.model&#39;</span><span class="p">,</span> <span class="s1">&#39;thinker.lm_head&#39;</span><span class="p">],</span>
        <span class="n">vision_tower</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;thinker.audio_tower&#39;</span><span class="p">,</span> <span class="s1">&#39;thinker.visual&#39;</span><span class="p">],</span>
        <span class="n">aligner</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;thinker.audio_tower.proj&#39;</span><span class="p">,</span> <span class="s1">&#39;thinker.visual.merger&#39;</span><span class="p">],</span>
        <span class="c1"># Generator parts will never be trained or remain frozen.</span>
        <span class="c1"># If you want `thinker.audio_tower` and `thinker.audio_tower.proj` to never be trained, you can place them in the generator and remove them from vision_tower and aligner.</span>
        <span class="n">generator</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;talker&#39;</span><span class="p">,</span> <span class="s1">&#39;token2wav&#39;</span><span class="p">],</span>
    <span class="p">))</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5OmniLoader</span><span class="p">(</span><span class="n">ModelLoader</span><span class="p">):</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PretrainedConfig</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen2_5OmniConfig</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">Qwen2_5OmniConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">enable_audio_output</span> <span class="o">=</span> <span class="n">get_env_args</span><span class="p">(</span><span class="s1">&#39;ENABLE_AUDIO_OUTPUT&#39;</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">enable_audio_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">enable_audio_output</span> <span class="o">=</span> <span class="n">enable_audio_output</span>
        <span class="k">return</span> <span class="n">config</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Processor</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen2_5OmniProcessor</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">qwen_omni_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">vision_process</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">Qwen2_5OmniProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Control constants in qwen_omni_utils library via environment variables,</span>
        <span class="c1"># e.g., `MAX_PIXELS`, etc.</span>
        <span class="n">patch_qwen_vl_utils</span><span class="p">(</span><span class="n">vision_process</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">processor</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">,</span> <span class="n">processor</span><span class="p">:</span> <span class="n">Processor</span><span class="p">,</span>
                  <span class="n">model_kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PreTrainedModel</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen2_5OmniForConditionalGeneration</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Run my_qwen2_5_omni...&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">auto_model_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_model_cls</span> <span class="ow">or</span> <span class="n">Qwen2_5OmniForConditionalGeneration</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">processor</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>
        <span class="c1"># For multimodal model consistency, we replace the model&#39;s forward/generate functions</span>
        <span class="c1"># with those of its language_model.</span>
        <span class="c1"># Handle additional parts separately.</span>
        <span class="n">use_submodel_func</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;thinker&#39;</span><span class="p">)</span>
        <span class="c1"># Avoid inplace operations on leaf_variable during training</span>
        <span class="c1"># (replacing parts of input_embeds with images_embeds)</span>
        <span class="n">patch_get_input_embeddings</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">visual</span><span class="p">,</span> <span class="s1">&#39;patch_embed&#39;</span><span class="p">)</span>
        <span class="c1"># Some custom settings for model/config (usually not needed; configure based on</span>
        <span class="c1"># specific model if errors occur during training/inference)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">keys_to_ignore_at_inference</span> <span class="o">+=</span> <span class="p">[</span><span class="s1">&#39;hidden_states&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">talker_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="n">register_model</span><span class="p">(</span>
    <span class="n">ModelMeta</span><span class="p">(</span>
        <span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span>
        <span class="p">[</span>
            <span class="n">ModelGroup</span><span class="p">([</span>
                <span class="n">Model</span><span class="p">(</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-3B&#39;</span><span class="p">,</span> <span class="s1">&#39;Qwen/Qwen2.5-Omni-3B&#39;</span><span class="p">),</span>
                <span class="n">Model</span><span class="p">(</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">,</span> <span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">),</span>
            <span class="p">]),</span>
        <span class="p">],</span>
        <span class="c1"># Function to get model and processor.</span>
        <span class="n">Qwen2_5OmniLoader</span><span class="p">,</span>
        <span class="n">template</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span>
        <span class="n">is_multimodal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Whether it&#39;s a multimodal model</span>
        <span class="n">model_arch</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span>  <span class="c1"># Usually set only for multimodal models</span>
        <span class="c1"># Used for automatic model_type matching</span>
        <span class="n">architectures</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Qwen2_5OmniModel&#39;</span><span class="p">,</span> <span class="s1">&#39;Qwen2_5OmniForConditionalGeneration&#39;</span><span class="p">],</span>
        <span class="c1"># Used to prompt users about dependency versions (can be removed)</span>
        <span class="n">requires</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformers&gt;=4.50&#39;</span><span class="p">,</span> <span class="s1">&#39;soundfile&#39;</span><span class="p">,</span> <span class="s1">&#39;qwen_omni_utils&#39;</span><span class="p">,</span> <span class="s1">&#39;decord&#39;</span><span class="p">],</span>
        <span class="c1"># Used to prompt users (can be removed)</span>
        <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;vision&#39;</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">,</span> <span class="s1">&#39;audio&#39;</span><span class="p">],</span>
        <span class="c1"># Additional files to save during full parameter training/merge-lora</span>
        <span class="n">additional_saved_files</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;spk_dict.pt&#39;</span><span class="p">],</span>
    <span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># Test and debug</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">get_model_processor</span><span class="p">(</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="template-registration">
<h2>Template Registration<a class="headerlink" href="#template-registration" title="Link to this heading">ÔÉÅ</a></h2>
<p>Second, we need to register a template to customize how text, images, videos, and audio are preprocessed (<code class="docutils literal notranslate"><span class="pre">_encode</span></code> and <code class="docutils literal notranslate"><span class="pre">_data_collator</span></code> methods). This is a key module for ms-swift's support of multimodal model training. Preprocessing methods should reference transformers inference implementation and align with it.</p>
<p>Template functions:</p>
<ol class="simple">
<li><p>Support normal inference and training, preprocess text and multimodal information, and support grounding tasks.</p></li>
<li><p>Support padding_free and packing training.</p></li>
<li><p>Support mixed modality data training.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.integrations</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_deepspeed_zero3_enabled</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_model_processor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.template</span><span class="w"> </span><span class="kn">import</span> <span class="n">StdTemplateInputs</span><span class="p">,</span> <span class="n">Template</span><span class="p">,</span> <span class="n">TemplateMeta</span><span class="p">,</span> <span class="n">get_template</span><span class="p">,</span> <span class="n">register_template</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.template.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Context</span><span class="p">,</span> <span class="n">findall</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.template.vision_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_audio</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Processor</span><span class="p">,</span> <span class="n">get_env_args</span><span class="p">,</span> <span class="n">get_logger</span><span class="p">,</span> <span class="n">get_packed_seq_params</span><span class="p">,</span> <span class="n">is_deepspeed_enabled</span><span class="p">,</span> <span class="n">to_float_dtype</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">get_logger</span><span class="p">()</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5OmniTemplate</span><span class="p">(</span><span class="n">Template</span><span class="p">):</span>
    <span class="n">use_model</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Whether model participation is required during preprocessing</span>
    <span class="c1"># Note: Not all multimodal models support padding_free/packing. Models in `transformers` library usually support it</span>
    <span class="n">support_padding_free</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Whether padding_free and packing are supported (multimodal models)</span>
    <span class="n">norm_bbox</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span>  <span class="c1"># Whether grounding tasks use absolute or norm1000 coordinates</span>

    <span class="c1"># These tokens will not be truncated (e.g., when setting `--truncation_strategy left/right`)</span>
    <span class="c1"># and will be printed in abbreviated form (calling `template.safe_decode`)</span>
    <span class="n">placeholder_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;|IMAGE|&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;|AUDIO|&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;|VIDEO|&gt;&#39;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">processor</span><span class="p">:</span> <span class="n">Processor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize some required constants when initializing the processor&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">processor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">init_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.qwen2_5_omni.processing_qwen2_5_omni</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen2_5OmniProcessorKwargs</span>
        <span class="n">default</span> <span class="o">=</span> <span class="n">Qwen2_5OmniProcessorKwargs</span><span class="o">.</span><span class="n">_defaults</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seconds_per_chunk</span> <span class="o">=</span> <span class="n">default</span><span class="p">[</span><span class="s1">&#39;videos_kwargs&#39;</span><span class="p">][</span><span class="s1">&#39;seconds_per_chunk&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_id_per_seconds</span> <span class="o">=</span> <span class="n">default</span><span class="p">[</span><span class="s1">&#39;videos_kwargs&#39;</span><span class="p">][</span><span class="s1">&#39;position_id_per_seconds&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_audio_in_video</span> <span class="o">=</span> <span class="n">get_env_args</span><span class="p">(</span><span class="s1">&#39;use_audio_in_video&#39;</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">get_env_args</span><span class="p">(</span><span class="s1">&#39;sampling_rate&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
        <span class="c1"># See grounding dataset customization documentation for `QWENVL_BBOX_FORMAT` meaning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bbox_format</span> <span class="o">=</span> <span class="n">get_env_args</span><span class="p">(</span><span class="s1">&#39;QWENVL_BBOX_FORMAT&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;legacy&#39;</span><span class="p">)</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">replace_tag</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">media_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">,</span> <span class="s1">&#39;audio&#39;</span><span class="p">],</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="p">:</span> <span class="n">StdTemplateInputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Context</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load multimodal data and replace generic multimodal tags.</span>
<span class="sd">        For example: image tag from `&lt;image&gt;` -&gt; `&lt;|vision_bos|&gt;&lt;|IMAGE|&gt;&lt;|vision_eos|&gt;`&quot;&quot;&quot;</span>
        <span class="c1"># Loading multimodal data can also be done in the `_encode` function, whichever is more convenient.</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">qwen_omni_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_image</span><span class="p">,</span> <span class="n">fetch_video</span>
        <span class="k">if</span> <span class="n">media_type</span> <span class="o">==</span> <span class="s1">&#39;image&#39;</span><span class="p">:</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fetch_image</span><span class="p">({</span><span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">inputs</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]})</span>
            <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;&lt;|vision_bos|&gt;&lt;|IMAGE|&gt;&lt;|vision_eos|&gt;&#39;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">media_type</span> <span class="o">==</span> <span class="s1">&#39;audio&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;vllm&#39;</span><span class="p">:</span>  <span class="c1"># No processing needed in &#39;vllm&#39; inference scenario</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">audios</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_audio</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">audios</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;&lt;|audio_bos|&gt;&lt;|AUDIO|&gt;&lt;|audio_eos|&gt;&#39;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">media_type</span> <span class="o">==</span> <span class="s1">&#39;video&#39;</span><span class="p">:</span>
            <span class="n">video</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">videos</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">_video</span> <span class="o">=</span> <span class="n">fetch_video</span><span class="p">({</span><span class="s1">&#39;video&#39;</span><span class="p">:</span> <span class="n">video</span><span class="p">})</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_video</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">_video</span> <span class="o">=</span> <span class="n">_video</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">videos</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">_video</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_audio_in_video</span><span class="p">:</span>
                <span class="kn">import</span><span class="w"> </span><span class="nn">librosa</span>
                <span class="k">if</span> <span class="n">video</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;http://&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">video</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;https://&#39;</span><span class="p">):</span>
                    <span class="kn">import</span><span class="w"> </span><span class="nn">audioread</span>
                    <span class="n">video</span> <span class="o">=</span> <span class="n">audioread</span><span class="o">.</span><span class="n">ffdec</span><span class="o">.</span><span class="n">FFmpegAudioFile</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
                <span class="n">video</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">video</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">audios</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">audio_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">video</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">))</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">audio_idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;&lt;|vision_bos|&gt;&lt;|audio_bos|&gt;&lt;|VIDEO|&gt;&lt;|audio_eos|&gt;&lt;|vision_eos|&gt;&#39;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;&lt;|vision_bos|&gt;&lt;|VIDEO|&gt;&lt;|vision_eos|&gt;&#39;</span><span class="p">]</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">replace_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ref</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">StdTemplateInputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Context</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Replace generic tag for grounding tasks: `&lt;ref-object&gt;`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bbox_format</span> <span class="o">==</span> <span class="s1">&#39;legacy&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;&lt;|object_ref_start|&gt;</span><span class="si">{</span><span class="n">ref</span><span class="si">}</span><span class="s1">&lt;|object_ref_end|&gt;&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">ref</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replace_bbox</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bbox</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">StdTemplateInputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Context</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Replace generic tag for grounding tasks: `&lt;bbox&gt;`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bbox_format</span> <span class="o">==</span> <span class="s1">&#39;legacy&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;&lt;|box_start|&gt;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_bbox_str</span><span class="p">(</span><span class="n">bbox</span><span class="p">)</span><span class="si">}</span><span class="s1">&lt;|box_end|&gt;&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">bbox</span><span class="p">)]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">packing_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Support packing &amp; mrope.</span>

<span class="sd">        Usually no need to inherit this function; here for customizing mrope&#39;s position_ids.&quot;&quot;&quot;</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">row</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">r</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])[</span><span class="kc">None</span><span class="p">]</span>
            <span class="n">position_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_position_ids</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
        <span class="n">packed</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">packing_row</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
        <span class="n">packed</span><span class="p">[</span><span class="s1">&#39;position_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">packed</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_new_tokens_use_audio_in_video</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">video_grid_thw</span><span class="p">,</span> <span class="n">video_second_per_grid</span><span class="p">,</span> <span class="n">audio_lengths</span><span class="p">,</span>
                                           <span class="n">video_token_id</span><span class="p">,</span> <span class="n">audio_token_id</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Helper function to support `use_audio_in_video` being True&quot;&quot;&quot;</span>
        <span class="n">merge_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">image_processor</span><span class="o">.</span><span class="n">merge_size</span>
        <span class="n">grid_thw</span> <span class="o">=</span> <span class="n">video_grid_thw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">height</span> <span class="o">=</span> <span class="n">grid_thw</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">merge_size</span>
        <span class="n">width</span> <span class="o">=</span> <span class="n">grid_thw</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="n">merge_size</span>
        <span class="n">audio_token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">audio_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">video_token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">grid_thw</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">video_token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">video_token_indices</span><span class="p">,</span>
                                                 <span class="p">(</span><span class="n">video_token_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">video_token_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">video_token_indices</span> <span class="o">*</span> <span class="n">video_second_per_grid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_id_per_seconds</span><span class="p">)</span>
        <span class="n">tokens_per_chunk</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position_id_per_seconds</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seconds_per_chunk</span><span class="p">)</span>
        <span class="n">video_chunk_indexes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">get_chunked_index</span><span class="p">(</span><span class="n">video_token_indices</span><span class="p">,</span> <span class="n">tokens_per_chunk</span><span class="p">)</span>
        <span class="n">audio_chunk_indexes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">get_chunked_index</span><span class="p">(</span><span class="n">audio_token_indices</span><span class="p">,</span> <span class="n">tokens_per_chunk</span><span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">video_chunk_indexes</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">audio_chunk_indexes</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">video_chunk_indexes</span><span class="p">):</span>
                <span class="n">video_seq_length</span> <span class="o">=</span> <span class="n">video_chunk_indexes</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">video_chunk_indexes</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">res</span> <span class="o">+=</span> <span class="n">video_token_id</span> <span class="o">*</span> <span class="n">video_seq_length</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">audio_chunk_indexes</span><span class="p">):</span>
                <span class="n">audio_seq_length</span> <span class="o">=</span> <span class="n">audio_chunk_indexes</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">audio_chunk_indexes</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">res</span> <span class="o">+=</span> <span class="n">audio_token_id</span> <span class="o">*</span> <span class="n">audio_seq_length</span>
        <span class="k">return</span> <span class="n">res</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">_encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">StdTemplateInputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This determines how to convert text/images/audios/videos -&gt; input_ids, labels, loss_scale, and multimodal content like pixel_values</span>
<span class="sd">        Processing logic can usually be borrowed from the corresponding model&#39;s preprocessing code implementation.</span>
<span class="sd">        Recommended: Perform inference alignment first, then training&quot;&quot;&quot;</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">Template</span><span class="o">.</span><span class="n">_encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>  <span class="c1"># Process text-only part; see custom model documentation for details</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info_once</span><span class="p">(</span><span class="s1">&#39;Run qwen2_5_omni template&#39;</span><span class="p">)</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span>
        <span class="c1"># Get multimodal content</span>
        <span class="n">media_inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
            <span class="n">audio</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">audios</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">images</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">images</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">videos</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">videos</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">do_resize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
        <span class="c1"># We don&#39;t use input_ids and attention_mask produced by `processor` because it doesn&#39;t produce `labels`.</span>
        <span class="n">media_inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;input_ids&#39;</span><span class="p">)</span>
        <span class="n">media_inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">)</span>
        <span class="n">media_inputs</span> <span class="o">=</span> <span class="n">to_float_dtype</span><span class="p">(</span><span class="n">media_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_info</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span>
        <span class="n">loss_scale</span> <span class="o">=</span> <span class="n">encoded</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;loss_scale&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># audio modality</span>
        <span class="n">audio_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenize</span><span class="p">(</span><span class="s1">&#39;&lt;|AUDIO|&gt;&#39;</span><span class="p">)</span>
        <span class="n">idx_list</span> <span class="o">=</span> <span class="n">findall</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">audio_token_id</span><span class="p">)</span>  <span class="c1"># Find all audio_tokens</span>
        <span class="n">feature_attention_mask</span> <span class="o">=</span> <span class="n">media_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;feature_attention_mask&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">feature_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">audio_feature_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">feature_attention_mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">audio_lengths</span> <span class="o">=</span> <span class="p">((</span><span class="n">audio_feature_lengths</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">audio_lengths</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">audio_lengths_origin</span> <span class="o">=</span> <span class="n">audio_lengths</span>
        <span class="c1"># video_audios_mask is used to handle `use_audio_in_video`, distinguishing pure audio(0) from audio in video(1)</span>
        <span class="n">video_audios_mask</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">audio</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">audios</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">audio</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;video&#39;</span><span class="p">:</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">audios</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">video_audios_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">video_audios_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">video_audios_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">video_audios_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">idx_list</span><span class="p">:</span>
            <span class="c1"># Filter out audio content in videos (will be handled in video section)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_audio_in_video</span><span class="p">:</span>
                <span class="n">audio_lengths</span> <span class="o">=</span> <span class="n">audio_lengths</span><span class="p">[</span><span class="o">~</span><span class="n">video_audios_mask</span><span class="p">]</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">_get_new_audio_tokens</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">audio_token_id</span> <span class="o">*</span> <span class="n">audio_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="c1"># Expand multimodal tokens in input_ids</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extend_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span><span class="p">,</span> <span class="n">idx_list</span><span class="p">,</span>
                                                                <span class="n">_get_new_audio_tokens</span><span class="p">)</span>

        <span class="c1"># image and video modalities</span>
        <span class="k">for</span> <span class="n">media_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">]:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;&lt;|</span><span class="si">{</span><span class="n">media_type</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s1">|&gt;&#39;</span>
            <span class="n">token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenize</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">idx_list</span> <span class="o">=</span> <span class="n">findall</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_id</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">idx_list</span><span class="p">:</span>
                <span class="n">merge_size</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">image_processor</span><span class="o">.</span><span class="n">merge_size</span>
                <span class="n">media_grid_thw</span> <span class="o">=</span> <span class="n">media_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">media_type</span><span class="si">}</span><span class="s1">_grid_thw&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">media_type</span> <span class="o">==</span> <span class="s1">&#39;video&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_audio_in_video</span><span class="p">:</span>
                    <span class="n">audio_lengths</span> <span class="o">=</span> <span class="n">audio_lengths_origin</span><span class="p">[</span><span class="n">video_audios_mask</span><span class="p">]</span>
                    <span class="n">video_second_per_grid</span> <span class="o">=</span> <span class="n">media_inputs</span><span class="p">[</span><span class="s1">&#39;video_second_per_grid&#39;</span><span class="p">]</span>
                    <span class="n">_get_new_tokens_use_audio_in_video</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_get_new_tokens_use_audio_in_video</span><span class="p">,</span>
                        <span class="n">video_grid_thw</span><span class="o">=</span><span class="n">media_grid_thw</span><span class="p">,</span>
                        <span class="n">video_second_per_grid</span><span class="o">=</span><span class="n">video_second_per_grid</span><span class="p">,</span>
                        <span class="n">audio_lengths</span><span class="o">=</span><span class="n">audio_lengths</span><span class="p">,</span>
                        <span class="n">video_token_id</span><span class="o">=</span><span class="n">token_id</span><span class="p">,</span>
                        <span class="n">audio_token_id</span><span class="o">=</span><span class="n">audio_token_id</span><span class="p">)</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extend_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span><span class="p">,</span> <span class="n">idx_list</span><span class="p">,</span>
                                                                        <span class="n">_get_new_tokens_use_audio_in_video</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>

                    <span class="k">def</span><span class="w"> </span><span class="nf">_get_new_tokens</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
                        <span class="n">token_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">media_grid_thw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span> <span class="o">//</span> <span class="p">(</span><span class="n">merge_size</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
                        <span class="k">return</span> <span class="n">token_id</span> <span class="o">*</span> <span class="n">token_len</span>

                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extend_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span><span class="p">,</span> <span class="n">idx_list</span><span class="p">,</span>
                                                                        <span class="n">_get_new_tokens</span><span class="p">)</span>

        <span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_ids</span>
        <span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;loss_scale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_scale</span>
        <span class="n">encoded</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">media_inputs</span><span class="p">)</span>  <span class="c1"># Add multimodal content</span>
        <span class="k">return</span> <span class="n">encoded</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This function is typically used to solve the zero2/zero3 hanging issue in mixed model training,</span>
<span class="sd">        i.e., some processes have pure text data without passing through vit, while others have image data that passed through vit.</span>
<span class="sd">        Here we create dummy_image to solve this.</span>

<span class="sd">        This function will be registered in the pre_forward_hook before `model.forward`.</span>
<span class="sd">        This function should return input_embeds containing multimodal information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;input_features&#39;</span><span class="p">)</span>
        <span class="n">feature_attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;feature_attention_mask&#39;</span><span class="p">)</span>

        <span class="n">base_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_base_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">base_model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">thinker_config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">thinker_config</span>
        <span class="c1"># Helper function for handling text/image/video mixed modality data scenarios. (internally creates dummy_image)</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_inputs_embeds_hf</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">visual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="p">,</span>
                                                   <span class="n">thinker_config</span><span class="p">)</span>
        <span class="c1"># Mixed modality data scenarios containing audio</span>
        <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_deepspeed_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_deepspeed_zero3_enabled</span><span class="p">():</span>
                <span class="c1"># Note: Due to transformers implementation, the number of passes through audio model layers is related to the number of audios</span>
                <span class="c1"># Therefore, zero3 will hang in scenarios where different processes have different numbers of audios (requires modification of transformers code to fix). Use zero2 in this scenario.</span>
                <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">audio_tower</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">feature_attention_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new_ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
                <span class="n">audio_res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">get_audio_features</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">feature_attention_mask</span><span class="p">)</span>
                <span class="c1"># Compatible with transformers 5.0</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">audio_res</span><span class="p">,</span> <span class="s1">&#39;last_hidden_state&#39;</span><span class="p">):</span>
                    <span class="n">audio_embeds</span> <span class="o">=</span> <span class="n">audio_res</span><span class="o">.</span><span class="n">last_hidden_state</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">audio_embeds</span> <span class="o">=</span> <span class="n">audio_res</span>
                <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">audio_embeds</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">audio_res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">get_audio_features</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">feature_attention_mask</span><span class="p">)</span>
            <span class="c1"># Compatible with transformers 5.0</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">audio_res</span><span class="p">,</span> <span class="s1">&#39;last_hidden_state&#39;</span><span class="p">):</span>
                <span class="n">audio_embeds</span> <span class="o">=</span> <span class="n">audio_res</span><span class="o">.</span><span class="n">last_hidden_state</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">audio_embeds</span> <span class="o">=</span> <span class="n">audio_res</span>
            <span class="n">audio_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">==</span> <span class="n">thinker_config</span><span class="o">.</span><span class="n">audio_token_index</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span>
            <span class="n">audio_embeds</span> <span class="o">=</span> <span class="n">audio_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">masked_scatter</span><span class="p">(</span><span class="n">audio_mask</span><span class="p">,</span> <span class="n">audio_embeds</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;inputs_embeds&#39;</span><span class="p">:</span> <span class="n">inputs_embeds</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_position_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Helper function to get mrope&#39;s position_ids&quot;&quot;&quot;</span>
        <span class="n">feature_attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;feature_attention_mask&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">feature_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">audio_feature_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">feature_attention_mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">audio_feature_lengths</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">video_second_per_grid</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;video_second_per_grid&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">position_ids</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">thinker</span><span class="o">.</span><span class="n">get_rope_index</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;image_grid_thw&#39;</span><span class="p">),</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;video_grid_thw&#39;</span><span class="p">),</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_audio_in_video</span><span class="p">,</span>
            <span class="n">audio_feature_lengths</span><span class="p">,</span>
            <span class="n">video_second_per_grid</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concat_text_position_ids</span><span class="p">(</span><span class="n">position_ids</span><span class="p">)</span>  <span class="c1"># First dimension is text_position_ids</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_data_collator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="o">*</span><span class="p">,</span> <span class="n">padding_to</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Passed to dataloader&#39;s `collate_fn`&quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_data_collator</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">padding_to</span><span class="o">=</span><span class="n">padding_to</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_free</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="c1"># padding_free/packing scenarios will handle position_ids in packing_row.</span>
            <span class="n">res</span><span class="p">[</span><span class="s1">&#39;position_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_position_ids</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;position_ids&#39;</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
            <span class="c1"># Create `packed_seq_params` to support padding_free/packing &amp; flash-attn</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;position_ids&#39;</span><span class="p">]</span>
            <span class="n">res</span><span class="p">[</span><span class="s1">&#39;position_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">res</span><span class="p">[</span><span class="s1">&#39;text_position_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># https://github.com/huggingface/transformers/pull/40194</span>
            <span class="n">res</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">get_packed_seq_params</span><span class="p">(</span><span class="n">text_position_ids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_data_collator_mm_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Handle multimodal part in `_data_collator` function. (This function is compatible with padding_free/packing)&quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_data_collator_mm_data</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">video_second_per_grid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_list</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="s1">&#39;video_second_per_grid&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">video_second_per_grid</span><span class="p">:</span>
            <span class="n">res</span><span class="p">[</span><span class="s1">&#39;video_second_per_grid&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">video_second_per_grid</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="s1">&#39;input_features&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;input_features&#39;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">feature_attention_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">b</span><span class="p">[</span><span class="s1">&#39;feature_attention_mask&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;feature_attention_mask&#39;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">input_features</span><span class="p">:</span>
            <span class="n">res</span><span class="p">[</span><span class="s1">&#39;input_features&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
            <span class="n">res</span><span class="p">[</span><span class="s1">&#39;feature_attention_mask&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">feature_attention_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;`TransformersEngine` will call template.generate method for text generation; inherit here for customization.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;video_grid_thw&#39;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;use_audio_in_video&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_audio_in_video</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="n">register_template</span><span class="p">(</span>
    <span class="n">TemplateMeta</span><span class="p">(</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="p">[],</span> <span class="n">prompt</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&lt;|im_start|&gt;user</span><span class="se">\n</span><span class="s1">{{QUERY}}&lt;|im_end|&gt;</span><span class="se">\n</span><span class="s1">&lt;|im_start|&gt;assistant</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">],</span>
                 <span class="n">chat_sep</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&lt;|im_end|&gt;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">],</span> <span class="n">suffix</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&lt;|im_end|&gt;&#39;</span><span class="p">],</span>
                 <span class="n">system_prefix</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&lt;|im_start|&gt;system</span><span class="se">\n</span><span class="s1">{{SYSTEM}}&lt;|im_end|&gt;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">],</span>
                 <span class="n">default_system</span><span class="o">=</span><span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&lt;|endoftext|&gt;&#39;</span><span class="p">],</span>
                 <span class="n">agent_template</span><span class="o">=</span><span class="s1">&#39;hermes&#39;</span><span class="p">,</span>
                 <span class="n">template_cls</span><span class="o">=</span><span class="n">Qwen2_5OmniTemplate</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># Test and debug</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">get_model_processor</span><span class="p">(</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">)</span>
    <span class="n">template</span> <span class="o">=</span> <span class="n">get_template</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="n">template_type</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Describe the video&lt;video&gt; and image&lt;image&gt; content.&#39;</span><span class="p">},</span>
            <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;A child and a cat.&#39;</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="s1">&#39;videos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">],</span>
        <span class="s1">&#39;images&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png&#39;</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="n">template</span><span class="o">.</span><span class="n">set_mode</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input_ids: &#39;</span> <span class="o">+</span> <span class="n">template</span><span class="o">.</span><span class="n">safe_decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;labels: &#39;</span> <span class="o">+</span> <span class="n">template</span><span class="o">.</span><span class="n">safe_decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;keys: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">encoded</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section id="inference-alignment">
<h2>Inference Alignment<a class="headerlink" href="#inference-alignment" title="Link to this heading">ÔÉÅ</a></h2>
<p>Next, you need to align inference between TransformersEngine and transformers. Typically you need to align <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> and output content. You can write the following test function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen2_5OmniForConditionalGeneration</span><span class="p">,</span> <span class="n">Qwen2_5OmniProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">qwen_omni_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">process_mm_info</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">modelscope</span><span class="w"> </span><span class="kn">import</span> <span class="n">snapshot_download</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">swift.infer_engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">TransformersEngine</span><span class="p">,</span> <span class="n">InferRequest</span><span class="p">,</span> <span class="n">RequestConfig</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="k">def</span><span class="w"> </span><span class="nf">infer_hf</span><span class="p">():</span>
    <span class="n">model_dir</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Qwen2_5OmniForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">model_dir</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s1">&#39;flash_attention_2&#39;</span><span class="p">)</span>
    <span class="n">processor</span> <span class="o">=</span> <span class="n">Qwen2_5OmniProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="c1"># Use decord to read video (url not yet supported)</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;_baby.mp4&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

    <span class="n">conversation</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;video&quot;</span><span class="p">,</span> <span class="s2">&quot;video&quot;</span><span class="p">:</span> <span class="s2">&quot;_baby.mp4&quot;</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="s2">&quot;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png&quot;</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe the video and image.&quot;</span><span class="p">},</span>
            <span class="p">],</span>
        <span class="p">},</span>
    <span class="p">]</span>

    <span class="n">USE_AUDIO_IN_VIDEO</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">audios</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">videos</span> <span class="o">=</span> <span class="n">process_mm_info</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span> <span class="n">use_audio_in_video</span><span class="o">=</span><span class="n">USE_AUDIO_IN_VIDEO</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">audio</span><span class="o">=</span><span class="n">audios</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">images</span><span class="p">,</span> <span class="n">videos</span><span class="o">=</span><span class="n">videos</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">use_audio_in_video</span><span class="o">=</span><span class="n">USE_AUDIO_IN_VIDEO</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">text_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">use_audio_in_video</span><span class="o">=</span><span class="n">USE_AUDIO_IN_VIDEO</span><span class="p">,</span> <span class="n">thinker_do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">return_audio</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">text_ids</span><span class="p">[:,</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_my_qwen2_5_omni</span><span class="p">():</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">TransformersEngine</span><span class="p">(</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span> <span class="n">attn_impl</span><span class="o">=</span><span class="s1">&#39;flash_attention_2&#39;</span><span class="p">)</span>
    <span class="n">infer_request</span> <span class="o">=</span> <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;video&gt;&lt;image&gt;Describe the video and image.&quot;</span><span class="p">,</span>
    <span class="p">}],</span>
        <span class="n">videos</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&quot;</span><span class="p">],</span>
        <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">request_config</span> <span class="o">=</span> <span class="n">RequestConfig</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">template</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">infer_request</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
    <span class="n">resp_list</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">infer</span><span class="p">([</span><span class="n">infer_request</span><span class="p">],</span> <span class="n">request_config</span><span class="p">)</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">resp_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
    <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">resp</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># Enable debug mode, will print input_ids and generate_ids from `TransformersEngine.infer`</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SWIFT_DEBUG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
    <span class="n">input_ids_hf</span><span class="p">,</span> <span class="n">response_hf</span> <span class="o">=</span> <span class="n">infer_hf</span><span class="p">()</span>
    <span class="n">input_ids_swift</span><span class="p">,</span> <span class="n">response_swift</span> <span class="o">=</span> <span class="n">test_my_qwen2_5_omni</span><span class="p">()</span>
    <span class="c1"># Test input_ids and response alignment</span>
    <span class="k">assert</span> <span class="n">input_ids_hf</span> <span class="o">==</span> <span class="n">input_ids_swift</span>
    <span class="k">assert</span> <span class="n">response_hf</span> <span class="o">==</span> <span class="n">response_swift</span>
</pre></div>
</div>
</section>
<section id="start-training">
<h2>Start Training<a class="headerlink" href="#start-training" title="Link to this heading">ÔÉÅ</a></h2>
<p>Train using Python code, which is usually easier to debug:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">swift</span><span class="w"> </span><span class="kn">import</span> <span class="n">sft_main</span><span class="p">,</span> <span class="n">SftArguments</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MAX_PIXELS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1003520&#39;</span>
    <span class="n">sft_main</span><span class="p">(</span><span class="n">SftArguments</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s1">&#39;Qwen/Qwen2.5-Omni-7B&#39;</span><span class="p">,</span>
        <span class="n">dataset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AI-ModelScope/LaTeX_OCR#5000&#39;</span><span class="p">],</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span>
        <span class="n">template</span><span class="o">=</span><span class="s1">&#39;my_qwen2_5_omni&#39;</span><span class="p">,</span>
        <span class="n">load_from_cache_file</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">split_dataset_ratio</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">tuner_type</span><span class="o">=</span><span class="s1">&#39;lora&#39;</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="s1">&#39;bfloat16&#39;</span><span class="p">,</span>
        <span class="n">attn_impl</span><span class="o">=</span><span class="s1">&#39;flash_attn&#39;</span><span class="p">,</span>
        <span class="n">padding_free</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">lora_rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;all-linear&#39;</span><span class="p">],</span>
        <span class="n">freeze_vit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">freeze_aligner</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">eval_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">save_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">,</span>
        <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">dataloader_num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">dataset_num_proc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">))</span>
</pre></div>
</div>
<p>Train using command line:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4 * 35GiB</span>
<span class="nv">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="s1">&#39;expandable_segments:True&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3<span class="w"> </span><span class="se">\</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="nv">VIDEO_MAX_PIXELS</span><span class="o">=</span><span class="m">50176</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FPS_MAX_FRAMES</span><span class="o">=</span><span class="m">12</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-Omni-7B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_type<span class="w"> </span>my_qwen2_5_omni<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--template<span class="w"> </span>my_qwen2_5_omni<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--external_plugins<span class="w"> </span><span class="s1">&#39;examples/custom/my_qwen2_5_omni/my_register.py&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-zh#2000&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;AI-ModelScope/LaTeX_OCR:human_handwrite#2000&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;speech_asr/speech_asr_aishell1_trainsets:validation#2000&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;swift/VideoChatGPT:all#2000&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_from_cache_file<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--split_dataset_ratio<span class="w"> </span><span class="m">0</span>.01<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--attn_impl<span class="w"> </span>flash_attn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--padding_free<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--packing<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_alpha<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--freeze_vit<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--freeze_aligner<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_steps<span class="w"> </span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_num_proc<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed<span class="w"> </span>zero2
</pre></div>
</div>
<p>Perform inference on the validation set after training: (Environment variables should align with training)</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="s1">&#39;expandable_segments:True&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">VIDEO_MAX_PIXELS</span><span class="o">=</span><span class="m">50176</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FPS_MAX_FRAMES</span><span class="o">=</span><span class="m">12</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>output/vx-xxx/checkpoint-xxx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stream<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_data_args<span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
<p>Use the following command to push training weights to Modelscope:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>swift<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>output/vx-xxx/checkpoint-xxx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--push_to_hub<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--hub_model_id<span class="w"> </span><span class="s1">&#39;&lt;your-model-id&gt;&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--hub_token<span class="w"> </span><span class="s1">&#39;&lt;your-sdk-token&gt;&#39;</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>