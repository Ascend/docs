

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pluginization &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Pluginization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Customization/Pluginization.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pluginization">
<h1>Pluginization<a class="headerlink" href="#pluginization" title="Link to this heading">ïƒ</a></h1>
<blockquote>
<div><p>[!WARNING]
This document is pending update to ms-swift 4.0</p>
</div></blockquote>
<p>Pluginization is a significant new feature introduced in SWIFT 3.0. We aim to make the customization of the development process more natural for developers through a plugin-based approach.</p>
<section id="callback-mechanism">
<h2>Callback Mechanism<a class="headerlink" href="#callback-mechanism" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/callbacks">here</a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">callback</span></code> mechanism is a customization feature in the Transformers Trainer that allows developers to control the training process. Typically, customizing a callback looks like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">TrainerState</span><span class="p">,</span> <span class="n">control</span><span class="p">:</span> <span class="n">TrainerControl</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Doing something when the training begins.</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">on_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">TrainerState</span><span class="p">,</span> <span class="n">control</span><span class="p">:</span> <span class="n">TrainerControl</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Doing something when saving a checkpoint.</span>
        <span class="k">pass</span>
</pre></div>
</div>
<p>Callbacks are registered with the trainer before it is instantiated. The example provided demonstrates a simple version of an EarlyStopping mechanism. Registering your own callback is straightforward:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">extra_callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">CustomCallback</span><span class="p">()]</span>
</pre></div>
</div>
<p>Developers can add new callbacks in <code class="docutils literal notranslate"><span class="pre">plugin/callback.py</span></code> and customize their training process. For detailed parameters of callbacks, refer to <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/callback">this documentation</a>.</p>
</section>
<section id="customizing-loss">
<h2>Customizing Loss<a class="headerlink" href="#customizing-loss" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss/mapping.py">here</a>.</p>
<p>SWIFT supports customizing the loss function in a plugin. If you donâ€™t use this capability, cross-entropy loss (CE Loss) will be used by default. You can write your code in this file, register it, and then enable your custom loss during training by setting <code class="docutils literal notranslate"><span class="pre">--loss_type</span> <span class="pre">custom_loss</span></code> to use your customized loss method.</p>
<p>For example, adding the following code in <code class="docutils literal notranslate"><span class="pre">plugin/loss.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_loss_func</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_items_in_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Write your own loss calculation here</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">loss_map</span><span class="p">[</span><span class="s1">&#39;custom_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">custom_loss_func</span>
</pre></div>
</div>
<p>It is important to note that the loss function is strongly related to the training task. Currently, loss customization supports PT and SFT tasks. For human alignment tasks (e.g., DPO, PPO) or classification tasks (seq_cls), loss customization through plugins is not supported.</p>
</section>
<section id="customizing-loss-scale">
<h2>Customizing Loss Scale<a class="headerlink" href="#customizing-loss-scale" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss_scale/mapping.py">here</a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> mechanism is one of the crucial features in SWIFT. In PT and SFT tasks, the loss for trainable tokens is uniform, meaning each token is equally involved in backpropagation. However, in certain situations, some tokens require higher weights and extra attention. In such cases, <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> allows developers to define custom token weights.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LastRoundLossScale</span><span class="p">(</span><span class="n">LossScale</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_loss_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context_type</span><span class="p">:</span> <span class="n">ContextType</span><span class="p">,</span> <span class="n">is_last_round</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">context_type</span> <span class="o">==</span> <span class="n">ContextType</span><span class="o">.</span><span class="n">RESPONSE</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">context</span><span class="p">],</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">is_last_round</span><span class="p">)]</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_loss_scale</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context_type</span><span class="p">,</span> <span class="n">is_last_round</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above code, a <code class="docutils literal notranslate"><span class="pre">Tuple</span></code> is returned where the first element is the <code class="docutils literal notranslate"><span class="pre">context</span></code> (or its split parts), and the second element is the corresponding <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code>. The float value represents the weight. For example, the following weight settings:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[&quot;å­¦ä¹ &quot;, &quot;å¥½&quot;, &quot;æ•°å­¦&quot;, &quot;æ˜¯&quot;, &quot;é‡è¦&quot;, &quot;çš„&quot;]
[1.0, 0.5, 2.0, 0.5, 2.0, 0.1]
</pre></div>
</div>
<p>Here, we place more emphasis on the words &quot;æ•°å­¦&quot; (mathematics) and &quot;é‡è¦&quot; (important) by increasing their weights to 2.0.</p>
<p>Referring back to the code, we check if the provided <code class="docutils literal notranslate"><span class="pre">context</span></code> is a response. If it is a response and is the last round in a multi-turn dialogue, we return a <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> of <code class="docutils literal notranslate"><span class="pre">[1]</span></code>. In other cases, we use the base implementation (which sets <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> to <code class="docutils literal notranslate"><span class="pre">[0]</span></code>). This approach ensures that only the responses from the last round participate in training, while other responses do not. Using this method, we can make all tokens (prompts and responses) participate in training or focus on specific special characters of the agent for training, etc.</p>
<p>In PT and SFT, <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> is uniformly supported (whether to participate in training and the size of the weights). However, in human alignment tasks, only the participation of certain tokens in training is supported, not the size of the weights.</p>
</section>
<section id="customizing-metrics">
<h2>Customizing Metrics<a class="headerlink" href="#customizing-metrics" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/metrics">here</a>.</p>
<p>Metrics can be customized to evaluate the training process:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metric_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">compute_acc_metrics</span><span class="p">,</span> <span class="n">preprocess_logits_for_acc</span><span class="p">),</span>
    <span class="s1">&#39;nlg&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">compute_nlg_metrics</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s1">&#39;custom&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">custom_metric</span><span class="p">,</span> <span class="n">custom_preprocess</span><span class="p">),</span>
<span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_metric</span><span class="p">(</span><span class="n">metric</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">metric_mapping</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span>
</pre></div>
</div>
<p>In the above definition, we added a new <code class="docutils literal notranslate"><span class="pre">custom</span></code> metric. Its value consists of two parts: the first is the metric computation process, which returns a dictionary containing metric key-value pairs, and the second is the preprocessing step for logits, which returns the actual predictions.</p>
</section>
<section id="customizing-optimizers">
<h2>Customizing Optimizers<a class="headerlink" href="#customizing-optimizers" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/optimizers/mapping.py">here</a>.</p>
<ul class="simple">
<li><p>Apply different learning rates to different parts of the model. For example, use separate learning rates for ViT and LLM, as referenced <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/lora_llm_full_vit/custom_plugin.py">here</a>.</p></li>
</ul>
<p>Users can add their own optimizers and learning rate schedulers here:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_custom_optimizers</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="c1"># Create your own optimizer</span>
    <span class="k">return</span> <span class="n">CustomOptimizer</span><span class="p">(</span><span class="n">optimizer_grouped_parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">optimizer_kwargs</span><span class="p">),</span> <span class="n">CustomScheduler</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="n">optimizers_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;custom&#39;</span><span class="p">:</span> <span class="n">create_custom_optimizers</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When developers need to use other optimizers, such as those defined in new research papers, they can define their creation process here and specify the parameter:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--optimizer<span class="w"> </span>custom
</pre></div>
</div>
<p>This will invoke the custom optimizer.</p>
</section>
<section id="customizing-agent-template">
<h2>Customizing Agent Template<a class="headerlink" href="#customizing-agent-template" title="Link to this heading">ïƒ</a></h2>
<p>The example is <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/agent_template/mapping.py">here</a>.</p>
</section>
<section id="customizing-tuners">
<h2>Customizing Tuners<a class="headerlink" href="#customizing-tuners" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/tuner_plugin">here</a>.</p>
<ul class="simple">
<li><p>For the multimodal model, full-parameter training is applied to the ViT part, while LoRA training is used for the LLM part. Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal/lora_llm_full_vit">here</a>.</p></li>
<li><p>For Phi4-multimodal, train its existing LoRA directly without adding extra LoRA. Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/plugins/tuner_phi4_mm.sh">here</a>.</p></li>
</ul>
<p>Tuner customization is another unique feature of SWIFT. Developers can bypass the complex tuner initialization process and code integration costs by registering new tuners here:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">IA3</span><span class="p">(</span><span class="n">Tuner</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_model</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="s1">&#39;SftArguments&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">model_arch</span><span class="p">:</span> <span class="n">ModelKeys</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model_meta</span><span class="o">.</span><span class="n">model_arch</span>
        <span class="n">ia3_config</span> <span class="o">=</span> <span class="n">IA3Config</span><span class="p">(</span>
            <span class="n">target_modules</span><span class="o">=</span><span class="n">find_all_linears</span><span class="p">(</span><span class="n">model</span><span class="p">),</span> <span class="n">feedforward_modules</span><span class="o">=</span><span class="s1">&#39;.*&#39;</span> <span class="o">+</span> <span class="n">model_arch</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">.&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;.*&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ia3_config</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_pretrained</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">PeftModel</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above example, we apply PEFT's IA3 to model training. This class includes three methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prepare_model</span></code>: How to wrap the original model using the tuner and set up trainable parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code>: How to save the model during training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code>: How to reload checkpoints saved earlier for subsequent training and inference.</p></li>
</ul>
<p>These three methods are invoked during the SWIFT training process, allowing developers to use their tuners without reading the complex training code.</p>
</section>
<section id="prm-process-reward-model">
<h2>PRM (Process Reward Model)<a class="headerlink" href="#prm-process-reward-model" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/rewards/prm.py">here</a>.</p>
<p>PRM stands for Process Reward Model, which is used in the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">sample</span></code> command. PRM needs to support simple interfaces:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PRM</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># init here</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">infer_requests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InferRequest</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
<p>The InferRequest comes from <code class="docutils literal notranslate"><span class="pre">swift.infer_engine</span></code>, and the returned <code class="docutils literal notranslate"><span class="pre">List[Union[float,</span> <span class="pre">List[float]]]</span></code> may contain a reward or several rewards. Developers can access queries and responses in infer_requests and split them according to their own methods, for example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Let&#39;s think step by step.

Step1: xxx

Step2: xxx

So, the answer is ...
</pre></div>
</div>
<p>Developers can split the process here, batch them into PRM for inference, and return rewards. More generally, developers can call a remote URL here, such as a closed-source PRM large model, and return rewards.</p>
</section>
<section id="orm-outcome-reward-model">
<h2>ORM (Outcome Reward Model)<a class="headerlink" href="#orm-outcome-reward-model" title="Link to this heading">ïƒ</a></h2>
<p>An example can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/rewards/orm.py">here</a>.</p>
<p>ORM stands for Outcome Reward Model. ORM typically uses regular expressions to determine whether a response is correct. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MathORM</span><span class="p">(</span><span class="n">ORM</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">extract_boxed_result</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">boxed{([^}]*)}&#39;</span>
        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">infer_requests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InferRequest</span><span class="p">],</span> <span class="n">ground_truths</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">request</span><span class="o">.</span><span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">request</span> <span class="ow">in</span> <span class="n">infer_requests</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">ground_truths</span><span class="p">):</span>
            <span class="n">res1</span> <span class="o">=</span> <span class="n">MathORM</span><span class="o">.</span><span class="n">extract_boxed_result</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;&#39;</span>
            <span class="n">res2</span> <span class="o">=</span> <span class="n">MathORM</span><span class="o">.</span><span class="n">extract_boxed_result</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;&#39;</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">res1</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="n">res2</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>

        <span class="k">return</span> <span class="n">rewards</span>


<span class="n">orms</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;math&#39;</span><span class="p">:</span> <span class="n">MathORM</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In the above code, we define a process to parse mathematical responses. If the results are the same, it returns a score of <code class="docutils literal notranslate"><span class="pre">1.0</span></code>; otherwise, it returns <code class="docutils literal notranslate"><span class="pre">0.0</span></code>. Unlike PRM, this class's <code class="docutils literal notranslate"><span class="pre">infer</span></code> method includes an additional parameter <code class="docutils literal notranslate"><span class="pre">ground_truths</span></code>, which corresponds to the actual labels (standard responses defined in the dataset) for the <code class="docutils literal notranslate"><span class="pre">infer_requests</span></code>.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>