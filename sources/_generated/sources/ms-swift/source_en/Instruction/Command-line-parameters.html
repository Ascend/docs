

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Command Line Parameters &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Command Line Parameters</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/Command-line-parameters.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="command-line-parameters">
<h1>Command Line Parameters<a class="headerlink" href="#command-line-parameters" title="Link to this heading">ïƒ</a></h1>
<p>The command-line arguments will be introduced in four categories: basic arguments, atomic arguments, integrated arguments, and model-specific arguments. <strong>The final list of arguments used in the command line consists of the integrated arguments, which inherit from the basic arguments and certain atomic arguments</strong>. Model-specific arguments are tailored for particular models and can be configured via <code class="docutils literal notranslate"><span class="pre">--model_kwargs</span></code> or environment variables. For a detailed introduction to Megatron-SWIFT command-line arguments, please refer to the <a class="reference internal" href="../Megatron-SWIFT/Command-line-parameters.html"><span class="doc">Megatron-SWIFT Training Documentation</span></a>.</p>
<p><strong>Tips:</strong></p>
<ul class="simple">
<li><p>To pass a list via the command line, separate the elements with spaces. For example: <code class="docutils literal notranslate"><span class="pre">--dataset</span> <span class="pre">&lt;dataset_path1&gt;</span> <span class="pre">&lt;dataset_path2&gt;</span></code>.</p></li>
<li><p>To pass a dictionary via the command line, use JSON format. For example: <code class="docutils literal notranslate"><span class="pre">--model_kwargs</span> <span class="pre">'{&quot;fps_max_frames&quot;:</span> <span class="pre">12}'</span></code>.</p></li>
<li><p>Parameters marked with ğŸ”¥ are important; new users of ms-swift should prioritize these command-line arguments.</p></li>
</ul>
<section id="base-arguments">
<h2>Base Arguments<a class="headerlink" href="#base-arguments" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>ğŸ”¥tuner_backend: Optional values are <code class="docutils literal notranslate"><span class="pre">'peft'</span></code> and <code class="docutils literal notranslate"><span class="pre">'unsloth'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'peft'</span></code>.</p></li>
<li><p>ğŸ”¥tuner_type: Optional values are <code class="docutils literal notranslate"><span class="pre">'lora'</span></code>, <code class="docutils literal notranslate"><span class="pre">'full'</span></code>, <code class="docutils literal notranslate"><span class="pre">'longlora'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adalora'</span></code>, <code class="docutils literal notranslate"><span class="pre">'llamapro'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adapter'</span></code>, <code class="docutils literal notranslate"><span class="pre">'vera'</span></code>, <code class="docutils literal notranslate"><span class="pre">'boft'</span></code>, <code class="docutils literal notranslate"><span class="pre">'fourierft'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reft'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'lora'</span></code>. (<strong>In ms-swift 3.x, the parameter name is <code class="docutils literal notranslate"><span class="pre">train_type</span></code></strong>)</p></li>
<li><p>ğŸ”¥adapters: A list specifying adapter IDs or paths. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. This parameter is typically used in inference/deployment commands, for example: <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span> <span class="pre">--model</span> <span class="pre">'&lt;model_id_or_path&gt;'</span> <span class="pre">--adapters</span> <span class="pre">'&lt;adapter_id_or_path&gt;'</span></code>. It can occasionally be used for resuming training from a checkpoint. The difference between this parameter and <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> is that <strong>this parameter only loads adapter weights</strong>, without restoring the optimizer state or random seed, and does not skip already-trained portions of the dataset.</p>
<ul>
<li><p>The difference between <code class="docutils literal notranslate"><span class="pre">--model</span></code> and <code class="docutils literal notranslate"><span class="pre">--adapters</span></code>: <code class="docutils literal notranslate"><span class="pre">--model</span></code> is followed by the directory path of the complete weights, which contains full weight information such as model/tokenizer/config, for example <code class="docutils literal notranslate"><span class="pre">model.safetensors</span></code>. <code class="docutils literal notranslate"><span class="pre">--adapters</span></code> is followed by a list of incremental adapter weight directory paths, which contain incremental weight information of the adapters, for example <code class="docutils literal notranslate"><span class="pre">adapter_model.safetensors</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥external_plugins: A list of external <code class="docutils literal notranslate"><span class="pre">plugin.py</span></code> files that will be additionally loaded (i.e., the modules will be imported). Defaults to <code class="docutils literal notranslate"><span class="pre">[]</span></code>. You can pass in <code class="docutils literal notranslate"><span class="pre">.py</span></code> file paths for custom model, template, and dataset registration, see <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/custom/sft.sh">here</a>; or for custom GRPO components, see <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/plugin/run_external_reward_func.sh">here</a>.</p></li>
<li><p>seed: Global random seed. Default is 42.</p>
<ul>
<li><p>Note: This random seed is independent of <code class="docutils literal notranslate"><span class="pre">data_seed</span></code>, which controls randomness in the dataset.</p></li>
</ul>
</li>
<li><p>model_kwargs: Additional arguments specific to certain models. This list of parameters will be logged during training/inference. For example: <code class="docutils literal notranslate"><span class="pre">--model_kwargs</span> <span class="pre">'{&quot;fps_max_frames&quot;:</span> <span class="pre">12}'</span></code>. You can also set it via environment variables, e.g., <code class="docutils literal notranslate"><span class="pre">FPS_MAX_FRAMES=12</span></code>. Default is None.</p>
<ul>
<li><p>Note: <strong>If you specify model-specific parameters during training, please also set the corresponding parameters during inference</strong>â€”this helps maintain consistent performance.</p></li>
<li><p>The meaning of model-specific parameters can usually be found in the official repository or inference code of the corresponding model. MS-Swift includes these parameters to ensure alignment between trained models and official inference behavior.</p></li>
</ul>
</li>
<li><p>load_args: When <code class="docutils literal notranslate"><span class="pre">--resume_from_checkpoint</span></code>, <code class="docutils literal notranslate"><span class="pre">--model</span></code>, or <code class="docutils literal notranslate"><span class="pre">--adapters</span></code> are specified, this flag controls whether to load <code class="docutils literal notranslate"><span class="pre">args.json</span></code> from the saved file. The loaded keys are defined in <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/arguments/base_args/base_args.py">base_args.py</a>. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code> for inference and export, and <code class="docutils literal notranslate"><span class="pre">False</span></code> for training. Usually, this parameter does not need to be modified.</p></li>
<li><p>load_data_args: If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, additional data-related arguments from <code class="docutils literal notranslate"><span class="pre">args.json</span></code> will be loaded. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. <strong>This is typically used during inference to run inference on validation sets split during training</strong>, for example: <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span> <span class="pre">--adapters</span> <span class="pre">xxx</span> <span class="pre">--load_data_args</span> <span class="pre">true</span> <span class="pre">--stream</span> <span class="pre">true</span> <span class="pre">--max_new_tokens</span> <span class="pre">512</span></code>.</p></li>
<li><p>use_hf: Controls whether ModelScope or HuggingFace is used for model downloading, dataset downloading, and model uploading. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code> (uses ModelScope).</p></li>
<li><p>hub_token: Hub authentication token. For ModelScope, see <a class="reference external" href="https://modelscope.cn/my/myaccesstoken">here</a>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>ddp_timeout: Default is 18000000, in seconds.</p></li>
<li><p>ddp_backend: Optional values are <code class="docutils literal notranslate"><span class="pre">&quot;nccl&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;gloo&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mpi&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;ccl&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;hccl&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cncl&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mccl&quot;</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which enables automatic selection.</p></li>
<li><p>ignore_args_error: Used for compatibility with Jupyter Notebook. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
<section id="model-arguments">
<h3>Model Arguments<a class="headerlink" href="#model-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>ğŸ”¥model:  The <a class="reference external" href="https://modelscope.cn/models">model ID</a> or local model path. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>model_type: The model type. In ms-swift, a <code class="docutils literal notranslate"><span class="pre">model_type</span></code> refers to a group of models that share the same architecture, model loading process, and template definition. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning it will be automatically inferred based on the suffix of <code class="docutils literal notranslate"><span class="pre">--model</span></code> and the 'architectures' field in config.json. Supported model types can be found in the <a class="reference internal" href="Supported-models-and-datasets.html"><span class="doc">List of Supported Models and Datasets</span></a></p>
<ul>
<li><p>Note: The concept of <code class="docutils literal notranslate"><span class="pre">model_type</span></code> in MS-Swift differs from the <code class="docutils literal notranslate"><span class="pre">model_type</span></code> in <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p></li>
<li><p>Custom models typically require manually registering a <code class="docutils literal notranslate"><span class="pre">model_type</span></code> and <code class="docutils literal notranslate"><span class="pre">template</span></code>. See the <a class="reference internal" href="../Customization/Custom-model.html"><span class="doc">Custom Model Documentation</span></a> for details.</p></li>
</ul>
</li>
<li><p>model_revision: Model version. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>task_type: Default is <code class="docutils literal notranslate"><span class="pre">'causal_lm'</span></code>. Options include <code class="docutils literal notranslate"><span class="pre">'causal_lm'</span></code>, <code class="docutils literal notranslate"><span class="pre">'seq_cls'</span></code>, <code class="docutils literal notranslate"><span class="pre">'embedding'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reranker'</span></code>, and <code class="docutils literal notranslate"><span class="pre">'generative_reranker'</span></code>. Examples for seq_cls can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/seq_cls">here</a>, and examples for embedding can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/embedding">here</a>.</p></li>
<li><p>ğŸ”¥torch_dtype: Data type for model weights. Supported values: <code class="docutils literal notranslate"><span class="pre">float16</span></code>, <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>, <code class="docutils literal notranslate"><span class="pre">float32</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which reads from the <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file.</p></li>
<li><p>attn_impl: Attention implementation. Options include <code class="docutils literal notranslate"><span class="pre">'sdpa'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eager'</span></code>, <code class="docutils literal notranslate"><span class="pre">'flash_attn'</span></code>, <code class="docutils literal notranslate"><span class="pre">'flash_attention_2'</span></code>, <code class="docutils literal notranslate"><span class="pre">'flash_attention_3'</span></code>, etc. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, reading from config.json.</p>
<ul>
<li><p>Note: Not all attention implementations may be supported, depending on the underlying Transformers library's support for the specific model.</p></li>
<li><p>If set to <code class="docutils literal notranslate"><span class="pre">'flash_attn'</span></code> (for backward compatibility), <code class="docutils literal notranslate"><span class="pre">'flash_attention_2'</span></code> will be used.</p></li>
</ul>
</li>
<li><p>ğŸ”¥experts_impl: Expert implementation type, options are 'grouped_mm', 'batched_mm', 'eager'. Defaults to None. This feature requires &quot;transformers&gt;=5.0.0&quot;.</p></li>
<li><p>new_special_tokens: List of additional special tokens to be added. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. Example usage can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/new_special_tokens">here</a>.</p>
<ul>
<li><p>Note: You can also pass a <code class="docutils literal notranslate"><span class="pre">.txt</span></code> file path where each line contains one special token.</p></li>
</ul>
</li>
<li><p>num_labels: Required for classification models (<code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">seq_cls</span></code>). Indicates the number of labels. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>problem_type: Required for classification models (<code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">seq_cls</span></code>). Options: <code class="docutils literal notranslate"><span class="pre">'regression'</span></code>, <code class="docutils literal notranslate"><span class="pre">'single_label_classification'</span></code>, <code class="docutils literal notranslate"><span class="pre">'multi_label_classification'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If the model is a reward model or <code class="docutils literal notranslate"><span class="pre">num_labels=1</span></code>, it defaults to <code class="docutils literal notranslate"><span class="pre">'regression'</span></code>; otherwise, it defaults to <code class="docutils literal notranslate"><span class="pre">'single_label_classification'</span></code>.</p></li>
<li><p>rope_scaling: Type of RoPE scaling. You can pass a string such as <code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'dynamic'</span></code>, <code class="docutils literal notranslate"><span class="pre">'yarn'</span></code>, along with <code class="docutils literal notranslate"><span class="pre">max_model_len</span></code>, and MS-Swift will automatically configure the corresponding <code class="docutils literal notranslate"><span class="pre">rope_scaling</span></code>, overriding the value in <code class="docutils literal notranslate"><span class="pre">config.json</span></code>. Alternatively, pass a JSON string like <code class="docutils literal notranslate"><span class="pre">'{&quot;factor&quot;:</span> <span class="pre">2.0,</span> <span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;yarn&quot;}'</span></code>, which will directly replace the <code class="docutils literal notranslate"><span class="pre">rope_scaling</span></code> in <code class="docutils literal notranslate"><span class="pre">config.json</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>max_model_len: When using <code class="docutils literal notranslate"><span class="pre">rope_scaling</span></code> with a string input, this parameter helps calculate the RoPE scaling <code class="docutils literal notranslate"><span class="pre">factor</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If specified, this value will <strong>override</strong> <code class="docutils literal notranslate"><span class="pre">max_position_embeddings</span></code> in <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p></li>
<li><p>device_map: Device placement configuration for the model, e.g., <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>, <code class="docutils literal notranslate"><span class="pre">'cpu'</span></code>, a JSON string, or a JSON file path. This argument is <strong>passed through</strong> to the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method in Transformers. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, automatically determined based on available devices and distributed training setup.</p></li>
<li><p>max_memory: When <code class="docutils literal notranslate"><span class="pre">device_map</span></code> is set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> or <code class="docutils literal notranslate"><span class="pre">'sequential'</span></code>, model weights are allocated across devices according to <code class="docutils literal notranslate"><span class="pre">max_memory</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">--max_memory</span> <span class="pre">'{0:</span> <span class="pre">&quot;20GB&quot;,</span> <span class="pre">1:</span> <span class="pre">&quot;20GB&quot;}'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Passed through to the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> interface in Transformers.</p></li>
<li><p>local_repo_path: Some models depend on GitHub repositories during loading, e.g., <a class="reference external" href="https://github.com/deepseek-ai/DeepSeek-VL2">deepseek-vl2</a>. To avoid network issues during <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span></code>, you can use a local repository. This parameter takes the path to the local repo. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>init_strategy: Strategy for initializing uninitialized parameters when loading a model (especially useful for custom architectures). Options: <code class="docutils literal notranslate"><span class="pre">'zero'</span></code>, <code class="docutils literal notranslate"><span class="pre">'uniform'</span></code>, <code class="docutils literal notranslate"><span class="pre">'normal'</span></code>, <code class="docutils literal notranslate"><span class="pre">'xavier_uniform'</span></code>, <code class="docutils literal notranslate"><span class="pre">'xavier_normal'</span></code>, <code class="docutils literal notranslate"><span class="pre">'kaiming_uniform'</span></code>, <code class="docutils literal notranslate"><span class="pre">'kaiming_normal'</span></code>, <code class="docutils literal notranslate"><span class="pre">'orthogonal'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</section>
<section id="data-arguments">
<h3>Data Arguments<a class="headerlink" href="#data-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>ğŸ”¥dataset: A list of dataset IDs or paths. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. Each dataset should be specified in the format: <code class="docutils literal notranslate"><span class="pre">'dataset_id_or_path:subset#sample_size'</span></code>, where subset and sample size are optional. Local datasets support formats such as jsonl, csv, json, and folders. <strong>Open-source datasets from the hub can be used offline by <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span></code>-ing them locally and passing the local folder path</strong>. For custom dataset formats, refer to the <a class="reference internal" href="../Customization/Custom-dataset.html"><span class="doc">Custom Dataset Documentation</span></a>. You can use multiple datasets by passing <code class="docutils literal notranslate"><span class="pre">--dataset</span> <span class="pre">&lt;dataset1&gt;</span> <span class="pre">&lt;dataset2&gt;</span></code>.</p>
<ul>
<li><p>Subset: This parameter is only effective when the dataset is a dataset ID or a folder. If subsets were specified during registration and only one exists, that subset is selected by default; otherwise, the default subset <code class="docutils literal notranslate"><span class="pre">'default'</span></code> is used. You can select multiple subsets using <code class="docutils literal notranslate"><span class="pre">/</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">&lt;dataset_id&gt;:subset1/subset2</span></code>. You can also use <code class="docutils literal notranslate"><span class="pre">'all'</span></code> to select all registered subsets, e.g., <code class="docutils literal notranslate"><span class="pre">&lt;dataset_id&gt;:all</span></code>. See an example of registration <a class="reference external" href="https://modelscope.cn/datasets/swift/garbage_competition">here</a>.</p></li>
<li><p>Sampling size: Uses the complete dataset by default. You can sample from the selected dataset by setting <code class="docutils literal notranslate"><span class="pre">#sample_size</span></code>, for example <code class="docutils literal notranslate"><span class="pre">&lt;dataset_path#1000&gt;</span></code>. If the sample size is less than the total number of samples, random sampling without replacement is performed. If the sample count exceeds the total, the dataset is repeated <code class="docutils literal notranslate"><span class="pre">sample_size</span> <span class="pre">//</span> <span class="pre">total_samples</span></code> times, with an additional <code class="docutils literal notranslate"><span class="pre">sample_size</span> <span class="pre">%</span> <span class="pre">total_samples</span></code> samples randomly sampled. Note: For streaming datasets (<code class="docutils literal notranslate"><span class="pre">--streaming</span> <span class="pre">true</span></code>), only sequential sampling is performed. If <code class="docutils literal notranslate"><span class="pre">--dataset_shuffle</span> <span class="pre">false</span></code> is set, non-streaming datasets also use sequential sampling.</p></li>
</ul>
</li>
<li><p>ğŸ”¥val_dataset: A list of validation dataset IDs or paths. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>ğŸ”¥cached_dataset: Use cached datasets (generated with the command <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span> <span class="pre">--to_cached_dataset</span> <span class="pre">true</span> <span class="pre">...</span></code>) to avoid GPU time being occupied by tokenization during training/inference on large datasets. This parameter is used to set the folder path(s) of cached training datasets, and defaults to <code class="docutils literal notranslate"><span class="pre">[]</span></code>. For examples, see <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/cached_dataset">here</a>.</p>
<ul>
<li><p>Note: cached_dataset only stores an additional length field in the dataset (to avoid storage pressure) and filters out data samples that would cause errors. During training/inference, the <code class="docutils literal notranslate"><span class="pre">--max_length</span></code> parameter is supported for filtering/truncating excessively long data and the <code class="docutils literal notranslate"><span class="pre">--packing</span></code> parameter is supported. The actual data preprocessing process occurs synchronously during training and overlaps with the training process, which does not affect training speed.</p></li>
<li><p>cached_dataset is compatible between <code class="docutils literal notranslate"><span class="pre">ms-swift</span></code> and <code class="docutils literal notranslate"><span class="pre">Megatron-SWIFT</span></code>, and supports pt/sft/infer/rlhf, set the training type using <code class="docutils literal notranslate"><span class="pre">--template_mode</span></code>; supports embedding/reranker/seq_cls tasks, set the task type using <code class="docutils literal notranslate"><span class="pre">--task_type</span></code>.</p></li>
<li><p>Supports sampling from cache_dataset with the syntax <code class="docutils literal notranslate"><span class="pre">&lt;cached_dataset_path&gt;#sample_count</span></code>, supports sampling counts both higher and lower than the number of samples. For functionality and implementation, refer to the <code class="docutils literal notranslate"><span class="pre">--dataset</span></code> description.</p></li>
</ul>
</li>
<li><p>cached_val_dataset: Folder path(s) for cached validation datasets, default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>ğŸ”¥split_dataset_ratio: The ratio for splitting a validation set from the training set when <code class="docutils literal notranslate"><span class="pre">val_dataset</span></code> is not specified. Default is <code class="docutils literal notranslate"><span class="pre">0.</span></code>, meaning no splitting occurs.</p></li>
<li><p>data_seed: Random seed for dataset operations. Default is <code class="docutils literal notranslate"><span class="pre">42</span></code>.</p></li>
<li><p>ğŸ”¥dataset_num_proc: Number of processes for dataset preprocessing. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<ul>
<li><p>Note: For text-only models, it is recommended to increase this value to accelerate preprocessing speed. For multimodal models, it is not recommended to set it too high, as this may lead to slower preprocessing speed (if multimodal models experience 100% CPU utilization but extremely slow processing speed, it is recommended to additionally set the <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> environment variable).</p></li>
</ul>
</li>
<li><p>ğŸ”¥load_from_cache_file: Whether to load the dataset from cache. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. <strong>Recommended to set to <code class="docutils literal notranslate"><span class="pre">True</span></code> during actual training and <code class="docutils literal notranslate"><span class="pre">False</span></code> during debugging</strong>. You can modify the <code class="docutils literal notranslate"><span class="pre">MODELSCOPE_CACHE</span></code> environment variable to control the cache path.</p></li>
<li><p>dataset_shuffle: Whether to shuffle the training dataset. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<ul>
<li><p>Note: <strong>Shuffling in CPT/SFT involves two parts</strong>: dataset-level shuffling (controlled by <code class="docutils literal notranslate"><span class="pre">dataset_shuffle</span></code>) and dataloader-level shuffling (controlled by <code class="docutils literal notranslate"><span class="pre">train_dataloader_shuffle</span></code>).</p></li>
</ul>
</li>
<li><p>val_dataset_shuffle: Whether to shuffle the validation dataset. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>streaming: Whether to stream and process the dataset on-the-fly. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. (The shuffling of streaming datasets is not thorough, which may lead to severe loss fluctuations.)</p>
<ul>
<li><p>Note: You must set <code class="docutils literal notranslate"><span class="pre">--max_steps</span></code> explicitly, as streaming datasets do not have a defined length. You can achieve behavior equivalent to <code class="docutils literal notranslate"><span class="pre">--num_train_epochs</span></code> by setting <code class="docutils literal notranslate"><span class="pre">--save_strategy</span> <span class="pre">epoch</span></code> and a large <code class="docutils literal notranslate"><span class="pre">max_steps</span></code>. Alternatively, set <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> to ensure training stops after the specified number of epochs, allowing model evaluation and checkpoint saving.</p></li>
<li><p>Note: Streaming avoids waiting for preprocessing by overlapping it with training. However, preprocessing is only performed on rank 0 and then distributed to other processes. <strong>This is typically less efficient than non-streaming data sharding</strong>. When the training <code class="docutils literal notranslate"><span class="pre">world_size</span></code> is large, preprocessing and data distribution can become a bottleneck.</p></li>
</ul>
</li>
<li><p>interleave_prob: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. By default, multiple datasets are combined using <code class="docutils literal notranslate"><span class="pre">concatenate_datasets</span></code> from the datasets library. If this parameter is set, <code class="docutils literal notranslate"><span class="pre">interleave_datasets</span></code> is used instead. This is typically used for combining streaming datasets and is passed directly to <code class="docutils literal notranslate"><span class="pre">interleave_datasets</span></code>. This parameter does not apply to <code class="docutils literal notranslate"><span class="pre">--val_dataset</span></code>.</p></li>
<li><p>stopping_strategy: Options are <code class="docutils literal notranslate"><span class="pre">&quot;first_exhausted&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;all_exhausted&quot;</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">&quot;first_exhausted&quot;</span></code>. Passed to the <code class="docutils literal notranslate"><span class="pre">interleave_datasets</span></code> function. This parameter does not apply to <code class="docutils literal notranslate"><span class="pre">--val_dataset</span></code>.</p></li>
<li><p>shuffle_buffer_size:  Specifies the shuffle buffer size for <strong>streaming datasets</strong>. Default is <code class="docutils literal notranslate"><span class="pre">1000</span></code>. Only effective when <code class="docutils literal notranslate"><span class="pre">dataset_shuffle</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>download_mode: Dataset download mode. Options: <code class="docutils literal notranslate"><span class="pre">'reuse_dataset_if_exists'</span></code> or <code class="docutils literal notranslate"><span class="pre">'force_redownload'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'reuse_dataset_if_exists'</span></code>.</p>
<ul>
<li><p>Typically set to <code class="docutils literal notranslate"><span class="pre">--download_mode</span> <span class="pre">force_redownload</span></code> when encountering errors with hub datasets.</p></li>
</ul>
</li>
<li><p>columns: Used to map dataset column names so that the dataset conforms to the format accepted by <code class="docutils literal notranslate"><span class="pre">AutoPreprocessor</span></code>. See <a class="reference internal" href="../Customization/Custom-dataset.html"><span class="doc">Custom Dataset Documentation</span></a> for supported formats. You can pass a JSON string, e.g., <code class="docutils literal notranslate"><span class="pre">'{&quot;text1&quot;:</span> <span class="pre">&quot;query&quot;,</span> <span class="pre">&quot;text2&quot;:</span> <span class="pre">&quot;response&quot;}'</span></code>, meaning column <code class="docutils literal notranslate"><span class="pre">&quot;text1&quot;</span></code> is mapped to <code class="docutils literal notranslate"><span class="pre">&quot;query&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;text2&quot;</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;response&quot;</span></code>, which <code class="docutils literal notranslate"><span class="pre">AutoPreprocessor</span></code> can process. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>strict: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, any malformed row in the dataset will raise an error; otherwise, erroneous samples are dropped. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. This is typically used for debugging.</p></li>
<li><p>ğŸ”¥remove_unused_columns: Whether to remove unused columns from the dataset. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<ul>
<li><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, extra columns are passed to the trainer's <code class="docutils literal notranslate"><span class="pre">compute_loss</span></code> function, <strong>facilitating custom loss functions that use additional dataset columns</strong>.</p></li>
<li><p>Default value is <code class="docutils literal notranslate"><span class="pre">False</span></code> for GPRO.</p></li>
</ul>
</li>
<li><p>ğŸ”¥model_name: <strong>Used only for self-cognition tasks</strong>, and only affects the <code class="docutils literal notranslate"><span class="pre">swift/self-cognition</span></code> dataset. Replaces the <code class="docutils literal notranslate"><span class="pre">{{NAME}}</span></code> placeholder in the dataset. Provide the model's Chinese and English names, separated by space, e.g., <code class="docutils literal notranslate"><span class="pre">--model_name</span> <span class="pre">å°é»„</span> <span class="pre">'Xiao</span> <span class="pre">Huang'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>ğŸ”¥model_author: Used only for self-cognition tasks, and only affects the <code class="docutils literal notranslate"><span class="pre">swift/self-cognition</span></code> dataset. Replaces the <code class="docutils literal notranslate"><span class="pre">{{AUTHOR}}</span></code> placeholder. Provide the model author's Chinese and English names, separated by space, e.g., <code class="docutils literal notranslate"><span class="pre">--model_author</span> <span class="pre">'é­”æ­'</span> <span class="pre">'ModelScope'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>custom_dataset_info: Path to a JSON file for custom dataset registration. See <a class="reference internal" href="../Customization/Custom-dataset.html"><span class="doc">Custom Dataset Guide</span></a> and the <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/dataset/data/dataset_info.json">built-in dataset_info.json</a>. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
</ul>
</section>
<section id="template-arguments">
<h3>Template Arguments<a class="headerlink" href="#template-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>ğŸ”¥template: The type of conversation template. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which automatically selects the corresponding template for the given model. See <a class="reference internal" href="Supported-models-and-datasets.html"><span class="doc">List of Supported Models</span></a> for mapping details.</p></li>
<li><p>ğŸ”¥system: Custom system message field. Accepts either a string or a <strong>path to a .txt file</strong>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, using the default system message defined in the registered template.</p>
<ul>
<li><p>Note: In terms of priority, the <code class="docutils literal notranslate"><span class="pre">system</span></code> field from the dataset takes precedence, followed by <code class="docutils literal notranslate"><span class="pre">--system</span></code>, and finally the <code class="docutils literal notranslate"><span class="pre">default_system</span></code> set in the registered template.</p></li>
</ul>
</li>
<li><p>ğŸ”¥max_length: Maximum token length after <code class="docutils literal notranslate"><span class="pre">tokenizer.encode</span></code> for a single data sample (to prevent OOM during training). Samples exceeding this limit are handled according to <code class="docutils literal notranslate"><span class="pre">truncation_strategy</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning it's set to the modelâ€™s maximum supported sequence length (<code class="docutils literal notranslate"><span class="pre">max_model_len</span></code>).</p>
<ul>
<li><p>In PPO, GRPO, GKD and inference scenarios, <code class="docutils literal notranslate"><span class="pre">max_length</span></code> refers to <code class="docutils literal notranslate"><span class="pre">max_prompt_length</span></code>.</p></li>
</ul>
</li>
<li><p>truncation_strategy: How to handle samples whose tokens exceed <code class="docutils literal notranslate"><span class="pre">max_length</span></code>. Supports 'delete', 'left', 'right', and 'split', which represent deleting, left truncation, right truncation, and splitting into multiple data samples, respectively. The default is 'delete'.</p>
<ul>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">--truncation_strategy</span> <span class="pre">split</span></code> is only supported during pretraining, i.e., in <code class="docutils literal notranslate"><span class="pre">swift/megatron</span> <span class="pre">pt</span></code> scenarios. This strategy will split oversized fields into multiple data samples to avoid token waste. (This feature is not compatible with cached_dataset)</p></li>
<li><p>Note: For multimodal models, if <code class="docutils literal notranslate"><span class="pre">truncation_strategy</span></code> is set to <code class="docutils literal notranslate"><span class="pre">'left'</span></code> or <code class="docutils literal notranslate"><span class="pre">'right'</span></code> during training, <strong>ms-swift preserves all image tokens and other modality-specific tokens</strong>, which may lead to OOM.</p></li>
</ul>
</li>
<li><p>ğŸ”¥max_pixels: Maximum pixel count (HÃ—W) for input images in multimodal models. Images exceeding this limit will be resized to avoid OOM during training. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code> (no restriction).</p>
<ul>
<li><p>Note: This parameter applies to all multimodal models. The Qwen2.5-VL specific parameter <code class="docutils literal notranslate"><span class="pre">MAX_PIXELS</span></code> (see bottom of doc) only affects Qwen2.5-VL.</p></li>
</ul>
</li>
<li><p>ğŸ”¥agent_template: Agent template that defines how the tool list <code class="docutils literal notranslate"><span class="pre">'tools'</span></code> is converted into the <code class="docutils literal notranslate"><span class="pre">'system'</span></code> message, how tool calls are extracted from model responses during inference/deployment, and the formatting of <code class="docutils literal notranslate"><span class="pre">{&quot;role&quot;:</span> <span class="pre">&quot;tool_call&quot;,</span> <span class="pre">&quot;content&quot;:</span> <span class="pre">&quot;xxx&quot;}</span></code> and <code class="docutils literal notranslate"><span class="pre">{&quot;role&quot;:</span> <span class="pre">&quot;tool_response&quot;,</span> <span class="pre">&quot;content&quot;:</span> <span class="pre">&quot;xxx&quot;}</span></code> in <code class="docutils literal notranslate"><span class="pre">messages</span></code>. Options include <code class="docutils literal notranslate"><span class="pre">'react_en'</span></code>, <code class="docutils literal notranslate"><span class="pre">'hermes'</span></code>, <code class="docutils literal notranslate"><span class="pre">'glm4'</span></code>, <code class="docutils literal notranslate"><span class="pre">'qwen_en'</span></code>, <code class="docutils literal notranslate"><span class="pre">'toolbench'</span></code>, etc. See <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/agent_template/mapping.py">here</a> for more. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, automatically selected based on model type. Refer to <a class="reference internal" href="Agent-support.html"><span class="doc">Agent Documentation</span></a>.</p></li>
<li><p>norm_bbox: Controls how bounding boxes (&quot;bbox&quot; in dataset, containing absolute coordinates; see <a class="reference external" href="../Customization/Custom-dataset.md#grounding">Custom Dataset Documentation</a>) are normalized. Options: <code class="docutils literal notranslate"><span class="pre">'norm1000'</span></code> (scale coordinates to thousandths), <code class="docutils literal notranslate"><span class="pre">'none'</span></code> (no scaling). Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, automatically chosen based on model.</p>
<ul>
<li><p>This also works correctly when <strong>images are resized during training</strong> (e.g., when <code class="docutils literal notranslate"><span class="pre">max_pixels</span></code> is set).</p></li>
</ul>
</li>
<li><p>use_chat_template: Whether to use a chat template or a generation template (the latter typically used in pretraining). Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<ul>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">pt</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>, using the generation template. This setting provides good <strong>compatibility with multimodal models</strong>.</p></li>
</ul>
</li>
<li><p>padding_side: Padding side when training with <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">&gt;=</span> <span class="pre">2</span></code>. Options: <code class="docutils literal notranslate"><span class="pre">'left'</span></code>, <code class="docutils literal notranslate"><span class="pre">'right'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'right'</span></code>.</p>
<ul>
<li><p>Note: PPO and GKD default to <code class="docutils literal notranslate"><span class="pre">'left'</span></code>. (During inference with <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">&gt;=</span> <span class="pre">2</span></code>, only left-padding is applied.)</p></li>
</ul>
</li>
<li><p>ğŸ”¥padding_free: Flattens data within a batch to avoid padding, reducing GPU memory usage and accelerating training (<strong>sequences in the same batch remain invisible to each other</strong>). Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Currently supported in CPT/SFT/DPO/GRPO/KTO/GKD.</p>
<ul>
<li><p>Note: Use <code class="docutils literal notranslate"><span class="pre">padding_free</span></code> together with <code class="docutils literal notranslate"><span class="pre">--attn_impl</span> <span class="pre">flash_attn</span></code> and <code class="docutils literal notranslate"><span class="pre">transformers&gt;=4.44</span></code>. See <a class="reference external" href="https://github.com/huggingface/transformers/pull/31629">this PR</a> for details. (Same as packing.)</p></li>
<li><p><strong>Compared to packing, padding_free avoids extra preprocessing time, but packing offers faster training and more stable memory usage</strong>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥loss_scale: Loss weight configuration for training tokens. Default is <code class="docutils literal notranslate"><span class="pre">'default'</span></code>. loss_scale includes 3 basic strategies: 'default', 'last_round', 'all', and other strategies: 'ignore_empty_think' and agent-specific ones: 'react', 'hermes', 'qwen', 'agentflan', 'alpha_umi', etc. For available options, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss_scale/mapping.py">loss_scale module</a>. ms-swift supports mixing basic strategies with other strategies, for example: <code class="docutils literal notranslate"><span class="pre">'default+ignore_empty_think'</span></code>, <code class="docutils literal notranslate"><span class="pre">'last_round+ignore_empty_think'</span></code>. If no basic strategy is specified, it defaults to 'default', for example: 'hermes' is equivalent to 'default+hermes'.</p>
<ul>
<li><p>'default': All responses (including history) are calculated with weight 1 for cross-entropy loss (<strong>system/user/multimodal tokens in messages and <code class="docutils literal notranslate"><span class="pre">tool_response</span></code> parts in Agent training are not included in loss calculation</strong>). (<strong>Default value for SFT</strong>)</p></li>
<li><p>'last_round': Only calculate loss for the last round response. The last round means all content after the last &quot;user&quot;. (<strong>Default value for RLHF</strong>)</p></li>
<li><p>'all': Calculate loss for all tokens. (<strong>Default value for <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">pt</span></code></strong>)</p></li>
<li><p>'ignore_empty_think': Ignore loss computation for empty <code class="docutils literal notranslate"><span class="pre">'&lt;think&gt;\n\n&lt;/think&gt;\n\n'</span></code> (as long as it matches the regex <code class="docutils literal notranslate"><span class="pre">'&lt;think&gt;\\s*&lt;/think&gt;\\s*'</span></code>).</p></li>
<li><p>'react', 'hermes', 'qwen': Adjust the loss weight of the <code class="docutils literal notranslate"><span class="pre">tool_call</span></code> part to 2.</p></li>
</ul>
</li>
<li><p>sequence_parallel_size: Size for sequence parallelism. Default is 1. Currently supported in CPT/SFT/DPO/GRPO. Training scripts can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel">here</a>.</p></li>
<li><p>template_backend: Backend for template processing. Options are <code class="docutils literal notranslate"><span class="pre">'swift'</span></code> or <code class="docutils literal notranslate"><span class="pre">'jinja'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'swift'</span></code>. If <code class="docutils literal notranslate"><span class="pre">'jinja'</span></code> is used, <code class="docutils literal notranslate"><span class="pre">apply_chat_template</span></code> from Transformers will be applied.</p>
<ul>
<li><p>Note: The <code class="docutils literal notranslate"><span class="pre">'jinja'</span></code> backend only supports inference and does not support training (as it cannot determine the token ranges for loss computation).</p></li>
</ul>
</li>
<li><p>response_prefix: The prefix string for responses, this parameter only takes effect during inference. Default is None, determined by the enable_thinking parameter and template type.</p></li>
<li><p>enable_thinking: This parameter takes effect during inference, indicating whether to enable thinking mode. Default is None, the default value is determined by the template (model) type (True for thinking/hybrid thinking templates, False for non-thinking templates). If enable_thinking is False, a non-thinking prefix is added, for example the Qwen3-8B hybrid thinking model adds the prefix <code class="docutils literal notranslate"><span class="pre">'&lt;think&gt;\n\n&lt;/think&gt;\n\n'</span></code>, while Qwen3-8B-Thinking does not add a prefix. If enable_thinking is True, a thinking prefix is added, for example <code class="docutils literal notranslate"><span class="pre">'&lt;think&gt;\n'</span></code>. Note: The priority of this parameter is lower than the response_prefix parameter.</p>
<ul>
<li><p>Note: For thinking models (thinking/hybrid thinking) or when enable_thinking is explicitly enabled, we will remove historical thinking content during both inference and training (the thinking content of the last round is retained, i.e., the content after the last user message). If the basic strategy of loss_scale during training is not last_round, for example 'default', then historical thinking content will not be removed.</p></li>
</ul>
</li>
<li><p>add_non_thinking_prefix: This parameter only takes effect during training, indicating whether to add a non-thinking prefix to data samples whose assistant part <strong>does not start with the thinking marker <code class="docutils literal notranslate"><span class="pre">'&lt;think&gt;'</span></code></strong> (typically hybrid thinking models contain a non-thinking prefix). This feature allows swift's built-in datasets to train hybrid thinking models. Default value is True. For example: the non-thinking prefix for the Qwen3-8B hybrid thinking model is <code class="docutils literal notranslate"><span class="pre">'&lt;think&gt;\n\n&lt;/think&gt;\n\n'</span></code>, while the non-thinking prefix for Qwen3-8B-Thinking/Instruct is <code class="docutils literal notranslate"><span class="pre">''</span></code>. Note: During training, if the basic strategy of loss_scale is last_round, this modification is only applied to the last round; otherwise, for example 'default' or 'all', this modification is applied to every round of data. If set to False, no non-thinking prefix is added to data samples.</p></li>
</ul>
</section>
<section id="generation-arguments">
<h3>Generation Arguments<a class="headerlink" href="#generation-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Refer to the <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig">generation_config</a> documentation.</p>
<ul class="simple">
<li><p>ğŸ”¥max_new_tokens: The maximum number of new tokens generated during inference. Defaults to None, meaning unlimited.</p></li>
<li><p>temperature: Sampling temperature. Higher values increase output randomness. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, reading from <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code>.</p>
<ul>
<li><p>You can set <code class="docutils literal notranslate"><span class="pre">--temperature</span> <span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">--top_k</span> <span class="pre">1</span></code> to disable randomness in generation.</p></li>
</ul>
</li>
<li><p>top_k: Top-k sampling parameter. Only the top <code class="docutils literal notranslate"><span class="pre">k</span></code> highest probability tokens are considered for generation. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, reading from <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code>.</p></li>
<li><p>top_p: Top-p (nucleus) sampling parameter. Only tokens whose cumulative probability reaches <code class="docutils literal notranslate"><span class="pre">top_p</span></code> are considered. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, reading from <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code>.</p></li>
<li><p>repetition_penalty: Penalty for repeated tokens. A value of 1.0 means no penalty. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, reading from <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code>.</p></li>
<li><p>num_beams: Number of beams for beam search. Default is 1.</p></li>
<li><p>ğŸ”¥stream: Enable streaming output. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning <code class="docutils literal notranslate"><span class="pre">True</span></code> when using an interactive interface, and <code class="docutils literal notranslate"><span class="pre">False</span></code> during batch inference on datasets.</p></li>
<li><p>stop_words: Additional stop words besides the <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p>
<ul>
<li><p>Note: The <code class="docutils literal notranslate"><span class="pre">eos_token</span></code> is removed from the output response, while additional stop words are preserved in the output.</p></li>
</ul>
</li>
<li><p>logprobs: Whether to return log probabilities. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>top_logprobs: Number of top log probabilities to return. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>structured_outputs_regex: A regular expression pattern for structured outputs (guided decoding). When set, the model's generation is constrained to match the specified regex pattern. Only effective when <code class="docutils literal notranslate"><span class="pre">infer_backend</span></code> is <code class="docutils literal notranslate"><span class="pre">vllm</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</section>
<section id="quantization-arguments">
<h3>Quantization Arguments<a class="headerlink" href="#quantization-arguments" title="Link to this heading">ïƒ</a></h3>
<p>The following are parameters for quantizing models upon loading. See the <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization">quantization documentation</a> for details. These do not include <code class="docutils literal notranslate"><span class="pre">gptq</span></code> or <code class="docutils literal notranslate"><span class="pre">awq</span></code> quantization parameters used in <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code>.</p>
<ul class="simple">
<li><p>ğŸ”¥quant_method: Quantization method used when loading the model. Options: <code class="docutils literal notranslate"><span class="pre">'bnb'</span></code>, <code class="docutils literal notranslate"><span class="pre">'hqq'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eetq'</span></code>, <code class="docutils literal notranslate"><span class="pre">'quanto'</span></code>, <code class="docutils literal notranslate"><span class="pre">'fp8'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<ul>
<li><p>If performing QLoRA training on already AWQ/GPTQ-quantized models, you do not need to set additional quantization parameters like <code class="docutils literal notranslate"><span class="pre">quant_method</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥quant_bits: Number of bits for quantization. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>hqq_axis: Axis for HQQ quantization. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>bnb_4bit_compute_dtype: Computation data type for 4-bit BNB quantization. Options: <code class="docutils literal notranslate"><span class="pre">float16</span></code>, <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>, <code class="docutils literal notranslate"><span class="pre">float32</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which uses the value of <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code>.</p></li>
<li><p>bnb_4bit_quant_type: Type for 4-bit BNB quantization. Options: <code class="docutils literal notranslate"><span class="pre">'fp4'</span></code>, <code class="docutils literal notranslate"><span class="pre">'nf4'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'nf4'</span></code>.</p></li>
<li><p>bnb_4bit_use_double_quant: Whether to use double quantization. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>bnb_4bit_quant_storage: Data type used to store quantized weights. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</section>
<section id="ray-arguments">
<h3>RAY Arguments<a class="headerlink" href="#ray-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>use_ray: Boolean type. Whether to use ray, defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>ray_exp_name: Ray experiment name. This field will be used as the prefix for cluster and worker names, can be empty.</p></li>
<li><p>device_groups: String (jsonstring) type. When using ray, this field must be configured. For details, please refer to the <a class="reference internal" href="Ray.html"><span class="doc">ray documentation</span></a>.</p></li>
</ul>
</section>
<section id="yaml-arguments">
<h3>YAML Arguments<a class="headerlink" href="#yaml-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>config: You can use config instead of command-line arguments, for example:</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>swift<span class="w"> </span>sft<span class="w"> </span>--config<span class="w"> </span>demo.yaml
</pre></div>
</div>
<p>The content of demo.yaml consists of other command-line configurations:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model args</span>
<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Qwen/Qwen2.5-7B-Instruct</span>
<span class="nt">dataset</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">swift/self-cognition</span>
<span class="nn">...</span>

<span class="c1"># Train args</span>
<span class="nt">output_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">xxx/xxx</span>
<span class="nt">gradient_checkpointing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nn">...</span>
</pre></div>
</div>
</section>
</section>
<section id="atomic-arguments">
<h2>Atomic Arguments<a class="headerlink" href="#atomic-arguments" title="Link to this heading">ïƒ</a></h2>
<section id="seq2seqtrainer-arguments">
<h3>Seq2SeqTrainer Arguments<a class="headerlink" href="#seq2seqtrainer-arguments" title="Link to this heading">ïƒ</a></h3>
<p>This list inherits from the Transformers <code class="docutils literal notranslate"><span class="pre">Seq2SeqTrainingArguments</span></code>, with ms-swift overriding certain default values. For arguments not listed here, please refer to the <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments">official HF documentation</a>.</p>
<ul class="simple">
<li><p>ğŸ”¥output_dir: The output directory where the model predictions and checkpoints will be written. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, automatically set to <code class="docutils literal notranslate"><span class="pre">'output/&lt;model_name&gt;'</span></code>.</p></li>
<li><p>ğŸ”¥gradient_checkpointing: Whether to use gradient checkpointing. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>. This significantly reduces GPU memory usage but slows down training.</p></li>
<li><p>ğŸ”¥vit_gradient_checkpointing: For multimodal model training, whether to enable gradient checkpointing for the ViT (Vision Transformer) component. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning it follows the value of <code class="docutils literal notranslate"><span class="pre">gradient_checkpointing</span></code>. For an example, please refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/vit_gradient_checkpointing.sh">here</a>.</p>
<ul>
<li><p>Note: When training multimodal models with LoRA and <code class="docutils literal notranslate"><span class="pre">--freeze_vit</span> <span class="pre">false</span></code>, if you see the warning: <code class="docutils literal notranslate"><span class="pre">UserWarning:</span> <span class="pre">None</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">inputs</span> <span class="pre">have</span> <span class="pre">requires_grad=True.</span> <span class="pre">Gradients</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">None</span></code>, try setting <code class="docutils literal notranslate"><span class="pre">--vit_gradient_checkpointing</span> <span class="pre">false</span></code> or open an issue. This issue does not occur in full-parameter training. (If this warning comes from the <code class="docutils literal notranslate"><span class="pre">ref_model</span></code> during RLHF LoRA training, it is normal.)</p></li>
</ul>
</li>
<li><p>ğŸ”¥deepspeed: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Can be set to <code class="docutils literal notranslate"><span class="pre">'zero0'</span></code>, <code class="docutils literal notranslate"><span class="pre">'zero1'</span></code>, <code class="docutils literal notranslate"><span class="pre">'zero2'</span></code>, <code class="docutils literal notranslate"><span class="pre">'zero3'</span></code>, <code class="docutils literal notranslate"><span class="pre">'zero2_offload'</span></code>, <code class="docutils literal notranslate"><span class="pre">'zero3_offload'</span></code> to use built-in DeepSpeed configurations in ms-swift. You can also pass a path to a custom DeepSpeed config file.</p></li>
<li><p>zero_hpz_partition_size: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. This enables ZeRO++ functionalityâ€”model sharding within nodes and data sharding across nodes. If encountering <code class="docutils literal notranslate"><span class="pre">grad_norm</span> <span class="pre">NaN</span></code>, try using <code class="docutils literal notranslate"><span class="pre">--torch_dtype</span> <span class="pre">float16</span></code>.</p></li>
<li><p>deepspeed_autotp_size: DeepSpeed tensor parallelism size. Default is 1. To use DeepSpeed AutoTP, set <code class="docutils literal notranslate"><span class="pre">--deepspeed</span></code> to <code class="docutils literal notranslate"><span class="pre">'zero0'</span></code>, <code class="docutils literal notranslate"><span class="pre">'zero1'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'zero2'</span></code>. (Note: Only supports full-parameter training)</p></li>
<li><p>ğŸ”¥fsdp: FSDP2 distributed training configuration. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Can be set to <code class="docutils literal notranslate"><span class="pre">'fsdp2'</span></code> to use built-in FSDP2 configurations in ms-swift. You can also pass a path to a custom FSDP config file. FSDP2 is PyTorch's native distributed training solution, use either this or DeepSpeed (not both).</p></li>
<li><p>ğŸ”¥per_device_train_batch_size: Default is 1.</p></li>
<li><p>ğŸ”¥per_device_eval_batch_size: Default is 1.</p></li>
<li><p>ğŸ”¥gradient_accumulation_steps: Gradient accumulation steps. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span></code> is automatically calculated so that <code class="docutils literal notranslate"><span class="pre">total_batch_size</span> <span class="pre">&gt;=</span> <span class="pre">16</span></code>. Total batch size is computed as <code class="docutils literal notranslate"><span class="pre">per_device_train_batch_size</span> <span class="pre">*</span> <span class="pre">gradient_accumulation_steps</span> <span class="pre">*</span> <span class="pre">world_size</span></code>. In GRPO training, default is 1.</p>
<ul>
<li><p>In CPT/SFT training, gradient accumulation has equivalent effects to using a larger batch size, but this equivalence does not hold in RLHF training.</p></li>
</ul>
</li>
<li><p>weight_decay:  Weight decay coefficient. Default is 0.1.</p></li>
<li><p>adam_beta1: The exponential decay rate for the first moment estimates (momentum) in Adam-based optimizers. Defaults to 0.9.</p></li>
<li><p>adam_beta2: The exponential decay rate for the second moment estimates (variance) in Adam-based optimizers. Defaults to 0.95.</p></li>
<li><p>adam_epsilon: Epsilon value for numerical stability in Adam-based optimizers. Defaults to 1e-8.</p></li>
<li><p>ğŸ”¥learning_rate:  Learning rate. <strong>Default is <code class="docutils literal notranslate"><span class="pre">1e-5</span></code> for full-parameter training, and <code class="docutils literal notranslate"><span class="pre">1e-4</span></code> for LoRA and other tuners</strong>.</p>
<ul>
<li><p>Tip: If you want to set <code class="docutils literal notranslate"><span class="pre">min_lr</span></code>, you can pass the arguments <code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span> <span class="pre">cosine_with_min_lr</span> <span class="pre">--lr_scheduler_kwargs</span> <span class="pre">'{&quot;min_lr&quot;:</span> <span class="pre">1e-6}'</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥vit_lr: Specifies the learning rate for the ViT module when training multimodal models. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, same as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. Typically used together with <code class="docutils literal notranslate"><span class="pre">--freeze_vit</span></code> and <code class="docutils literal notranslate"><span class="pre">--freeze_aligner</span></code>.</p>
<ul>
<li><p>Note: The &quot;learning_rate&quot; printed in the logs is the learning rate of <code class="docutils literal notranslate"><span class="pre">param_groups[0]</span></code>, where the order of param_groups is vit, aligner, llm (if it contains trainable parameters).</p></li>
</ul>
</li>
<li><p>ğŸ”¥aligner_lr: Specifies the learning rate for the aligner module in multimodal models. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, same as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p></li>
<li><p>lr_scheduler_type: Type of learning rate scheduler. Default is <code class="docutils literal notranslate"><span class="pre">'cosine'</span></code>.</p></li>
<li><p>lr_scheduler_kwargs: Additional arguments for the learning rate scheduler. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>gradient_checkpointing_kwargs: Arguments passed to <code class="docutils literal notranslate"><span class="pre">torch.utils.checkpoint</span></code>. For example: <code class="docutils literal notranslate"><span class="pre">--gradient_checkpointing_kwargs</span> <span class="pre">'{&quot;use_reentrant&quot;:</span> <span class="pre">false}'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<ul>
<li><p>Note: When using DDP without DeepSpeed/FSDP and <code class="docutils literal notranslate"><span class="pre">gradient_checkpointing_kwargs</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it defaults to <code class="docutils literal notranslate"><span class="pre">'{&quot;use_reentrant&quot;:</span> <span class="pre">false}'</span></code> to prevent errors.</p></li>
</ul>
</li>
<li><p>full_determinism: Ensures reproducible results during training. Note: This may negatively impact performance. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>ğŸ”¥report_to: Default is <code class="docutils literal notranslate"><span class="pre">'tensorboard'</span></code>. You can specify multiple loggers, e.g., <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">tensorboard</span> <span class="pre">wandb</span> <span class="pre">swanlab</span></code>, or <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">all</span></code>.</p>
<ul>
<li><p>If you specify <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">wandb</span></code>, you can set the project name through <code class="docutils literal notranslate"><span class="pre">WANDB_PROJECT</span></code> and specify the API KEY corresponding to your account through <code class="docutils literal notranslate"><span class="pre">WANDB_API_KEY</span></code>.</p></li>
</ul>
</li>
<li><p>logging_first_step: Whether to log metrics at the first step. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>logging_steps: Interval for logging. Default is 5.</p></li>
<li><p>router_aux_loss_coef: Used in MoE model training to set the weight of auxiliary loss. Default is <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
<li><p>enable_dft_loss: Whether to use <a class="reference external" href="https://arxiv.org/abs/2508.05629">DFT</a> (Dynamic Fine-Tuning) loss during SFT training. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>enable_channel_loss: Enable channel-based loss. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Requires a <code class="docutils literal notranslate"><span class="pre">&quot;channel&quot;</span></code> field in the dataset. ms-swift groups and computes loss by this field (samples without <code class="docutils literal notranslate"><span class="pre">&quot;channel&quot;</span></code> are grouped into the default <code class="docutils literal notranslate"><span class="pre">None</span></code> channel). Dataset format reference: <a class="reference external" href="../Customization/Custom-dataset.md#channel-loss">channel loss</a>.  Channel loss is compatible with packing, padding_free, and loss_scale techniques.</p></li>
<li><p>logging_dir: Directory for TensorBoard logs. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, automatically set to <code class="docutils literal notranslate"><span class="pre">f'{self.output_dir}/runs'</span></code>.</p></li>
<li><p>predict_with_generate: Use generation during evaluation. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>metric_for_best_model: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If <code class="docutils literal notranslate"><span class="pre">predict_with_generate=False</span></code>, it's set to <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>; otherwise <code class="docutils literal notranslate"><span class="pre">'rouge-l'</span></code> (in PPO training, no default; in GRPO, set to <code class="docutils literal notranslate"><span class="pre">'reward'</span></code>).</p></li>
<li><p>greater_is_better: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Set to <code class="docutils literal notranslate"><span class="pre">False</span></code> if <code class="docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> contains <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>max_epochs: Force training to stop after reaching <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code>, then evaluate and save the model. Useful when using streaming datasets. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
<p>Other important parameters:</p>
<ul class="simple">
<li><p>ğŸ”¥num_train_epochs: Number of training epochs. Default is 3.</p></li>
<li><p>ğŸ”¥save_strategy: Strategy for saving checkpoints. Options: <code class="docutils literal notranslate"><span class="pre">'no'</span></code>, <code class="docutils literal notranslate"><span class="pre">'steps'</span></code>, <code class="docutils literal notranslate"><span class="pre">'epoch'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'steps'</span></code>.</p></li>
<li><p>ğŸ”¥save_steps: Default is 500.</p></li>
<li><p>ğŸ”¥eval_strategy: Evaluation strategy. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, following <code class="docutils literal notranslate"><span class="pre">save_strategy</span></code>.</p>
<ul>
<li><p>If neither <code class="docutils literal notranslate"><span class="pre">val_dataset</span></code> nor <code class="docutils literal notranslate"><span class="pre">eval_dataset</span></code> is used and <code class="docutils literal notranslate"><span class="pre">split_dataset_ratio=0</span></code>, defaults to <code class="docutils literal notranslate"><span class="pre">'no'</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥eval_steps: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If evaluation dataset exists, follows <code class="docutils literal notranslate"><span class="pre">save_steps</span></code>.</p></li>
<li><p>eval_on_start: Whether to perform an evaluation step before training to ensure the validation steps work correctly. Defaults to False.</p></li>
<li><p>ğŸ”¥save_total_limit: Maximum number of checkpoints to save. Expired checkpoints will be deleted. Default is None, which saves all checkpoints. If set to 2, it will save the best checkpoint and the last checkpoint.</p></li>
<li><p>max_steps: Maximum number of training steps. Must be set when using streaming datasets. Default is -1.</p></li>
<li><p>ğŸ”¥warmup_ratio: Default is 0.</p></li>
<li><p>save_on_each_node: Save weights on every node. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Relevant in multi-node training.</p>
<ul>
<li><p>Tip: In multi-node training, <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> is typically set to a shared directory, so this parameter usually doesn't need to be set.</p></li>
</ul>
</li>
<li><p>save_only_model: Whether to save only model weights (excluding optimizer states, random seed states, etc.), reducing time and space overhead in full-parameter training. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>ğŸ”¥resume_from_checkpoint: Path to resume training from. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<ul>
<li><p>Tip: <strong>To resume training, keep other parameters unchanged and add <code class="docutils literal notranslate"><span class="pre">--resume_from_checkpoint</span> <span class="pre">checkpoint_dir</span></code></strong>. Weights and states will be loaded by the trainer.</p></li>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> loads model weights, optimizer state, random seed, and resumes training from the last step. Use <code class="docutils literal notranslate"><span class="pre">--resume_only_model</span></code> to load only model weights.</p></li>
</ul>
</li>
<li><p>resume_only_model: Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. If set to <code class="docutils literal notranslate"><span class="pre">True</span></code> along with <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code>, only model weights are resumed, ignoring optimizer state and random seed.</p>
<ul>
<li><p>Note: <strong><code class="docutils literal notranslate"><span class="pre">resume_only_model</span></code> skips already-trained data by default</strong>, controlled via the <code class="docutils literal notranslate"><span class="pre">ignore_data_skip</span></code> argument.</p></li>
</ul>
</li>
<li><p>ignore_data_skip: When <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> and <code class="docutils literal notranslate"><span class="pre">resume_only_model</span></code> are set, this controls whether to skip already-trained data and restore training states (epoch, step count, etc.). Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, training starts from step 0 without loading previous states or skipping data.</p></li>
<li><p>ğŸ”¥ddp_find_unused_parameters: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>ğŸ”¥dataloader_num_workers: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. On Windows, set to 0; otherwise, 1.</p></li>
<li><p>dataloader_pin_memory: Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>dataloader_persistent_workers: Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>dataloader_prefetch_factor: Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If <code class="docutils literal notranslate"><span class="pre">dataloader_num_workers</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>, it is set to 2. Number of batches loaded in advance by each worker. 2 means there will be a total of 2 * num_workers batches prefetched across all workers.</p></li>
<li><p>train_dataloader_shuffle: Whether to shuffle the dataloader for CPT/SFT training, default is True. This parameter is ineffective for IterableDataset (i.e., it doesn't work for streaming datasets). IterableDataset reads data sequentially.</p></li>
<li><p>optim: The optimizer, defaults to <code class="docutils literal notranslate"><span class="pre">&quot;adamw_torch&quot;</span></code> (for torch&gt;=2.8 <code class="docutils literal notranslate"><span class="pre">&quot;adamw_torch_fused&quot;</span></code>). For a complete list of optimizers, please see <code class="docutils literal notranslate"><span class="pre">OptimizerNames</span></code> in <a class="reference external" href="https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py">training_args.py</a>.</p></li>
<li><p>optim_args: Optional arguments to pass to the optimizer, defaults to None.</p></li>
<li><p>group_by_length: Whether to group samples with approximately the same length together in the training dataset (with a random factor) to minimize padding and ensure load balancing across nodes and processes for improved efficiency. Defaults to False. For the specific algorithm, refer to <code class="docutils literal notranslate"><span class="pre">transformers.trainer_pt_utils.get_length_grouped_indices</span></code>.</p></li>
<li><p>ğŸ”¥neftune_noise_alpha: Noise magnitude for NEFTune. Default is 0. Common values: 5, 10, 15.</p></li>
<li><p>ğŸ”¥use_liger_kernel: Whether to enable the <a class="reference external" href="https://github.com/linkedin/Liger-Kernel">Liger</a> kernel to accelerate training and reduce GPU memory consumption. Defaults to False. Example shell script can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/liger">here</a>.</p>
<ul>
<li><p>Note: Liger kernel does not support <code class="docutils literal notranslate"><span class="pre">device_map</span></code>. Use DDP or DeepSpeed for multi-GPU training. Currently, liger_kernel only supports <code class="docutils literal notranslate"><span class="pre">task_type='causal_lm'</span></code>.</p></li>
</ul>
</li>
<li><p>average_tokens_across_devices: Whether to average token counts across devices. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">num_tokens_in_batch</span></code> is synchronized via <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code> for accurate loss computation. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>max_grad_norm: Gradient clipping. Default is 1.</p>
<ul>
<li><p>Note: The logged <code class="docutils literal notranslate"><span class="pre">grad_norm</span></code> reflects the value <strong>before</strong> clipping.</p></li>
</ul>
</li>
<li><p>push_to_hub: Push checkpoints to the hub. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>hub_model_id: Model ID on the hub. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>hub_private_repo: Whether the repo is private. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</section>
<section id="tuner-arguments">
<h3>Tuner Arguments<a class="headerlink" href="#tuner-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>ğŸ”¥freeze_llm: This argument only takes effect for multimodal models and can be used in both full-parameter and LoRA training, but with different behaviors. In full-parameter training, setting <code class="docutils literal notranslate"><span class="pre">freeze_llm=True</span></code> freezes the LLM component's weights. In LoRA training with <code class="docutils literal notranslate"><span class="pre">target_modules=['all-linear']</span></code>, setting <code class="docutils literal notranslate"><span class="pre">freeze_llm=True</span></code> prevents LoRA modules from being added to the LLM part. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>ğŸ”¥freeze_vit: This argument only applies to multimodal models and behaves differently depending on the training mode. In full-parameter training, setting <code class="docutils literal notranslate"><span class="pre">freeze_vit=True</span></code> freezes the ViT (vision transformer) component's weights. In LoRA training with <code class="docutils literal notranslate"><span class="pre">target_modules=['all-linear']</span></code>, setting <code class="docutils literal notranslate"><span class="pre">freeze_vit=True</span></code> prevents LoRA modules from being added to the ViT part. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<ul>
<li><p>Note: <strong>Here, &quot;vit&quot; refers not only to <code class="docutils literal notranslate"><span class="pre">vision_tower</span></code>, but also to <code class="docutils literal notranslate"><span class="pre">audio_tower</span></code></strong>. For Omni models, if you want to apply LoRA only to <code class="docutils literal notranslate"><span class="pre">vision_tower</span></code> and not <code class="docutils literal notranslate"><span class="pre">audio_tower</span></code>, you can modify <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/a5d4c0a2ce0658cef8332d6c0fa619a52afa26ff/swift/llm/model/model_arch.py#L544-L554">this code</a>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥freeze_aligner: This argument only affects multimodal models. In full-parameter training, setting <code class="docutils literal notranslate"><span class="pre">freeze_aligner=True</span></code> freezes the aligner (also known as projector) weights. In LoRA training with <code class="docutils literal notranslate"><span class="pre">target_modules=['all-linear']</span></code>, setting <code class="docutils literal notranslate"><span class="pre">freeze_aligner=True</span></code> prevents LoRA modules from being added to the aligner component. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>ğŸ”¥target_modules: Specifies which modules to apply LoRA to. Default is <code class="docutils literal notranslate"><span class="pre">['all-linear']</span></code>. You can also specify suffixes of modules, e.g., <code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">q_proj</span> <span class="pre">k_proj</span> <span class="pre">v_proj</span></code>. This argument is not limited to LoRA and can be used with other tuners.</p>
<ul>
<li><p>Note: The behavior of <code class="docutils literal notranslate"><span class="pre">'all-linear'</span></code> differs between LLMs and multimodal LLMs. For standard LLMs, it automatically finds all linear layers except <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> and attaches tuners. <strong>For multimodal LLMs, tuners are by default only attached to the LLM component; this behavior can be controlled via <code class="docutils literal notranslate"><span class="pre">freeze_llm</span></code>, <code class="docutils literal notranslate"><span class="pre">freeze_vit</span></code>, and <code class="docutils literal notranslate"><span class="pre">freeze_aligner</span></code></strong>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥target_regex: A regular expression to specify LoRA modules. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If provided, <code class="docutils literal notranslate"><span class="pre">target_modules</span></code> is ignored. For example: <code class="docutils literal notranslate"><span class="pre">--target_regex</span> <span class="pre">'^(language_model).*\.(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$'</span></code> applies LoRA to modules matching the pattern. This argument is not limited to LoRA and can be used with other tuners.</p></li>
<li><p>target_parameters: List of parameter names (not module names) to replace with LoRA. Similar in behavior to <code class="docutils literal notranslate"><span class="pre">target_modules</span></code>, but operates at the parameter level. Requires &quot;peft&gt;=0.17.0&quot;. This is useful for models like Mixture-of-Experts (MoE) layers in Hugging Face Transformers, which may use <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code> instead of <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>.</p></li>
<li><p>init_weights: Method for initializing weights. For LoRA: options are <code class="docutils literal notranslate"><span class="pre">'true'</span></code>, <code class="docutils literal notranslate"><span class="pre">'false'</span></code>, <code class="docutils literal notranslate"><span class="pre">'gaussian'</span></code>, <code class="docutils literal notranslate"><span class="pre">'pissa'</span></code>, <code class="docutils literal notranslate"><span class="pre">'pissa_niter_[number</span> <span class="pre">of</span> <span class="pre">iters]'</span></code>. For Bone: <code class="docutils literal notranslate"><span class="pre">'true'</span></code>, <code class="docutils literal notranslate"><span class="pre">'false'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bat'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'true'</span></code>.</p></li>
<li><p>ğŸ”¥modules_to_save: Additional original model modules to include in training and saving, even after attaching a tuner. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. Applies to tuners beyond LoRA. For example: <code class="docutils literal notranslate"><span class="pre">--modules_to_save</span> <span class="pre">embed_tokens</span> <span class="pre">lm_head</span></code> enables training of <code class="docutils literal notranslate"><span class="pre">embed_tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> during LoRA training, and their weights will be saved in <code class="docutils literal notranslate"><span class="pre">adapter_model.safetensors</span></code>.</p></li>
</ul>
<section id="full-arguments">
<h4>Full Arguments<a class="headerlink" href="#full-arguments" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>freeze_parameters: List of parameter name prefixes to freeze. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>freeze_parameters_regex: Regular expression to match parameters to freeze. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>freeze_parameters_ratio: Proportion of parameters to freeze, from bottom to top layers. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>. Setting to <code class="docutils literal notranslate"><span class="pre">1</span></code> freezes all parameters; can be combined with <code class="docutils literal notranslate"><span class="pre">trainable_parameters</span></code> to specify trainable parts.</p></li>
<li><p>trainable_parameters: Prefixes of additional parameters to keep trainable. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>trainable_parameters_regex: Regex to match additional trainable parameters. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<ul>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">trainable_parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">trainable_parameters_regex</span></code> have higher priority than <code class="docutils literal notranslate"><span class="pre">freeze_parameters</span></code>, <code class="docutils literal notranslate"><span class="pre">freeze_parameters_regex</span></code>, and <code class="docutils literal notranslate"><span class="pre">freeze_parameters_ratio</span></code>. For example, in full-parameter training, all modules are first set to trainable, then some are frozen based on the freeze rules, and finally some are re-enabled via <code class="docutils literal notranslate"><span class="pre">trainable_parameters</span></code> or <code class="docutils literal notranslate"><span class="pre">trainable_parameters_regex</span></code>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="lora">
<h4>LoRA<a class="headerlink" href="#lora" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>ğŸ”¥lora_rank: Default is <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p></li>
<li><p>ğŸ”¥lora_alpha: Default is <code class="docutils literal notranslate"><span class="pre">32</span></code>.</p></li>
<li><p>lora_dropout: Default is <code class="docutils literal notranslate"><span class="pre">0.05</span></code>.</p></li>
<li><p>lora_bias: Defaults to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>. Possible values are 'none', 'all'. If you want to make all biases trainable, you can set it to <code class="docutils literal notranslate"><span class="pre">'all'</span></code>.</p></li>
<li><p>lora_dtype: Specifies the data type (dtype) for the LoRA modules. Supported values are 'float16', 'bfloat16', 'float32'. Default is None, which follows the default behavior of PEFT.</p></li>
<li><p>ğŸ”¥use_dora: Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>, indicating whether to use <code class="docutils literal notranslate"><span class="pre">DoRA</span></code>.</p></li>
<li><p>use_rslora: Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>, indicating whether to use <code class="docutils literal notranslate"><span class="pre">RS-LoRA</span></code>.</p></li>
<li><p>ğŸ”¥lorap_lr_ratio: Parameter for LoRA+. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Recommended values: <code class="docutils literal notranslate"><span class="pre">10â€“16</span></code>. Setting this when using LoRA enables the LoRA+ variant.</p></li>
</ul>
<section id="lora-ga">
<h5>LoRA-GA<a class="headerlink" href="#lora-ga" title="Link to this heading">ïƒ</a></h5>
<ul class="simple">
<li><p>lora_ga_batch_size: The default value is <code class="docutils literal notranslate"><span class="pre">2</span></code>. The batch size used for estimating gradients during initialization in LoRA-GA.</p></li>
<li><p>lora_ga_iters: The default value is <code class="docutils literal notranslate"><span class="pre">2</span></code>. The number of iterations for estimating gradients during initialization in LoRA-GA.</p></li>
<li><p>lora_ga_max_length: The default value is <code class="docutils literal notranslate"><span class="pre">1024</span></code>. The maximum input length for estimating gradients during initialization in LoRA-GA.</p></li>
<li><p>lora_ga_direction: The default value is <code class="docutils literal notranslate"><span class="pre">ArB2r</span></code>. The initial direction used for gradient estimation during initialization in LoRA-GA. Allowed values are: <code class="docutils literal notranslate"><span class="pre">ArBr</span></code>, <code class="docutils literal notranslate"><span class="pre">A2rBr</span></code>, <code class="docutils literal notranslate"><span class="pre">ArB2r</span></code>, and <code class="docutils literal notranslate"><span class="pre">random</span></code>.</p></li>
<li><p>lora_ga_scale: The default value is <code class="docutils literal notranslate"><span class="pre">stable</span></code>. The scaling method for initialization in LoRA-GA. Allowed values are: <code class="docutils literal notranslate"><span class="pre">gd</span></code>, <code class="docutils literal notranslate"><span class="pre">unit</span></code>, <code class="docutils literal notranslate"><span class="pre">stable</span></code>, and <code class="docutils literal notranslate"><span class="pre">weightS</span></code>.</p></li>
<li><p>lora_ga_stable_gamma: The default value is <code class="docutils literal notranslate"><span class="pre">16</span></code>. The gamma value when choosing <code class="docutils literal notranslate"><span class="pre">stable</span></code> scaling for initialization.</p></li>
</ul>
</section>
</section>
<section id="fourierft">
<h4>FourierFt<a class="headerlink" href="#fourierft" title="Link to this heading">ïƒ</a></h4>
<p>FourierFt uses three parameters: <code class="docutils literal notranslate"><span class="pre">target_modules</span></code>, <code class="docutils literal notranslate"><span class="pre">target_regex</span></code>, and <code class="docutils literal notranslate"><span class="pre">modules_to_save</span></code>, whose meanings are described in the documentation above. Additional parameters include:</p>
<ul class="simple">
<li><p>fourier_n_frequency: Number of frequencies in Fourier transform, an <code class="docutils literal notranslate"><span class="pre">int</span></code>, similar to <code class="docutils literal notranslate"><span class="pre">r</span></code> in LoRA. Default value is <code class="docutils literal notranslate"><span class="pre">2000</span></code>.</p></li>
<li><p>fourier_scaling: Scaling value of matrix W, a <code class="docutils literal notranslate"><span class="pre">float</span></code>, similar to <code class="docutils literal notranslate"><span class="pre">lora_alpha</span></code> in LoRA. Default value is <code class="docutils literal notranslate"><span class="pre">300.0</span></code>.</p></li>
</ul>
</section>
<section id="boft">
<h4>BOFT<a class="headerlink" href="#boft" title="Link to this heading">ïƒ</a></h4>
<p>BOFT uses the three parameters <code class="docutils literal notranslate"><span class="pre">target_modules</span></code>, <code class="docutils literal notranslate"><span class="pre">target_regex</span></code>, and <code class="docutils literal notranslate"><span class="pre">modules_to_save</span></code>, whose meanings are described in the documentation above. Additional parameters include:</p>
<ul class="simple">
<li><p>boft_block_size: Size of BOFT blocks, default value is 4.</p></li>
<li><p>boft_block_num: Number of BOFT blocks, cannot be used simultaneously with <code class="docutils literal notranslate"><span class="pre">boft_block_size</span></code>.</p></li>
<li><p>boft_dropout: Dropout value for BOFT, default is 0.0.</p></li>
</ul>
</section>
<section id="vera">
<h4>Vera<a class="headerlink" href="#vera" title="Link to this heading">ïƒ</a></h4>
<p>Vera uses the three parameters <code class="docutils literal notranslate"><span class="pre">target_modules</span></code>, <code class="docutils literal notranslate"><span class="pre">target_regex</span></code>, and <code class="docutils literal notranslate"><span class="pre">modules_to_save</span></code>, whose meanings are described in the documentation above. Additional parameters include:</p>
<ul class="simple">
<li><p>vera_rank: Size of Vera Attention, default value is 256.</p></li>
<li><p>vera_projection_prng_key: Whether to store the Vera mapping matrix, default is True.</p></li>
<li><p>vera_dropout: Dropout value for Vera, default is <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p>vera_d_initial: Initial value of Vera's d matrix, default is <code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p></li>
</ul>
</section>
<section id="galore">
<h4>GaLore<a class="headerlink" href="#galore" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>ğŸ”¥use_galore: Default value is False, whether to use GaLore.</p></li>
<li><p>galore_target_modules: Default is None, if not provided, applies GaLore to attention and MLP.</p></li>
<li><p>galore_rank: Default value is 128, GaLore rank value.</p></li>
<li><p>galore_update_proj_gap: Default is 50, interval for updating decomposed matrices.</p></li>
<li><p>galore_scale: Default is 1.0, matrix weight coefficient.</p></li>
<li><p>galore_proj_type: Default is <code class="docutils literal notranslate"><span class="pre">std</span></code>, type of GaLore matrix decomposition.</p></li>
<li><p>galore_optim_per_parameter: Default value is False, whether to set a separate optimizer for each Galore target parameter.</p></li>
<li><p>galore_with_embedding: Default value is False, whether to apply GaLore to embedding.</p></li>
<li><p>galore_quantization: Whether to use q-galore, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>galore_proj_quant: Whether to quantize the SVD decomposition matrix, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>galore_proj_bits: Number of bits for SVD quantization.</p></li>
<li><p>galore_proj_group_size: Number of groups for SVD quantization.</p></li>
<li><p>galore_cos_threshold: Cosine similarity threshold for updating projection matrices. Default value is 0.4.</p></li>
<li><p>galore_gamma_proj: As the projection matrix becomes more similar over time, this parameter is the coefficient for extending the update interval. Default value is 2.</p></li>
<li><p>galore_queue_size: Length of the queue for calculating projection matrix similarity, default is 5.</p></li>
</ul>
</section>
<section id="lisa">
<h4>LISA<a class="headerlink" href="#lisa" title="Link to this heading">ïƒ</a></h4>
<p>Note: LISA only supports full parameters, i.e., <code class="docutils literal notranslate"><span class="pre">--tuner_type</span> <span class="pre">full</span></code>.</p>
<ul class="simple">
<li><p>ğŸ”¥lisa_activated_layers: Default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>, representing LISA is not used. Setting to a non-zero value activates that many layers, it is recommended to set to 2 or 8.</p></li>
<li><p>lisa_step_interval: Default value is <code class="docutils literal notranslate"><span class="pre">20</span></code>, number of iter to switch to layers that can be backpropagated.</p></li>
</ul>
</section>
<section id="unsloth">
<h4>UNSLOTH<a class="headerlink" href="#unsloth" title="Link to this heading">ïƒ</a></h4>
<p>ğŸ”¥Unsloth has no additional parameters; it can be supported by adjusting existing parameters, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">tuner_backend</span> <span class="n">unsloth</span>
<span class="o">--</span><span class="n">tuner_type</span> <span class="n">full</span><span class="o">/</span><span class="n">lora</span>
<span class="o">--</span><span class="n">quant_bits</span> <span class="mi">4</span>
</pre></div>
</div>
</section>
<section id="llamapro">
<h4>LLAMAPRO<a class="headerlink" href="#llamapro" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>ğŸ”¥llamapro_num_new_blocks: Default value is <code class="docutils literal notranslate"><span class="pre">4</span></code>, total number of new layers to insert.</p></li>
<li><p>llamapro_num_groups: Default value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, number of groups to insert new blocks. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, it equals <code class="docutils literal notranslate"><span class="pre">llamapro_num_new_blocks</span></code>, meaning each new layer is inserted separately into the original model.</p></li>
</ul>
</section>
<section id="adalora">
<h4>AdaLoRA<a class="headerlink" href="#adalora" title="Link to this heading">ïƒ</a></h4>
<p>When the <code class="docutils literal notranslate"><span class="pre">tuner_type</span></code> parameter is set to <code class="docutils literal notranslate"><span class="pre">adalora</span></code>, the following parameters take effect. The <code class="docutils literal notranslate"><span class="pre">adalora</span></code> parameters such as <code class="docutils literal notranslate"><span class="pre">target_modules</span></code> inherit from the corresponding parameters of <code class="docutils literal notranslate"><span class="pre">lora</span></code>, but the <code class="docutils literal notranslate"><span class="pre">lora_dtype</span></code> parameter does not take effect.</p>
<ul class="simple">
<li><p>adalora_target_r: Default value is <code class="docutils literal notranslate"><span class="pre">8</span></code>, average rank of AdaLoRA.</p></li>
<li><p>adalora_init_r: Default value is <code class="docutils literal notranslate"><span class="pre">12</span></code>, initial rank of AdaLoRA.</p></li>
<li><p>adalora_tinit: Default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>, initial warmup of AdaLoRA.</p></li>
<li><p>adalora_tfinal: Default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>, final warmup of AdaLoRA.</p></li>
<li><p>adalora_deltaT: Default value is <code class="docutils literal notranslate"><span class="pre">1</span></code>, step interval of AdaLoRA.</p></li>
<li><p>adalora_beta1: Default value is <code class="docutils literal notranslate"><span class="pre">0.85</span></code>, EMA parameter of AdaLoRA.</p></li>
<li><p>adalora_beta2: Default value is <code class="docutils literal notranslate"><span class="pre">0.85</span></code>, EMA parameter of AdaLoRA.</p></li>
<li><p>adalora_orth_reg_weight: Default value is <code class="docutils literal notranslate"><span class="pre">0.5</span></code>, regularization parameter for AdaLoRA.</p></li>
</ul>
</section>
<section id="reft">
<h4>ReFT<a class="headerlink" href="#reft" title="Link to this heading">ïƒ</a></h4>
<p>The following parameters are effective when <code class="docutils literal notranslate"><span class="pre">tuner_type</span></code> is set to <code class="docutils literal notranslate"><span class="pre">reft</span></code>.</p>
<blockquote>
<div><ol class="simple">
<li><p>ReFT cannot merge tuners.</p></li>
<li><p>ReFT is not compatible with gradient checkpointing.</p></li>
<li><p>If experiencing issues while using DeepSpeed, please uninstall DeepSpeed temporarily.</p></li>
</ol>
</div></blockquote>
<ul class="simple">
<li><p>ğŸ”¥reft_layers: Which layers ReFT is applied to, default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, representing all layers. You can input a list of layer numbers, e.g., <code class="docutils literal notranslate"><span class="pre">reft_layers</span> <span class="pre">1</span> <span class="pre">2</span> <span class="pre">3</span> <span class="pre">4</span></code>.</p></li>
<li><p>ğŸ”¥reft_rank: Rank of ReFT matrix, default is <code class="docutils literal notranslate"><span class="pre">4</span></code>.</p></li>
<li><p>reft_intervention_type: Type of ReFT, supports 'NoreftIntervention', 'LoreftIntervention', 'ConsreftIntervention', 'LobireftIntervention', 'DireftIntervention', 'NodireftIntervention', default is <code class="docutils literal notranslate"><span class="pre">LoreftIntervention</span></code>.</p></li>
<li><p>reft_args: Other supported parameters for ReFT Intervention, input in json-string format.</p></li>
</ul>
</section>
</section>
<section id="vllm-arguments">
<h3>vLLM Arguments<a class="headerlink" href="#vllm-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Parameter meanings can be found in the <a class="reference external" href="https://docs.vllm.ai/en/latest/serving/engine_args.html">vllm documentation</a>.</p>
<ul class="simple">
<li><p>ğŸ”¥vllm_gpu_memory_utilization: GPU memory ratio, ranging from 0 to 1. Default is <code class="docutils literal notranslate"><span class="pre">0.9</span></code>.</p></li>
<li><p>ğŸ”¥vllm_tensor_parallel_size: Tensor parallelism size. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>vllm_pipeline_parallel_size: Pipeline parallelism size. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>vllm_data_parallel_size: Data parallelism size, default is <code class="docutils literal notranslate"><span class="pre">1</span></code>, effective in the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">deploy/rollout</span></code> command.</p>
<ul>
<li><p>In <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span></code>, use <code class="docutils literal notranslate"><span class="pre">NPROC_PER_NODE</span></code> to set the data parallelism (DP) degree. See the example <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/vllm/mllm_ddp.sh">here</a>.</p></li>
</ul>
</li>
<li><p>vllm_enable_expert_parallel: Enable expert parallelism. Default is False.</p></li>
<li><p>vllm_max_num_seqs: Maximum number of sequences to be processed in a single iteration. Default is <code class="docutils literal notranslate"><span class="pre">256</span></code>.</p></li>
<li><p>ğŸ”¥vllm_max_model_len: The maximum sequence length supported by the model. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning it will be read from <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p></li>
<li><p>vllm_disable_custom_all_reduce: Disables the custom all-reduce kernel and falls back to NCCL. For stability, the default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>vllm_enforce_eager: Determines whether vllm uses PyTorch eager mode or constructs a CUDA graph, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Setting it to True can save memory but may affect efficiency.</p></li>
<li><p>vllm_mm_processor_cache_gb: The size (in GiB) of the multimodal processor cache, used to store processed multimodal inputs (e.g., images, videos) to avoid redundant processing. Default is 4. Setting it to 0 disables the cache but may degrade performance (not recommended). This option takes effect only for multimodal models.</p></li>
<li><p>vllm_speculative_config: Speculative decoding configuration, passed as a JSON string. Default: None.</p></li>
<li><p>vllm_disable_cascade_attn: Whether to forcibly disable the V1 engineâ€™s cascade-attention implementation to avoid potential numerical issues. Defaults to False; vLLMâ€™s internal heuristics determine whether cascade attention is actually used.</p></li>
<li><p>ğŸ”¥vllm_limit_mm_per_prompt: Controls the use of multiple media in vllm, default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. For example, you can pass in <code class="docutils literal notranslate"><span class="pre">--vllm_limit_mm_per_prompt</span> <span class="pre">'{&quot;image&quot;:</span> <span class="pre">5,</span> <span class="pre">&quot;video&quot;:</span> <span class="pre">2}'</span></code>.</p></li>
<li><p>vllm_max_lora_rank: Default is <code class="docutils literal notranslate"><span class="pre">16</span></code>. This is the parameter supported by vllm for lora.</p></li>
<li><p>vllm_quantization: vllm is able to quantize model with this argument, supported values can be found <a class="reference external" href="https://docs.vllm.ai/en/latest/serving/engine_args.html">here</a>.</p></li>
<li><p>ğŸ”¥vllm_enable_prefix_caching: Enables vLLM's automatic prefix caching to save processing time for repeated prompt prefixes, improving inference efficiency. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, following vLLM's default behavior.</p></li>
<li><p>vllm_use_async_engine: Whether to use the async engine under the vLLM backend. Default is None, which is automatically set based on the scenario: encode tasks (embedding, seq_cls, reranker, generative_reranker) default to True, deployment scenarios (swift deploy) default to True, and other scenarios default to False. Note: Encode tasks must use the async engine.</p></li>
<li><p>vllm_reasoning_parser: Reasoning parser type, used for parsing the chain of thought content of reasoning models. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Only used for the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">deploy</span></code> command. Available types can be found in the <a class="reference external" href="https://docs.vllm.ai/en/latest/features/reasoning_outputs.html#streaming-chat-completions">vLLM documentation</a>.</p></li>
<li><p>vllm_engine_kwargs: Extra arguments for vllm, formatted as a JSON string. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</section>
<section id="sglang-arguments">
<h3>SGLang Arguments<a class="headerlink" href="#sglang-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Parameter meanings can be found in the <a class="reference external" href="https://docs.sglang.ai/backend/server_arguments.html">sglang documentation</a>.</p>
<ul class="simple">
<li><p>ğŸ”¥sglang_tp_size: Tensor parallelism size. Default is 1.</p></li>
<li><p>sglang_pp_size: Pipeline parallelism size. Default is 1.</p></li>
<li><p>sglang_dp_size: Data parallelism size. Default is 1.</p></li>
<li><p>sglang_ep_size: Expert parallelism size. Default is 1.</p></li>
<li><p>sglang_enable_ep_moe: Whether to enable EP MoE. Default is False. This parameter has been removed in the latest version of SGLang.</p></li>
<li><p>sglang_mem_fraction_static: The fraction of GPU memory used for static allocation (model weights and KV cache memory pool). If you encounter out-of-memory errors, try reducing this value. Default is None.</p></li>
<li><p>sglang_context_length: The maximum context length of the model. Default is None, which means it will use the value from the model's <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p></li>
<li><p>sglang_disable_cuda_graph: Disables CUDA graph. Default is False.</p></li>
<li><p>sglang_quantization: Quantization method. Default is None.</p></li>
<li><p>sglang_kv_cache_dtype: Data type for KV cache storage. 'auto' means it will use the model's data type. 'fp8_e5m2' and 'fp8_e4m3' are supported on CUDA 11.8 and above. Default is 'auto'.</p></li>
<li><p>sglang_enable_dp_attention: Enables data parallelism for attention and tensor parallelism for FFN. The data parallelism size (dp size) should be equal to the tensor parallelism size (tp size). Currently supports DeepSeek-V2/3 and Qwen2/3 MoE models. Default is False.</p></li>
<li><p>sglang_disable_custom_all_reduce: Disables the custom all-reduce kernel and falls back to NCCL. For stability, the default is True.</p></li>
<li><p>sglang_speculative_algorithm: Speculative algorithm. Available options: None, &quot;EAGLE&quot;, &quot;EAGLE3&quot;, &quot;NEXTN&quot;, &quot;STANDALONE&quot;, &quot;NGRAM&quot;. Default is None.</p></li>
<li><p>sglang_speculative_num_steps: The number of steps sampled from the draft model in speculative decoding. Default is None.</p></li>
<li><p>sglang_speculative_eagle_topk: The number of tokens sampled from the draft model at each step in the EAGLE2 algorithm. Default is None.</p></li>
<li><p>sglang_speculative_num_draft_tokens: The number of tokens sampled from the draft model in speculative decoding. Default is None.</p></li>
</ul>
</section>
<section id="lmdeploy-arguments">
<h3>LMDeploy Arguments<a class="headerlink" href="#lmdeploy-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Parameter meanings can be found in the <a class="reference external" href="https://lmdeploy.readthedocs.io/en/latest/api/pipeline.html#turbomindengineconfig">lmdeploy documentation</a>.</p>
<ul class="simple">
<li><p>ğŸ”¥lmdeploy_tp: tensor parallelism degree. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>lmdeploy_session_len: Maximum session length. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>lmdeploy_cache_max_entry_count: The percentage of GPU memory occupied by the k/v cache. Default is <code class="docutils literal notranslate"><span class="pre">0.8</span></code>.</p></li>
<li><p>lmdeploy_quant_policy: Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>. Set it to <code class="docutils literal notranslate"><span class="pre">4</span></code> or <code class="docutils literal notranslate"><span class="pre">8</span></code> when quantizing k/v to 4-bit or 8-bit, respectively.</p></li>
<li><p>lmdeploy_vision_batch_size: The <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code> parameter passed to <code class="docutils literal notranslate"><span class="pre">VisionConfig</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</section>
<section id="merge-arguments">
<h3>Merge Arguments<a class="headerlink" href="#merge-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>ğŸ”¥merge_lora: Indicates whether to merge lora; this parameter supports lora, llamapro, and longlora, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Example parameters <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/export/merge_lora.sh">here</a>.</p></li>
<li><p>safe_serialization: Whether to save the model in safetensors format. Default is True.</p></li>
<li><p>max_shard_size: Maximum size of a single storage file, default is '5GB'.</p></li>
</ul>
</section>
</section>
<section id="integration-arguments">
<h2>Integration Arguments<a class="headerlink" href="#integration-arguments" title="Link to this heading">ïƒ</a></h2>
<section id="training-arguments">
<h3>Training Arguments<a class="headerlink" href="#training-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Training arguments include the <a class="reference external" href="#base-arguments">base arguments</a>, <a class="reference external" href="#Seq2SeqTrainer-arguments">Seq2SeqTrainer arguments</a>, <a class="reference external" href="#tuner-arguments">tuner arguments</a>, and also include the following parts:</p>
<ul class="simple">
<li><p>add_version: Add directory to output_dir with <code class="docutils literal notranslate"><span class="pre">'&lt;version&gt;-&lt;timestamp&gt;'</span></code> to prevent weight overwrite, default is True.</p></li>
<li><p>check_model: Check local model files for corruption or modification and give a prompt, default is True. <strong>If in an offline environment, please set to False.</strong></p></li>
<li><p>ğŸ”¥create_checkpoint_symlink: Creates additional checkpoint symlinks to facilitate writing automated training scripts. The symlink paths for <code class="docutils literal notranslate"><span class="pre">best_model</span></code> and <code class="docutils literal notranslate"><span class="pre">last_model</span></code> are <code class="docutils literal notranslate"><span class="pre">f'{output_dir}/best'</span></code> and <code class="docutils literal notranslate"><span class="pre">f'{output_dir}/last'</span></code> respectively.</p></li>
<li><p>ğŸ”¥packing: Uses the <code class="docutils literal notranslate"><span class="pre">padding_free</span></code> approach to pack data samples of different lengths into samples of <strong>approximately</strong> uniform length (packing ensures complete sequences are not split), achieving load balancing across nodes and processes during training (avoiding long texts slowing down short text training speed), thereby improving GPU utilization and maintaining stable memory usage. When using <code class="docutils literal notranslate"><span class="pre">--attn_impl</span> <span class="pre">flash_attn</span></code>, it ensures that different sequences within packed samples are independent and invisible to each other. This parameter defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code> and currently supports packing for CPT/SFT/DPO/KTO/GKD as well as embedding/reranker/seq_cls tasks. Note: <strong>packing will reduce the number of dataset samples, please adjust gradient accumulation and learning rate accordingly</strong>.</p></li>
<li><p>packing_length: the length to use for packing. Defaults to None, in which case it is set to max_length.</p></li>
<li><p>packing_num_proc: Number of processes for packing, default is 1. Note that different values of <code class="docutils literal notranslate"><span class="pre">packing_num_proc</span></code> will result in different packed datasets. (This parameter does not take effect during streaming packing). Usually there is no need to modify this value, as packing speed is much faster than tokenization speed.</p></li>
<li><p>lazy_tokenize: Whether to use lazy tokenization. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, all dataset samples will be tokenized (and for multimodal models, images will be loaded from disk) before training begins. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>: in LLM training, it defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>; in MLLM training, it defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> to save memory.</p>
<ul>
<li><p>Note: If you want to perform image data augmentation, you need to set <code class="docutils literal notranslate"><span class="pre">lazy_tokenize</span></code> (or <code class="docutils literal notranslate"><span class="pre">streaming</span></code>) to True and modify the <code class="docutils literal notranslate"><span class="pre">encode</span></code> method in the Template class.</p></li>
</ul>
</li>
<li><p>use_logits_to_keep: Pass <code class="docutils literal notranslate"><span class="pre">logits_to_keep</span></code> in the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method based on labels to reduce the computation and storage of unnecessary logits, thereby reducing memory usage and accelerating training. The default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which enables automatic selection.</p></li>
<li><p>acc_strategy: Strategy for calculating accuracy during training and validation. Options are <code class="docutils literal notranslate"><span class="pre">seq</span></code>-level and <code class="docutils literal notranslate"><span class="pre">token</span></code>-level accuracy, with <code class="docutils literal notranslate"><span class="pre">token</span></code> as the default.</p></li>
<li><p>max_new_tokens: Generation parameter override. The maximum number of tokens to generate when <code class="docutils literal notranslate"><span class="pre">predict_with_generate=True</span></code>, defaulting to 64.</p></li>
<li><p>temperature: Generation parameter override. The temperature setting when <code class="docutils literal notranslate"><span class="pre">predict_with_generate=True</span></code>, defaulting to 0.</p></li>
<li><p>optimizer: The optimizer plugin to use (takes priority over <code class="docutils literal notranslate"><span class="pre">--optim</span></code>), default is None. Available optimizers can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/optimizers/mapping.py">here</a>.</p></li>
<li><p>loss_type: Custom loss_type name. Default is None, uses the model's built-in loss function. Available loss options can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss/mapping.py">here</a>.</p></li>
<li><p>eval_metric: Custom eval metric name. Default is None. Available eval_metric options can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/eval_metric/mapping.py">here</a>.</p>
<ul>
<li><p>Regarding default values: When <code class="docutils literal notranslate"><span class="pre">task_type</span></code> is 'causal_lm' and <code class="docutils literal notranslate"><span class="pre">predict_with_generate=True</span></code>, it defaults to 'nlg'. When <code class="docutils literal notranslate"><span class="pre">task_type</span></code> is 'embedding', the default value is 'infonce' or 'paired' based on loss_type. When <code class="docutils literal notranslate"><span class="pre">task_type</span></code> is 'reranker/generative_reranker', the default value is 'reranker'.</p></li>
</ul>
</li>
<li><p>callbacks: Custom trainer callbacks, default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. Available callbacks can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/callbacks/mapping.py">here</a>.For example,enable elastic training by adding <code class="docutils literal notranslate"><span class="pre">deepspeed_elastic</span></code> (and optionally <code class="docutils literal notranslate"><span class="pre">graceful_exit</span></code>) in <code class="docutils literal notranslate"><span class="pre">callbacks</span></code>. See the <a class="reference internal" href="../BestPractices/Elastic.html"><span class="doc">Elastic guide</span></a>.</p></li>
<li><p>early_stop_interval: The interval for early stopping. Training will terminate when best_metric shows no improvement within early_stop_interval periods (based on <code class="docutils literal notranslate"><span class="pre">save_steps</span></code>; it's recommended to set <code class="docutils literal notranslate"><span class="pre">eval_steps</span></code> and <code class="docutils literal notranslate"><span class="pre">save_steps</span></code> to the same value). The specific implementation can be found in <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/callbacks/early_stop.py">early_stop.py</a>. Additionally, if you have more complex early stopping requirements, you can directly override the existing implementation in callback.py. When this parameter is set, the <code class="docutils literal notranslate"><span class="pre">early_stop</span></code> trainer callback is automatically added.</p></li>
<li><p>eval_use_evalscope: Whether to use evalscope for evaluation, this parameter needs to be set to enable evaluation, refer to <a class="reference external" href="../Instruction/Evaluation.md#evaluation-during-training">example</a>. Default is False.</p></li>
<li><p>eval_dataset: Evaluation datasets, multiple datasets can be set, separated by spaces</p></li>
<li><p>eval_dataset_args: Evaluation dataset parameters in JSON format, parameters for multiple datasets can be set</p></li>
<li><p>eval_limit: Number of samples from the evaluation dataset</p></li>
<li><p>eval_generation_config: Model inference configuration during evaluation, in JSON format, default is <code class="docutils literal notranslate"><span class="pre">{'max_tokens':</span> <span class="pre">512}</span></code></p></li>
<li><p>use_flash_ckpt: Whether to use <a class="reference external" href="https://github.com/intelligent-machine-learning/dlrover">DLRover Flash Checkpoint</a>. Default is <code class="docutils literal notranslate"><span class="pre">false</span></code>. If enabled, checkpoints are saved to memory synchronously, then persisted to storage asynchronously. It's recommended to use this with the environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_CUDA_ALLOC_CONF=&quot;expandable_segments:True&quot;</span></code> to avoid CUDA OOM.</p></li>
</ul>
<section id="swanlab">
<h4>SWANLAB<a class="headerlink" href="#swanlab" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>swanlab_token: The API key for SwanLab. You can also specify it using the <code class="docutils literal notranslate"><span class="pre">SWANLAB_API_KEY</span></code> environment variable.</p></li>
<li><p>swanlab_project: The SwanLab project, which can be created in advance on the page <a class="reference external" href="https://swanlab.cn/space/~">https://swanlab.cn/space/~</a> or created automatically. The default is &quot;ms-swift&quot;.</p></li>
<li><p>swanlab_workspace: Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the username associated with the API key.</p></li>
<li><p>swanlab_exp_name: Experiment name, can be left empty. If empty, the value of <code class="docutils literal notranslate"><span class="pre">--output_dir</span></code> will be used by default.</p></li>
<li><p>swanlab_notification_method: The notification method for SwanLab when training completes or errors occur. For details, refer to <a class="reference external" href="https://docs.swanlab.cn/plugin/notification-dingtalk.html">here</a>. Supports 'dingtalk', 'lark', 'email', 'discord', 'wxwork', 'slack'.</p></li>
<li><p>swanlab_webhook_url: Defaults to None. The webhook URL corresponding to SwanLab's <code class="docutils literal notranslate"><span class="pre">swanlab_notification_method</span></code>.</p></li>
<li><p>swanlab_secret: Defaults to None. The secret corresponding to SwanLab's <code class="docutils literal notranslate"><span class="pre">swanlab_notification_method</span></code>.</p></li>
<li><p>swanlab_mode: Optional values are <code class="docutils literal notranslate"><span class="pre">cloud</span></code> and <code class="docutils literal notranslate"><span class="pre">local</span></code>, representing cloud mode or local mode.</p></li>
</ul>
</section>
</section>
<section id="rlhf-arguments">
<h3>RLHF Arguments<a class="headerlink" href="#rlhf-arguments" title="Link to this heading">ïƒ</a></h3>
<p>RLHF arguments inherit from the <a class="reference external" href="#training-arguments">training arguments</a>.</p>
<ul class="simple">
<li><p>ğŸ”¥rlhf_type: Type of human alignment algorithm, supporting 'dpo', 'orpo', 'simpo', 'kto', 'cpo', 'rm', 'ppo', 'grpo' and 'gkd'. Default is 'dpo'.</p></li>
<li><p>ref_model: Required for full parameter training when using the dpo, kto, ppo or grpo algorithms. Default is None, set to <code class="docutils literal notranslate"><span class="pre">--model</span></code>.</p></li>
<li><p>ref_adapters: Defaults to <code class="docutils literal notranslate"><span class="pre">[]</span></code>. If you want to use LoRA weights generated from SFT for DPO/KTO/GRPO, set <code class="docutils literal notranslate"><span class="pre">--adapters</span> <span class="pre">sft_ckpt</span> <span class="pre">--ref_adapters</span> <span class="pre">sft_ckpt</span></code> during training. For checkpoint resumption in this scenario, set <code class="docutils literal notranslate"><span class="pre">--resume_from_checkpoint</span> <span class="pre">rlhf_ckpt</span> <span class="pre">--ref_adapters</span> <span class="pre">sft_ckpt</span></code>.</p></li>
<li><p>ref_model_type: Same as model_type. Default is None.</p></li>
<li><p>ref_model_revision: Same as model_revision. Default is None.</p></li>
<li><p>ğŸ”¥beta: A parameter controlling the degree of deviation from the reference model. A higher beta value indicates smaller deviation from the reference model. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, with different default values depending on the RLHF algorithm: <code class="docutils literal notranslate"><span class="pre">2.0</span></code> for SimPO, <code class="docutils literal notranslate"><span class="pre">0.04</span></code> for GRPO, <code class="docutils literal notranslate"><span class="pre">0.5</span></code> for GKD, and <code class="docutils literal notranslate"><span class="pre">0.1</span></code> for other algorithms. See <a class="reference internal" href="RLHF.html"><span class="doc">documentation</span></a> for details.</p></li>
<li><p>label_smoothing: Whether to use DPO smoothing, default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p>max_completion_length: The maximum generation length in the GRPO/PPO/GKD algorithms. Default is 512.</p></li>
<li><p>ğŸ”¥rpo_alpha: A parameter from the <a class="reference external" href="https://arxiv.org/abs/2404.19733">RPO paper</a> that controls the weight of the NLL term (i.e., the SFT loss) in the loss function, where <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">dpo_loss</span> <span class="pre">+</span> <span class="pre">rpo_alpha</span> <span class="pre">*</span> <span class="pre">sft_loss</span></code>. The paper recommends setting it to <code class="docutils literal notranslate"><span class="pre">1.</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning the SFT loss is not included by default.</p></li>
<li><p>ld_alpha: From the <a class="reference external" href="https://arxiv.org/abs/2409.06411">LD-DPO paper</a>. Applies a weight Î± &lt; 1 to the log-probabilities of tokens that lie beyond the shared prefix of the chosen and rejected responses, thereby mitigating length bias.</p></li>
<li><p>discopop_tau: Temperature parameter Ï„ from the <a class="reference external" href="https://arxiv.org/abs/2406.08414">DiscoPOP paper</a> used to scale the log-ratio before the sigmoid modulation. Default 0.05; only active when loss_type is discopop.</p></li>
<li><p>loss_type: Type of loss function. Default is None, with different defaults depending on the RLHF algorithm used.</p>
<ul>
<li><p>DPO: Available options can be found in the <a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions">documentation</a>. Multiple values can be provided to enable mixed training (<a class="reference external" href="https://arxiv.org/abs/2411.10442">MPO</a>); when multiple values are given, the loss_weights parameter must also be set. Default is <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>.</p></li>
<li><p>GRPO: See <a class="reference external" href="#grpo-arguments">GRPO parameters</a> for reference.</p></li>
</ul>
</li>
<li><p>loss_weights: When setting multiple loss_type values in DPO training, this parameter specifies the weight for each loss component.</p></li>
<li><p>cpo_alpha: Coefficient for nll loss in CPO/SimPO loss, default is <code class="docutils literal notranslate"><span class="pre">1.</span></code>.</p></li>
<li><p>simpo_gamma: Reward margin term in the SimPO algorithm, with a paper-suggested setting of 0.5-1.5, default is <code class="docutils literal notranslate"><span class="pre">1.</span></code>.</p></li>
<li><p>desirable_weight: In the KTO algorithm, this weight compensates for the imbalance between the number of desirable and undesirable samples by scaling the desirable loss. Default is <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p>undesirable_weight: In the KTO algorithm, this weight compensates for the imbalance between desirable and undesirable samples by scaling the undesirable loss. Default is <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p>center_rewards_coefficient: A coefficient used in reward model (RM) training to incentivize the model to output rewards with zero mean. See this <a class="reference external" href="https://huggingface.co/papers/2312.09244">paper</a> for details. Recommended value: 0.01.</p></li>
<li><p>loss_scale: Overrides the template parameter. During RLHF training, the default is <code class="docutils literal notranslate"><span class="pre">'last_round'</span></code>.</p></li>
<li><p>temperature: Default is 0.9; this parameter will be used in PPO, GRPO and GKD.</p></li>
</ul>
<section id="gkd-arguments">
<h4>GKD Arguments<a class="headerlink" href="#gkd-arguments" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>lmbda: Default is 0.5. This parameter is used in GKD. It controls the lambda parameter for the proportion of student data (i.e., the proportion of student-generated outputs within the strategy). If lmbda is 0, student-generated data is not used.</p></li>
<li><p>sft_alpha: The default value is 0. It controls the weight of sft_loss added in GKD. The final loss is <code class="docutils literal notranslate"><span class="pre">gkd_loss</span> <span class="pre">+</span> <span class="pre">sft_alpha</span> <span class="pre">*</span> <span class="pre">sft_loss</span></code>.</p></li>
<li><p>seq_kd: Default is False. This parameter is used in GKD. It is the <code class="docutils literal notranslate"><span class="pre">seq_kd</span></code> parameter that controls whether to perform Sequence-Level KD (can be viewed as supervised fine-tuning on teacher-generated output).</p>
<ul>
<li><p>Note: You can perform inference on the dataset using the teacher model in advance (accelerated by inference engines such as vLLM, SGLang, or lmdeploy), and set <code class="docutils literal notranslate"><span class="pre">seq_kd</span></code> to False during training. Alternatively, you can set <code class="docutils literal notranslate"><span class="pre">seq_kd</span></code> to True, which will use the teacher model to generate sequences during training (ensuring different generated data across multiple epochs, but at a slower efficiency).</p></li>
</ul>
</li>
<li><p>offload_teacher_model: Whether to offload the teacher model to save GPU memory. If set to True, the teacher model will be loaded only during generate/logps computation. Default: False.</p></li>
<li><p>truncation_strategy: The method to handle inputs exceeding <code class="docutils literal notranslate"><span class="pre">max_length</span></code>. Supported values are <code class="docutils literal notranslate"><span class="pre">delete</span></code> and <code class="docutils literal notranslate"><span class="pre">left</span></code>, representing deletion and left-side truncation respectively. The default is <code class="docutils literal notranslate"><span class="pre">left</span></code>. With the delete strategy, over-long or encoding-failed samples are discarded, and new samples are resampled from the original dataset to maintain the intended batch size.</p></li>
<li><p>log_completions: Whether to log the model-generated content during training, to be used in conjunction with <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">wandb/swanlab</span></code>, default is False.</p>
<ul>
<li><p>Note: If <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">wandb/swanlab</span></code> is not set, a <code class="docutils literal notranslate"><span class="pre">completions.jsonl</span></code> will be created in the checkpoint to store the generated content.</p></li>
<li><p>Log vLLM rollout results only.</p></li>
</ul>
</li>
</ul>
</section>
<section id="reward-teacher-model-parameters">
<h4>Reward/Teacher Model Parameters<a class="headerlink" href="#reward-teacher-model-parameters" title="Link to this heading">ïƒ</a></h4>
<p>The reward model parameters will be used in PPO and GRPO.</p>
<ul class="simple">
<li><p>reward_model: Default is None.</p></li>
<li><p>reward_adapters: Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>reward_model_type: Default is None.</p></li>
<li><p>reward_model_revision: Default is None.</p></li>
<li><p>teacher_model: Default is None. This parameter must be provided when <code class="docutils literal notranslate"><span class="pre">rlhf_type</span></code> is <code class="docutils literal notranslate"><span class="pre">'gkd'</span></code>.</p></li>
<li><p>teacher_adapters: Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>teacher_model_type: Default is None.</p></li>
<li><p>teacher_model_revision: Default is None.</p></li>
<li><p>teacher_deepspeed: Same as the deepspeed parameter, controls the DeepSpeed configuration for the teacher model. By default, uses the DeepSpeed configuration of the training model.</p></li>
</ul>
</section>
<section id="ppo-arguments">
<h4>PPO Arguments<a class="headerlink" href="#ppo-arguments" title="Link to this heading">ïƒ</a></h4>
<p>The meanings of the following parameters can be referenced <a class="reference external" href="https://huggingface.co/docs/trl/main/ppo_trainer">here</a>:</p>
<ul class="simple">
<li><p>num_ppo_epochs: Defaults to 4</p></li>
<li><p>whiten_rewards: Defaults to False</p></li>
<li><p>kl_coef: Defaults to 0.05</p></li>
<li><p>cliprange: Defaults to 0.2</p></li>
<li><p>vf_coef: Defaults to 0.1</p></li>
<li><p>cliprange_value: Defaults to 0.2</p></li>
<li><p>gamma: Defaults to 1.0</p></li>
<li><p>lam: Defaults to 0.95</p></li>
<li><p>num_mini_batches: Defaults to 1</p></li>
<li><p>local_rollout_forward_batch_size: Defaults to 64</p></li>
<li><p>num_sample_generations: Defaults to 10</p></li>
<li><p>missing_eos_penalty: Defaults to None</p></li>
</ul>
</section>
<section id="grpo-arguments">
<h4>GRPO Arguments<a class="headerlink" href="#grpo-arguments" title="Link to this heading">ïƒ</a></h4>
<ul class="simple">
<li><p>beta: KL regularization coefficient; default 0.04. Setting it to 0 disables the reference model.</p></li>
<li><p>per_device_train_batch_size: The training batch size per device. In GRPO, this refers to the batch size of completions during training.</p></li>
<li><p>per_device_eval_batch_size: The evaluation batch size per device. In GRPO, this refers to the batch size of completions during evaluation.</p></li>
<li><p>steps_per_generation: Number of optimization steps per generation. It defaults to gradient_accumulation_steps. This parameter and generation_batch_size cannot be set simultaneously.</p></li>
<li><p>generation_batch_size: Total batch size of sampling completions. It should be a multiple of num_processes * per_device_train_batch_size. It defaults to per_device_train_batch_size * steps_per_generation * num_processes.</p></li>
<li><p>num_generations: The number of samples generated per prompt (corresponding to the G value in the paper). generation_batch_size must be divisible by num_generations. The default value is 8.</p></li>
<li><p>num_generations_eval: Number of generations to sample during evaluation. This allows using fewer generations during evaluation to save computation. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the value of <code class="docutils literal notranslate"><span class="pre">num_generations</span></code>. Default is None.</p></li>
<li><p>ds3_gather_for_generation: This parameter applies to DeepSpeed ZeRO-3. If enabled, the policy model weights are gathered for generation, improving generation speed. However, disabling this option allows training models that exceed the VRAM capacity of a single GPU, albeit at the cost of slower generation. Disabling this option is not compatible with vLLM generation. The default is True.</p></li>
<li><p>reward_funcs: Reward functions in the GRPO algorithm; options include <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>,<code class="docutils literal notranslate"><span class="pre">format</span></code>,<code class="docutils literal notranslate"><span class="pre">cosine</span></code>,<code class="docutils literal notranslate"><span class="pre">repetition</span></code> and <code class="docutils literal notranslate"><span class="pre">soft_overlong</span></code>, as seen in <code class="docutils literal notranslate"><span class="pre">swift/rewards/orm.py</span></code>. You can also customize your own reward functions in the plugin. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>reward_weights: Weights for each reward function. The number should be equal to the sum of the number of reward functions and reward models. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all rewards are weighted equally with weight <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<ul>
<li><p>Note: If <code class="docutils literal notranslate"><span class="pre">--reward_model</span></code> is included in GRPO training, it is added to the end of the reward functions.</p></li>
</ul>
</li>
<li><p>reward_model_plugin: The logic for the reward model, which defaults to ORM logic. For more information, please refer to <a class="reference external" href="./GRPO/DeveloperGuide/reward_model.md#custom-reward-model">Customized Reward Models</a>.</p></li>
<li><p>dataset_shuffle: Whether to shuffle the dataset randomly. Default is True.</p></li>
<li><p>truncation_strategy: The method to handle inputs exceeding <code class="docutils literal notranslate"><span class="pre">max_length</span></code>. Supported values are <code class="docutils literal notranslate"><span class="pre">delete</span></code> and <code class="docutils literal notranslate"><span class="pre">left</span></code>, representing deletion and left-side truncation respectively. The default is <code class="docutils literal notranslate"><span class="pre">left</span></code>. With the delete strategy, over-long or encoding-failed samples are discarded, and new samples are resampled from the original dataset to maintain the intended batch size.</p></li>
<li><p>loss_type: The type of loss normalization. Options are ['grpo', 'bnpo', 'dr_grpo', 'dapo', 'cispo', 'sapo'], default is 'grpo'. For details, refer to this <a class="reference internal" href="GRPO/DeveloperGuide/loss_types.html"><span class="doc">doc</span></a></p></li>
<li><p>log_completions: Whether to log the model-generated content during training, to be used in conjunction with <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">wandb/swanlab</span></code>, default is False.</p>
<ul>
<li><p>Note: If <code class="docutils literal notranslate"><span class="pre">--report_to</span> <span class="pre">wandb/swanlab</span></code> is not set, a <code class="docutils literal notranslate"><span class="pre">completions.jsonl</span></code> will be created in the checkpoint to store the generated content.</p></li>
</ul>
</li>
<li><p>use_vllm: Whether to use vLLM as the infer_backend for GRPO generation, default is False.</p></li>
<li><p>vllm_mode: Mode to use for vLLM integration when <code class="docutils literal notranslate"><span class="pre">use_vllm</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Must be one of <code class="docutils literal notranslate"><span class="pre">server</span></code> or <code class="docutils literal notranslate"><span class="pre">colocate</span></code></p></li>
<li><p>vllm_mode server parameter</p>
<ul>
<li><p>vllm_server_host: The host address of the vLLM server. Default is None.</p></li>
<li><p>vllm_server_port: The service port of the vLLM server. Default is 8000.</p></li>
<li><p>vllm_server_base_url: Base URL for the vLLM server (e.g., 'http://localhost:8000'). If provided, <code class="docutils literal notranslate"><span class="pre">vllm_server_host</span></code> &quot; &quot;and <code class="docutils literal notranslate"><span class="pre">vllm_server_port</span></code> are ignored. Default is None.</p></li>
<li><p>vllm_server_timeout: The connection timeout for the vLLM server. Default is 240 seconds.</p></li>
<li><p>vllm_server_pass_dataset: pass additional dataset information through to the vLLM server for multi-turn training.</p></li>
<li><p>vllm_server_group_port: The internal communication port for the vLLM server. Generally, there is no need to set it unless the port is occupied. The default value is 51216.</p></li>
<li><p>async_generate: Use async rollout to improve train speed. Note that rollout will use the model updated in the previous round when enabled. Multi-turn scenarios are not supported. Default is <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>enable_flattened_weight_sync: Whether to use flattened tensor for weight synchronization. When enabled, multiple parameters are packed into a single contiguous tensor for transfer, which can improve synchronization efficiency; Only takes effect in Server Mode. Default is True.</p></li>
<li><p>SWIFT_UPDATE_WEIGHTS_BUCKET_SIZE: An environment variable that controls the bucket size (in MB) for flattened tensor weight synchronization during full-parameter training in Server Mode. Default is 512 MB.</p></li>
</ul>
</li>
<li><p>vllm_mode colocate parameter (For more parameter support, refer to the <a class="reference external" href="#vLLM-Arguments">vLLM Arguments</a>.)</p>
<ul>
<li><p>vllm_gpu_memory_utilization: vLLM passthrough parameter, default is 0.9.</p></li>
<li><p>vllm_max_model_len: vLLM passthrough parameter, the total length limit of model, default is None.</p></li>
<li><p>vllm_enforce_eager: vLLM passthrough parameter, default is False.</p></li>
<li><p>vllm_limit_mm_per_prompt: vLLM passthrough parameter, default is None.</p></li>
<li><p>vllm_enable_prefix_caching: A pass-through parameter for vLLM, default is True.</p></li>
<li><p>vllm_tensor_parallel_size: the tensor parallel size of vLLM engine, default is 1.</p></li>
<li><p>vllm_enable_lora: Enable the vLLM engine to load LoRA adapters; defaults to False. Used to accelerate weight synchronization during LoRA training. See the <a class="reference external" href="./GRPO/GetStarted/GRPO.md#weight-sync-acceleration">documentation</a> for details.</p></li>
<li><p>sleep_level: make vllm sleep when model is training. Options are 0/1/2, default is 0, no sleep</p></li>
<li><p>offload_optimizer: Whether to offload optimizer parameters during inference with vLLM. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>offload_model: Whether to offload the model during inference with vLLM. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>completion_length_limit_scope: Specifies the scope of the <code class="docutils literal notranslate"><span class="pre">max_completion_length</span></code> limit in multi-turn conversations.
When set to <code class="docutils literal notranslate"><span class="pre">total</span></code>, the total output length across all turns must not exceed <code class="docutils literal notranslate"><span class="pre">max_completion_length</span></code>.
When set to <code class="docutils literal notranslate"><span class="pre">per_round</span></code>, each individual turn's output length is limited separately.
Defaults to <code class="docutils literal notranslate"><span class="pre">per_round</span></code>. Currently only takes effect in colocate mode.</p></li>
</ul>
</li>
<li><p>num_iterations: The number of updates per data sample, corresponding to the $\mu$ value in the GRPO paper. Default is 1.</p></li>
<li><p>epsilon: epsilon value for clipping. Default is 0.2.</p></li>
<li><p>epsilon_high: Upper clip coefficient, default is None. When set, it forms a clipping range of [epsilon, epsilon_high] together with epsilon.</p></li>
<li><p>tau_pos: Temperature parameter for positive advantages in <a class="reference external" href="https://arxiv.org/abs/2511.20347">SAPO</a> algorithm, controlling the sharpness of the soft gating function. Larger values make the gate sharper (closer to hard clipping), smaller values make it smoother. Default is 1.0.</p></li>
<li><p>tau_neg: Temperature parameter for negative advantages in SAPO algorithm, controlling the sharpness of the soft gating function. Typically set <code class="docutils literal notranslate"><span class="pre">tau_neg</span> <span class="pre">&gt;</span> <span class="pre">tau_pos</span></code> to apply stronger constraints on negative advantages. Default is 1.05.</p></li>
<li><p>dynamic_sample: Exclude data within the group where the reward standard deviation is 0, and additionally sample new data. Default is False.</p></li>
<li><p>max_resample_times: Under the dynamic_sample setting, limit the number of resampling attempts to a maximum of 3. Default is 3 times.</p></li>
<li><p>overlong_filter: Skip overlong truncated samples, which will not be included in loss calculation. Default is False.
The hyperparameters for the reward function can be found in the <a class="reference external" href="#built-in-reward-functions">Built-in Reward Functions section</a>.</p></li>
<li><p>delta: Delta value for the upper clipping bound in two-sided GRPO. Recommended to be &gt; 1 + epsilon. This method was introduced in the <a class="reference external" href="https://huggingface.co/papers/2505.07291">INTELLECT-2 tech report</a>.</p></li>
<li><p>importance_sampling_level: Controls how the importance sampling ratio is computed. Options are <code class="docutils literal notranslate"><span class="pre">token</span></code> and <code class="docutils literal notranslate"><span class="pre">sequence</span></code>. In <code class="docutils literal notranslate"><span class="pre">token</span></code> mode, the raw per-token log-probability ratios are used. In <code class="docutils literal notranslate"><span class="pre">sequence</span></code> mode, the log-probability ratios of all valid tokens in the sequence are averaged to produce a single ratio per sequence. The <a class="reference external" href="https://arxiv.org/abs/2507.18071">GSPO paper</a> uses sequence-level importance sampling to stabilize training. The default is <code class="docutils literal notranslate"><span class="pre">token</span></code>.</p></li>
<li><p>advantage_estimator: Advantage estimator. Default is <code class="docutils literal notranslate"><span class="pre">grpo</span></code> (group-relative advantage). Options: <code class="docutils literal notranslate"><span class="pre">grpo</span></code>, <a class="reference internal" href="GRPO/AdvancedResearch/RLOO.html"><span class="doc">rloo</span></a>, <a class="reference internal" href="GRPO/AdvancedResearch/REINFORCEPP.html"><span class="doc">reinforce_plus_plus</span></a>.</p></li>
<li><p>kl_in_reward: Controls where the KL regularization is applied. <code class="docutils literal notranslate"><span class="pre">false</span></code>: KL is a separate loss term. <code class="docutils literal notranslate"><span class="pre">true</span></code>: KL is subtracted from the reward. The default is bound to <code class="docutils literal notranslate"><span class="pre">advantage_estimator</span></code>: <code class="docutils literal notranslate"><span class="pre">false</span></code> for <code class="docutils literal notranslate"><span class="pre">grpo</span></code>, and <code class="docutils literal notranslate"><span class="pre">true</span></code> for <code class="docutils literal notranslate"><span class="pre">rloo</span></code> and <code class="docutils literal notranslate"><span class="pre">reinforce_plus_plus</span></code>.</p></li>
<li><p>scale_rewards: Specifies the reward scaling strategy. Options: <code class="docutils literal notranslate"><span class="pre">group</span></code> (scale by intra-group std), <code class="docutils literal notranslate"><span class="pre">batch</span></code> (scale by batch-wide std), <code class="docutils literal notranslate"><span class="pre">none</span></code> (no scaling), <code class="docutils literal notranslate"><span class="pre">gdpo</span></code> (normalize each reward function separately within groups before weighted aggregation, see <a class="reference external" href="https://arxiv.org/abs/2601.05242">GDPO paper</a>). In ms-swift &lt; 3.10, this was a boolean where <code class="docutils literal notranslate"><span class="pre">true</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">group</span></code> and <code class="docutils literal notranslate"><span class="pre">false</span></code> to <code class="docutils literal notranslate"><span class="pre">none</span></code>. The default is bound to <code class="docutils literal notranslate"><span class="pre">advantage_estimator</span></code>: <code class="docutils literal notranslate"><span class="pre">group</span></code> for <code class="docutils literal notranslate"><span class="pre">grpo</span></code>, <code class="docutils literal notranslate"><span class="pre">none</span></code> for <code class="docutils literal notranslate"><span class="pre">rloo</span></code>, and <code class="docutils literal notranslate"><span class="pre">batch</span></code> for <code class="docutils literal notranslate"><span class="pre">reinforce_plus_plus</span></code>.</p>
<ul>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">gdpo</span></code> mode does not support <code class="docutils literal notranslate"><span class="pre">kl_in_reward=True</span></code>. If both are set, <code class="docutils literal notranslate"><span class="pre">kl_in_reward</span></code> will be automatically set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>GDPO is designed for multi-reward optimization: When using multiple reward functions, GDPO normalizes each reward function separately within groups (subtract mean, divide by std), then performs weighted aggregation using <code class="docutils literal notranslate"><span class="pre">reward_weights</span></code>, and finally applies batch-level normalization. This approach better preserves the relative differences between rewards and prevents different reward combinations from collapsing into identical advantage values.</p></li>
</ul>
</li>
<li><p>sync_ref_model: Whether to synchronize the reference model. Default is False.</p>
<ul>
<li><p>ref_model_mixup_alpha: The Parameter controls the mix between the current policy and the previous reference policy during updates. The reference policy is updated according to the equation: $Ï€_{ref} = Î± * Ï€_Î¸ + (1 - Î±) * Ï€_{ref_{prev}}$. Default is 0.6.</p></li>
<li><p>ref_model_sync_stepsï¼šThe parameter determines how frequently the current policy is synchronized with the reference policy. Default is 512.</p></li>
</ul>
</li>
<li><p>move_model_batches: When moving model parameters to fast inference frameworks such as vLLM/LMDeploy, determines how many batches to divide the layers into. The default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which means the entire model is not split. Otherwise, the model is split into <code class="docutils literal notranslate"><span class="pre">move_model_batches</span> <span class="pre">+</span> <span class="pre">1</span></code> (non-layer parameters) + <code class="docutils literal notranslate"><span class="pre">1</span></code> (multi-modal component parameters) batches.</p></li>
<li><p>multi_turn_scheduler: Multi-turn GRPO parameter; pass the corresponding plugin name, and make sure to implement it in plugin/multi_turn.py.</p></li>
<li><p>max_turns: Maximum number of rounds for multi-turn GRPO. The default is None, which means there is no limit.</p></li>
<li><p>top_entropy_quantile: Only tokens whose entropy ranks within the specified top quantile are included in the loss calculation. The default is 1.0, which means low-entropy tokens are not filtered. For details, refer to the <a class="reference internal" href="GRPO/AdvancedResearch/entropy_mask.html"><span class="doc">documentation</span></a>.</p></li>
<li><p>log_entropy: Logs the entropy values during training. The default is False. For more information, refer to the <a class="reference external" href="./GRPO/GetStarted/GRPO.md#logged-metrics">documentation</a>.</p></li>
<li><p>rollout_importance_sampling_mode: Training-inference mismatch correction mode. Options are <code class="docutils literal notranslate"><span class="pre">token_truncate</span></code>, <code class="docutils literal notranslate"><span class="pre">token_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">sequence_truncate</span></code>, <code class="docutils literal notranslate"><span class="pre">sequence_mask</span></code>. Default is None (disabled). For details, refer to the <a class="reference internal" href="GRPO/AdvancedResearch/training_inference_mismatch.html"><span class="doc">documentation</span></a>.</p></li>
<li><p>rollout_importance_sampling_threshold: Threshold for importance sampling weights, used for truncating or masking extreme weights. Default is 2.0.</p></li>
<li><p>log_rollout_offpolicy_metrics: Whether to log training-inference mismatch diagnostic metrics (KL, PPL, Ï‡Â², etc.) when <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code> is not set. When <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code> is set, metrics are always logged. Default is False.</p></li>
<li><p>off_policy_sequence_mask_delta: Off-Policy Sequence Masking threshold from <a class="reference external" href="https://arxiv.org/abs/2512.02556">DeepSeek-V3.2 paper</a>. When set, computes <code class="docutils literal notranslate"><span class="pre">mean(old_policy_logps</span> <span class="pre">-</span> <span class="pre">policy_logps)</span></code> for each sequence. If this value exceeds the threshold AND the sequence has negative advantage, the sequence is masked out from loss computation. Default is None (disabled). For details, refer to the <a class="reference external" href="./GRPO/AdvancedResearch/training_inference_mismatch.md#off-policy-sequence-masking">documentation</a>.</p></li>
</ul>
<section id="reward-function-parameters">
<h5>Reward function parameters<a class="headerlink" href="#reward-function-parameters" title="Link to this heading">ïƒ</a></h5>
<p>Refer to the <a class="reference internal" href="GRPO/DeveloperGuide/reward_function.html"><span class="doc">documentation</span></a> for built-in reward functions.</p>
<p>cosine reward function arguments</p>
<ul class="simple">
<li><p>cosine_min_len_value_wrong (default: -0.5): Reward value corresponding to the minimum length when the answer is incorrect.</p></li>
<li><p>cosine_max_len_value_wrong (default: 0.0): Reward value corresponding to the maximum length when the answer is incorrect.</p></li>
<li><p>cosine_min_len_value_correct (default: 1.0): Reward value corresponding to the minimum length when the answer is correct.</p></li>
<li><p>cosine_max_len_value_correct (default: 0.5): Reward value corresponding to the maximum length when the answer is correct.</p></li>
<li><p>cosine_max_len (default value equal to the model's maximum generation capacity): Maximum length limit for generated text. Default value equal to max_completion_length</p></li>
</ul>
<p>repetition penalty function arguments</p>
<ul class="simple">
<li><p>repetition_n_grams (default: 3): Size of the n-gram used to detect repetition.</p></li>
<li><p>repetition_max_penalty (default: -1.0): Maximum penalty value, which controls the intensity of the penalty.</p></li>
</ul>
<p>Soft overlong reward parameters:</p>
<ul class="simple">
<li><p>soft_max_length: L_max in the paper, the maximum generation length of the model, default is equal to max_completion_length.</p></li>
<li><p>soft_cache_length: L_cache in the paper, controls the length penalty interval, which is defined as [soft_max_length - soft_cache_length, soft_max_length].</p></li>
</ul>
</section>
</section>
</section>
<section id="inference-arguments">
<h3>Inference Arguments<a class="headerlink" href="#inference-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Inference arguments include the <a class="reference external" href="#base-arguments">base arguments</a>, <a class="reference external" href="#merge-arguments">merge arguments</a>, <a class="reference external" href="#vllm-arguments">vLLM arguments</a>, <a class="reference external" href="#LMDeploy-arguments">LMDeploy arguments</a>, and also contain the following:</p>
<ul class="simple">
<li><p>ğŸ”¥infer_backend: Inference acceleration backend, supporting four inference engines: 'transformers', 'vllm', 'sglang', and 'lmdeploy'. The default is 'transformers'.</p>
<ul>
<li><p>Note: All four engines use SWIFT's template, controlled by <code class="docutils literal notranslate"><span class="pre">--template_backend</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥max_batch_size: Effective when infer_backend is set to 'transformers'; used for batch inference, with a default value of 1. If set to -1, there is no restriction.</p></li>
<li><p>ğŸ”¥result_path: Path to store inference results (jsonl). Defaults to None. When performing inference/evaluation on datasets, results are saved by default in the checkpoint directory (containing args.json file) or the './result' directory. The final storage path will be printed in the command line (interactive inference or deployment does not save results by default).</p>
<ul>
<li><p>Note: If the <code class="docutils literal notranslate"><span class="pre">result_path</span></code> file already exists, results will be appended to it.</p></li>
</ul>
</li>
<li><p>write_batch_size: The batch size for writing results to result_path. Defaults to 1000. If set to -1, there is no restriction.</p></li>
<li><p>metric: Evaluate the results of the inference, currently supporting 'acc' and 'rouge'. The default is None, meaning no evaluation is performed.</p></li>
<li><p>val_dataset_sample: Number of samples from the inference dataset, default is None.</p></li>
<li><p>reranker_use_activation: Whether to apply sigmoid activation after the score during reranker inference. Default is True.</p></li>
</ul>
</section>
<section id="deployment-arguments">
<h3>Deployment Arguments<a class="headerlink" href="#deployment-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Deployment Arguments inherit from the <a class="reference external" href="#inference-arguments">inference arguments</a>.</p>
<ul class="simple">
<li><p>host: Service host, default is '0.0.0.0'.</p></li>
<li><p>port: Port number, default is 8000.</p></li>
<li><p>api_key: The API key required for access; the default is None.</p></li>
<li><p>owned_by: Default is <code class="docutils literal notranslate"><span class="pre">swift</span></code>.</p></li>
<li><p>ğŸ”¥served_model_name: Model name for serving, defaults to the model's suffix.</p></li>
<li><p>verbose: Print detailed logs, with a default value of True.</p>
<ul>
<li><p>Note: In <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">app</span></code> or <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">eval</span></code>, the default is False.</p></li>
</ul>
</li>
<li><p>log_interval: Interval for printing tokens/s statistics, default is 20 seconds. If set to -1, it will not be printed.</p></li>
<li><p>max_logprobs: Maximum number of logprobs returned to the client, with a default value of 20.</p></li>
</ul>
</section>
<section id="rollout-arguments">
<h3>Rollout Arguments<a class="headerlink" href="#rollout-arguments" title="Link to this heading">ïƒ</a></h3>
<p>The rollout parameters inherit from the <a class="reference external" href="#deployment-arguments">deployment parameters</a>.</p>
<ul class="simple">
<li><p>multi_turn_scheduler: The scheduler for multi-turn GRPO training. Pass the corresponding plugin name, and ensure the implementation is added in <code class="docutils literal notranslate"><span class="pre">plugin/multi_turn.py</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. See <a class="reference internal" href="GRPO/DeveloperGuide/multi_turn.html"><span class="doc">documentation</span></a> for details.</p></li>
<li><p>max_turns: Maximum number of turns in multi-turn GRPO training. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning no limit.</p></li>
<li><p>vllm_enable_lora: Enable the vLLM engine to load LoRA adapters; defaults to False. Used to accelerate weight synchronization during LoRA training. See the <a class="reference external" href="./GRPO/GetStarted/GRPO.md#weight-sync-acceleration">documentation</a> for details.</p></li>
<li><p>vllm_max_lora_rank: LoRA parameter for the vLLM engine. Must be greater than or equal to the training lora_rank; it is recommended to set them equal. Defaults to 16.</p></li>
</ul>
</section>
<section id="web-ui-arguments">
<h3>Web-UI Arguments<a class="headerlink" href="#web-ui-arguments" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>server_name: Host for the web UI, default is '0.0.0.0'.</p></li>
<li><p>server_port: Port for the web UI, default is 7860.</p></li>
<li><p>share: Default is False.</p></li>
<li><p>lang: Language for the web UI, options are 'zh', 'en'. Default is 'zh'.</p></li>
</ul>
</section>
<section id="app-arguments">
<h3>App Arguments<a class="headerlink" href="#app-arguments" title="Link to this heading">ïƒ</a></h3>
<p>App parameters inherit from <a class="reference external" href="#deployment-arguments">deployment arguments</a> and <a class="reference external" href="#web-ui-arguments">Web-UI Arguments</a>.</p>
<ul class="simple">
<li><p>base_url: The base URL for model deployment, for example, <code class="docutils literal notranslate"><span class="pre">http://localhost:8000/v1</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which means using local deployment.</p></li>
<li><p>studio_title: Title of the studio. Default is None, set to the model name.</p></li>
<li><p>is_multimodal: Whether to launch the multimodal version of the app. Defaults to None, automatically determined based on the model; if it cannot be determined, set to False.</p></li>
<li><p>lang: Overrides the Web-UI Arguments, default is 'en'.</p></li>
</ul>
</section>
<section id="evaluation-arguments">
<h3>Evaluation Arguments<a class="headerlink" href="#evaluation-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Evaluation Arguments inherit from the <a class="reference external" href="#deployment-arguments">deployment arguments</a>.</p>
<ul class="simple">
<li><p>ğŸ”¥eval_backend: Evaluation backend, defaults to 'Native'. It can also be specified as 'OpenCompass' or 'VLMEvalKit'.</p></li>
<li><p>ğŸ”¥eval_dataset: Evaluation dataset, please refer to the <a class="reference internal" href="Evaluation.html"><span class="doc">evaluation documentation</span></a>.</p></li>
<li><p>eval_limit: Number of samples per evaluation set, defaults to None.</p></li>
<li><p>eval_output_dir: Directory to store evaluation results, defaults to 'eval_output'.</p></li>
<li><p>temperature: Override generation parameters, defaults to 0.</p></li>
<li><p>eval_num_proc: Maximum client concurrency during evaluation, defaults to 16.</p></li>
<li><p>eval_url: Evaluation URL, e.g., <code class="docutils literal notranslate"><span class="pre">http://localhost:8000/v1</span></code>. Examples can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/eval/eval_url">here</a>. Defaults to None for local deployment evaluation.</p></li>
<li><p>eval_generation_config: Model inference configuration during evaluation, should be passed as a JSON string, e.g., <code class="docutils literal notranslate"><span class="pre">'{&quot;max_new_tokens&quot;:</span> <span class="pre">512}'</span></code>; defaults to None.</p></li>
<li><p>extra_eval_args: Additional evaluation parameters, should be passed as a JSON string, defaults to empty. Only effective for Native evaluation. For more parameter descriptions, please refer to <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/get_started/parameters.html">here</a>.</p></li>
<li><p>local_dataset: Some evaluation sets, such as <code class="docutils literal notranslate"><span class="pre">CMB</span></code>, require additional data packages to be downloaded for utilization. Setting this parameter to <code class="docutils literal notranslate"><span class="pre">true</span></code> will automatically download the full data package, create a <code class="docutils literal notranslate"><span class="pre">data</span></code> folder in the current directory, and start the evaluation. The data package will only be downloaded once, and future evaluations will use the cache. This parameter defaults to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p>
<ul>
<li><p>Note: By default, evaluation uses the dataset under <code class="docutils literal notranslate"><span class="pre">~/.cache/opencompass</span></code>. After specifying this parameter, it will directly use the data folder in the current directory.</p></li>
</ul>
</li>
</ul>
</section>
<section id="export-arguments">
<h3>Export Arguments<a class="headerlink" href="#export-arguments" title="Link to this heading">ïƒ</a></h3>
<p>Export Arguments include the <a class="reference external" href="#base-arguments">basic arguments</a> and <a class="reference external" href="#merge-arguments">merge arguments</a>, and also contain the following:</p>
<ul class="simple">
<li><p>ğŸ”¥output_dir: The path for storing exported results. The default value is None, and an appropriate suffix path will be automatically set.</p></li>
<li><p>exist_ok: If output_dir exists, do not raise an exception and overwrite the contents. The default value is False.</p></li>
<li><p>ğŸ”¥quant_method: Options are 'gptq', 'awq', 'bnb' or 'fp8', with the default being None. Examples can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/export/quantize">here</a>.</p></li>
<li><p>quant_n_samples: The number of samples for the validation set used by gptq/awq, with a default of 256.</p></li>
<li><p>quant_batch_size: Quantization batch size, default is 1.</p></li>
<li><p>group_size: Group size for quantization, default is 128.</p></li>
<li><p>to_cached_dataset: pre-tokenize the dataset and export it in advance, default is False. See the example <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/cached_dataset">here</a>. For more information, please refer to cached_dataset.</p>
<ul>
<li><p>Note: You can specify the validation set content through <code class="docutils literal notranslate"><span class="pre">--split_dataset_ratio</span></code> or <code class="docutils literal notranslate"><span class="pre">--val_dataset</span></code>.</p></li>
</ul>
</li>
<li><p>template_mode: Used to support the <code class="docutils literal notranslate"><span class="pre">cached_dataset</span></code> feature for <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">rlhf</span></code> training. This parameter only takes effect when <code class="docutils literal notranslate"><span class="pre">--to_cached_dataset</span> <span class="pre">true</span></code> is set. Available options include: 'train', 'rlhf', and 'kto'. Among them, <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">pt/sft</span></code> uses 'train', <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">rlhf</span> <span class="pre">--rlhf_type</span> <span class="pre">kto</span></code> uses 'kto', and other rlhf algorithms use 'rlhf'. Note: Currently, 'gkd', 'ppo', and 'grpo' algorithms do not support the <code class="docutils literal notranslate"><span class="pre">cached_dataset</span></code> feature. Default is 'train'.</p></li>
<li><p>to_ollama: Generate the Modelfile required by Ollama. Default is False.</p></li>
<li><p>ğŸ”¥to_mcore: Convert weights from HF format to Megatron format. Default is False.</p></li>
<li><p>to_hf: Convert weights from Megatron format to HF format. Default is False.</p></li>
<li><p>mcore_model: Path to the mcore format model. Default is None.</p></li>
<li><p>mcore_adapter: The adapter path for mcore format models, default is None.</p></li>
<li><p>thread_count: The number of model slices when <code class="docutils literal notranslate"><span class="pre">--to_mcore</span> <span class="pre">true</span></code> is set. Defaults to None, and is automatically configured based on the model size, ensuring that the largest slice is less than 10GB.</p></li>
<li><p>ğŸ”¥offload_bridge: Store Megatron exported HF format weights for vLLM updates in CPU main memory to reduce GPU memory usage. Default is False.</p></li>
<li><p>ğŸ”¥test_convert_precision: Test the precision error when converting weights between HF and Megatron formats. Default is False.</p></li>
<li><p>test_convert_dtype: The dtype used for conversion precision testing, defaults to 'float32'.</p></li>
<li><p>ğŸ”¥push_to_hub: Whether to push to the hub, with the default being False. Examples can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/export/push_to_hub.sh">here</a>.</p></li>
<li><p>hub_model_id: Model ID for pushing, default is None.</p></li>
<li><p>hub_private_repo: Whether it is a private repo, default is False.</p></li>
<li><p>commit_message: Commit message, default is 'update files'.</p></li>
</ul>
</section>
<section id="sampling-parameters">
<h3>Sampling Parameters<a class="headerlink" href="#sampling-parameters" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>prm_model: The type of process reward model. It can be a model ID (triggered using <code class="docutils literal notranslate"><span class="pre">transformers</span></code>) or a <code class="docutils literal notranslate"><span class="pre">prm</span></code> key defined in a plugin (for custom inference processes).</p></li>
<li><p>orm_model: The type of outcome reward model, typically a wildcard or test case, usually defined in a plugin.</p></li>
<li><p>sampler_type: The type of sampling. Currently supports <code class="docutils literal notranslate"><span class="pre">sample</span></code> and <code class="docutils literal notranslate"><span class="pre">distill</span></code>.</p></li>
<li><p>sampler_engine: Supports <code class="docutils literal notranslate"><span class="pre">transformers</span></code>, <code class="docutils literal notranslate"><span class="pre">lmdeploy</span></code>, <code class="docutils literal notranslate"><span class="pre">vllm</span></code>, <code class="docutils literal notranslate"><span class="pre">no</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">transformers</span></code>. Specifies the inference engine for the sampling model.</p></li>
<li><p>output_dir: The output directory. Defaults to <code class="docutils literal notranslate"><span class="pre">sample_output</span></code>.</p></li>
<li><p>output_file: The name of the output file. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, which uses a timestamp as the filename. When provided, only the filename should be passed without the directory, and only JSONL format is supported.</p></li>
<li><p>override_exist_file: Whether to overwrite if <code class="docutils literal notranslate"><span class="pre">output_file</span></code> already exists.</p></li>
<li><p>num_sampling_batch_size: The batch size for each sampling operation.</p></li>
<li><p>num_sampling_batches: The total number of batches to sample.</p></li>
<li><p>n_best_to_keep: The number of best sequences to return.</p></li>
<li><p>data_range: The partition of the dataset being processed for this sampling operation. The format should be <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">3</span></code>, meaning the dataset is divided into 3 parts, and this instance is processing the 3rd partition (this implies that typically three <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">sample</span></code> processes are running in parallel).</p></li>
<li><p>temperature: Defaults to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p>prm_threshold: The PRM threshold. Results below this value will be filtered out. The default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p>easy_query_threshold: For each query, if the ORM evaluation is correct for more than this proportion of all samples, the query will be discarded to prevent overly simple queries from appearing in the results. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning no filtering is applied.</p></li>
<li><p>engine_kwargs: Additional parameters for the <code class="docutils literal notranslate"><span class="pre">sampler_engine</span></code>, passed as a JSON string, for example, <code class="docutils literal notranslate"><span class="pre">{&quot;cache_max_entry_count&quot;:0.7}</span></code>.</p></li>
<li><p>num_return_sequences: The number of original sequences returned by sampling. Defaults to <code class="docutils literal notranslate"><span class="pre">64</span></code>. This parameter is effective for <code class="docutils literal notranslate"><span class="pre">sample</span></code> sampling.</p></li>
<li><p>cache_files: To avoid loading both <code class="docutils literal notranslate"><span class="pre">prm</span></code> and <code class="docutils literal notranslate"><span class="pre">generator</span></code> simultaneously and causing GPU memory OOM, sampling can be done in two steps. In the first step, set <code class="docutils literal notranslate"><span class="pre">prm</span></code> and <code class="docutils literal notranslate"><span class="pre">orm</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code>, and all results will be output to a file. In the second run, set <code class="docutils literal notranslate"><span class="pre">sampler_engine</span></code> to <code class="docutils literal notranslate"><span class="pre">no</span></code> and pass <code class="docutils literal notranslate"><span class="pre">--cache_files</span></code> with the output file from the first sampling. This will use the results from the first run for <code class="docutils literal notranslate"><span class="pre">prm</span></code> and <code class="docutils literal notranslate"><span class="pre">orm</span></code> evaluation and output the final results.</p>
<ul>
<li><p>Note: When using <code class="docutils literal notranslate"><span class="pre">cache_files</span></code>, the <code class="docutils literal notranslate"><span class="pre">--dataset</span></code> still needs to be provided because the ID for <code class="docutils literal notranslate"><span class="pre">cache_files</span></code> is calculated using the MD5 of the original data. Both pieces of information need to be used together.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="specific-model-arguments">
<h2>Specific Model Arguments<a class="headerlink" href="#specific-model-arguments" title="Link to this heading">ïƒ</a></h2>
<p>In addition to the parameters listed above, some models support additional model-specific arguments. The meanings of these parameters can usually be found in the corresponding model's official repository or its inference code. <strong>MS-Swift includes these parameters to ensure that the trained model aligns with the behavior of the official inference implementation</strong>.</p>
<ul class="simple">
<li><p>Model-specific parameters can be set via <code class="docutils literal notranslate"><span class="pre">--model_kwargs</span></code> or environment variables. For example: <code class="docutils literal notranslate"><span class="pre">--model_kwargs</span> <span class="pre">'{&quot;fps_max_frames&quot;:</span> <span class="pre">12}'</span></code> or <code class="docutils literal notranslate"><span class="pre">FPS_MAX_FRAMES=12</span></code>.</p></li>
<li><p>Note: If you specify model-specific parameters during training, please also set the corresponding parameters during inference to achieve optimal performance.</p></li>
</ul>
<section id="qwen2-vl-qvq-qwen2-5-vl-mimo-vl-keye-vl-keye-vl-1-5">
<h3>qwen2_vl, qvq, qwen2_5_vl, mimo_vl, keye_vl, keye_vl_1_5<a class="headerlink" href="#qwen2-vl-qvq-qwen2-5-vl-mimo-vl-keye-vl-keye-vl-1-5" title="Link to this heading">ïƒ</a></h3>
<p>These parameters have the same meaning as in <code class="docutils literal notranslate"><span class="pre">qwen_vl_utils&lt;0.0.12</span></code> or the <code class="docutils literal notranslate"><span class="pre">qwen_omni_utils</span></code> library. See <a class="reference external" href="https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L24">here</a> for details. MS-Swift adjusts these constant values to control image resolution and video frame rate, preventing out-of-memory (OOM) errors during training.</p>
<ul class="simple">
<li><p>IMAGE_FACTOR: Default is 28.</p></li>
<li><p>MIN_PIXELS: Default is <code class="docutils literal notranslate"><span class="pre">4</span> <span class="pre">*</span> <span class="pre">28</span> <span class="pre">*</span> <span class="pre">28</span></code>. Minimum image resolution. It is recommended to set this as a multiple of 28Ã—28.</p></li>
<li><p>ğŸ”¥MAX_PIXELS: Default is <code class="docutils literal notranslate"><span class="pre">16384</span> <span class="pre">*</span> <span class="pre">28</span> <span class="pre">*</span> <span class="pre">28</span></code>. Maximum image resolution. It is recommended to set this as a multiple of 28Ã—28.</p></li>
<li><p>MAX_RATIO: Default is 200.</p></li>
<li><p>VIDEO_MIN_PIXELS: Default is <code class="docutils literal notranslate"><span class="pre">128</span> <span class="pre">*</span> <span class="pre">28</span> <span class="pre">*</span> <span class="pre">28</span></code>. Minimum resolution per frame in a video. Recommended to be a multiple of 28Ã—28.</p></li>
<li><p>ğŸ”¥VIDEO_MAX_PIXELS: Default is <code class="docutils literal notranslate"><span class="pre">768</span> <span class="pre">*</span> <span class="pre">28</span> <span class="pre">*</span> <span class="pre">28</span></code>. Maximum resolution per frame in a video. Recommended to be a multiple of 28Ã—28.</p></li>
<li><p>VIDEO_TOTAL_PIXELS: Default is <code class="docutils literal notranslate"><span class="pre">24576</span> <span class="pre">*</span> <span class="pre">28</span> <span class="pre">*</span> <span class="pre">28</span></code>.</p></li>
<li><p>FRAME_FACTOR: Default is 2.</p></li>
<li><p>FPS: Default is 2.0.</p></li>
<li><p>FPS_MIN_FRAMES: Default is 4. Minimum number of frames extracted from a video clip.</p></li>
<li><p>ğŸ”¥FPS_MAX_FRAMES: Default is 768. Maximum number of frames extracted from a video clip.</p></li>
<li><p>ğŸ”¥QWENVL_BBOX_FORMAT: Specifies whether to use <code class="docutils literal notranslate"><span class="pre">'legacy'</span></code> or <code class="docutils literal notranslate"><span class="pre">'new'</span></code> format for grounding. The <code class="docutils literal notranslate"><span class="pre">'legacy'</span></code> format is: <code class="docutils literal notranslate"><span class="pre">&lt;|object_ref_start|&gt;a</span> <span class="pre">dog&lt;|object_ref_end|&gt;&lt;|box_start|&gt;(432,991),(1111,2077)&lt;|box_end|&gt;</span></code>. The <code class="docutils literal notranslate"><span class="pre">'new'</span></code> format refers to: <a class="reference external" href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb">Qwen3-VL Cookbook</a>. For dataset formatting, see the <a class="reference external" href="../Customization/Custom-dataset.md#grounding">Grounding Dataset Format Documentation</a>. Default: <code class="docutils literal notranslate"><span class="pre">'legacy'</span></code>.</p>
<ul>
<li><p>Note: This environment variable applies to Qwen2/2.5/3-VL and Qwen2.5/3-Omni series models.</p></li>
</ul>
</li>
</ul>
</section>
<section id="qwen2-audio">
<h3>qwen2_audio<a class="headerlink" href="#qwen2-audio" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>SAMPLING_RATE: Default is 16000</p></li>
</ul>
</section>
<section id="qwen2-5-omni-qwen3-omni">
<h3>qwen2_5_omni, qwen3_omni<a class="headerlink" href="#qwen2-5-omni-qwen3-omni" title="Link to this heading">ïƒ</a></h3>
<p>qwen2_5_omni not only includes the model-specific parameters of qwen2_5_vl and qwen2_audio, but also contains the following parameter:</p>
<ul class="simple">
<li><p>USE_AUDIO_IN_VIDEO: Whether to use audio information from video. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>ğŸ”¥ENABLE_AUDIO_OUTPUT: Defaults to None, which means the value from <code class="docutils literal notranslate"><span class="pre">config.json</span></code> will be used. If training with zero3, please set it to False.</p>
<ul>
<li><p>Tip: ms-swift only fine-tunes the &quot;thinker&quot; component; it is recommended to set this to <code class="docutils literal notranslate"><span class="pre">False</span></code> to reduce GPU memory usage (only the thinker part of the model structure will be created).</p></li>
</ul>
</li>
</ul>
</section>
<section id="qwen3-vl">
<h3>qwen3_vl<a class="headerlink" href="#qwen3-vl" title="Link to this heading">ïƒ</a></h3>
<p>The parameter meanings are the same as in the <code class="docutils literal notranslate"><span class="pre">qwen_vl_utils&gt;=0.0.14</span></code> library â€” see here: https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L24. By passing the following environment variables you can override the library's global default values: (It is also compatible with environment variables used by <code class="docutils literal notranslate"><span class="pre">qwen2_5_vl</span></code>, such as: <code class="docutils literal notranslate"><span class="pre">MAX_PIXELS</span></code>, <code class="docutils literal notranslate"><span class="pre">VIDEO_MAX_PIXELS</span></code>, and will perform automatic conversion.)</p>
<ul class="simple">
<li><p>SPATIAL_MERGE_SIZE: default 2.</p></li>
<li><p>IMAGE_MIN_TOKEN_NUM: default <code class="docutils literal notranslate"><span class="pre">4</span></code>, denotes the minimum number of image tokens per image.</p></li>
<li><p>ğŸ”¥IMAGE_MAX_TOKEN_NUM: default <code class="docutils literal notranslate"><span class="pre">16384</span></code>, denotes the maximum number of image tokens per image. (used to avoid OOM)</p>
<ul>
<li><p>Note: The equivalent maximum image pixel count is <code class="docutils literal notranslate"><span class="pre">IMAGE_MAX_TOKEN_NUM</span> <span class="pre">*</span> <span class="pre">32</span> <span class="pre">*</span> <span class="pre">32</span></code>.</p></li>
</ul>
</li>
<li><p>VIDEO_MIN_TOKEN_NUM: default <code class="docutils literal notranslate"><span class="pre">128</span></code>, denotes the minimum number of video tokens per frame.</p></li>
<li><p>ğŸ”¥VIDEO_MAX_TOKEN_NUM: default <code class="docutils literal notranslate"><span class="pre">768</span></code>, denotes the maximum number of video tokens per frame. (used to avoid OOM)</p></li>
<li><p>MAX_RATIO: default 200.</p></li>
<li><p>FRAME_FACTOR: default 2.</p></li>
<li><p>FPS: default 2.0.</p></li>
<li><p>FPS_MIN_FRAMES: default 4, denotes the minimum number of sampled frames for a video segment.</p></li>
<li><p>ğŸ”¥FPS_MAX_FRAMES: default 768, denotes the maximum number of sampled frames for a video segment. (used to avoid OOM)</p></li>
</ul>
</section>
<section id="qwen3-vl-emb-qwen3-vl-reranker">
<h3>qwen3_vl_emb, qwen3_vl_reranker<a class="headerlink" href="#qwen3-vl-emb-qwen3-vl-reranker" title="Link to this heading">ïƒ</a></h3>
<p>The parameter meanings are the same as <code class="docutils literal notranslate"><span class="pre">qwen3_vl</span></code>, see the description above. The following are overrides to the default values:</p>
<ul class="simple">
<li><p>IMAGE_MAX_TOKEN_NUM: Default is 1800 for qwen3_vl_emb, and 1280 for qwen3_vl_reranker. For details, please refer to: <a class="reference external" href="https://modelscope.cn/models/Qwen/Qwen3-VL-Embedding-2B/file/view/master/scripts%2Fqwen3_vl_embedding.py?status=1#L26">qwen3_vl_embedding</a>, <a class="reference external" href="https://modelscope.cn/models/Qwen/Qwen3-VL-Reranker-2B/file/view/master/scripts%2Fqwen3_vl_reranker.py?status=1#L16">qwen3_vl_reranker</a>.</p></li>
<li><p>FPS: Default is 1.</p></li>
<li><p>FPS_MAX_FRAMES: Default is 64.</p></li>
</ul>
</section>
<section id="internvl-internvl-phi3">
<h3>internvl, internvl_phi3<a class="headerlink" href="#internvl-internvl-phi3" title="Link to this heading">ïƒ</a></h3>
<p>For the meaning of the arguments, please refer to <a class="reference external" href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-2B-V1-5">here</a></p>
<ul class="simple">
<li><p>MAX_NUM: Default is 12</p></li>
<li><p>INPUT_SIZE: Default is 448</p></li>
</ul>
</section>
<section id="internvl2-internvl2-phi3-internvl2-5-internvl3-internvl3-5">
<h3>internvl2, internvl2_phi3, internvl2_5, internvl3, internvl3_5<a class="headerlink" href="#internvl2-internvl2-phi3-internvl2-5-internvl3-internvl3-5" title="Link to this heading">ïƒ</a></h3>
<p>For the meaning of the arguments, please refer to <a class="reference external" href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B">here</a></p>
<ul class="simple">
<li><p>MAX_NUM: Default is 12</p></li>
<li><p>INPUT_SIZE: Default is 448</p></li>
<li><p>VIDEO_MAX_NUM: Default is 1, which is the MAX_NUM for videos</p></li>
<li><p>VIDEO_SEGMENTS: Default is 8</p></li>
</ul>
</section>
<section id="minicpmv2-6-minicpmv4-minicpmo">
<h3>minicpmv2_6, minicpmv4, minicpmo<a class="headerlink" href="#minicpmv2-6-minicpmv4-minicpmo" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>MAX_SLICE_NUMS: Default is 9, refer to <a class="reference external" href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6/file/view/master?fileName=config.json&amp;status=1">here</a></p></li>
<li><p>VIDEO_MAX_SLICE_NUMS: Default is 1, which is the MAX_SLICE_NUMS for videos, refer to <a class="reference external" href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6">here</a></p></li>
<li><p>MAX_NUM_FRAMES: Default is 64, refer to <a class="reference external" href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6">here</a></p></li>
</ul>
</section>
<section id="minicpmo">
<h3>minicpmo<a class="headerlink" href="#minicpmo" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>INIT_TTS: Default is False</p></li>
<li><p>INIT_AUDIO: Default is False</p></li>
</ul>
</section>
<section id="ovis1-6-ovis2">
<h3>ovis1_6, ovis2<a class="headerlink" href="#ovis1-6-ovis2" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>MAX_PARTITION: Default is 9, refer to <a class="reference external" href="https://github.com/AIDC-AI/Ovis/blob/d248e34d755a95d24315c40e2489750a869c5dbc/ovis/model/modeling_ovis.py#L312">here</a></p></li>
</ul>
</section>
<section id="ovis2-5">
<h3>ovis2_5<a class="headerlink" href="#ovis2-5" title="Link to this heading">ïƒ</a></h3>
<p>The meanings of the following parameters can be found in the example code <a class="reference external" href="https://modelscope.cn/models/AIDC-AI/Ovis2.5-2B">here</a>.</p>
<ul class="simple">
<li><p>MIX_PIXELS: int type, default is <code class="docutils literal notranslate"><span class="pre">448</span> <span class="pre">*</span> <span class="pre">448</span></code>.</p></li>
<li><p>MAX_PIXELS: int type, default is <code class="docutils literal notranslate"><span class="pre">1344</span> <span class="pre">*</span> <span class="pre">1792</span></code>. If OOM (out of memory) occurs, you can reduce this value.</p></li>
<li><p>VIDEO_MAX_PIXELS: int type, default is <code class="docutils literal notranslate"><span class="pre">896</span> <span class="pre">*</span> <span class="pre">896</span></code>.</p></li>
<li><p>NUM_FRAMES: default is 8. Used for video frame sampling.</p></li>
</ul>
</section>
<section id="mplug-owl3-mplug-owl3-241101">
<h3>mplug_owl3, mplug_owl3_241101<a class="headerlink" href="#mplug-owl3-mplug-owl3-241101" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>MAX_NUM_FRAMES: Default is 16, refer to <a class="reference external" href="https://modelscope.cn/models/iic/mPLUG-Owl3-7B-240728">here</a></p></li>
</ul>
</section>
<section id="xcomposer2-4khd">
<h3>xcomposer2_4khd<a class="headerlink" href="#xcomposer2-4khd" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>HD_NUM: Default is 55, refer to <a class="reference external" href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-4khd-7b">here</a></p></li>
</ul>
</section>
<section id="xcomposer2-5">
<h3>xcomposer2_5<a class="headerlink" href="#xcomposer2-5" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>HD_NUM: Default is 24 when the number of images is 1. Greater than 1, the default is 6. Refer to <a class="reference external" href="https://modelscope.cn/models/AI-ModelScope/internlm-xcomposer2d5-7b/file/view/master?fileName=modeling_internlm_xcomposer2.py&amp;status=1#L254">here</a></p></li>
</ul>
</section>
<section id="video-cogvlm2">
<h3>video_cogvlm2<a class="headerlink" href="#video-cogvlm2" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>NUM_FRAMES: Default is 24, refer to <a class="reference external" href="https://github.com/zai-org/CogVLM2/blob/main/video_demo/inference.py#L22">here</a></p></li>
</ul>
</section>
<section id="phi3-vision">
<h3>phi3_vision<a class="headerlink" href="#phi3-vision" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>NUM_CROPS: Default is 4, refer to <a class="reference external" href="https://modelscope.cn/models/LLM-Research/Phi-3.5-vision-instruct">here</a></p></li>
</ul>
</section>
<section id="llama3-1-omni">
<h3>llama3_1_omni<a class="headerlink" href="#llama3-1-omni" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>N_MELS: Default is 128, refer to <a class="reference external" href="https://github.com/ictnlp/LLaMA-Omni/blob/544d0ff3de8817fdcbc5192941a11cf4a72cbf2b/omni_speech/infer/infer.py#L57">here</a></p></li>
</ul>
</section>
<section id="video-llava">
<h3>video_llava<a class="headerlink" href="#video-llava" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>NUM_FRAMES: Default is 16</p></li>
</ul>
</section>
</section>
<section id="other-environment-variables">
<h2>Other Environment Variables<a class="headerlink" href="#other-environment-variables" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>CUDA_VISIBLE_DEVICES: Controls which GPU to use. By default, all GPUs are used.</p></li>
<li><p>ASCEND_RT_VISIBLE_DEVICES: Controls which NPU (effective for ASCEND cards) are used. By default, all NPUs are used.</p></li>
<li><p>MODELSCOPE_CACHE: Controls the cache path. (Recommended to set this value during multi-node training to ensure all nodes use the same dataset cache.)</p></li>
<li><p>NPROC_PER_NODE: Pass-through for the <code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code> parameter in torchrun. The default is 1. If the <code class="docutils literal notranslate"><span class="pre">NPROC_PER_NODE</span></code> or <code class="docutils literal notranslate"><span class="pre">NNODES</span></code> environment variables are set, torchrun is used to start training or inference.</p></li>
<li><p>PYTORCH_CUDA_ALLOC_CONF: It is recommended to set it to <code class="docutils literal notranslate"><span class="pre">'expandable_segments:True'</span></code>, which reduces GPU memory fragmentation. For more details, please refer to the <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management">PyTorch documentation</a>.</p></li>
<li><p>MASTER_PORT: Pass-through for the <code class="docutils literal notranslate"><span class="pre">--master_port</span></code> parameter in torchrun. The default is 29500.</p></li>
<li><p>MASTER_ADDR: Pass-through for the <code class="docutils literal notranslate"><span class="pre">--master_addr</span></code> parameter in torchrun.</p></li>
<li><p>NNODES: Pass-through for the <code class="docutils literal notranslate"><span class="pre">--nnodes</span></code> parameter in torchrun.</p></li>
<li><p>NODE_RANK: Pass-through for the <code class="docutils literal notranslate"><span class="pre">--node_rank</span></code> parameter in torchrun.</p></li>
<li><p>LOG_LEVEL: The log level, default is 'INFO'. You can set it to 'WARNING', 'ERROR', etc.</p></li>
<li><p>SWIFT_DEBUG: When set to <code class="docutils literal notranslate"><span class="pre">'1'</span></code> during <code class="docutils literal notranslate"><span class="pre">engine.infer(...)</span></code>, TransformersEngine will print the contents of <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> and <code class="docutils literal notranslate"><span class="pre">generate_ids</span></code> to facilitate debugging and alignment.</p></li>
<li><p>VLLM_USE_V1: Used to switch between V0 and V1 versions of vLLM.</p></li>
<li><p>SWIFT_TIMEOUT: If the multimodal dataset contains image URLs, this parameter controls the timeout for fetching images, defaulting to 20 seconds.</p></li>
<li><p>ROOT_IMAGE_DIR: The root directory for image (multimodal) resources. By setting this parameter, relative paths in the dataset can be interpreted relative to <code class="docutils literal notranslate"><span class="pre">ROOT_IMAGE_DIR</span></code>. By default, paths are relative to the current working directory.</p></li>
<li><p>SWIFT_SINGLE_DEVICE_MODE: Single device mode, valid values are &quot;0&quot;(default)/&quot;1&quot;. In this mode, each process can only see one device.</p></li>
<li><p>SWIFT_PATCH_CONV3D: If using torch==2.9, you may encounter slow Conv3d performance issues. You can work around this problem by setting <code class="docutils literal notranslate"><span class="pre">SWIFT_PATCH_CONV3D=1</span></code>. For more details, see <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/7108">this issue</a>.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>