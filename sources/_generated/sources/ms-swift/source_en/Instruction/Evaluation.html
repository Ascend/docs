

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Evaluation &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Evaluation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/Evaluation.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">ÔÉÅ</a></h1>
<p>SWIFT supports eval (evaluation) capabilities to provide standardized evaluation metrics for both raw models and trained models.</p>
<section id="capability-introduction">
<h2>Capability Introduction<a class="headerlink" href="#capability-introduction" title="Link to this heading">ÔÉÅ</a></h2>
<p>SWIFT's eval capability utilizes the EvalScope evaluation framework from the Magic Tower community, which has been advanced in its encapsulation to support the evaluation needs of various models.</p>
<blockquote>
<div><p>Note: EvalScope supports many other complex capabilities, such as <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/quick_start.html">model performance evaluation</a>, so please use the EvalScope framework directly.</p>
</div></blockquote>
<p>Currently, we support the evaluation process of <strong>standard evaluation datasets</strong> as well as the evaluation process of <strong>user-defined</strong> evaluation datasets. The <strong>standard evaluation datasets</strong> are supported by three evaluation backends:</p>
<p>Below are the names of the supported datasets. For detailed information on the datasets, please refer to <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html">all supported datasets</a>.</p>
<ol>
<li><p>Native (default):</p>
<p>Primarily supports pure text evaluation, while <strong>supporting</strong> visualization of evaluation results.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;arc&#39;, &#39;bbh&#39;, &#39;ceval&#39;, &#39;cmmlu&#39;, &#39;competition_math&#39;,
&#39;general_qa&#39;, &#39;gpqa&#39;, &#39;gsm8k&#39;, &#39;hellaswag&#39;, &#39;humaneval&#39;,
&#39;ifeval&#39;, &#39;iquiz&#39;, &#39;mmlu&#39;, &#39;mmlu_pro&#39;,
&#39;race&#39;, &#39;trivia_qa&#39;, &#39;truthful_qa&#39;
</pre></div>
</div>
</li>
<li><p>OpenCompass:</p>
<p>Primarily supports pure text evaluation, currently <strong>does not support</strong> visualization of evaluation results.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;obqa&#39;, &#39;cmb&#39;, &#39;AX_b&#39;, &#39;siqa&#39;, &#39;nq&#39;, &#39;mbpp&#39;, &#39;winogrande&#39;, &#39;mmlu&#39;, &#39;BoolQ&#39;, &#39;cluewsc&#39;, &#39;ocnli&#39;, &#39;lambada&#39;,
&#39;CMRC&#39;, &#39;ceval&#39;, &#39;csl&#39;, &#39;cmnli&#39;, &#39;bbh&#39;, &#39;ReCoRD&#39;, &#39;math&#39;, &#39;humaneval&#39;, &#39;eprstmt&#39;, &#39;WSC&#39;, &#39;storycloze&#39;,
&#39;MultiRC&#39;, &#39;RTE&#39;, &#39;chid&#39;, &#39;gsm8k&#39;, &#39;AX_g&#39;, &#39;bustm&#39;, &#39;afqmc&#39;, &#39;piqa&#39;, &#39;lcsts&#39;, &#39;strategyqa&#39;, &#39;Xsum&#39;, &#39;agieval&#39;,
&#39;ocnli_fc&#39;, &#39;C3&#39;, &#39;tnews&#39;, &#39;race&#39;, &#39;triviaqa&#39;, &#39;CB&#39;, &#39;WiC&#39;, &#39;hellaswag&#39;, &#39;summedits&#39;, &#39;GaokaoBench&#39;,
&#39;ARC_e&#39;, &#39;COPA&#39;, &#39;ARC_c&#39;, &#39;DRCD&#39;
</pre></div>
</div>
</li>
<li><p>VLMEvalKit:</p>
<p>Primarily supports multimodal evaluation and currently <strong>does not support</strong> visualization of evaluation results.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;COCO_VAL&#39;, &#39;MME&#39;, &#39;HallusionBench&#39;, &#39;POPE&#39;, &#39;MMBench_DEV_EN&#39;, &#39;MMBench_TEST_EN&#39;, &#39;MMBench_DEV_CN&#39;, &#39;MMBench_TEST_CN&#39;,
&#39;MMBench&#39;, &#39;MMBench_CN&#39;, &#39;MMBench_DEV_EN_V11&#39;, &#39;MMBench_TEST_EN_V11&#39;, &#39;MMBench_DEV_CN_V11&#39;,
&#39;MMBench_TEST_CN_V11&#39;, &#39;MMBench_V11&#39;, &#39;MMBench_CN_V11&#39;, &#39;SEEDBench_IMG&#39;, &#39;SEEDBench2&#39;,
&#39;SEEDBench2_Plus&#39;, &#39;ScienceQA_VAL&#39;, &#39;ScienceQA_TEST&#39;, &#39;MMT-Bench_ALL_MI&#39;, &#39;MMT-Bench_ALL&#39;,
&#39;MMT-Bench_VAL_MI&#39;, &#39;MMT-Bench_VAL&#39;, &#39;AesBench_VAL&#39;, &#39;AesBench_TEST&#39;, &#39;CCBench&#39;, &#39;AI2D_TEST&#39;, &#39;MMStar&#39;,
&#39;RealWorldQA&#39;, &#39;MLLMGuard_DS&#39;, &#39;BLINK&#39;, &#39;OCRVQA_TEST&#39;, &#39;OCRVQA_TESTCORE&#39;, &#39;TextVQA_VAL&#39;, &#39;DocVQA_VAL&#39;,
&#39;DocVQA_TEST&#39;, &#39;InfoVQA_VAL&#39;, &#39;InfoVQA_TEST&#39;, &#39;ChartQA_TEST&#39;, &#39;MathVision&#39;, &#39;MathVision_MINI&#39;,
&#39;MMMU_DEV_VAL&#39;, &#39;MMMU_TEST&#39;, &#39;OCRBench&#39;, &#39;MathVista_MINI&#39;, &#39;LLaVABench&#39;, &#39;MMVet&#39;, &#39;MTVQA_TEST&#39;,
&#39;MMLongBench_DOC&#39;, &#39;VCR_EN_EASY_500&#39;, &#39;VCR_EN_EASY_100&#39;, &#39;VCR_EN_EASY_ALL&#39;, &#39;VCR_EN_HARD_500&#39;,
&#39;VCR_EN_HARD_100&#39;, &#39;VCR_EN_HARD_ALL&#39;, &#39;VCR_ZH_EASY_500&#39;, &#39;VCR_ZH_EASY_100&#39;, &#39;VCR_ZH_EASY_ALL&#39;,
&#39;VCR_ZH_HARD_500&#39;, &#39;VCR_ZH_HARD_100&#39;, &#39;VCR_ZH_HARD_ALL&#39;, &#39;MMDU&#39;, &#39;MMBench-Video&#39;, &#39;Video-MME&#39;
</pre></div>
</div>
</li>
</ol>
</section>
<section id="environment-preparation">
<h2>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ms-swift<span class="o">[</span>eval<span class="o">]</span><span class="w"> </span>-U
</pre></div>
</div>
<p>Or install from source:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/modelscope/ms-swift.git
<span class="nb">cd</span><span class="w"> </span>ms-swift
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s1">&#39;.[eval]&#39;</span>
</pre></div>
</div>
</section>
<section id="id1">
<h2>Evaluation<a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h2>
<p>Supports four methods of evaluation: pure text evaluation, multimodal evaluation, URL evaluation, and custom dataset evaluation.</p>
<p><strong>Basic Example</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-0.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_backend<span class="w"> </span>Native<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_limit<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset<span class="w"> </span>gsm8k
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>model: Can specify a local model path or a model ID on modelscope</p></li>
<li><p>eval_backend: Options are Native, OpenCompass, VLMEvalKit; default is Native</p></li>
<li><p>infer_backend: Options are transformers, vllm, sglang, lmdeploy; default is transformers</p></li>
<li><p>eval_limit: Sample size for each evaluation set; default is None, which means using all data; can be used for quick validation</p></li>
<li><p>eval_dataset: Evaluation dataset(s); multiple datasets can be set, separated by spaces</p></li>
</ul>
<p><strong>Complex Evaluation Example</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-0.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_backend<span class="w"> </span>Native<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_limit<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset<span class="w"> </span>gsm8k<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset_args<span class="w"> </span><span class="s1">&#39;{&quot;gsm8k&quot;: {&quot;few_shot_num&quot;: 0, &quot;filters&quot;: {&quot;remove_until&quot;: &quot;&lt;/think&gt;&quot;}}}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_generation_config<span class="w"> </span><span class="s1">&#39;{&quot;max_tokens&quot;: 512, &quot;temperature&quot;: 0}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--extra_eval_args<span class="w"> </span><span class="s1">&#39;{&quot;ignore_errors&quot;: true, &quot;debug&quot;: true}&#39;</span>
</pre></div>
</div>
<p>For a specific list of evaluation parameters, please refer to <a class="reference external" href="./Command-line-parameters.md#evaluation-arguments">here</a>.</p>
</section>
<section id="evaluation-during-training">
<h2>Evaluation During Training<a class="headerlink" href="#evaluation-during-training" title="Link to this heading">ÔÉÅ</a></h2>
<p>SWIFT supports using EvalScope to evaluate the current model during the training process, allowing for timely understanding of the model's training effectiveness.</p>
<p><strong>Basic Example</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w"> </span><span class="s2">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tuner_type<span class="w"> </span><span class="s2">&quot;lora&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dataset<span class="w"> </span><span class="s2">&quot;AI-ModelScope/alpaca-gpt4-data-zh#100&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--torch_dtype<span class="w"> </span><span class="s2">&quot;bfloat16&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--num_train_epochs<span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--per_device_train_batch_size<span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--learning_rate<span class="w"> </span><span class="s2">&quot;1e-4&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora_rank<span class="w"> </span><span class="s2">&quot;8&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora_alpha<span class="w"> </span><span class="s2">&quot;32&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--target_modules<span class="w"> </span><span class="s2">&quot;all-linear&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gradient_accumulation_steps<span class="w"> </span><span class="s2">&quot;16&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--save_steps<span class="w"> </span><span class="s2">&quot;50&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--save_total_limit<span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--logging_steps<span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max_length<span class="w"> </span><span class="s2">&quot;2048&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--eval_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--eval_steps<span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--per_device_eval_batch_size<span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--eval_use_evalscope<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--eval_dataset<span class="w"> </span><span class="s2">&quot;gsm8k&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--eval_dataset_args<span class="w"> </span><span class="s1">&#39;{&quot;gsm8k&quot;: {&quot;few_shot_num&quot;: 0}}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--eval_limit<span class="w"> </span><span class="s2">&quot;10&quot;</span>
</pre></div>
</div>
<p>Note that the launch command is <code class="docutils literal notranslate"><span class="pre">sft</span></code>, and the evaluation-related parameters include:</p>
<ul class="simple">
<li><p>eval_strategy: Evaluation strategy. Defaults to None, following the <code class="docutils literal notranslate"><span class="pre">save_strategy</span></code> policy</p></li>
<li><p>eval_steps: Defaults to None. If an evaluation dataset exists, it follows the <code class="docutils literal notranslate"><span class="pre">save_steps</span></code> policy</p></li>
<li><p>eval_use_evalscope: Whether to use evalscope for evaluation, this parameter needs to be set to enable evaluation</p></li>
<li><p>eval_dataset: Evaluation datasets, multiple datasets can be set, separated by spaces</p></li>
<li><p>eval_dataset_args: Evaluation dataset parameters in JSON format, parameters for multiple datasets can be set</p></li>
<li><p>eval_limit: Number of samples from the evaluation dataset</p></li>
<li><p>eval_generation_config: Model inference configuration during evaluation, in JSON format, default is <code class="docutils literal notranslate"><span class="pre">{'max_tokens':</span> <span class="pre">512}</span></code></p></li>
</ul>
<p>More evaluation examples can be found in <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/eval">examples</a>.</p>
</section>
<section id="custom-evaluation-datasets">
<h2>Custom Evaluation Datasets<a class="headerlink" href="#custom-evaluation-datasets" title="Link to this heading">ÔÉÅ</a></h2>
<p>This framework supports two predefined dataset formats: multiple-choice questions (MCQ) and question-and-answer (QA). The usage process is as follows:</p>
<p><em>Note: When using a custom evaluation, the <code class="docutils literal notranslate"><span class="pre">eval_backend</span></code> parameter must be set to <code class="docutils literal notranslate"><span class="pre">Native</span></code>.</em></p>
<section id="multiple-choice-question-format-mcq">
<h3>Multiple-Choice Question Format (MCQ)<a class="headerlink" href="#multiple-choice-question-format-mcq" title="Link to this heading">ÔÉÅ</a></h3>
<p>This format is suitable for scenarios involving multiple-choice questions, and the evaluation metric is accuracy.</p>
<p><strong>Data Preparation</strong></p>
<p>Prepare a CSV file in the multiple-choice question format, structured as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mcq/
‚îú‚îÄ‚îÄ example_dev.csv  # (Optional) The filename should follow the format `{subset_name}_dev.csv` for few-shot evaluation
‚îî‚îÄ‚îÄ example_val.csv  # The filename should follow the format `{subset_name}_val.csv` for the actual evaluation data
</pre></div>
</div>
<p>The CSV file should follow this format:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>id,question,A,B,C,D,answer
1,Generally speaking, the amino acids that make up animal proteins are____,4 types,22 types,20 types,19 types,C
2,Among the substances present in the blood, which is not a metabolic end product?____,Urea,Uric acid,Pyruvate,Carbon dioxide,C
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">id</span></code> is an optional index</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">question</span></code> is the question</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span></code>, <code class="docutils literal notranslate"><span class="pre">B</span></code>, <code class="docutils literal notranslate"><span class="pre">C</span></code>, <code class="docutils literal notranslate"><span class="pre">D</span></code>, etc. are the options, with a maximum of 10 options</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">answer</span></code> is the correct option</p></li>
</ul>
<p><strong>Launching Evaluation</strong></p>
<p>Run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-0.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_backend<span class="w"> </span>Native<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset<span class="w"> </span>general_mcq<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset_args<span class="w"> </span><span class="s1">&#39;{&quot;general_mcq&quot;: {&quot;local_path&quot;: &quot;/path/to/mcq&quot;, &quot;subset_list&quot;: [&quot;example&quot;]}}&#39;</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">eval_dataset</span></code> should be set to <code class="docutils literal notranslate"><span class="pre">general_mcq</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eval_dataset_args</span></code> should be set with:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">local_path</span></code> as the path to the custom dataset folder</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">subset_list</span></code> as the name of the evaluation dataset, taken from the <code class="docutils literal notranslate"><span class="pre">*_dev.csv</span></code> mentioned above</p></li>
</ul>
</li>
</ul>
<p><strong>Running Results</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Model               | Dataset     | Metric          | Subset   |   Num |   Score | Cat.0   |
+=====================+=============+=================+==========+=======+=========+=========+
| Qwen2-0.5B-Instruct | general_mcq | AverageAccuracy | example  |    12 |  0.5833 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
</pre></div>
</div>
</section>
</section>
<section id="question-and-answer-format-qa">
<h2>Question-and-Answer Format (QA)<a class="headerlink" href="#question-and-answer-format-qa" title="Link to this heading">ÔÉÅ</a></h2>
<p>This format is suitable for scenarios involving question-and-answer, and the evaluation metrics are <code class="docutils literal notranslate"><span class="pre">ROUGE</span></code> and <code class="docutils literal notranslate"><span class="pre">BLEU</span></code>.</p>
<p><strong>Data Preparation</strong></p>
<p>Prepare a JSON Lines file in the question-and-answer format, containing one file in the following structure:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>qa/
‚îî‚îÄ‚îÄ example.jsonl
</pre></div>
</div>
<p>The JSON Lines file should follow this format:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;What is the capital of China?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;response&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;The capital of China is Beijing&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;What is the highest mountain in the world?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;response&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;It is Mount Everest&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Why can&#39;t penguins be seen in the Arctic?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;response&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Because most penguins live in Antarctica&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p><strong>Launching Evaluation</strong></p>
<p>Run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-0.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_backend<span class="w"> </span>Native<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset<span class="w"> </span>general_qa<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_dataset_args<span class="w"> </span><span class="s1">&#39;{&quot;general_qa&quot;: {&quot;local_path&quot;: &quot;/path/to/qa&quot;, &quot;subset_list&quot;: [&quot;example&quot;]}}&#39;</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">eval_dataset</span></code> should be set to <code class="docutils literal notranslate"><span class="pre">general_qa</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eval_dataset_args</span></code> is a JSON string that needs to be set with:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">local_path</span></code> as the path to the custom dataset folder</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">subset_list</span></code> as the name of the evaluation dataset, taken from the <code class="docutils literal notranslate"><span class="pre">*.jsonl</span></code> mentioned above</p></li>
</ul>
</li>
</ul>
<p><strong>Running Results</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Model               | Dataset     | Metric          | Subset   |   Num |   Score | Cat.0   |
+=====================+=============+=================+==========+=======+=========+=========+
| Qwen2-0.5B-Instruct | general_qa  | bleu-1          | default  |    12 |  0.2324 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | bleu-2          | default  |    12 |  0.1451 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | bleu-3          | default  |    12 |  0.0625 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | bleu-4          | default  |    12 |  0.0556 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-1-f       | default  |    12 |  0.3441 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-1-p       | default  |    12 |  0.2393 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-1-r       | default  |    12 |  0.8889 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-2-f       | default  |    12 |  0.2062 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-2-p       | default  |    12 |  0.1453 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-2-r       | default  |    12 |  0.6167 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-l-f       | default  |    12 |  0.333  | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-l-p       | default  |    12 |  0.2324 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
| Qwen2-0.5B-Instruct | general_qa  | rouge-l-r       | default  |    12 |  0.8889 | default |
+---------------------+-------------+-----------------+----------+-------+---------+---------+
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>