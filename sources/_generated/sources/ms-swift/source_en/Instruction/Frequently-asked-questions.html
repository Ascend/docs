

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frequently-asked-questions &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Frequently-asked-questions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/Frequently-asked-questions.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frequently-asked-questions">
<h1>Frequently-asked-questions<a class="headerlink" href="#frequently-asked-questions" title="Link to this heading">ïƒ</a></h1>
<blockquote>
<div><p>[!WARNING]
This document is pending update to ms-swift 4.0</p>
</div></blockquote>
<p>Here are some common questions encountered during the use of Swift.</p>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">ïƒ</a></h2>
<section id="q1-what-models-and-datasets-are-supported-for-fine-tuning-in-swift">
<h3>Q1: What models and datasets are supported for fine-tuning in Swift?<a class="headerlink" href="#q1-what-models-and-datasets-are-supported-for-fine-tuning-in-swift" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html">Supported Models and Datasets</a>.</p>
</section>
<section id="q2-what-data-formats-are-supported-when-training-with-custom-datasets">
<h3>Q2: What data formats are supported when training with custom datasets?<a class="headerlink" href="#q2-what-data-formats-are-supported-when-training-with-custom-datasets" title="Link to this heading">ïƒ</a></h3>
<p>For custom dataset formats, see the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html">Custom Dataset</a>.</p>
</section>
<section id="q3-what-is-the-format-for-dataset-info-json-for-custom-datasets-and-how-can-i-use-it">
<h3>Q3: What is the format for dataset_info.json for custom datasets, and how can I use it?<a class="headerlink" href="#q3-what-is-the-format-for-dataset-info-json-for-custom-datasets-and-how-can-i-use-it" title="Link to this heading">ïƒ</a></h3>
<p>The dataset_info.json format can be found in the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html">Custom Dataset</a>. Use the command line with <code class="docutils literal notranslate"><span class="pre">--custom_dataset_info</span> <span class="pre">xxx.json</span></code>, <code class="docutils literal notranslate"><span class="pre">--dataset</span> <span class="pre">&lt;dataset_id_or_path&gt;</span></code>.</p>
</section>
<section id="q4-how-can-i-train-with-a-custom-dataset-using-the-interface">
<h3>Q4: How can I train with a custom dataset using the interface?<a class="headerlink" href="#q4-how-can-i-train-with-a-custom-dataset-using-the-interface" title="Link to this heading">ïƒ</a></h3>
<p>Using a custom dataset through the interface is the same as using the command line. Refer to the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html">Custom Dataset</a>.</p>
</section>
<section id="q5-can-i-write-a-line-in-the-jsonl-file-like-this-index-00000-query-11111-response-22222-source-qqq">
<h3>Q5: Can I write a line in the jsonl file like this? {&quot;index&quot;: &quot;00000&quot;, &quot;query&quot;: &quot;11111&quot;, &quot;response&quot;: &quot;22222&quot;, 'source':'qqq'}<a class="headerlink" href="#q5-can-i-write-a-line-in-the-jsonl-file-like-this-index-00000-query-11111-response-22222-source-qqq" title="Link to this heading">ïƒ</a></h3>
<p>Yes, you can have extra fields. By default, these fields will not be used. For details, see the parameter <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#data-arguments">remove_unused_columns</a>.</p>
</section>
<section id="q6-where-can-i-find-the-command-line-parameters">
<h3>Q6: Where can I find the command line parameters?<a class="headerlink" href="#q6-where-can-i-find-the-command-line-parameters" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q7-what-parameters-need-to-be-configured-for-training-in-an-offline-environment">
<h3>Q7: What parameters need to be configured for training in an offline environment?<a class="headerlink" href="#q7-what-parameters-need-to-be-configured-for-training-in-an-offline-environment" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">--model</span> <span class="pre">local_path</span></code>, <code class="docutils literal notranslate"><span class="pre">--check_model</span> <span class="pre">false</span></code>. For more details, see the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q8-where-can-i-check-model-type">
<h3>Q8: Where can I check model_type?<a class="headerlink" href="#q8-where-can-i-check-model-type" title="Link to this heading">ïƒ</a></h3>
<p>Check the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html">Supported Models and Datasets</a>.</p>
</section>
<section id="q9-can-i-directly-convert-the-model-to-gguf-format-after-training">
<h3>Q9: Can I directly convert the model to gguf format after training?<a class="headerlink" href="#q9-can-i-directly-convert-the-model-to-gguf-format-after-training" title="Link to this heading">ïƒ</a></h3>
<p>Currently, only export to ModelFile is supported. See the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q10-does-swift-support-pre-training-i-only-see-sft">
<h3>Q10: Does Swift support pre-training? I only see SFT.<a class="headerlink" href="#q10-does-swift-support-pre-training-i-only-see-sft" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it supports it. Use the command line <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">pt</span></code>, <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/pretrain">pretrain example</a>. The dataset format is detailed in <a class="reference external" href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html">Custom Dataset</a>.</p>
</section>
<section id="q11-for-models-fine-tuned-with-lora-should-i-merge-them-into-one-model-for-resuming-training-or-can-i-specify-the-original-model-and-lora-block-by-path-directly">
<h3>Q11: For models fine-tuned with LoRA, should I merge them into one model for resuming training, or can I specify the original model and LoRA block by path directly?<a class="headerlink" href="#q11-for-models-fine-tuned-with-lora-should-i-merge-them-into-one-model-for-resuming-training-or-can-i-specify-the-original-model-and-lora-block-by-path-directly" title="Link to this heading">ïƒ</a></h3>
<p>You do not need to merge. Use <code class="docutils literal notranslate"><span class="pre">--resume_from_checkpoint</span> <span class="pre">output/xxx/vx-xxx/checkpoint-xxx</span></code>. See the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q12-i-would-like-to-control-the-location-where-the-original-model-weights-downloaded-from-the-internet-are-stored-how-can-i-place-the-original-model-in-a-specific-folder">
<h3>Q12: I would like to control the location where the original model weights downloaded from the internet are stored. How can I place the original model in a specific folder?<a class="headerlink" href="#q12-i-would-like-to-control-the-location-where-the-original-model-weights-downloaded-from-the-internet-are-stored-how-can-i-place-the-original-model-in-a-specific-folder" title="Link to this heading">ïƒ</a></h3>
<p>You can set the environment variable <code class="docutils literal notranslate"><span class="pre">MODELSCOPE_CACHE=your_path</span></code> to store the original model in the specified path. For SDK downloads, use <code class="docutils literal notranslate"><span class="pre">cache_dir=&quot;local_path&quot;</span></code>. You can also use the <code class="docutils literal notranslate"><span class="pre">modelscope</span> <span class="pre">download</span></code> command-line tool or <code class="docutils literal notranslate"><span class="pre">git</span></code> to download it. For details, refer to the <a class="reference external" href="https://modelscope.cn/docs/Models/Download-Model">Download Model</a>. During training, set <code class="docutils literal notranslate"><span class="pre">--model</span></code> to the local path. For offline training, configure <code class="docutils literal notranslate"><span class="pre">--check_model</span> <span class="pre">false</span></code>. See the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q14-is-there-a-complete-tutorial-and-command-line-for-fine-tuning-qwen-2-vl">
<h3>Q14: Is there a complete tutorial and command line for fine-tuning Qwen-2-VL?<a class="headerlink" href="#q14-is-there-a-complete-tutorial-and-command-line-for-fine-tuning-qwen-2-vl" title="Link to this heading">ïƒ</a></h3>
<p>Reference the <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal">example</a> for multimodal model training.</p>
</section>
<section id="q15-are-there-any-tricks-supported-for-fine-tuning-multi-modal-large-models-similar-to-the-llm-s-neftune">
<h3>Q15: Are there any tricks supported for fine-tuning multi-modal large models, similar to the LLM's neftune?<a class="headerlink" href="#q15-are-there-any-tricks-supported-for-fine-tuning-multi-modal-large-models-similar-to-the-llm-s-neftune" title="Link to this heading">ïƒ</a></h3>
<p>You can try variations of <code class="docutils literal notranslate"><span class="pre">lora</span></code> like <code class="docutils literal notranslate"><span class="pre">piassa/olora/dora</span></code>, or <code class="docutils literal notranslate"><span class="pre">fourierft</span></code>. Refer to the tricks in the <code class="docutils literal notranslate"><span class="pre">sft</span></code> parameters, as some may not apply to multi-modal.</p>
</section>
<section id="q16-the-accuracy-from-eval-during-training-and-the-accuracy-computed-from-re-inference-with-the-saved-checkpoint-are-not-consistent">
<h3>Q16: The accuracy from eval during training and the accuracy computed from re-inference with the saved checkpoint are not consistent.<a class="headerlink" href="#q16-the-accuracy-from-eval-during-training-and-the-accuracy-computed-from-re-inference-with-the-saved-checkpoint-are-not-consistent" title="Link to this heading">ïƒ</a></h3>
<p>The methods for calculating eval accuracy during training and inference are different. The default <code class="docutils literal notranslate"><span class="pre">acc_strategy</span></code> is <code class="docutils literal notranslate"><span class="pre">token</span></code>, and the selectable values are: <code class="docutils literal notranslate"><span class="pre">token</span></code>, <code class="docutils literal notranslate"><span class="pre">seq</span></code>.</p>
</section>
<section id="q17-official-magic-mirror-image-and-swift-environment">
<h3>Q17: Official Magic Mirror image and Swift environment.<a class="headerlink" href="#q17-official-magic-mirror-image-and-swift-environment" title="Link to this heading">ïƒ</a></h3>
<p>You can start the container using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code>, for example: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--gpus</span> <span class="pre">all</span> <span class="pre">-p</span> <span class="pre">8000:8000</span> <span class="pre">-it</span> <span class="pre">-d</span> <span class="pre">--name</span> <span class="pre">ms</span> <span class="pre">modelscope-registry.cn-beijing.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-1.26.0-LLM</span> <span class="pre">/bin/bash</span></code>. After starting the container, pull the latest code to install Swift. Additionally, for large model training scenarios, the <code class="docutils literal notranslate"><span class="pre">ms-swift</span></code> image is provided, which includes additional dependencies for <code class="docutils literal notranslate"><span class="pre">Megatron-SWIFT</span></code>, such as: <code class="docutils literal notranslate"><span class="pre">modelscope-registry.cn-beijing.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-vllm0.8.5.post1-modelscope1.26.0-swift3.4.1.post1</span></code>. For more details, refer to the <a class="reference external" href="https://swift.readthedocs.io/en/latest/GetStarted/SWIFT-installation.html">Swift installation documentation</a>.</p>
</section>
<section id="q18-command-line-for-multi-machine-multi-card-training">
<h3>Q18: Command line for multi-machine multi-card training.<a class="headerlink" href="#q18-command-line-for-multi-machine-multi-card-training" title="Link to this heading">ïƒ</a></h3>
<p>For details, see the <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node">Multi-node Example</a>.</p>
</section>
<section id="q19-how-to-choose-a-template">
<h3>Q19: How to choose a template?<a class="headerlink" href="#q19-how-to-choose-a-template" title="Link to this heading">ïƒ</a></h3>
<p>See <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/1813">issue</a>.</p>
</section>
<section id="q20-how-to-use-torchrun-and-swift-sft-for-multi-card-training">
<h3>Q20: How to use torchrun and swift sft for multi-card training?<a class="headerlink" href="#q20-how-to-use-torchrun-and-swift-sft-for-multi-card-training" title="Link to this heading">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">sft</span></code> uses <code class="docutils literal notranslate"><span class="pre">torchrun</span></code>.</p>
</section>
<section id="q21-i-have-a-question-about-my-sft-dataset-being-too-large-tokenizing-takes-a-long-time-is-there-a-solution">
<h3>Q21: I have a question about my SFT dataset being too large; tokenizing takes a long time. Is there a solution?<a class="headerlink" href="#q21-i-have-a-question-about-my-sft-dataset-being-too-large-tokenizing-takes-a-long-time-is-there-a-solution" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">lazy_tokenize</span></code>or stream reading (<code class="docutils literal notranslate"><span class="pre">streaming</span></code>). See <a class="reference external" href="https://swift.readthedocs.io/zh-cn/latest/Instruction/Command-line-parameters.html">Command Line Parameters documentation</a>.</p>
</section>
<section id="q22-when-two-datasets-are-simply-appended-together-in-the-training-set-does-the-model-shuffle-internally-during-training-or-does-it-take-data-in-order-to-train">
<h3>Q22: When two datasets are simply appended together in the training set, does the model shuffle internally during training, or does it take data in order to train?<a class="headerlink" href="#q22-when-two-datasets-are-simply-appended-together-in-the-training-set-does-the-model-shuffle-internally-during-training-or-does-it-take-data-in-order-to-train" title="Link to this heading">ïƒ</a></h3>
<p>Command-line parameter <code class="docutils literal notranslate"><span class="pre">dataset_shuffle</span></code>. For more details, see the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">command-line parameters documentation</a>.</p>
</section>
<section id="q23-if-the-model-is-on-two-cards-and-the-data-is-not-parallelized-deepspeed-will-throw-an-error-how-to-handle-this">
<h3>Q23: If the model is on two cards and the data is not parallelized, deepspeed will throw an error. How to handle this?<a class="headerlink" href="#q23-if-the-model-is-on-two-cards-and-the-data-is-not-parallelized-deepspeed-will-throw-an-error-how-to-handle-this" title="Link to this heading">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> and <code class="docutils literal notranslate"><span class="pre">device_map</span></code> are incompatible; you can only choose one.</p>
</section>
<section id="q24-how-to-reduce-gpu-memory-usage-when-training-vlm-models">
<h3>Q24: How to reduce GPU memory usage when training VLM models?<a class="headerlink" href="#q24-how-to-reduce-gpu-memory-usage-when-training-vlm-models" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--freeze_vit</span> <span class="pre">true</span></code> and the parameter <code class="docutils literal notranslate"><span class="pre">--max_pixels</span></code> to limit the maximum pixels.</p>
</section>
<section id="q25-for-a-model-that-doesn-t-have-a-matching-model-type-can-i-customize-the-special-tokens-and-chat-template-during-sft">
<h3>Q25: For a model that doesn't have a matching model_type, can I customize the special_tokens and chat_template during SFT?<a class="headerlink" href="#q25-for-a-model-that-doesn-t-have-a-matching-model-type-can-i-customize-the-special-tokens-and-chat-template-during-sft" title="Link to this heading">ïƒ</a></h3>
<p>Yes, you can. Please refer to <a class="reference external" href="https://swift.readthedocs.io/en/latest/BestPractices/MLLM-Registration.html">Best Practices for MLLM Registration</a>.</p>
</section>
<section id="q26-can-i-use-dpo-to-train-qwen2-vl-in-a-python-script">
<h3>Q26: Can I use DPO to train Qwen2-VL in a Python script?<a class="headerlink" href="#q26-can-i-use-dpo-to-train-qwen2-vl-in-a-python-script" title="Link to this heading">ïƒ</a></h3>
<p>Yes, import <code class="docutils literal notranslate"><span class="pre">rlhf_main</span></code> and <code class="docutils literal notranslate"><span class="pre">RLHFArguments</span></code> from <code class="docutils literal notranslate"><span class="pre">swift.pipelines</span></code>.</p>
</section>
<section id="q27-can-i-pre-train-with-pure-text-before-fine-tuning-on-a-vqa-dataset-for-mllm">
<h3>Q27: Can I pre-train with pure text before fine-tuning on a VQA dataset for MLLM?<a class="headerlink" href="#q27-can-i-pre-train-with-pure-text-before-fine-tuning-on-a-vqa-dataset-for-mllm" title="Link to this heading">ïƒ</a></h3>
<p>Yes, you can mix training as well.</p>
</section>
<section id="q28-when-conducting-dpo-training-based-on-the-qwen2-sft-model-on-a-v100-machine-the-training-shows-nan">
<h3>Q28: When conducting DPO training based on the qwen2 SFT model on a V100 machine, the training shows NaN?<a class="headerlink" href="#q28-when-conducting-dpo-training-based-on-the-qwen2-sft-model-on-a-v100-machine-the-training-shows-nan" title="Link to this heading">ïƒ</a></h3>
<p>Use fp32 for training with the V100 machine.</p>
</section>
<section id="q29-does-swift-support-distillation">
<h3>Q29: Does Swift support distillation?<a class="headerlink" href="#q29-does-swift-support-distillation" title="Link to this heading">ïƒ</a></h3>
<p>Refer to this <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/sampler/distill/distill.sh">example</a>.</p>
</section>
<section id="q30-how-many-checkpoints-are-saved-by-default-after-training">
<h3>Q30: How many checkpoints are saved by default after training?<a class="headerlink" href="#q30-how-many-checkpoints-are-saved-by-default-after-training" title="Link to this heading">ïƒ</a></h3>
<p>By default, all checkpoints are saved. For details, see the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">command-line parameter save_total_limit</a>.</p>
</section>
<section id="q31-in-grounding-tasks-does-the-universal-data-format-support-multiple-instances-for-one-category">
<h3>Q31: In grounding tasks, does the universal data format support multiple instances for one category?<a class="headerlink" href="#q31-in-grounding-tasks-does-the-universal-data-format-support-multiple-instances-for-one-category" title="Link to this heading">ïƒ</a></h3>
<p>Currently, it supports one object corresponding to multiple bounding boxes. Refer to the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html#grounding">Custom Dataset</a>.</p>
</section>
<section id="q32-why-am-i-getting-the-error-that-numpy-object-cannot-be-found">
<h3>Q32: Why am I getting the error that numpy.object cannot be found?<a class="headerlink" href="#q32-why-am-i-getting-the-error-that-numpy-object-cannot-be-found" title="Link to this heading">ïƒ</a></h3>
<p>Try using <code class="docutils literal notranslate"><span class="pre">numpy==1.26.3</span></code>.</p>
</section>
<section id="q33-does-the-swift-framework-support-sequence-parallelism-now">
<h3>Q33: Does the Swift framework support sequence parallelism now?<a class="headerlink" href="#q33-does-the-swift-framework-support-sequence-parallelism-now" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it supports it. Refer to the <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel">example</a> here.</p>
</section>
<section id="q34-when-fine-tuning-qwen2-1-5b-on-a-v100-i-see-loss-0-0-acc-0-0-grad-norm-nan-what-is-the-issue">
<h3>Q34: When fine-tuning qwen2-1.5B on a V100, I see <code class="docutils literal notranslate"><span class="pre">loss':</span> <span class="pre">0.0,</span> <span class="pre">'acc':</span> <span class="pre">0.0,</span> <span class="pre">'grad_norm':</span> <span class="pre">nan</span></code>. What is the issue?<a class="headerlink" href="#q34-when-fine-tuning-qwen2-1-5b-on-a-v100-i-see-loss-0-0-acc-0-0-grad-norm-nan-what-is-the-issue" title="Link to this heading">ïƒ</a></h3>
<p>Try using fp32.</p>
</section>
<section id="q35-is-it-possible-to-fully-fine-tune-gptq-quantized-models">
<h3>Q35: Is it possible to fully fine-tune GPTQ quantized models?<a class="headerlink" href="#q35-is-it-possible-to-fully-fine-tune-gptq-quantized-models" title="Link to this heading">ïƒ</a></h3>
<p>No, GPTQ model's int-type parameters cannot participate in gradients; they can only be updated with additional structures like LoRA.</p>
</section>
<section id="q36-what-parameters-should-i-set-for-fine-tuning-using-qlora-on-glm4-chat">
<h3>Q36: What parameters should I set for fine-tuning using QLoRA on glm4-chat?<a class="headerlink" href="#q36-what-parameters-should-i-set-for-fine-tuning-using-qlora-on-glm4-chat" title="Link to this heading">ïƒ</a></h3>
<p>Refer to the QLoRA <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora">example</a>.</p>
</section>
<section id="q37-how-do-i-expand-my-vocabulary-within-the-swift-framework">
<h3>Q37: How do I expand my vocabulary within the Swift framework?<a class="headerlink" href="#q37-how-do-i-expand-my-vocabulary-within-the-swift-framework" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to the command-line-parameters documentation for <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#model-arguments">new_special_tokens</a> for more information.</p>
</section>
<section id="q38-can-i-directly-use-models-with-the-same-name-from-hugging-face">
<h3>Q38: Can I directly use models with the same name from Hugging Face?<a class="headerlink" href="#q38-can-i-directly-use-models-with-the-same-name-from-hugging-face" title="Link to this heading">ïƒ</a></h3>
<p>Set the environment variable <code class="docutils literal notranslate"><span class="pre">USE_HF=1</span></code>.</p>
</section>
<section id="q39-can-qwen2-vl-2b-conduct-incremental-pre-training-is-there-guidance-available">
<h3>Q39: Can Qwen2-VL-2B conduct incremental pre-training? Is there guidance available?<a class="headerlink" href="#q39-can-qwen2-vl-2b-conduct-incremental-pre-training-is-there-guidance-available" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it supports incremental pre-training. Just include all the content in the response.</p>
</section>
<section id="q40-when-training-with-videos-how-can-i-control-the-frame-sampling-rate-in-the-parameters-the-frame-rate-setting-doesn-t-seem-to-work-and-i-m-using-minicpmv">
<h3>Q40: When training with videos, how can I control the frame sampling rate in the parameters? The <code class="docutils literal notranslate"><span class="pre">frame_rate</span></code> setting doesn't seem to work, and I'm using MiniCPMV.<a class="headerlink" href="#q40-when-training-with-videos-how-can-i-control-the-frame-sampling-rate-in-the-parameters-the-frame-rate-setting-doesn-t-seem-to-work-and-i-m-using-minicpmv" title="Link to this heading">ïƒ</a></h3>
<p>Set the environment variable <code class="docutils literal notranslate"><span class="pre">MAX_NUM_FRAMES</span></code>.</p>
</section>
<section id="q41-can-i-save-the-inference-results-of-the-validation-set-during-training-in-swift">
<h3>Q41: Can I save the inference results of the validation set during training in Swift?<a class="headerlink" href="#q41-can-i-save-the-inference-results-of-the-validation-set-during-training-in-swift" title="Link to this heading">ïƒ</a></h3>
<p>After training, run <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span></code> to save the results.</p>
</section>
<section id="q42-why-is-the-saved-checkpoint-larger-than-the-original-model-file-after-full-parameter-dpo">
<h3>Q42: Why is the saved checkpoint larger than the original model file after full parameter DPO?<a class="headerlink" href="#q42-why-is-the-saved-checkpoint-larger-than-the-original-model-file-after-full-parameter-dpo" title="Link to this heading">ïƒ</a></h3>
<p>Using V100 for fine-tuning stores the data in fp32 format.</p>
</section>
<section id="q43-training-speed-slows-down-when-using-multi-machine-training-using-swift-framework-for-llm-training-with-deepspeed-zero3-causes-significant-performance-drop">
<h3>Q43: Training speed slows down when using multi-machine training; using Swift framework for LLM training with deepspeed zero3 causes significant performance drop.<a class="headerlink" href="#q43-training-speed-slows-down-when-using-multi-machine-training-using-swift-framework-for-llm-training-with-deepspeed-zero3-causes-significant-performance-drop" title="Link to this heading">ïƒ</a></h3>
<p>See the <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/1825">issue</a>.</p>
</section>
<section id="q44-does-swift-now-support-multi-stage-pre-training-for-qwen2-vl-it-looks-like-the-official-best-practices-only-show-sft-training-with-vit-llm-together-not-sure-if-separate-fine-tuning-is-supported">
<h3>Q44: Does Swift now support multi-stage pre-training for qwen2-vl? It looks like the official best practices only show SFT training with vit+llm together, not sure if separate fine-tuning is supported.<a class="headerlink" href="#q44-does-swift-now-support-multi-stage-pre-training-for-qwen2-vl-it-looks-like-the-official-best-practices-only-show-sft-training-with-vit-llm-together-not-sure-if-separate-fine-tuning-is-supported" title="Link to this heading">ïƒ</a></h3>
<p>You can control this using the parameters <code class="docutils literal notranslate"><span class="pre">--freeze_vit</span></code>, <code class="docutils literal notranslate"><span class="pre">--freeze_aligner</span></code>, and <code class="docutils literal notranslate"><span class="pre">--freeze_llm</span></code>. For more details, see the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#tuner-arguments">Command Line Parameters Documentation</a>.</p>
</section>
<section id="q45-does-qwen2-vl-support-mixing-pure-text-data">
<h3>Q45: Does qwen2-vl support mixing pure text data?<a class="headerlink" href="#q45-does-qwen2-vl-support-mixing-pure-text-data" title="Link to this heading">ïƒ</a></h3>
<p>It supports both mixed visual-text and pure text data.</p>
</section>
<section id="q46-can-i-plot-loss-curves-for-different-datasets-during-fine-tuning">
<h3>Q46: Can I plot loss curves for different datasets during fine-tuning?<a class="headerlink" href="#q46-can-i-plot-loss-curves-for-different-datasets-during-fine-tuning" title="Link to this heading">ïƒ</a></h3>
<p>Channel loss is supported. Please refer to <code class="docutils literal notranslate"><span class="pre">--enable_channel_loss</span></code>.</p>
</section>
<section id="q47-after-model-training-the-responses-have-a-lot-of-repeated-content">
<h3>Q47: After model training, the responses have a lot of repeated content.<a class="headerlink" href="#q47-after-model-training-the-responses-have-a-lot-of-repeated-content" title="Link to this heading">ïƒ</a></h3>
<p>Refer to the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Pre-training-and-Fine-tuning.html">Pre-training and Fine-tuning</a>. If you notice repetitions during training, try training for more epochs, cleaning the data, and conducting full parameter training, using RLHF to mitigate this issue.</p>
</section>
<section id="q48-does-swift-currently-support-prompt-tuning-or-prefix-tuning">
<h3>Q48: Does Swift currently support prompt tuning or prefix tuning?<a class="headerlink" href="#q48-does-swift-currently-support-prompt-tuning-or-prefix-tuning" title="Link to this heading">ïƒ</a></h3>
<p>No, it does not support these methods, as both methods suffer from serious forgetting issues and are not currently recommended.</p>
</section>
<section id="q49-i-encountered-the-following-error-when-training-with-two-a10s">
<h3>Q49: I encountered the following error when training with two A10s:<a class="headerlink" href="#q49-i-encountered-the-following-error-when-training-with-two-a10s" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[rank0]: torch.distributed.DistBackendError: NCCL error in:../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970ï¼Œ unhandled system error (run with NCCL_DEBUG=INFO for details),NCCL version 2.20.5
[rank0]:ncclSystemError: System call (e.g. socket,malloc) or external library call failed or device error.
</pre></div>
</div>
<p>Please check if shared memory is too small; NCCL requires shared memory.</p>
</section>
<section id="q50-how-to-solve-the-issue-of-certain-parameters-not-participating-in-backpropagation-when-freezing-layers-during-ddp-fine-tuning">
<h3>Q50: How to solve the issue of certain parameters not participating in backpropagation when freezing layers during DDP fine-tuning?<a class="headerlink" href="#q50-how-to-solve-the-issue-of-certain-parameters-not-participating-in-backpropagation-when-freezing-layers-during-ddp-fine-tuning" title="Link to this heading">ïƒ</a></h3>
<p>Set the parameter <code class="docutils literal notranslate"><span class="pre">--ddp_find_unused_parameters</span> <span class="pre">true</span></code>.</p>
</section>
<section id="q51-does-swift-have-a-dataset-quality-inspection-tool">
<h3>Q51: Does Swift have a dataset quality inspection tool?<a class="headerlink" href="#q51-does-swift-have-a-dataset-quality-inspection-tool" title="Link to this heading">ïƒ</a></h3>
<p><a class="reference external" href="https://github.com/modelscope/data-juicer">data-juicer</a>.</p>
</section>
<section id="q52-where-to-start-model-parallelism-on-the-web-i-only-found-the-option-to-check-for-data-parallelism">
<h3>Q52: Where to start model parallelism on the web? I only found the option to check for data parallelism.<a class="headerlink" href="#q52-where-to-start-model-parallelism-on-the-web-i-only-found-the-option-to-check-for-data-parallelism" title="Link to this heading">ïƒ</a></h3>
<p>You can specify visible GPUs to enable model parallelism.</p>
</section>
<section id="q53-how-can-i-set-a-fixed-location-for-dataset-downloads-when-using-dataset-i-can-t-find-this-in-command-line-parameters-how-can-i-read-from-the-download-location-next-time">
<h3>Q53: How can I set a fixed location for dataset downloads when using --dataset? I can't find this in command line parameters. How can I read from the download location next time?<a class="headerlink" href="#q53-how-can-i-set-a-fixed-location-for-dataset-downloads-when-using-dataset-i-can-t-find-this-in-command-line-parameters-how-can-i-read-from-the-download-location-next-time" title="Link to this heading">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">dataset_path</span></code> supports folders, typically for datasets downloaded via <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span></code>. See <a class="reference external" href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html#dataset-info-json">Custom Dataset Documentation</a>.</p>
</section>
<section id="q54-when-using-streaming-true-i-get-an-error-asking-me-to-set-max-steps-when-setting-num-train-epochs-can-t-i-just-set-num-train-epochs">
<h3>Q54: When using --streaming true, I get an error asking me to set max_steps when setting num_train_epochs. Can't I just set num_train_epochs?<a class="headerlink" href="#q54-when-using-streaming-true-i-get-an-error-asking-me-to-set-max-steps-when-setting-num-train-epochs-can-t-i-just-set-num-train-epochs" title="Link to this heading">ïƒ</a></h3>
<p>See the streaming parameter description, <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#data-arguments">Command Line Parameters Documentation</a>.</p>
</section>
<section id="q55-why-is-tools-in-format-rather-than-directly-using-could-you-explain-why-tools-uses-this-format-instead-of-direct-notation">
<h3>Q55: Why is tools in &quot;[]&quot; format rather than directly using []? Could you explain why tools uses this &quot;[]&quot; format instead of direct [] notation?<a class="headerlink" href="#q55-why-is-tools-in-format-rather-than-directly-using-could-you-explain-why-tools-uses-this-format-instead-of-direct-notation" title="Link to this heading">ïƒ</a></h3>
<p>This is because the underlying pyarrow in datasets has strict type control. For the same reason, the objects part in our official grounding dataset also uses str, otherwise pyarrow would report errors about inconsistent types across rows.</p>
</section>
<section id="q56-can-t-this-parameter-be-used-check-dataset-strategy-discard">
<h3>Q56: Can't this parameter be used? check_dataset_strategy==discard<a class="headerlink" href="#q56-can-t-this-parameter-be-used-check-dataset-strategy-discard" title="Link to this heading">ïƒ</a></h3>
<p>This parameter no longer exists in swift3.0, use the <code class="docutils literal notranslate"><span class="pre">strict</span></code> parameter instead.</p>
</section>
<section id="q57-getting-this-error-when-running-sft-command">
<h3>Q57: Getting this error when running sft command:<a class="headerlink" href="#q57-getting-this-error-when-running-sft-command" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: Expected to mark a variable ready only once.This error is caused by one of the following reasons: 1) Use of a module parameter outsid forward function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph( ) as round if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple oint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph( ) as a workaround if dule graph does not change over iterations.
</pre></div>
</div>
<p>Add the parameter <code class="docutils literal notranslate"><span class="pre">--gradient_checkpointing_kwargs</span> <span class="pre">'{&quot;use_reentrant&quot;:</span> <span class="pre">false}'</span></code>. Alternatively, upgrade ms-swift, which sets this parameter by default.</p>
</section>
<section id="q58-have-you-encountered-this-issue-attributeerror-trainerstate-object-has-no-attribute-last-model-checkpoint">
<h3>Q58: Have you encountered this issue? AttributeError:'TrainerState' object has no attribute 'last_model_checkpoint'<a class="headerlink" href="#q58-have-you-encountered-this-issue-attributeerror-trainerstate-object-has-no-attribute-last-model-checkpoint" title="Link to this heading">ïƒ</a></h3>
<p>Dataset is too small, need to add more data. Error occurs when data quantity is less than one step.</p>
</section>
<section id="q59-i-see-preprocess-can-be-defined-in-custompreprocessor-is-this-processed-all-at-once-before-training-starts-or-loaded-during-training">
<h3>Q59: I see preprocess can be defined in CustomPreprocessor. Is this processed all at once before training starts, or loaded during training?<a class="headerlink" href="#q59-i-see-preprocess-can-be-defined-in-custompreprocessor-is-this-processed-all-at-once-before-training-starts-or-loaded-during-training" title="Link to this heading">ïƒ</a></h3>
<p>If <code class="docutils literal notranslate"><span class="pre">--streaming</span> <span class="pre">true</span></code> is set, it loads while training. By default, it processes everything before training.</p>
</section>
<section id="q60-for-full-parameter-training-of-internvl2-5-why-do-vision-model-and-mlp1-appear-in-freeze-parameters-by-default-documentation-shows-freeze-parameters-defaults-to-and-command-line-settings-for-freeze-vit-freeze-aligner-freeze-llm-are-all-false-it-prints-trainable-parameters-mlp1-unclear-if-only-mlp1-is-trainable-or-all-parameters">
<h3>Q60: For full-parameter training of internvl2_5, why do vision_model and mlp1 appear in freeze parameters by default? Documentation shows freeze_parameters defaults to [], and command line settings for freeze vit, freeze aligner, freeze llm are all False. It prints trainable parameters: ['mlp1'] - unclear if only mlp1 is trainable or all parameters<a class="headerlink" href="#q60-for-full-parameter-training-of-internvl2-5-why-do-vision-model-and-mlp1-appear-in-freeze-parameters-by-default-documentation-shows-freeze-parameters-defaults-to-and-command-line-settings-for-freeze-vit-freeze-aligner-freeze-llm-are-all-false-it-prints-trainable-parameters-mlp1-unclear-if-only-mlp1-is-trainable-or-all-parameters" title="Link to this heading">ïƒ</a></h3>
<p>First freeze parameters then active parameters. The three parameters <code class="docutils literal notranslate"><span class="pre">freeze</span> <span class="pre">vit/freeze</span> <span class="pre">aligner/freeze</span> <span class="pre">llm</span></code> adjust freeze parameters and trainable parameters. Since some models' <code class="docutils literal notranslate"><span class="pre">vit</span></code> contains <code class="docutils literal notranslate"><span class="pre">aligner</span></code>, aligner is separately added to trainable_parameters.</p>
</section>
<section id="q61-does-llamapro-in-swift-support-multimodal-adaptation">
<h3>Q61: Does LlamaPro in swift support multimodal adaptation?<a class="headerlink" href="#q61-does-llamapro-in-swift-support-multimodal-adaptation" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it's supported.</p>
</section>
<section id="q62-i-noticed-2-x-supports-max-pixels-is-the-max-pixel-parameter-in-3-x-documentation-the-same-thing-what-s-the-processing-logic-using-12000-9000-images-with-internvl-still-crashes-in-2-x-even-with-resacle-image">
<h3>Q62: I noticed 2.x supports MAX_PIXELS. Is the --max_pixel parameter in 3.x documentation the same thing? What's the processing logic? Using 12000*9000 images with internvl still crashes in 2.x even with resacle_image<a class="headerlink" href="#q62-i-noticed-2-x-supports-max-pixels-is-the-max-pixel-parameter-in-3-x-documentation-the-same-thing-what-s-the-processing-logic-using-12000-9000-images-with-internvl-still-crashes-in-2-x-even-with-resacle-image" title="Link to this heading">ïƒ</a></h3>
<p>Environment variable parameters correspond to model parameters. <code class="docutils literal notranslate"><span class="pre">MAX_PIXELS</span></code> only supports qwen2vl, internvl has its own environment variables. See <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#specific-model-argumen">Specific Model Parameters</a>.</p>
</section>
<section id="q63-is-there-documentation-for-fine-tuning-qwen-base-model-to-chat-model-any-special-configurations-needed">
<h3>Q63: Is there documentation for fine-tuning qwen base model to chat model? Any special configurations needed?<a class="headerlink" href="#q63-is-there-documentation-for-fine-tuning-qwen-base-model-to-chat-model-any-special-configurations-needed" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">sft</span></code>, no special configuration needed. See <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/base_to_chat">example</a>.</p>
</section>
<section id="q64-where-can-i-find-sequence-parallel-examples">
<h3>Q64:  Where can I find sequence parallel examples?<a class="headerlink" href="#q64-where-can-i-find-sequence-parallel-examples" title="Link to this heading">ïƒ</a></h3>
<p>See this example: <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel">sequence_parallel</a>.</p>
</section>
<section id="q65-can-swift-support-training-custom-model-structures">
<h3>Q65: Can swift support training custom model structures?<a class="headerlink" href="#q65-can-swift-support-training-custom-model-structures" title="Link to this heading">ïƒ</a></h3>
<p>Yes, just customize the <code class="docutils literal notranslate"><span class="pre">get_model_tokenizer_xxx</span></code> function to return <code class="docutils literal notranslate"><span class="pre">model</span></code> and <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>.</p>
</section>
<section id="q66-getting-an-error-using-longlora-with-name-or-path-mnt-workspace-model-qwen2-5-14b-instruct-is-longlora-only-for-llama-series">
<h3>Q66: Getting an error using longlora with &quot;name_or_path&quot;: &quot;/mnt/workspace/model/Qwen2.5-14B-Instruct&quot;. Is longlora only for llama series?<a class="headerlink" href="#q66-getting-an-error-using-longlora-with-name-or-path-mnt-workspace-model-qwen2-5-14b-instruct-is-longlora-only-for-llama-series" title="Link to this heading">ïƒ</a></h3>
<p>Yes, <code class="docutils literal notranslate"><span class="pre">longlora</span></code> only works with llama series.</p>
</section>
<section id="q67-how-to-add-custom-special-tokens-in-swift">
<h3>Q67: How to add custom special tokens in swift?<a class="headerlink" href="#q67-how-to-add-custom-special-tokens-in-swift" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to the command-line-parameters documentation for <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#model-arguments">new_special_tokens</a> for more information.</p>
</section>
<section id="q68-for-freeze-parameters-ratio-parameter-if-set-to-0-7-does-it-mean-only-30-of-llm-parameters-are-updated-during-training-is-it-random-30-what-s-the-update-mechanism">
<h3>Q68: For --freeze_parameters_ratio parameter, if set to 0.7, does it mean only 30% of llm parameters are updated during training? Is it random 30%? What's the update mechanism?<a class="headerlink" href="#q68-for-freeze-parameters-ratio-parameter-if-set-to-0-7-does-it-mean-only-30-of-llm-parameters-are-updated-during-training-is-it-random-30-what-s-the-update-mechanism" title="Link to this heading">ïƒ</a></h3>
<p>Freezes from bottom to top.</p>
</section>
<section id="q69-why-is-the-map-process-so-slow-is-this-normal">
<h3>Q69: Why is the map process so slow? Is this normal?<a class="headerlink" href="#q69-why-is-the-map-process-so-slow-is-this-normal" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Map: 4%|â–ˆâ–ˆ | 9000/203823 [02:18&lt;50:34, 64.19 examples/s]
</pre></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">--dataset_num_proc</span></code> parameter to enable multiple processes.</p>
</section>
<section id="q70-how-can-i-delete-and-redownload-a-dataset-i-think-there-might-be-an-issue-with-the-dataset">
<h3>Q70: How can I delete and redownload a dataset? I think there might be an issue with the dataset.<a class="headerlink" href="#q70-how-can-i-delete-and-redownload-a-dataset-i-think-there-might-be-an-issue-with-the-dataset" title="Link to this heading">ïƒ</a></h3>
<p>Set the <code class="docutils literal notranslate"><span class="pre">--download_mode</span></code> parameter.</p>
</section>
<section id="q71-how-to-solve-this-error-safetensors-rust-safetensorerror-error-while-deserializing-header-headertoolarge">
<h3>Q71: How to solve this error: safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge?<a class="headerlink" href="#q71-how-to-solve-this-error-safetensors-rust-safetensorerror-error-while-deserializing-header-headertoolarge" title="Link to this heading">ïƒ</a></h3>
<p>The disk space is insufficient, and the model wasn't saved completely.</p>
</section>
<section id="q72-does-swift3-0-not-support-get-default-template-type">
<h3>Q72: Does swift3.0 not support get_default_template_type?<a class="headerlink" href="#q72-does-swift3-0-not-support-get-default-template-type" title="Link to this heading">ïƒ</a></h3>
<p>Please check <code class="docutils literal notranslate"><span class="pre">model.model_meta.template</span></code>, the information is available in <code class="docutils literal notranslate"><span class="pre">model.model_meta</span></code> and <code class="docutils literal notranslate"><span class="pre">model.model_info</span></code>.</p>
</section>
<section id="q73-is-the-default-model-training-using-left-padding">
<h3>Q73: Is the default model training using left padding?<a class="headerlink" href="#q73-is-the-default-model-training-using-left-padding" title="Link to this heading">ïƒ</a></h3>
<p>Training can use either left or right padding. The default is right padding, while <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">infer</span></code> uses left padding.</p>
</section>
<section id="q74-does-it-support-grounding-tasks-now">
<h3>Q74: Does it support grounding tasks now?<a class="headerlink" href="#q74-does-it-support-grounding-tasks-now" title="Link to this heading">ïƒ</a></h3>
<p>Yes, there's an <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/grounding.sh">example</a> under examples.</p>
</section>
<section id="q75-does-ms-swift-support-contrastive-learning-for-training-llm-emb">
<h3>Q75: Does ms-swift support contrastive learning for training llm_emb?<a class="headerlink" href="#q75-does-ms-swift-support-contrastive-learning-for-training-llm-emb" title="Link to this heading">ïƒ</a></h3>
<p>Yes, here's an <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding">example</a>.</p>
</section>
<section id="q76-is-there-a-big-difference-in-performance-between-manually-coding-fine-tuning-and-grpo-using-peft-and-trl-libraries-compared-to-swift-official-training-with-the-same-parameters">
<h3>Q76: Is there a big difference in performance between manually coding fine-tuning and GRPO using peft and trl libraries compared to Swift official training with the same parameters?<a class="headerlink" href="#q76-is-there-a-big-difference-in-performance-between-manually-coding-fine-tuning-and-grpo-using-peft-and-trl-libraries-compared-to-swift-official-training-with-the-same-parameters" title="Link to this heading">ïƒ</a></h3>
<p>The difference is minimal, with Swift additionally supporting multimodality.</p>
</section>
<section id="q77-does-swift-currently-not-support-audio-modal-input-training-for-minicpmo-it-shows-error-assert-media-type-in-image-video">
<h3>Q77: Does Swift currently not support audio modal input training for minicpmo? It shows error: assert media_type in {'image', 'video'}<a class="headerlink" href="#q77-does-swift-currently-not-support-audio-modal-input-training-for-minicpmo-it-shows-error-assert-media-type-in-image-video" title="Link to this heading">ïƒ</a></h3>
<p>Audio is not currently supported.</p>
</section>
<section id="q78-can-swift-fine-tune-deepseek-r1-671b">
<h3>Q78: Can Swift fine-tune deepseek R1 671B?<a class="headerlink" href="#q78-can-swift-fine-tune-deepseek-r1-671b" title="Link to this heading">ïƒ</a></h3>
<p>Yes, the template is integrated, but the process is complicated as it requires converting fp8 to bf16 first.</p>
</section>
<section id="q79-isn-t-the-latest-swift-framework-supposed-to-specify-the-model-location-using-this-command-this-is-the-location-of-the-model-i-ve-already-downloaded-but-i-don-t-know-why-it-still-tries-to-download-and-fails-with-a-git-clone-error">
<h3>Q79: Isn't the latest Swift framework supposed to specify the model location using this command? This is the location of the model I've already downloaded, but I don't know why it still tries to download and fails with a git clone error<a class="headerlink" href="#q79-isn-t-the-latest-swift-framework-supposed-to-specify-the-model-location-using-this-command-this-is-the-location-of-the-model-i-ve-already-downloaded-but-i-don-t-know-why-it-still-tries-to-download-and-fails-with-a-git-clone-error" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--model<span class="w"> </span>/mnt/workspace/.cache/modelscope/hub/deepseek-ai/deepseek-vl2/<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Some models require cloning the repo and then specifying through <code class="docutils literal notranslate"><span class="pre">local_repo_path</span></code>.</p>
</section>
<section id="q80-does-swift-now-support-multimodal-grpo">
<h3>Q80: Does Swift now support multimodal GRPO?<a class="headerlink" href="#q80-does-swift-now-support-multimodal-grpo" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it does.</p>
</section>
<section id="q81-can-the-grpo-reward-function-be-customized">
<h3>Q81: Can the GRPO reward function be customized?<a class="headerlink" href="#q81-can-the-grpo-reward-function-be-customized" title="Link to this heading">ïƒ</a></h3>
<p>Yes, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/plugin">examples/train/grpo/plugin</a>.</p>
</section>
<section id="q82-why-do-i-get-the-error-when-using-torch-dtype-float16-card-cannot-use-bf16-lib-python3-12-site-packages-torch-amp-grad-scaler-py-line-260-in-unscale-grads-raise-valueerror-attempting-to-unscale-fp16-gradients-valueerror-attempting-to-unscale-fp16-gradients">
<h3>Q82: Why do I get the error when using --torch_dtype float16 (card cannot use bf16): lib/python3.12/site-packages/torch/amp/grad_scaler.py&quot;, line 260, in unscale_grads raise ValueError(&quot;Attempting to unscale FP16 gradients.&quot;) ValueError: Attempting to unscale FP16 gradients.<a class="headerlink" href="#q82-why-do-i-get-the-error-when-using-torch-dtype-float16-card-cannot-use-bf16-lib-python3-12-site-packages-torch-amp-grad-scaler-py-line-260-in-unscale-grads-raise-valueerror-attempting-to-unscale-fp16-gradients-valueerror-attempting-to-unscale-fp16-gradients" title="Link to this heading">ïƒ</a></h3>
<p>FP16 does not support full-parameter training.</p>
</section>
<section id="q83-i-have-a-question-i-trained-a-reward-model-using-swift-baseline-is-qwen2-5-7b-but-when-loading-it-in-ppo-or-grpo-it-shows-an-error-the-reward-model-was-trained-using-lora">
<h3>Q83: I have a question. I trained a reward model using Swift (baseline is qwen2.5-7b), but when loading it in PPO or GRPO, it shows an error. The reward model was trained using LoRA.<a class="headerlink" href="#q83-i-have-a-question-i-trained-a-reward-model-using-swift-baseline-is-qwen2-5-7b-but-when-loading-it-in-ppo-or-grpo-it-shows-an-error-the-reward-model-was-trained-using-lora" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--rlhf_type<span class="w"> </span>ppo<span class="w"> </span><span class="se">\</span>
--model<span class="w"> </span>Qwen/Qwen2.5-14B-Instruct<span class="w"> </span><span class="se">\</span>
--reward_model<span class="w"> </span>/mnt/workspace/output/rm/model<span class="w"> </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-zh#20000&#39;</span><span class="w"> </span>--torch_dtype<span class="w"> </span>float32<span class="w"> </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--learning_rate<span class="w"> </span>1e-5<span class="w"> </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span>--lora_alpha<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
--gradient_accumulation_steps<span class="w"> </span><span class="m">16</span><span class="w"> </span>--eval_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span>--save_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>The LoRA-trained reward model needs to be merged.</p>
</section>
<section id="q84-what-version-of-transformers-is-needed-to-fine-tune-deepseek-vl2-official-docs-say-4-42-but-it-also-shows-errors-with-4-42-and-below-does-the-peft-version-need-to-be-lowered-too">
<h3>Q84: What version of transformers is needed to fine-tune deepseek_vl2? Official docs say &lt;4.42, but it also shows errors with 4.42 and below. Does the peft version need to be lowered too?<a class="headerlink" href="#q84-what-version-of-transformers-is-needed-to-fine-tune-deepseek-vl2-official-docs-say-4-42-but-it-also-shows-errors-with-4-42-and-below-does-the-peft-version-need-to-be-lowered-too" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">peft==0.11.*</span></code>.</p>
</section>
<section id="q85-generate-train-split-is-too-slow-about-30-datasets-with-around-a-million-total-data-points-previously-swift-2-x-wasn-t-this-slow-lazy-tokenize-is-already-enabled">
<h3>Q85: Generate train split is too slow (about 30+ datasets with around a million total data points). Previously Swift 2.x wasn't this slow. Lazy tokenize is already enabled.<a class="headerlink" href="#q85-generate-train-split-is-too-slow-about-30-datasets-with-around-a-million-total-data-points-previously-swift-2-x-wasn-t-this-slow-lazy-tokenize-is-already-enabled" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--dataset_num_proc</span> <span class="pre">16</span></code>.</p>
</section>
<section id="q86-how-can-i-full-parameter-fine-tune-the-visual-encoder-while-using-lora-to-fine-tune-llm-when-fine-tuning-qwen2-5vl">
<h3>Q86: How can I full-parameter fine-tune the visual encoder while using LoRA to fine-tune LLM when fine-tuning qwen2.5vl?<a class="headerlink" href="#q86-how-can-i-full-parameter-fine-tune-the-visual-encoder-while-using-lora-to-fine-tune-llm-when-fine-tuning-qwen2-5vl" title="Link to this heading">ïƒ</a></h3>
<p>Refer to this <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal/lora_llm_full_vit">example</a>.</p>
</section>
<section id="q87-how-to-use-custom-loss-functions-in-swift">
<h3>Q87: How to use custom loss functions in Swift?<a class="headerlink" href="#q87-how-to-use-custom-loss-functions-in-swift" title="Link to this heading">ïƒ</a></h3>
<p>Add it in the plugin.</p>
</section>
<section id="q88-what-are-the-parameters-for-moe-can-t-find-keywords-in-the-parameter-table-how-to-set-expert-numbers-and-expert-routing-parameters">
<h3>Q88: What are the parameters for MoE? Can't find keywords in the parameter table. How to set expert numbers and expert routing parameters?<a class="headerlink" href="#q88-what-are-the-parameters-for-moe-can-t-find-keywords-in-the-parameter-table-how-to-set-expert-numbers-and-expert-routing-parameters" title="Link to this heading">ïƒ</a></h3>
<p>Use parameters directly from <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p>
</section>
<section id="q89-using-lmdeploy-in-grpo-training-reports-missing-functions-the-load-weights-function-isn-t-found-in-lmdeployengine-class">
<h3>Q89: Using lmdeploy in grpo training reports missing functions. The load_weights function isn't found in lmdeployengine class.<a class="headerlink" href="#q89-using-lmdeploy-in-grpo-training-reports-missing-functions-the-load-weights-function-isn-t-found-in-lmdeployengine-class" title="Link to this heading">ïƒ</a></h3>
<p>Only supported under turbomind engine.</p>
</section>
<section id="q90-getting-errors-when-fine-tuning-moonlight-16b-a3b-instruct-model-seems-ms-swift-doesn-t-support-fine-tuning-this-model">
<h3>Q90: Getting errors when fine-tuning Moonlight-16B-A3B-Instruct model. Seems ms-swift doesn't support fine-tuning this model?<a class="headerlink" href="#q90-getting-errors-when-fine-tuning-moonlight-16b-a3b-instruct-model-seems-ms-swift-doesn-t-support-fine-tuning-this-model" title="Link to this heading">ïƒ</a></h3>
<p>Training is disabled in model files. Refer to deepseek_vl2's solution in the issues.</p>
</section>
<section id="q91-how-to-solve-this-error-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16">
<h3>Q91: How to solve this error: RuntimeError: &quot;triu_tril_cuda_template&quot; not implemented for 'BFloat16'?<a class="headerlink" href="#q91-how-to-solve-this-error-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">01</span>,2,3,4,5,6,7<span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Internlm3-8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span>train.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed<span class="w"> </span>zero3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eval_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<p>Upgrade torch.</p>
</section>
<section id="q92-is-it-normal-that-both-loss-and-grad-norm-are-0-during-grpo-training">
<h3>Q92: Is it normal that both loss and grad_norm are 0 during GRPO training?<a class="headerlink" href="#q92-is-it-normal-that-both-loss-and-grad-norm-are-0-during-grpo-training" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;:    0.0.    &#39;grad norm&#39;:0.0,    &#39;learning_rate&#39;:9e-08,    &#39;memory(GiB)&#39;:88.1ï¼Œ    &#39;train_speed(iter/s)&#39;:0.009252ï¼Œ    &#39;completion_length&#39;:    150.00000763ï¼Œ    &#39;response_clip ratio&#39;: 0.0,    &#39;rewards/Format&#39;:1.0,    &#39;reward
: 1.0,    &#39;reward std&#39;:0.0ï¼Œ    &#39;kl&#39;: 0.0, &#39;clip_ratio&#39;: 0.0,    &#39;epoch&#39;: 0.0ï¼Œ &#39;qlobal step/max steps&#39;:&#39;1/1052&#39;ï¼Œ    &#39;percentage&#39;:&#39;0.10%    &#39;elapsed time&#39;:    &#39;36s    &#39;remaining time&#39;: &#39;10h 43m 54s&#39;}
{&#39;loss&#39;: 0.0ï¼Œ&#39;grad_norm&#39;:0.0ï¼Œ&#39;learning_rate&#39;: 1.8e-07,&#39;memory(GiB)&#39;:94.15ï¼Œ&#39;train_speed(iter/s)&#39;:0.014782ï¼Œ&#39;completion_length&#39;: 133.25000763ï¼Œ&#39;response_clip_ratio&#39;: 0.0ï¼Œ&#39;rewards/Format&#39;: 1.0, &#39;rewa rd&#39;: 1.0ï¼Œ&#39;reward_std&#39;: 0.0, &#39;kl&#39;: 0.0ï¼Œ&#39;clip_ratio&#39;: 0.0,&#39;epoch&#39;: 0.0, &#39;global_step/max_steps&#39;: &#39;2/1052&#39;ï¼Œ&#39;percentage&#39;: &#39;0.19%&#39;, &#39;elapsed_time&#39;: &#39;1m 3s&#39;ï¼Œ &#39;remaining_time&#39;: &#39;9h 19m 49s&#39;}
{&#39;loss&#39;: 0.0ï¼Œ &#39;qrad norm&#39;: 0.0, &#39;learning rate&#39;: 2.7e-07,&#39;memory(GiB)&#39;: 94.15ï¼Œ&#39;train_speed(iter/s)&#39;: 0.018695ï¼Œ&#39;completion_length&#39;: 123.08333969ï¼Œï¼Œ&#39;response_clip_ratio&#39;: 0.0ï¼Œ&#39;rewards/Format&#39;: 1.0, &#39;rewa rd&#39;: 1.0ï¼Œ &#39;reward_ std&#39;: 0.0,&#39;kl&#39;: 0.0,&#39;clip_ratio&#39;: 0.0ï¼Œ &#39;epoch&#39;: 0.0ï¼Œ &#39;global_step/max_steps&#39;: &#39;3/1052&#39;ï¼Œ&#39;percentage&#39;: &#39;0.29%ï¼Œ&#39;elapsed_time&#39;: &#39;1m 29s&#39;ï¼Œ&#39;remaining_time&#39;: &#39;8h 39m 34s&#39;}
</pre></div>
</div>
<p>Training with loss close to 0 is normal, refer to this <a class="reference external" href="https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851">issue</a>.</p>
</section>
<section id="q93-where-can-i-pass-in-accuracy-orm-for-grpo-s-built-in-reward-function">
<h3>Q93: Where can I pass in accuracy_orm for GRPO's built-in reward function?<a class="headerlink" href="#q93-where-can-i-pass-in-accuracy-orm-for-grpo-s-built-in-reward-function" title="Link to this heading">ïƒ</a></h3>
<p>Currently it requires modifying the code directly.</p>
</section>
<section id="q94-i-notice-the-reward-function-has-a-solution-parameter-does-it-need-to-be-passed-from-the-dataset-does-my-dataset-must-have-a-solution-field">
<h3>Q94: I notice the reward function has a solution parameter, does it need to be passed from the dataset? Does my dataset must have a solution field?<a class="headerlink" href="#q94-i-notice-the-reward-function-has-a-solution-parameter-does-it-need-to-be-passed-from-the-dataset-does-my-dataset-must-have-a-solution-field" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it's necessary for math problems to calculate accuracy.</p>
</section>
<section id="q95-why-is-there-no-token-acc-during-training">
<h3>Q95: Why is there no token_acc during training?<a class="headerlink" href="#q95-why-is-there-no-token-acc-during-training" title="Link to this heading">ïƒ</a></h3>
<p>Some models have mismatched <code class="docutils literal notranslate"><span class="pre">logits</span></code> and <code class="docutils literal notranslate"><span class="pre">labels</span></code> counts, so token accuracy isn't calculated.</p>
</section>
<section id="q96-when-fine-tuning-ovis2-lora-parameters-don-t-seem-to-work-memory-usage-doesn-t-change-with-or-without-tuner-type-lora">
<h3>Q96: When fine-tuning Ovis2, LoRA parameters don't seem to work? Memory usage doesn't change with or without --tuner_type lora.<a class="headerlink" href="#q96-when-fine-tuning-ovis2-lora-parameters-don-t-seem-to-work-memory-usage-doesn-t-change-with-or-without-tuner-type-lora" title="Link to this heading">ïƒ</a></h3>
<p>Limit <code class="docutils literal notranslate"><span class="pre">--max_length</span></code>, this model is special and needs padding to max_length.</p>
</section>
<section id="q97-getting-valueerror-when-running-classification-task-with-qwen2-5-the-model-did-not-return-a-loss-from-the-inputs-only-the-following-keys-logits-for-reference-the-inputs-it-received-are-input-ids-attention-mask">
<h3>Q97: Getting ValueError when running classification task with Qwen2.5: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.<a class="headerlink" href="#q97-getting-valueerror-when-running-classification-task-with-qwen2-5-the-model-did-not-return-a-loss-from-the-inputs-only-the-following-keys-logits-for-reference-the-inputs-it-received-are-input-ids-attention-mask" title="Link to this heading">ïƒ</a></h3>
<p>dataset format: {&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;xxxxx&quot;}, {&quot;label&quot;: 1}]}
Put <code class="docutils literal notranslate"><span class="pre">label</span></code> at the same level as <code class="docutils literal notranslate"><span class="pre">messages</span></code>, not inside it.</p>
</section>
<section id="q98-how-to-exit-vllmengine-i-want-to-release-gpu-memory-after-inference-rather-than-keeping-it-occupied">
<h3>Q98: How to exit VllmEngine? I want to release GPU memory after inference rather than keeping it occupied.<a class="headerlink" href="#q98-how-to-exit-vllmengine-i-want-to-release-gpu-memory-after-inference-rather-than-keeping-it-occupied" title="Link to this heading">ïƒ</a></h3>
<p>Use sleep mode: <code class="docutils literal notranslate"><span class="pre">engine.sleep(level=1)/engine.wake_up()</span></code> with <code class="docutils literal notranslate"><span class="pre">enable_sleep_mode=True</span></code> during initialization.</p>
</section>
<section id="q99-does-trainer-sampler-random-have-no-effect-in-streaming-mode">
<h3>Q99: Does trainer_sampler_random have no effect in streaming mode?<a class="headerlink" href="#q99-does-trainer-sampler-random-have-no-effect-in-streaming-mode" title="Link to this heading">ïƒ</a></h3>
<p>Streaming is not random.</p>
</section>
<section id="q100-can-trust-remote-code-be-set-when-using-vllm-for-grpo-training">
<h3>Q100: Can trust_remote_code be set when using VLLM for GRPO training?<a class="headerlink" href="#q100-can-trust-remote-code-be-set-when-using-vllm-for-grpo-training" title="Link to this heading">ïƒ</a></h3>
<p>It's true by default.</p>
</section>
<section id="q101-for-large-dataset-pretraining-using-streaming-and-packing-is-there-a-way-to-calculate-total-steps-based-on-epochs-batch-size-etc-when-setting-max-steps">
<h3>Q101: For large dataset pretraining using streaming and packing, is there a way to calculate total steps based on epochs, batch size etc when setting max_steps?<a class="headerlink" href="#q101-for-large-dataset-pretraining-using-streaming-and-packing-is-there-a-way-to-calculate-total-steps-based-on-epochs-batch-size-etc-when-setting-max-steps" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--max_steps</span></code> or <code class="docutils literal notranslate"><span class="pre">--max_epochs</span></code>. For more details, see the streaming parameter description in the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#data-arguments">Command Line Parameters Documentation</a>.</p>
</section>
<section id="q102-unsloth-training-error-assert-type-target-modules-in-list-tuple-when-using-target-modules-all-linear">
<h3>Q102: Unsloth training error: &quot;assert(type(target modules) in (list,tuple,))&quot; when using --target_modules all-linear<a class="headerlink" href="#q102-unsloth-training-error-assert-type-target-modules-in-list-tuple-when-using-target-modules-all-linear" title="Link to this heading">ïƒ</a></h3>
<p>Don't use <code class="docutils literal notranslate"><span class="pre">all-linear</span></code>, specify concrete module list like <code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">q</span> <span class="pre">k</span> <span class="pre">v</span></code>.</p>
</section>
<section id="q103-does-swift-support-multi-label-classification-now">
<h3>Q103: Does Swift support multi-label classification now?<a class="headerlink" href="#q103-does-swift-support-multi-label-classification-now" title="Link to this heading">ïƒ</a></h3>
<p>Yes. Check custom dataset docs for format and search for <code class="docutils literal notranslate"><span class="pre">problem_type</span></code> in command line parameter docs.</p>
</section>
<section id="q104-how-does-flash-attn-handle-packing-separately-or-merged">
<h3>Q104: How does flash_attn handle packing - separately or merged?<a class="headerlink" href="#q104-how-does-flash-attn-handle-packing-separately-or-merged" title="Link to this heading">ïƒ</a></h3>
<p>Flash attention is required to avoid errors, otherwise attention_mask will have issues.</p>
</section>
<section id="q105-for-qwen2-5-omni-does-setting-freeze-vit-false-mean-both-the-visual-encoder-and-the-audio-encoder-are-enabled-is-there-a-way-to-enable-only-the-audio-encoder-without-enabling-the-visual-encoder">
<h3>Q105: For qwen2.5-omni, does setting --freeze_vit false mean both the visual encoder and the audio encoder are enabled? Is there a way to enable only the audio encoder without enabling the visual encoder?<a class="headerlink" href="#q105-for-qwen2-5-omni-does-setting-freeze-vit-false-mean-both-the-visual-encoder-and-the-audio-encoder-are-enabled-is-there-a-way-to-enable-only-the-audio-encoder-without-enabling-the-visual-encoder" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">--target_regex</span></code>.</p>
</section>
<section id="q106-does-swift-currently-support-sequence-parallelism-for-those-reinforcement-learning-training-methods">
<h3>Q106: Does swift currently support sequence parallelism for those reinforcement learning training methods?<a class="headerlink" href="#q106-does-swift-currently-support-sequence-parallelism-for-those-reinforcement-learning-training-methods" title="Link to this heading">ïƒ</a></h3>
<p>It supports pt, sft, dpo, and grpo.</p>
</section>
<section id="q107-after-using-lora-sft-is-tokenizer-json-not-saved">
<h3>Q107: After using lora sft, is tokenizer.json not saved?<a class="headerlink" href="#q107-after-using-lora-sft-is-tokenizer-json-not-saved" title="Link to this heading">ïƒ</a></h3>
<p>Lora doesn't save it; it is migrated after merging because the lora directory needs to work with the original model.</p>
</section>
<section id="q108-can-the-reward-model-and-reward-funcs-of-grpo-be-used-together">
<h3>Q108: Can the reward_model and reward_funcs of GRPO be used together?<a class="headerlink" href="#q108-can-the-reward-model-and-reward-funcs-of-grpo-be-used-together" title="Link to this heading">ïƒ</a></h3>
<p>Yes, they can.</p>
</section>
<section id="q109-i-want-to-ask-if-there-is-a-parameter-that-can-be-adjusted-to-avoid-introducing-the-kl-term-in-grpo">
<h3>Q109: I want to ask if there is a parameter that can be adjusted to avoid introducing the KL term in GRPO?<a class="headerlink" href="#q109-i-want-to-ask-if-there-is-a-parameter-that-can-be-adjusted-to-avoid-introducing-the-kl-term-in-grpo" title="Link to this heading">ïƒ</a></h3>
<p>Search for <code class="docutils literal notranslate"><span class="pre">beta</span></code> in the command line parameters.</p>
</section>
<section id="q110-when-doing-grpo-how-can-i-access-the-original-labels-in-the-orm-custom-reward-function-i-printed-the-messages-field-in-kwargs-and-the-value-of-assistant-s-content-in-each-item-is-replaced-by-the-generated-result">
<h3>Q110: When doing GRPO, how can I access the original labels in the orm custom reward function? I printed the messages field in kwargs, and the value of assistant's content in each item is replaced by the generated result.<a class="headerlink" href="#q110-when-doing-grpo-how-can-i-access-the-original-labels-in-the-orm-custom-reward-function-i-printed-the-messages-field-in-kwargs-and-the-value-of-assistant-s-content-in-each-item-is-replaced-by-the-generated-result" title="Link to this heading">ïƒ</a></h3>
<p>Place it in another column.</p>
</section>
<section id="q111-if-you-use-the-default-num-iterations-1-does-clip-become-ineffective-the-clip-higher-in-dapo-is-also-useless-i-see-that-verl-has-a-micro-batch-setting-to-update-the-policy-model-in-small-batches-for-the-clip-term-to-take-effect-in-ms-swift-it-seems-mini-batch-only-does-gradient-accumulation-according-to-the-source-code">
<h3>Q111: If you use the default num_iterations=1, does clip become ineffective? The clip higher in dapo is also useless. I see that veRL has a micro batch setting to update the policy model in small batches for the clip term to take effect. In ms-swift, it seems mini batch only does gradient accumulation according to the source code?<a class="headerlink" href="#q111-if-you-use-the-default-num-iterations-1-does-clip-become-ineffective-the-clip-higher-in-dapo-is-also-useless-i-see-that-verl-has-a-micro-batch-setting-to-update-the-policy-model-in-small-batches-for-the-clip-term-to-take-effect-in-ms-swift-it-seems-mini-batch-only-does-gradient-accumulation-according-to-the-source-code" title="Link to this heading">ïƒ</a></h3>
<p>Yes, num_iterations needs to be &gt;1.</p>
</section>
<section id="q112-does-qwen2-5-omni-training-support-full-parameter-training-and-does-it-support-talker-training">
<h3>Q112: Does qwen2.5-omni training support full parameter training, and does it support talker training?<a class="headerlink" href="#q112-does-qwen2-5-omni-training-support-full-parameter-training-and-does-it-support-talker-training" title="Link to this heading">ïƒ</a></h3>
<p>Currently, it does not support talker training, only thinker.</p>
</section>
<section id="q113-can-sequence-parallel-be-enabled-at-the-same-time-as-the-liger-kernel">
<h3>Q113: Can sequence parallel be enabled at the same time as the liger kernel?<a class="headerlink" href="#q113-can-sequence-parallel-be-enabled-at-the-same-time-as-the-liger-kernel" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it can.</p>
</section>
<section id="q114-what-are-the-requirements-for-rm-and-policy-in-ppo-training">
<h3>Q114: What are the requirements for rm and policy in ppo training?<a class="headerlink" href="#q114-what-are-the-requirements-for-rm-and-policy-in-ppo-training" title="Link to this heading">ïƒ</a></h3>
<p>PPO currently only supports rm and policy being from the same model series (tokenizer/template).</p>
</section>
<section id="q115-i-want-to-use-the-3-2-1b-model-for-fine-tuning-because-llama3-1-doesn-t-have-models-smaller-than-8b-can-i-still-use-the-llama-3-1-reward-model">
<h3>Q115: I want to use the 3.2 1B model for fine-tuning because llama3.1 doesn't have models smaller than 8B. Can I still use the Llama-3.1 reward model?<a class="headerlink" href="#q115-i-want-to-use-the-3-2-1b-model-for-fine-tuning-because-llama3-1-doesn-t-have-models-smaller-than-8b-can-i-still-use-the-llama-3-1-reward-model" title="Link to this heading">ïƒ</a></h3>
<p>The requirement is that template and tokenizer must be the same, so 3.1 and 3.2 should be fine.</p>
</section>
<section id="q116-can-swift-cache-a-mapped-version-of-data-for-troubleshooting-training-data-issues">
<h3>Q116: Can swift cache a mapped version of data for troubleshooting training data issues?<a class="headerlink" href="#q116-can-swift-cache-a-mapped-version-of-data-for-troubleshooting-training-data-issues" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--load_from_cache_file</span> <span class="pre">false</span></code>.</p>
</section>
<section id="q117-why-is-there-a-warning-none-of-the-inputs-have-requires-grad-true-during-full-parameter-training">
<h3>Q117: Why is there a warning: none of the inputs have requires_grad=True during full parameter training?<a class="headerlink" href="#q117-why-is-there-a-warning-none-of-the-inputs-have-requires-grad-true-during-full-parameter-training" title="Link to this heading">ïƒ</a></h3>
<p>If vit is not being trained, getting this warning is normal; if it is being trained, then it should not occur.</p>
</section>
<section id="q118-does-qwen2-5vl-ulysses-currently-support-sdpa">
<h3>Q118: Does qwen2.5vl ulysses currently support sdpa?<a class="headerlink" href="#q118-does-qwen2-5vl-ulysses-currently-support-sdpa" title="Link to this heading">ïƒ</a></h3>
<p>The vl model currently only supports flash-attn, but both are supported for pure text.</p>
</section>
<section id="q119-is-the-image-list-format-for-videos-now-supported-the-format-is-as-follows">
<h3>Q119: Is the image list format for videos now supported? The format is as follows:<a class="headerlink" href="#q119-is-the-image-list-format-for-videos-now-supported-the-format-is-as-follows" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;video&gt;æ˜¯ä¸€åªç‹®å­åœ¨è·‘æ­¥&quot;</span><span class="p">}],</span><span class="w"> </span><span class="nt">&quot;videos&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[[</span><span class="s2">&quot;1.jpg&quot;</span><span class="p">,</span><span class="s2">&quot;2.jpg&quot;</span><span class="p">]]}</span>
</pre></div>
</div>
<p>It is supported, using the file directory method.</p>
</section>
<section id="q120-in-the-grpo-script-does-save-steps-refer-to-step-or-global-step-the-local-training-shows-a-global-step-of-18-while-wandb-shows-a-step-of-628">
<h3>Q120: In the grpo script, does save_steps refer to step or global step? The local training shows a global step of 18, while wandb shows a step of 628.<a class="headerlink" href="#q120-in-the-grpo-script-does-save-steps-refer-to-step-or-global-step-the-local-training-shows-a-global-step-of-18-while-wandb-shows-a-step-of-628" title="Link to this heading">ïƒ</a></h3>
<p>It refers to global_step, as shown by local tqdm.</p>
</section>
<section id="q121-can-use-logits-to-keep-be-used-on-large-multimodal-models-now">
<h3>Q121: Can use_logits_to_keep be used on large multimodal models now?<a class="headerlink" href="#q121-can-use-logits-to-keep-be-used-on-large-multimodal-models-now" title="Link to this heading">ïƒ</a></h3>
<p>If the expansion of multimodal tokens occurs within the model's forward, it will cause an error.</p>
</section>
<section id="q122-why-does-memory-increase-significantly-multiple-times-during-training-even-after-50-or-100-steps">
<h3>Q122: Why does memory increase significantly multiple times during training, even after 50 or 100 steps?<a class="headerlink" href="#q122-why-does-memory-increase-significantly-multiple-times-during-training-even-after-50-or-100-steps" title="Link to this heading">ïƒ</a></h3>
<p>Set the environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_CUDA_ALLOC_CONF</span></code>, and check the torch documentation for details.</p>
</section>
<section id="q123-with-the-packing-cache-parameter-set-i-am-encountering-errors-when-training-on-multiple-machines-even-after-setting-the-folder-path-are-there-any-special-requirements">
<h3>Q123: With the packing_cache parameter set, I am encountering errors when training on multiple machines, even after setting the folder path. Are there any special requirements?<a class="headerlink" href="#q123-with-the-packing-cache-parameter-set-i-am-encountering-errors-when-training-on-multiple-machines-even-after-setting-the-folder-path-are-there-any-special-requirements" title="Link to this heading">ïƒ</a></h3>
<p>The path must be set to a shared disk directory.</p>
</section>
<section id="q124-for-qwen3-are-there-differences-in-datasets-and-parameter-settings-between-non-thinking-and-thinking-modes">
<h3>Q124: For Qwen3, are there differences in datasets and parameter settings between non-thinking and thinking modes?<a class="headerlink" href="#q124-for-qwen3-are-there-differences-in-datasets-and-parameter-settings-between-non-thinking-and-thinking-modes" title="Link to this heading">ïƒ</a></h3>
<p>Check this <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/4030">issue</a>.</p>
</section>
<section id="q125-how-do-i-configure-resuming-training-from-a-checkpoint-in-megatron-swift">
<h3>Q125: How do I configure resuming training from a checkpoint in megatron-swift?<a class="headerlink" href="#q125-how-do-i-configure-resuming-training-from-a-checkpoint-in-megatron-swift" title="Link to this heading">ïƒ</a></h3>
<p>Configure <code class="docutils literal notranslate"><span class="pre">--mcore_model</span></code> to load the checkpoint. Additionally, configure these parameters as needed: <code class="docutils literal notranslate"><span class="pre">--finetune</span></code>, <code class="docutils literal notranslate"><span class="pre">--no_load_optim</span></code>, and <code class="docutils literal notranslate"><span class="pre">--no_load_rng</span></code>. For resuming LoRA training, configure --mcore_adapter; other settings are the same as for full-parameter training. For details, refer to the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Megatron-SWIFT/Command-line-parameters.html">megatron-swift command-line-parameters documentation</a>.</p>
</section>
<section id="q126-has-anyone-encountered-the-following-error-while-reproducing-kimi-vl-a3b-instruct">
<h3>Q126: Has anyone encountered the following error while reproducing Kimi-VL-A3B-Instruct?<a class="headerlink" href="#q126-has-anyone-encountered-the-following-error-while-reproducing-kimi-vl-a3b-instruct" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[rank7]:    File &quot;/root/.cache/huggingface/modules/transformers_modules/Kimi-VL-A3B-Instruet/modeling_kimi_v1.py&quot;, line 946, in forward
[rank7]:      assert not self.training
[rank7]:
[rank7]:    AssertionError
</pre></div>
</div>
<p>You need to change <code class="docutils literal notranslate"><span class="pre">topk_method</span></code> to <code class="docutils literal notranslate"><span class="pre">greedy</span></code> in <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p>
</section>
<section id="q127-when-training-qwenvl2-5-how-can-i-ensure-the-bounding-box-coordinates-are-correct-after-setting-max-pixels-how-does-swift-handle-this">
<h3>Q127: When training qwenvl2.5, how can I ensure the bounding box coordinates are correct after setting max_pixels? How does Swift handle this?<a class="headerlink" href="#q127-when-training-qwenvl2-5-how-can-i-ensure-the-bounding-box-coordinates-are-correct-after-setting-max-pixels-how-does-swift-handle-this" title="Link to this heading">ïƒ</a></h3>
<p>Swift saves the states before and after preprocessing and adjusts the bounding box accordingly. However, this adjustment is not performed during inference, so you need to manually preprocess the images in advance.</p>
</section>
<section id="q128-i-have-a-question-i-want-to-return-a-solution-field-in-my-dataset-i-ve-written-a-preprocessing-function-but-the-dataset-only-returns-messages-and-images-not-solution-how-can-i-fix-this">
<h3>Q128: I have a question. I want to return a solution field in my dataset. I've written a preprocessing function, but the dataset only returns messages and images, not solution. How can I fix this?<a class="headerlink" href="#q128-i-have-a-question-i-want-to-return-a-solution-field-in-my-dataset-i-ve-written-a-preprocessing-function-but-the-dataset-only-returns-messages-and-images-not-solution-how-can-i-fix-this" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--remove_unused_columns</span> <span class="pre">false</span></code>.</p>
</section>
<section id="q129-i-m-getting-an-error-when-merging-lora-parameters-my-current-peft-version-is-0-11-0-is-this-error-because-i-need-to-upgrade-it">
<h3>Q129: I'm getting an error when merging LoRA parameters. My current peft version is 0.11.0. Is this error because I need to upgrade it?<a class="headerlink" href="#q129-i-m-getting-an-error-when-merging-lora-parameters-my-current-peft-version-is-0-11-0-is-this-error-because-i-need-to-upgrade-it" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>File &quot;/opt/conda/lib/python3.9/site-packages/peft/config.py&quot;, line 118, in from_peft_type
  return config_cls(**kwargs)
TypeError: __init__() got an unexpected keyword argument &#39;corda_config&#39;
</pre></div>
</div>
<p>This is caused by a version mismatch in peft between the training and merging processes.</p>
</section>
<section id="q130-does-it-currently-support-multi-turn-dpo">
<h3>Q130: Does it currently support multi-turn DPO?<a class="headerlink" href="#q130-does-it-currently-support-multi-turn-dpo" title="Link to this heading">ïƒ</a></h3>
<p>No, it is not supported.</p>
</section>
<section id="q131-i-have-a-question-i-ve-run-the-dataset-registration-process-before-after-that-i-modified-the-action-preprocessor-code-but-the-data-seems-to-be-unchanged-as-if-my-modifications-were-ignored-what-should-i-do">
<h3>Q131: I have a question. I've run the dataset registration process before. After that, I modified the action_preprocessor code, but the data seems to be unchanged, as if my modifications were ignored. What should I do?<a class="headerlink" href="#q131-i-have-a-question-i-ve-run-the-dataset-registration-process-before-after-that-i-modified-the-action-preprocessor-code-but-the-data-seems-to-be-unchanged-as-if-my-modifications-were-ignored-what-should-i-do" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--load_from_cache_file</span> <span class="pre">false</span></code>.</p>
</section>
<section id="q132-is-there-a-conflict-between-sequence-parallel-size-and-a-custom-loss-function-i-added-some-debug-print-statements-in-my-custom-loss-but-after-enabling-sequence-parallel-size-nothing-is-printed-and-there-are-no-errors-the-sft-training-runs-normally-so-i-m-concerned-it-might-be-automatically-using-another-library">
<h3>Q132: Is there a conflict between sequence_parallel_size and a custom loss function? I added some debug print statements in my custom loss, but after enabling sequence_parallel_size, nothing is printed, and there are no errors. The SFT training runs normally, so I'm concerned it might be automatically using another library.<a class="headerlink" href="#q132-is-there-a-conflict-between-sequence-parallel-size-and-a-custom-loss-function-i-added-some-debug-print-statements-in-my-custom-loss-but-after-enabling-sequence-parallel-size-nothing-is-printed-and-there-are-no-errors-the-sft-training-runs-normally-so-i-m-concerned-it-might-be-automatically-using-another-library" title="Link to this heading">ïƒ</a></h3>
<p>Yes, they conflict. Sequence parallelism has its own customized loss implementation. You can modify it yourself <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/trainers/sequence_parallel/ulysses.py">here</a>.</p>
</section>
<section id="q133-in-swift-s-pt-pre-training-mode-when-an-image-token-is-passed-is-it-not-masked-meaning-it-contributes-to-the-loss-calculation">
<h3>Q133: In Swift's PT (pre-training) mode, when an <image> token is passed, is it not masked, meaning it contributes to the loss calculation?<a class="headerlink" href="#q133-in-swift-s-pt-pre-training-mode-when-an-image-token-is-passed-is-it-not-masked-meaning-it-contributes-to-the-loss-calculation" title="Link to this heading">ïƒ</a></h3>
<p>No, it does not contribute to the loss. You can verify this by checking the printed labels in the command-line logs.</p>
</section>
<section id="q134-i-have-a-question-about-multimodal-pre-training-with-packing-it-seems-my-gpu-memory-usage-increases-slightly-after-each-pytorch-allocator-cache-flushes-since-last-step-leading-to-an-oom-out-of-memory-error-after-many-steps-what-should-i-do">
<h3>Q134: I have a question about multimodal pre-training with packing. It seems my GPU memory usage increases slightly after each pytorch allocator cache flushes since last step, leading to an OOM (Out of Memory) error after many steps. What should I do?<a class="headerlink" href="#q134-i-have-a-question-about-multimodal-pre-training-with-packing-it-seems-my-gpu-memory-usage-increases-slightly-after-each-pytorch-allocator-cache-flushes-since-last-step-leading-to-an-oom-out-of-memory-error-after-many-steps-what-should-i-do" title="Link to this heading">ïƒ</a></h3>
<p>Add the environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True'</span></code>.</p>
</section>
<section id="q135-how-can-i-use-focal-loss-during-training-where-can-i-find-the-list-of-currently-supported-loss-types">
<h3>Q135: How can I use focal loss during training? Where can I find the list of currently supported loss types?<a class="headerlink" href="#q135-how-can-i-use-focal-loss-during-training-where-can-i-find-the-list-of-currently-supported-loss-types" title="Link to this heading">ïƒ</a></h3>
<p>You can add new losses <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/loss/mapping.py">here</a>.</p>
</section>
<section id="q136-when-pipeline-parallel-size-is-set-for-rollout-it-seems-that-the-world-size-value-cannot-be-retrieved-in-trl-and-vllm">
<h3>Q136: When pipeline_parallel_size is set for rollout, it seems that the world_size value cannot be retrieved in TRL and vLLM.<a class="headerlink" href="#q136-when-pipeline-parallel-size-is-set-for-rollout-it-seems-that-the-world-size-value-cannot-be-retrieved-in-trl-and-vllm" title="Link to this heading">ïƒ</a></h3>
<p>Rollout is likely incompatible with pipeline parallelism.</p>
</section>
<section id="q137-does-sft-for-qwen2audio-support-packing">
<h3>Q137: Does SFT for qwen2audio support packing?<a class="headerlink" href="#q137-does-sft-for-qwen2audio-support-packing" title="Link to this heading">ïƒ</a></h3>
<p>It is not supported.</p>
</section>
<section id="q138-does-gspo-training-support-the-top-entropy-quantile-parameter-after-setting-importance-sampling-level-sequence-can-we-still-optimize-for-the-top-x-of-tokens-based-on-the-entropy-distribution">
<h3>Q138: Does GSPO training support the top_entropy_quantile parameter? After setting --importance_sampling_level sequence, can we still optimize for the top x% of tokens based on the entropy distribution?<a class="headerlink" href="#q138-does-gspo-training-support-the-top-entropy-quantile-parameter-after-setting-importance-sampling-level-sequence-can-we-still-optimize-for-the-top-x-of-tokens-based-on-the-entropy-distribution" title="Link to this heading">ïƒ</a></h3>
<p>Yes. The process is to first calculate the loss normally (which is affected by importance_sampling_level), and then mask the loss according to top_entropy_quantile.</p>
</section>
<section id="q139-for-gkd-training-do-the-model-type-for-the-student-and-teacher-models-have-to-be-the-same-can-one-be-a-dense-model-and-the-other-an-moe-model">
<h3>Q139: For GKD training, do the model_type for the student and teacher models have to be the same? Can one be a dense model and the other an MoE model?<a class="headerlink" href="#q139-for-gkd-training-do-the-model-type-for-the-student-and-teacher-models-have-to-be-the-same-can-one-be-a-dense-model-and-the-other-an-moe-model" title="Link to this heading">ïƒ</a></h3>
<p>Yes, that's possible. The only requirement is that they share the same vocabulary. However, training with an MoE model will be slower.</p>
</section>
<section id="q140-on-devices-that-don-t-support-flash-attention-what-is-the-default-attention-implementation-the-documentation-states-the-default-is-none">
<h3>Q140: On devices that don't support Flash Attention, what is the default attention implementation? The documentation states the default is none.<a class="headerlink" href="#q140-on-devices-that-don-t-support-flash-attention-what-is-the-default-attention-implementation-the-documentation-states-the-default-is-none" title="Link to this heading">ïƒ</a></h3>
<p>It defaults to using sdpa.</p>
</section>
<section id="q141-is-the-freeze-parameters-ratio-calculated-starting-from-the-attention-layers">
<h3>Q141: Is the freeze_parameters_ratio calculated starting from the attention layers?<a class="headerlink" href="#q141-is-the-freeze-parameters-ratio-calculated-starting-from-the-attention-layers" title="Link to this heading">ïƒ</a></h3>
<p>It is calculated starting from the embedding layer. You can use it in combination with the trainable_parameters setting.</p>
</section>
<section id="q142-after-lora-fine-tuning-is-it-not-possible-to-subsequently-perform-dpo-or-grpo-training-on-the-same-adapter">
<h3>Q142: After LoRA fine-tuning, is it not possible to subsequently perform DPO or GRPO training on the same adapter?<a class="headerlink" href="#q142-after-lora-fine-tuning-is-it-not-possible-to-subsequently-perform-dpo-or-grpo-training-on-the-same-adapter" title="Link to this heading">ïƒ</a></h3>
<p>It is supported now. Please search for <code class="docutils literal notranslate"><span class="pre">--adapters</span></code> in the command-line-parameters documentation.</p>
</section>
<section id="q143-for-a-multi-modal-dataset-i-d-like-to-perform-dynamic-data-augmentation-after-the-data-is-loaded-such-as-randomly-adding-noise-which-parts-should-i-inherit-or-override-to-implement-custom-dynamic-data-augmentation">
<h3>Q143: For a multi-modal dataset, I'd like to perform dynamic data augmentation after the data is loaded, such as randomly adding noise. Which parts should I inherit or override to implement custom dynamic data augmentation?<a class="headerlink" href="#q143-for-a-multi-modal-dataset-i-d-like-to-perform-dynamic-data-augmentation-after-the-data-is-loaded-such-as-randomly-adding-noise-which-parts-should-i-inherit-or-override-to-implement-custom-dynamic-data-augmentation" title="Link to this heading">ïƒ</a></h3>
<p>Modify the encode method in the template.</p>
</section>
<section id="q144-does-ppo-training-support-gradient-clipping">
<h3>Q144: Does PPO training support gradient clipping?<a class="headerlink" href="#q144-does-ppo-training-support-gradient-clipping" title="Link to this heading">ïƒ</a></h3>
<p>It is not supported.</p>
</section>
<section id="q145-is-it-not-possible-to-enable-the-liger-kernel-and-padding-free-simultaneously-during-the-grpo-phase">
<h3>Q145: Is it not possible to enable the Liger kernel and padding-free simultaneously during the GRPO phase?<a class="headerlink" href="#q145-is-it-not-possible-to-enable-the-liger-kernel-and-padding-free-simultaneously-during-the-grpo-phase" title="Link to this heading">ïƒ</a></h3>
<p>That's correct. Enabling them together would require modifying the Liger GRPO loss implementation, which is located in the Liger kernel library and is difficult to change.</p>
</section>
<section id="q146-in-the-command-line-arguments-for-swift-sft-are-lora-training-and-the-trainable-parameters-argument-compatible-for-instance-i-want-to-use-lora-to-train-the-language-model-while-also-adding-a-score-head-to-the-final-layer-and-training-it">
<h3>Q146: In the command-line arguments for Swift SFT, are LoRA training and the --trainable_parameters argument compatible? For instance, I want to use LoRA to train the language model while also adding a score head to the final layer and training it.<a class="headerlink" href="#q146-in-the-command-line-arguments-for-swift-sft-are-lora-training-and-the-trainable-parameters-argument-compatible-for-instance-i-want-to-use-lora-to-train-the-language-model-while-also-adding-a-score-head-to-the-final-layer-and-training-it" title="Link to this heading">ïƒ</a></h3>
<p>They are incompatible. Use modules_to_save for this purpose.</p>
</section>
<section id="q147-during-multi-node-multi-gpu-training-is-it-normal-that-only-the-main-node-produces-logs">
<h3>Q147: During multi-node, multi-GPU training, is it normal that only the main node produces logs?<a class="headerlink" href="#q147-during-multi-node-multi-gpu-training-is-it-normal-that-only-the-main-node-produces-logs" title="Link to this heading">ïƒ</a></h3>
<p>Yes, that is normal.</p>
</section>
<section id="q148-can-swift-support-setting-a-minimum-learning-rate-i-feel-like-it-decreases-too-much-in-the-end">
<h3>Q148: Can Swift support setting a minimum learning rate? I feel like it decreases too much in the end.<a class="headerlink" href="#q148-can-swift-support-setting-a-minimum-learning-rate-i-feel-like-it-decreases-too-much-in-the-end" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it's possible. Use <code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span> <span class="pre">cosine_with_min_lr</span> <span class="pre">--lr_scheduler_kwargs</span> <span class="pre">'{&quot;min_lr&quot;:</span> <span class="pre">1e-6}'</span></code>.</p>
</section>
<section id="q149-i-ve-set-split-dataset-ratio-but-there-s-no-validation-process-for-the-validation-set-even-by-the-end-of-training-is-there-a-configuration-i-missed">
<h3>Q149: I've set split_dataset_ratio, but there's no validation process for the validation set even by the end of training. Is there a configuration I missed?<a class="headerlink" href="#q149-i-ve-set-split-dataset-ratio-but-there-s-no-validation-process-for-the-validation-set-even-by-the-end-of-training-is-there-a-configuration-i-missed" title="Link to this heading">ïƒ</a></h3>
<p>Streaming mode does not split the dataset for validation. You need to set a val_dataset.</p>
</section>
<section id="q150-does-grpo-support-channel-loss">
<h3>Q150: Does GRPO support channel_loss?<a class="headerlink" href="#q150-does-grpo-support-channel-loss" title="Link to this heading">ïƒ</a></h3>
<p>No, it doesn't.</p>
</section>
<section id="q151-is-there-a-way-to-pass-through-a-task-id-in-grpo-i-want-to-distinguish-between-different-tasks-in-the-training-set">
<h3>Q151: Is there a way to pass through a task_id in GRPO? I want to distinguish between different tasks in the training set.<a class="headerlink" href="#q151-is-there-a-way-to-pass-through-a-task-id-in-grpo-i-want-to-distinguish-between-different-tasks-in-the-training-set" title="Link to this heading">ïƒ</a></h3>
<p>See <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/DeveloperGuide/multi_task.html">Multi-task Training</a>.</p>
</section>
<section id="q152-is-it-currently-supported-to-configure-grpo-and-sft-using-a-yaml-file">
<h3>Q152: Is it currently supported to configure GRPO and SFT using a YAML file?<a class="headerlink" href="#q152-is-it-currently-supported-to-configure-grpo-and-sft-using-a-yaml-file" title="Link to this heading">ïƒ</a></h3>
<p>Yes, both are supported. The configuration is directly processed into command-line arguments in main.py.</p>
</section>
<section id="q153-does-swift-support-multi-node-distributed-training">
<h3>Q153: Does Swift support multi-node distributed training?<a class="headerlink" href="#q153-does-swift-support-multi-node-distributed-training" title="Link to this heading">ïƒ</a></h3>
<p>Refer to the <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node">example</a> here.</p>
</section>
<section id="q154-when-fine-tuning-a-vlm-with-several-tasks-together-the-video-sampling-rules-are-different-for-each-task-does-ms-swift-support-this-and-where-can-i-configure-it">
<h3>Q154: When fine-tuning a VLM with several tasks together, the video sampling rules are different for each task. Does MS-Swift support this? And where can I configure it?<a class="headerlink" href="#q154-when-fine-tuning-a-vlm-with-several-tasks-together-the-video-sampling-rules-are-different-for-each-task-does-ms-swift-support-this-and-where-can-i-configure-it" title="Link to this heading">ïƒ</a></h3>
<p>Check the interleave_prob parameter in the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command-line Parameters Documentation</a>.</p>
</section>
<section id="q155-excuse-me-does-gkd-currently-support-using-different-tokenizers-for-the-teacher-and-student-models">
<h3>Q155: Excuse me, does GKD currently support using different tokenizers for the teacher and student models?<a class="headerlink" href="#q155-excuse-me-does-gkd-currently-support-using-different-tokenizers-for-the-teacher-and-student-models" title="Link to this heading">ïƒ</a></h3>
<p>No, it doesn't.</p>
</section>
<section id="q156-excuse-me-are-use-liger-kernel-and-log-entropy-currently-not-supported-to-be-used-together">
<h3>Q156: Excuse me, are use_liger_kernel and log_entropy currently not supported to be used together?<a class="headerlink" href="#q156-excuse-me-are-use-liger-kernel-and-log-entropy-currently-not-supported-to-be-used-together" title="Link to this heading">ïƒ</a></h3>
<p>They are not supported together.</p>
</section>
<section id="q157-when-training-grpo-why-isn-t-there-an-entropy-curve-during-monitoring">
<h3>Q157: When training GRPO, why isn't there an entropy curve during monitoring?<a class="headerlink" href="#q157-when-training-grpo-why-isn-t-there-an-entropy-curve-during-monitoring" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--log_entropy</span> <span class="pre">true</span></code>. Calculating entropy incurs some extra overhead, so it's not logged by default.</p>
</section>
<section id="q158-in-swift-where-is-the-location-in-the-code-that-reads-an-image-and-converts-it-into-a-tensor">
<h3>Q158: In Swift, where is the location in the code that reads an image and converts it into a tensor?<a class="headerlink" href="#q158-in-swift-where-is-the-location-in-the-code-that-reads-an-image-and-converts-it-into-a-tensor" title="Link to this heading">ïƒ</a></h3>
<p>In the _encode method of the Template class.</p>
</section>
<section id="q159-i-have-a-question-my-original-dataset-has-question-and-answer-columns-after-mapping-them-via-the-command-line-with-columns-question-query-answer-response-the-reward-function-reports-a-column-not-found-error-whether-i-use-answer-or-response-how-can-i-pass-the-dataset-columns-through-to-the-reward-function">
<h3>Q159: I have a question. My original dataset has 'question' and 'answer' columns. After mapping them via the command line with --columns {&quot;question&quot;: &quot;query&quot;, &quot;answer&quot;: &quot;response&quot;}, the reward function reports a 'column not found' error whether I use 'answer' or 'response'. How can I pass the dataset columns through to the reward function?<a class="headerlink" href="#q159-i-have-a-question-my-original-dataset-has-question-and-answer-columns-after-mapping-them-via-the-command-line-with-columns-question-query-answer-response-the-reward-function-reports-a-column-not-found-error-whether-i-use-answer-or-response-how-can-i-pass-the-dataset-columns-through-to-the-reward-function" title="Link to this heading">ïƒ</a></h3>
<p>These are reserved column names. Please use different column names.</p>
</section>
<section id="q160-i-have-a-question-about-fine-tuning-the-qwen3-30b-a3b-moe-model-with-lora-in-ms-swift-the-aux-loss-barely-changes-even-when-i-set-aux-loss-coef-to-1">
<h3>Q160: I have a question about fine-tuning the qwen3-30b-a3b MoE model with LoRA in MS-Swift. The aux_loss barely changes, even when I set aux_loss_coef to 1.<a class="headerlink" href="#q160-i-have-a-question-about-fine-tuning-the-qwen3-30b-a3b-moe-model-with-lora-in-ms-swift-the-aux-loss-barely-changes-even-when-i-set-aux-loss-coef-to-1" title="Link to this heading">ïƒ</a></h3>
<p>Add all-router to target_modules as well.</p>
</section>
<section id="q161-with-the-script-below-is-it-possible-to-save-checkpoints-per-epoch">
<h3>Q161: With the script below, is it possible to save checkpoints per epoch?<a class="headerlink" href="#q161-with-the-script-below-is-it-possible-to-save-checkpoints-per-epoch" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>megatron<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mcore_model<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$MODEL_PATH</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$DATA_PATH</span><span class="s2">&quot;</span><span class="w">  </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_alpha<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--sequence_parallel<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--micro_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--global_batch_size<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_granularity<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_method<span class="w"> </span>uniform<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_num_layers<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--attention_backend<span class="w"> </span>flash<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--sequence_parallel<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cross_entropy_loss_fusion<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_warmup_fraction<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--min_lr<span class="w"> </span>1e-5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$OUTPUT_PATH</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--split_dataset_ratio<span class="w"> </span><span class="m">0</span>.02<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">25</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--finetune<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_load_rng<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_load_optim<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_optim<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_rng<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_num_proc<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_author<span class="w"> </span>swift<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name<span class="w"> </span>swift-robot
</pre></div>
</div>
<p>Saving checkpoints per epoch is not yet supported.</p>
</section>
<section id="q162-i-encountered-this-error-how-can-i-fix-it-installing-apex-didn-t-help">
<h3>Q162: I encountered this error. How can I fix it? Installing Apex didn't help.<a class="headerlink" href="#q162-i-encountered-this-error-how-can-i-fix-it-installing-apex-didn-t-help" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: ColumnParallelLinear was called with gradient_accumulation_fusion set to True but the custom CUDA extension fused_weight_gradient_mlp_cuda module is not found. To use gradient_accumulation_fusion you must install APEX with --cpp_ext and --cuda_ext. For example: pip install --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext .&quot; Note that the extension requires CUDA&gt;=11. Otherwise, you must turn off gradient accumulation fusion.
</pre></div>
</div>
<p>Set <code class="docutils literal notranslate"><span class="pre">--gradient_accumulation_fusion</span> <span class="pre">false</span></code>.</p>
</section>
<section id="q163-for-moe-lora-training-if-target-modules-is-set-to-all-linear-does-this-include-the-router-modules">
<h3>Q163: For MoE LoRA training, if target_modules is set to all-linear, does this include the router modules?<a class="headerlink" href="#q163-for-moe-lora-training-if-target-modules-is-set-to-all-linear-does-this-include-the-router-modules" title="Link to this heading">ïƒ</a></h3>
<p>It depends on whether the gate is implemented as nn.Linear. If it's an nn.Parameter, it won't be trained. For details, see the command-line parameter <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#tuner-arguments">target_parameters</a>.</p>
</section>
<section id="q164-for-grpo-training-does-the-colocate-mode-not-support-use-async-engine">
<h3>Q164: For GRPO training, does the colocate mode not support use_async_engine?<a class="headerlink" href="#q164-for-grpo-training-does-the-colocate-mode-not-support-use-async-engine" title="Link to this heading">ïƒ</a></h3>
<p>No, it doesn't.</p>
</section>
<section id="q165-can-a-model-trained-with-qlora-be-merged">
<h3>Q165: Can a model trained with QLoRA be merged?<a class="headerlink" href="#q165-can-a-model-trained-with-qlora-be-merged" title="Link to this heading">ïƒ</a></h3>
<p>Refer to the <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora">QLoRA example</a>.</p>
</section>
</section>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Link to this heading">ïƒ</a></h2>
<section id="q1-is-there-documentation-for-swift-inference">
<h3>Q1: Is there documentation for Swift inference?<a class="headerlink" href="#q1-is-there-documentation-for-swift-inference" title="Link to this heading">ïƒ</a></h3>
<p>Swift supports inference via Python scripts, command line, and UI interface. See the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Inference-and-deployment.html">Inference and Deployment</a>.</p>
</section>
<section id="q2-how-to-use-the-trained-model-for-inference-with-a-dataset">
<h3>Q2: How to use the trained model for inference with a dataset?<a class="headerlink" href="#q2-how-to-use-the-trained-model-for-inference-with-a-dataset" title="Link to this heading">ïƒ</a></h3>
<p>Use the parameters <code class="docutils literal notranslate"><span class="pre">--load_data_args</span> <span class="pre">true</span></code> or <code class="docutils literal notranslate"><span class="pre">--val_dataset</span> <span class="pre">&lt;your-val-dataset&gt;</span></code>. Refer to the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q3-can-i-specify-a-locally-saved-model-during-swift-inference">
<h3>Q3: Can I specify a locally saved model during Swift inference?<a class="headerlink" href="#q3-can-i-specify-a-locally-saved-model-during-swift-inference" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--model</span></code> to the local path. See <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q4-how-do-i-infer-on-a-dataset-without-labels-i-see-that-the-dataset-format-in-the-documentation-is-all-for-the-training-set">
<h3>Q4: How do I infer on a dataset without labels? I see that the dataset format in the documentation is all for the training set.<a class="headerlink" href="#q4-how-do-i-infer-on-a-dataset-without-labels-i-see-that-the-dataset-format-in-the-documentation-is-all-for-the-training-set" title="Link to this heading">ïƒ</a></h3>
<p>Configure the parameter <code class="docutils literal notranslate"><span class="pre">--val_dataset</span> <span class="pre">&lt;your-val-dataset&gt;</span></code>.</p>
</section>
<section id="q5-how-to-resolve-the-error-valueerror-input-length-of-input-ids-is-35-but-max-length-is-set-to-20">
<h3>Q5: How to resolve the error <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Input</span> <span class="pre">length</span> <span class="pre">of</span> <span class="pre">input_ids</span> <span class="pre">is</span> <span class="pre">35,</span> <span class="pre">but</span> <span class="pre">max_length</span> <span class="pre">is</span> <span class="pre">set</span> <span class="pre">to</span> <span class="pre">20</span></code>?<a class="headerlink" href="#q5-how-to-resolve-the-error-valueerror-input-length-of-input-ids-is-35-but-max-length-is-set-to-20" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>raise ValueError(
ValueError: Input length of input_ids is 35, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
</pre></div>
</div>
<p>Set <code class="docutils literal notranslate"><span class="pre">model.generation_config.max_new_tokens</span></code>.</p>
</section>
<section id="q6-qwen2-vl-inference-training-runs-out-of-memory">
<h3>Q6: qwen2-vl inference (training) runs out of memory<a class="headerlink" href="#q6-qwen2-vl-inference-training-runs-out-of-memory" title="Link to this heading">ïƒ</a></h3>
<p>Set the command line parameter <code class="docutils literal notranslate"><span class="pre">--max_pixels</span> <span class="pre">xxx</span></code>, environment variable <code class="docutils literal notranslate"><span class="pre">MAX_PIXELS=xxx</span></code>, or specific model parameter <code class="docutils literal notranslate"><span class="pre">--model_kwargs</span> <span class="pre">'{&quot;max_pixels&quot;:</span> <span class="pre">xxx}'</span></code>. Note that the environment variable only takes effect for the corresponding models in the documentation. For more details, please refer to the documentation <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#specific-model-arguments">Specific Model Parameters</a>.</p>
</section>
<section id="q7-on-a-v100-gpu-in-a-python-virtual-environment-following-the-environment-setup-instructions-from-https-swift2x-readthedocs-io-zh-cn-latest-multi-modal-qwen2-vl-e6-9c-80-e4-bd-b3-e5-ae-9e-e8-b7-b5-html-when-testing-the-inference-command-cuda-visible-devices-0-1-2-3-swift-infer-model-type-qwen2-vl-7b-instruct-an-error-occurs-runtimeerror-probability-tensor-contains-either-inf-nan-or-element-0">
<h3>Q7: On a V100 GPU, in a Python virtual environment, following the environment setup instructions from https://swift2x.readthedocs.io/zh-cn/latest/Multi-Modal/qwen2-vl%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html, when testing the inference command: <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES=0,1,2,3</span> <span class="pre">swift</span> <span class="pre">infer</span> <span class="pre">--model_type</span> <span class="pre">qwen2-vl-7b-instruct</span></code>, an error occurs: <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">probability</span> <span class="pre">tensor</span> <span class="pre">contains</span> <span class="pre">either</span> <span class="pre">'inf',</span> <span class="pre">'nan'</span> <span class="pre">or</span> <span class="pre">element</span> <span class="pre">&lt;</span> <span class="pre">0</span></code>.<a class="headerlink" href="#q7-on-a-v100-gpu-in-a-python-virtual-environment-following-the-environment-setup-instructions-from-https-swift2x-readthedocs-io-zh-cn-latest-multi-modal-qwen2-vl-e6-9c-80-e4-bd-b3-e5-ae-9e-e8-b7-b5-html-when-testing-the-inference-command-cuda-visible-devices-0-1-2-3-swift-infer-model-type-qwen2-vl-7b-instruct-an-error-occurs-runtimeerror-probability-tensor-contains-either-inf-nan-or-element-0" title="Link to this heading">ïƒ</a></h3>
<p>Try inference on A10 or 3090 machines.</p>
</section>
<section id="q8-after-running-the-prediction-command-where-are-the-results-saved-cuda-visible-devices-0-swift-infer-ckpt-dir-output-glm4v-9b-chat-vx-xxx-checkpoint-xxx-merged-load-data-args-true">
<h3>Q8: After running the prediction command, where are the results saved? CUDA_VISIBLE_DEVICES=0 swift infer --ckpt_dir output/glm4v-9b-chat/vx-xxx/checkpoint-xxx-merged --load_data_args true<a class="headerlink" href="#q8-after-running-the-prediction-command-where-are-the-results-saved-cuda-visible-devices-0-swift-infer-ckpt-dir-output-glm4v-9b-chat-vx-xxx-checkpoint-xxx-merged-load-data-args-true" title="Link to this heading">ïƒ</a></h3>
<p>Results will be printed in the log.</p>
</section>
<section id="q9-for-the-latest-version-of-swift-can-the-infer-command-output-probability-values-through-the-logprobs-parameter">
<h3>Q9: For the latest version of swift, can the infer command output probability values through the logprobs parameter?<a class="headerlink" href="#q9-for-the-latest-version-of-swift-can-the-infer-command-output-probability-values-through-the-logprobs-parameter" title="Link to this heading">ïƒ</a></h3>
<p>Yes, logprobs can be output. For command line inference, set <code class="docutils literal notranslate"><span class="pre">--logprobs</span> <span class="pre">true</span></code>. For Python script inference, set <code class="docutils literal notranslate"><span class="pre">request_config</span> <span class="pre">=</span> <span class="pre">RequestConfig(...,</span> <span class="pre">logprobs=True,</span> <span class="pre">top_logprobs=2)</span></code>. Please refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/tests/infer/test_logprobs.py">test_logprobs.py</a>.</p>
</section>
<section id="q10-getting-the-error-assert-factor-in-rope-scaling-with-vllm">
<h3>Q10: Getting the error <code class="docutils literal notranslate"><span class="pre">assert</span> <span class="pre">factor</span> <span class="pre">in</span> <span class="pre">rope_scaling</span></code> with vllm?<a class="headerlink" href="#q10-getting-the-error-assert-factor-in-rope-scaling-with-vllm" title="Link to this heading">ïƒ</a></h3>
<p>For more details, see qwen2-vl <a class="reference external" href="https://github.com/QwenLM/Qwen2.5-VL/issues/96">issue#96</a>.</p>
</section>
<section id="q11-does-vllm-require-the-models-to-be-merged-before-calling-them-during-inference">
<h3>Q11: Does vllm require the models to be merged before calling them during inference?<a class="headerlink" href="#q11-does-vllm-require-the-models-to-be-merged-before-calling-them-during-inference" title="Link to this heading">ïƒ</a></h3>
<p>Models do not have to be merged. See the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html">Command Line Parameters</a>.</p>
</section>
<section id="q12-how-to-use-cpu-when-performing-inference-with-python-scripts">
<h3>Q12: How to use CPU when performing inference with Python scripts?<a class="headerlink" href="#q12-how-to-use-cpu-when-performing-inference-with-python-scripts" title="Link to this heading">ïƒ</a></h3>
<p>Set the environment variable: <code class="docutils literal notranslate"><span class="pre">os.environ['CUDA_VISIBLE_DEVICES']</span> <span class="pre">=</span> <span class="pre">'-1'</span></code>.</p>
</section>
<section id="q13-has-anyone-encountered-the-error-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16">
<h3>Q13: Has anyone encountered the error <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">&quot;triu_tril_cuda_template&quot;</span> <span class="pre">not</span> <span class="pre">implemented</span> <span class="pre">for'BFloat16'</span></code>?<a class="headerlink" href="#q13-has-anyone-encountered-the-error-runtimeerror-triu-tril-cuda-template-not-implemented-for-bfloat16" title="Link to this heading">ïƒ</a></h3>
<p>Upgrade Torch, as the current version may not have implemented this operator.</p>
</section>
<section id="q14-does-qwen2-audio-support-streaming-inference">
<h3>Q14: Does qwen2-audio support streaming inference?<a class="headerlink" href="#q14-does-qwen2-audio-support-streaming-inference" title="Link to this heading">ïƒ</a></h3>
<p>Yes, see the <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/1653">issue</a>.</p>
</section>
<section id="q15-where-to-set-do-sample-for-multi-modal-inference-using-inference-client">
<h3>Q15: Where to set <code class="docutils literal notranslate"><span class="pre">do_sample</span></code> for multi-modal inference using inference client?<a class="headerlink" href="#q15-where-to-set-do-sample-for-multi-modal-inference-using-inference-client" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">temperature=0</span></code>.</p>
</section>
<section id="q16-does-ms-swift-support-batch-processing-for-large-models">
<h3>Q16: Does ms-swift support batch processing for large models?<a class="headerlink" href="#q16-does-ms-swift-support-batch-processing-for-large-models" title="Link to this heading">ïƒ</a></h3>
<p>Supported.  See the <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo.py">demo</a>.</p>
</section>
<section id="q17-when-quantizing-models-with-ms-swift-there-is-an-insufficient-memory-display-can-we-reduce-resource-usage-during-quantization-even-if-it-s-slower">
<h3>Q17: When quantizing models with ms-swift, there is an insufficient memory display. Can we reduce resource usage during quantization, even if it's slower?<a class="headerlink" href="#q17-when-quantizing-models-with-ms-swift-there-is-an-insufficient-memory-display-can-we-reduce-resource-usage-during-quantization-even-if-it-s-slower" title="Link to this heading">ïƒ</a></h3>
<p>Try setting <code class="docutils literal notranslate"><span class="pre">--device_map</span> <span class="pre">cpu</span></code>.</p>
</section>
<section id="q18-does-swift-support-quantization-for-multi-modal-models">
<h3>Q18: Does Swift support quantization for multi-modal models?<a class="headerlink" href="#q18-does-swift-support-quantization-for-multi-modal-models" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it supports quantization.</p>
</section>
<section id="q19-encountering-the-following-error-while-using-gptq-what-is-the-cause">
<h3>Q19: Encountering the following error while using GPTQ, what is the cause?<a class="headerlink" href="#q19-encountering-the-following-error-while-using-gptq-what-is-the-cause" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>if llm_config[&#39;architectures&#39;][0] == &#39;LlamaForCausalLM&#39;:
KeyError: &#39;architectures&#39;
</pre></div>
</div>
<p>Try using <code class="docutils literal notranslate"><span class="pre">transformers==4.44.*</span></code>.</p>
</section>
<section id="q20-how-can-i-specify-where-to-save-evaluation-results-during-swift-infer-i-can-t-find-where-the-results-are-saved">
<h3>Q20: How can I specify where to save evaluation results during swift infer? I can't find where the results are saved.<a class="headerlink" href="#q20-how-can-i-specify-where-to-save-evaluation-results-during-swift-infer-i-can-t-find-where-the-results-are-saved" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--result_path</span> <span class="pre">your_path</span></code>. See <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/arguments/infer_args.py">InferArguments</a>.</p>
</section>
<section id="q21-i-get-an-error-while-using-awq-quantized-yi-vl-6b">
<h3>Q21: I get an error while using AWQ quantized yi-vl-6b:<a class="headerlink" href="#q21-i-get-an-error-while-using-awq-quantized-yi-vl-6b" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: swift.llm.utils.model.get_model_tokenizer_with_flash_attn() got multiple values for keyword argument &#39;automodel_class&#39;.
</pre></div>
</div>
<p>Please use GPTQ quantization.</p>
</section>
<section id="q22-i-would-like-to-ask-about-using-swift-export-to-perform-gptq-int4-quantization-on-the-qwen2-5-72b-model-with-a-max-model-length-of-32768-which-is-the-default-value-the-calibration-dataset-provided-has-128-samples-but-an-error-occurred-during-quantization-the-error-log-is-factorization-could-not-be-completed-because-the-input-is-not-positive-definite-the-leading-minor-of-order-18145-is-not-positive-definite-what-is-the-cause">
<h3>Q22: I would like to ask about using swift export to perform GPTQ INT4 quantization on the qwen2.5 72B model with a max model length of 32768, which is the default value. The calibration dataset provided has 128 samples, but an error occurred during quantization. The error log is: &quot;factorization could not be completed because the input is not positive-definite (the leading minor of order 18145 is not positive-definite).&quot; What is the cause?<a class="headerlink" href="#q22-i-would-like-to-ask-about-using-swift-export-to-perform-gptq-int4-quantization-on-the-qwen2-5-72b-model-with-a-max-model-length-of-32768-which-is-the-default-value-the-calibration-dataset-provided-has-128-samples-but-an-error-occurred-during-quantization-the-error-log-is-factorization-could-not-be-completed-because-the-input-is-not-positive-definite-the-leading-minor-of-order-18145-is-not-positive-definite-what-is-the-cause" title="Link to this heading">ïƒ</a></h3>
<p>This indicates a problem with the Hessian matrix being non-positive definite. Try using a different dataset.</p>
</section>
<section id="q23-can-batch-inference-only-be-done-through-custom-code-can-t-it-be-done-like-sft-with-script-parameters">
<h3>Q23: Can batch inference only be done through custom code? Can't it be done like SFT with script parameters?<a class="headerlink" href="#q23-can-batch-inference-only-be-done-through-custom-code-can-t-it-be-done-like-sft-with-script-parameters" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it can be done using <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span> <span class="pre">--val_dataset</span> <span class="pre">xxx</span> <span class="pre">--max_batch_size</span> <span class="pre">16</span> <span class="pre">...</span></code>.</p>
</section>
<section id="q24-what-s-the-default-temperature-value-when-using-swift-app-for-inference">
<h3>Q24: What's the default temperature value when using swift app for inference?<a class="headerlink" href="#q24-what-s-the-default-temperature-value-when-using-swift-app-for-inference" title="Link to this heading">ïƒ</a></h3>
<p>It's read from <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code> by default.</p>
</section>
<section id="q25-can-export-and-quantization-be-done-using-multiple-gpus">
<h3>Q25: Can export and quantization be done using multiple GPUs?<a class="headerlink" href="#q25-can-export-and-quantization-be-done-using-multiple-gpus" title="Link to this heading">ïƒ</a></h3>
<p>Model loading can use multiple GPUs, but quantization is single-GPU only.</p>
</section>
<section id="q26-when-using-swift-export-with-a-custom-template-type-does-it-permanently-change-the-template-type-if-we-use-swift-export-template-type-custom-does-it-change-the-model-s-template">
<h3>Q26: When using swift export with a custom template_type, does it permanently change the template_type? If we use swift export --template_type custom, does it change the model's template?<a class="headerlink" href="#q26-when-using-swift-export-with-a-custom-template-type-does-it-permanently-change-the-template-type-if-we-use-swift-export-template-type-custom-does-it-change-the-model-s-template" title="Link to this heading">ïƒ</a></h3>
<p>No, it won't be modified. Templates in swift are defined internally, not saved in jinja format.</p>
</section>
<section id="q27-awq-quantization-for-qwen2vl-gives-error-typeerror-qwen2vlforconditionalgeneration-init-got-an-unexpected-keyword-argument-use-cache">
<h3>Q27: AWQ quantization for Qwen2VL gives error: TypeError: Qwen2VLForConditionalGeneration.init() got an unexpected keyword argument 'use_cache'<a class="headerlink" href="#q27-awq-quantization-for-qwen2vl-gives-error-typeerror-qwen2vlforconditionalgeneration-init-got-an-unexpected-keyword-argument-use-cache" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">gptq</span></code> quantization instead.</p>
</section>
<section id="q28-for-ddp-inference-does-max-batch-size-in-infer-refer-to-batch-size-per-gpu-or-total-batch-size">
<h3>Q28: For DDP inference, does max_batch_size in infer refer to batch size per GPU or total batch size?<a class="headerlink" href="#q28-for-ddp-inference-does-max-batch-size-in-infer-refer-to-batch-size-per-gpu-or-total-batch-size" title="Link to this heading">ïƒ</a></h3>
<p>It refers to batch size per GPU.</p>
</section>
<section id="q29-does-swift-inference-now-support-messages-format-input-it-seems-to-only-support-query-format-currently-the-answer-contains-part-of-the-prompt-how-should-i-modify-the-inference-to-complete-the-answer">
<h3>Q29: Does swift.inference now support messages format input? It seems to only support query format currently. The answer contains part of the prompt, how should I modify the inference to complete the answer?<a class="headerlink" href="#q29-does-swift-inference-now-support-messages-format-input-it-seems-to-only-support-query-format-currently-the-answer-contains-part-of-the-prompt-how-should-i-modify-the-inference-to-complete-the-answer" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&lt;system&gt;&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;query1&gt;&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;answer1, &quot;}]}
</pre></div>
</div>
<p>This is supported in swift3, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_agent.py">examples/infer/demo_agent</a>.</p>
</section>
<section id="q30-how-can-i-make-swift-infer-write-results-to-result-path-in-real-time-instead-of-writing-everything-at-once-at-the-end">
<h3>Q30: How can I make swift infer write results to result_path in real-time instead of writing everything at once at the end?<a class="headerlink" href="#q30-how-can-i-make-swift-infer-write-results-to-result-path-in-real-time-instead-of-writing-everything-at-once-at-the-end" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
--ckpt_dir<span class="w"> </span>model_dir<span class="w"> </span><span class="se">\</span>
--streaming<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
--val_dataset<span class="w"> </span>dataset.jsonl<span class="w"> </span><span class="se">\</span>
--result_path<span class="w"> </span>result.jsonl
</pre></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">--stream</span> <span class="pre">true</span></code>. This will write results one by one, but it's non-batch inference.</p>
</section>
<section id="q31-when-i-trained-and-did-inference-in-swift-it-worked-but-after-merge-lora-when-using-ollama-s-api-the-effect-disappeared">
<h3>Q31: When I trained and did inference in Swift it worked, but after merge_lora when using Ollama's API the effect disappeared.<a class="headerlink" href="#q31-when-i-trained-and-did-inference-in-swift-it-worked-but-after-merge-lora-when-using-ollama-s-api-the-effect-disappeared" title="Link to this heading">ïƒ</a></h3>
<p>Try loading with transformers, Swift's template is aligned with transformers.</p>
</section>
<section id="q32-which-parameter-should-i-set-if-i-need-to-continue-inference-under-a-specific-prefix-during-model-inference">
<h3>Q32: Which parameter should I set if I need to continue inference under a specific prefix during model inference?<a class="headerlink" href="#q32-which-parameter-should-i-set-if-i-need-to-continue-inference-under-a-specific-prefix-during-model-inference" title="Link to this heading">ïƒ</a></h3>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">--response_prefix</span></code>.</p>
</section>
<section id="q33-how-do-i-fix-this-error-that-keeps-appearing">
<h3>Q33: How do I fix this error that keeps appearing?<a class="headerlink" href="#q33-how-do-i-fix-this-error-that-keeps-appearing" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>File &quot;/mnt/workspace/swift/swift/1lm/dataset/preprocessor/core. py&quot;, line 69, in _check_messages raise
ValueError(f&#39;assistant_message; {assistant_message}&#39;)
ValueError: assistant_message: {&#39;role&#39; :&#39;assistant&#39;, &#39;content&#39;: &#39;&#39;}
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span>swift<span class="w"> </span>sft<span class="w"> </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-7B-Instruct<span class="w"> </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span>--dataset<span class="w"> </span>/mnt/workspace/data.json<span class="w"> </span>--deepspeed<span class="w"> </span>zero2<span class="w"> </span>--max_length<span class="w"> </span><span class="m">16384</span>
</pre></div>
</div>
<p>The assistant field in the dataset is empty. If this is for inference, delete this empty string because it will cause NaN during training and will be checked.</p>
</section>
<section id="q34-inference-error-importerror-cannot-import-name-shard-checkpoint-from-transformers-modeling-utils-usr-local-lib-python3-10-dist-packages-transformers-modeling-utils-py">
<h3>Q34: Inference error, ImportError: cannot import name 'shard_checkpoint' from 'transformers.modeling_utils' (/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py)<a class="headerlink" href="#q34-inference-error-importerror-cannot-import-name-shard-checkpoint-from-transformers-modeling-utils-usr-local-lib-python3-10-dist-packages-transformers-modeling-utils-py" title="Link to this heading">ïƒ</a></h3>
<p>Try uninstalling autoawq.</p>
</section>
<section id="q35-when-using-swift-sample-it-seems-that-batch-processing-is-not-supported-it-appears-to-sample-examples-one-by-one-in-a-loop-which-is-somewhat-slow">
<h3>Q35: When using swift sample, it seems that batch processing is not supported? It appears to sample examples one by one in a loop, which is somewhat slow.<a class="headerlink" href="#q35-when-using-swift-sample-it-seems-that-batch-processing-is-not-supported-it-appears-to-sample-examples-one-by-one-in-a-loop-which-is-somewhat-slow" title="Link to this heading">ïƒ</a></h3>
<p>There is a <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/rft/rft.py">script</a> that can use multiprocessing to split and sample the dataset.</p>
</section>
<section id="q36-does-swift-support-inference-for-embedding-models">
<h3>Q36: Does Swift support inference for embedding models?<a class="headerlink" href="#q36-does-swift-support-inference-for-embedding-models" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to the <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_embedding.py">example</a>.</p>
</section>
<section id="q37-does-the-swift-framework-support-model-or-tensor-parallelism-for-inference-there-is-no-oom-during-training-but-oom-occurs-during-inference">
<h3>Q37: Does the swift framework support model or tensor parallelism for inference? There is no OOM during training, but OOM occurs during inference.<a class="headerlink" href="#q37-does-the-swift-framework-support-model-or-tensor-parallelism-for-inference-there-is-no-oom-during-training-but-oom-occurs-during-inference" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>/path/to/checkpoint-xxx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--merge_lora<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>vllm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--load_data_args<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_gpu_memory_utilization<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_max_model_len<span class="w"> </span><span class="m">32768</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">15536</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_limit_mm_per_prompt<span class="w"> </span><span class="s1">&#39;{&quot;image&quot;: 8, &quot;video&quot;: 2}&#39;</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:368 &#39;invalid argument&#39;
</pre></div>
</div>
<p>Add the option <code class="docutils literal notranslate"><span class="pre">--disable_custom_all_reduce</span> <span class="pre">true</span></code>.</p>
</section>
<section id="q38-does-streaming-inference-support-ddp">
<h3>Q38: Does streaming inference support DDP?<a class="headerlink" href="#q38-does-streaming-inference-support-ddp" title="Link to this heading">ïƒ</a></h3>
<p>Streaming does not support DDP.</p>
</section>
<section id="q39-problem-encountered-during-ovis2-2b-inference">
<h3>Q39: Problem encountered during Ovis2-2B inference<a class="headerlink" href="#q39-problem-encountered-during-ovis2-2b-inference" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[rank1]: safetensors_rust.SafetensorError: Error while deserializing header:MetadataIncompleteBuffer
Downloading Model from https://www.modelscope.cn to directory:/mnt/workspace/.cache/modelscope/hub/models/AIDC-AI/Ovis2-2B
</pre></div>
</div>
<p>The model weights are corrupted.</p>
</section>
<section id="q40-i-have-a-question-how-do-i-set-up-greedy-search-during-grpo-training-the-documentation-states-that-for-inference-greedy-search-can-be-enabled-by-setting-temperature-0-however-applying-this-setting-during-training-leads-to-many-nans-when-calculating-logits-self-temperature-if-i-set-temperature-1-top-k-1-and-top-p-1-will-the-model-s-output-correspond-to-a-greedy-search-result">
<h3>Q40: I have a question: how do I set up greedy search during GRPO training? The documentation states that for inference, greedy search can be enabled by setting temperature=0. However, applying this setting during training leads to many NaNs when calculating logits / self.temperature. If I set temperature=1, top_k=1, and top_p=1, will the model's output correspond to a greedy search result?<a class="headerlink" href="#q40-i-have-a-question-how-do-i-set-up-greedy-search-during-grpo-training-the-documentation-states-that-for-inference-greedy-search-can-be-enabled-by-setting-temperature-0-however-applying-this-setting-during-training-leads-to-many-nans-when-calculating-logits-self-temperature-if-i-set-temperature-1-top-k-1-and-top-p-1-will-the-model-s-output-correspond-to-a-greedy-search-result" title="Link to this heading">ïƒ</a></h3>
<p>Setting top_k=1 is sufficient.</p>
</section>
<section id="q41-when-using-swift-for-quantization-which-componentsactivations-or-weightsare-quantized-by-the-gptq-awq-and-fp8-methods-respectively">
<h3>Q41: When using Swift for quantization, which componentsâ€”activations or weightsâ€”are quantized by the GPTQ, AWQ, and FP8 methods respectively?<a class="headerlink" href="#q41-when-using-swift-for-quantization-which-componentsactivations-or-weightsare-quantized-by-the-gptq-awq-and-fp8-methods-respectively" title="Link to this heading">ïƒ</a></h3>
<p>Only the weights.</p>
</section>
<section id="q42-when-using-ms-swift-for-inference-i-m-seeing-a-large-discrepancy-in-the-results-between-the-transformers-engine-and-the-vllm-engine-what-could-be-the-reason">
<h3>Q42: When using ms-swift for inference, I'm seeing a large discrepancy in the results between the Transformers engine and the vLLM engine. What could be the reason?<a class="headerlink" href="#q42-when-using-ms-swift-for-inference-i-m-seeing-a-large-discrepancy-in-the-results-between-the-transformers-engine-and-the-vllm-engine-what-could-be-the-reason" title="Link to this heading">ïƒ</a></h3>
<p>Check if the parameters are aligned. Also, note that there are inherent differences between the vLLM engine and the Transformers engine. The Transformers engine's inference is aligned with the standard Transformers library.</p>
</section>
<section id="q43-when-using-swift-for-qwen2-audio-inference-the-output-is-garbled-chaotic-what-could-be-the-potential-cause">
<h3>Q43: When using Swift for Qwen2-Audio inference, the output is garbled/chaotic. What could be the potential cause?<a class="headerlink" href="#q43-when-using-swift-for-qwen2-audio-inference-the-output-is-garbled-chaotic-what-could-be-the-potential-cause" title="Link to this heading">ïƒ</a></h3>
<p>Use transformers==4.48.</p>
</section>
<section id="q44-for-multi-gpu-inference-does-max-batch-size-represent-the-total-batch-size-summed-across-all-gpus">
<h3>Q44: For multi-GPU inference, does --max_batch_size represent the total batch size summed across all GPUs?<a class="headerlink" href="#q44-for-multi-gpu-inference-does-max-batch-size-represent-the-total-batch-size-summed-across-all-gpus" title="Link to this heading">ïƒ</a></h3>
<p>No, it's the batch size per GPU.</p>
</section>
<section id="q45-for-inference-can-i-only-pass-an-image-file-path-or-is-there-a-way-to-pass-an-image-object-directly">
<h3>Q45: For inference, can I only pass an image file path, or is there a way to pass an Image object directly?<a class="headerlink" href="#q45-for-inference-can-i-only-pass-an-image-file-path-or-is-there-a-way-to-pass-an-image-object-directly" title="Link to this heading">ïƒ</a></h3>
<p>You can pass a PIL.Image object.</p>
</section>
<section id="q46-a-model-trained-with-swift-cannot-be-loaded-via-modelscope-how-can-this-be-resolved">
<h3>Q46: A model trained with Swift cannot be loaded via ModelScope. How can this be resolved?<a class="headerlink" href="#q46-a-model-trained-with-swift-cannot-be-loaded-via-modelscope-how-can-this-be-resolved" title="Link to this heading">ïƒ</a></h3>
<p>For details, see <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/5440">issue#5440</a>. LoRA adapters trained with transformers==4.55.2 can no longer be loaded by versions older than 4.52.</p>
</section>
<section id="q47-is-there-an-example-script-that-supports-outputting-last-hidden-state-during-large-language-model-inference">
<h3>Q47: Is there an example script that supports outputting last_hidden_state during large language model inference?<a class="headerlink" href="#q47-is-there-an-example-script-that-supports-outputting-last-hidden-state-during-large-language-model-inference" title="Link to this heading">ïƒ</a></h3>
<p>No, but you can refer to the <code class="docutils literal notranslate"><span class="pre">_get_last_hidden_state</span></code> method in the GRPO trainer for implementation.</p>
</section>
<section id="q48-after-fine-tuning-an-adapter-using-init-weights-lora-ga-i-encounter-the-following-error-during-inference-how-can-i-resolve-it">
<h3>Q48: After fine-tuning an adapter using --init_weights lora-ga, I encounter the following error during inference. How can I resolve it?<a class="headerlink" href="#q48-after-fine-tuning-an-adapter-using-init-weights-lora-ga-i-encounter-the-following-error-during-inference-how-can-i-resolve-it" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>raise ValueError(f&quot;Unknown initialization {init_lora_weights=}&quot;) ValueError: Unknown initialization init_lora_weights=&#39;lora-ga&#39;
</pre></div>
</div>
<p>Use the model located in the converted folder inside your checkpoint directory.</p>
</section>
<section id="q49-does-the-swift-infer-command-support-distributed-inference-across-multiple-nodes">
<h3>Q49: Does the swift infer command support distributed inference across multiple nodes?<a class="headerlink" href="#q49-does-the-swift-infer-command-support-distributed-inference-across-multiple-nodes" title="Link to this heading">ïƒ</a></h3>
<p>It is supported if the model can be fully loaded onto a single node; in this case, you can use Kubernetes (k8s) to manage multiple instances. However, if the model itself is too large to fit on one node, then it is not supported.</p>
</section>
<section id="q50-how-can-i-calculate-metrics-like-acc-rouge-during-inference">
<h3>Q50: How can I calculate metrics like acc/rouge during inference?<a class="headerlink" href="#q50-how-can-i-calculate-metrics-like-acc-rouge-during-inference" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to the <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#inference-arguments">inference parameter metric</a>.</p>
</section>
<section id="q51-how-do-i-set-the-system-prompt-to-be-empty-i-removed-the-system-parameter-but-it-still-adds-a-default-system-prompt">
<h3>Q51: How do I set the system_prompt to be empty? I removed the --system parameter, but it still adds a default system prompt.<a class="headerlink" href="#q51-how-do-i-set-the-system-prompt-to-be-empty-i-removed-the-system-parameter-but-it-still-adds-a-default-system-prompt" title="Link to this heading">ïƒ</a></h3>
<p>Set it with <code class="docutils literal notranslate"><span class="pre">--system</span> <span class="pre">''</span></code>.</p>
</section>
<section id="q52-my-inference-data-has-an-extra-field-but-this-field-is-not-saved-in-the-inference-results-how-can-i-configure-it-to-save-these-extra-fields">
<h3>Q52: My inference data has an &quot;extra&quot; field, but this field is not saved in the inference results. How can I configure it to save these extra fields?<a class="headerlink" href="#q52-my-inference-data-has-an-extra-field-but-this-field-is-not-saved-in-the-inference-results-how-can-i-configure-it-to-save-these-extra-fields" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">--remove_unused_columns</span> <span class="pre">false</span></code>.</p>
</section>
</section>
<section id="deployment">
<h2>Deployment<a class="headerlink" href="#deployment" title="Link to this heading">ïƒ</a></h2>
<section id="q1-how-to-deploy-a-trained-model">
<h3>Q1: How to deploy a trained model?<a class="headerlink" href="#q1-how-to-deploy-a-trained-model" title="Link to this heading">ïƒ</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">deploy</span> <span class="pre">--adapters</span> <span class="pre">xxx</span></code>. Refer to the documentation on <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Inference-and-deployment.html">Inference and Deployment</a>.</p>
</section>
<section id="q2-how-to-use-vllm-for-multi-card-deployment">
<h3>Q2: How to use vllm for multi-card deployment?<a class="headerlink" href="#q2-how-to-use-vllm-for-multi-card-deployment" title="Link to this heading">ïƒ</a></h3>
<p>For details, see the <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy">example</a>.</p>
</section>
<section id="q3-how-can-clients-pass-images-during-vllm-deployment">
<h3>Q3: How can clients pass images during vllm deployment?<a class="headerlink" href="#q3-how-can-clients-pass-images-during-vllm-deployment" title="Link to this heading">ïƒ</a></h3>
<p>See <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client/mllm">client examples</a> for details..</p>
</section>
<section id="q4-i-have-a-question-about-deploying-qwen2-7b-and-using-it-with-a-client-when-calling-the-openai-api-should-i-use-client-completions-create-instead-of-client-chat-completions-create-but-when-using-qwen2-7b-instruct-q5-k-m-gguf-i-can-use-client-chat-completions-create-why-is-that">
<h3>Q4: I have a question about deploying qwen2-7b and using it with a client. When calling the OpenAI API, should I use <code class="docutils literal notranslate"><span class="pre">client.completions.create</span></code> instead of <code class="docutils literal notranslate"><span class="pre">client.chat.completions.create</span></code>, but when using <code class="docutils literal notranslate"><span class="pre">qwen2-7b-instruct-q5_k_m.gguf</span></code>, I can use <code class="docutils literal notranslate"><span class="pre">client.chat.completions.create</span></code>. Why is that?<a class="headerlink" href="#q4-i-have-a-question-about-deploying-qwen2-7b-and-using-it-with-a-client-when-calling-the-openai-api-should-i-use-client-completions-create-instead-of-client-chat-completions-create-but-when-using-qwen2-7b-instruct-q5-k-m-gguf-i-can-use-client-chat-completions-create-why-is-that" title="Link to this heading">ïƒ</a></h3>
<p>The base model can use <code class="docutils literal notranslate"><span class="pre">client.chat.completions.create</span></code>, but this is a compatibility behavior.</p>
</section>
<section id="q5-after-launching-the-server-with-swift-deploy-using-two-cards-when-i-exit-with-ctrl-c-there-is-always-a-python-process-that-continues-to-occupy-the-memory-of-one-card-is-this-a-normal-phenomenon">
<h3>Q5: After launching the server with swift deploy using two cards, when I exit with Ctrl+C, there is always a Python process that continues to occupy the memory of one card. Is this a normal phenomenon?<a class="headerlink" href="#q5-after-launching-the-server-with-swift-deploy-using-two-cards-when-i-exit-with-ctrl-c-there-is-always-a-python-process-that-continues-to-occupy-the-memory-of-one-card-is-this-a-normal-phenomenon" title="Link to this heading">ïƒ</a></h3>
<p>You may need to kill it. This is an issue with vllm.</p>
</section>
<section id="q6-where-to-check-if-a-model-supports-lmdeploy-or-vllm-acceleration">
<h3>Q6: Where to check if a model supports lmdeploy or vllm acceleration?<a class="headerlink" href="#q6-where-to-check-if-a-model-supports-lmdeploy-or-vllm-acceleration" title="Link to this heading">ïƒ</a></h3>
<p>Vllm and lmdeploy have their own range of supported models. Please check their respective official documentation to determine availability.</p>
</section>
<section id="q7-why-does-tongyi-qianwen-2-5-math-7b-instruct-sometimes-return-garbled-characters-when-using-vllm-deployment-using-vllm-to-deploy-fp16">
<h3>Q7: Why does Tongyi Qianwen 2.5-Math-7B-Instruct sometimes return garbled characters when using vllm deployment? Using vllm to deploy,fp16<a class="headerlink" href="#q7-why-does-tongyi-qianwen-2-5-math-7b-instruct-sometimes-return-garbled-characters-when-using-vllm-deployment-using-vllm-to-deploy-fp16" title="Link to this heading">ïƒ</a></h3>
<p>Try using bf16.</p>
</section>
<section id="q8-after-starting-the-swift-inference-service-how-can-i-set-configurations-like-temperature-interactively">
<h3>Q8: After starting the swift inference service, how can I set configurations like temperature interactively?<a class="headerlink" href="#q8-after-starting-the-swift-inference-service-how-can-i-set-configurations-like-temperature-interactively" title="Link to this heading">ïƒ</a></h3>
<p>Inference only has preset configurations at startup, while deployment can set defaults initially and allow overriding them later on the client side.</p>
</section>
<section id="q9-when-deploying-qwen2vl-model-locally-how-can-i-input-videos-during-inference-can-i-use-base64-how-to-call-video-with-curl">
<h3>Q9: When deploying qwen2vl model locally, how can I input videos during inference? Can I use base64? How to call video with curl?<a class="headerlink" href="#q9-when-deploying-qwen2vl-model-locally-how-can-i-input-videos-during-inference-can-i-use-base64-how-to-call-video-with-curl" title="Link to this heading">ïƒ</a></h3>
<p>base64, see <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client/mllm">mllm client example</a> for details.</p>
</section>
<section id="q10-when-deploying-qwen2-vl-i-encounter-an-error-about-the-vllm-version-not-being-correct">
<h3>Q10: When deploying qwen2-vl, I encounter an error about the vllm version not being correct?<a class="headerlink" href="#q10-when-deploying-qwen2-vl-i-encounter-an-error-about-the-vllm-version-not-being-correct" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Unrecognized keys in `rope_scaling`for &#39;rope_type&#39;=&#39;default&#39;: {&#39;mrope_section&#39;}
</pre></div>
</div>
<p>Refer to the <a class="reference external" href="https://github.com/QwenLM/Qwen2.5-VL/issues/209">issue</a>.</p>
</section>
<section id="q11-when-using-swift-deploy-for-inference-i-want-to-output-token-probabilities-i-added-logprobs-true-but-it-outputs-null-what-s-the-reason">
<h3>Q11: When using Swift deploy for inference, I want to output token probabilities. I added logprobs True, but it outputs null. What's the reason?<a class="headerlink" href="#q11-when-using-swift-deploy-for-inference-i-want-to-output-token-probabilities-i-added-logprobs-true-but-it-outputs-null-what-s-the-reason" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">RAY_memory_monitor_refresh_ms</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>nohup<span class="w"> </span>swift<span class="w"> </span>deploy<span class="w"> </span>--ckpt_dir<span class="w"> </span>/mnt/workspace/checkpoint_600<span class="w"> </span>--infer_backend<span class="w"> </span>vllm<span class="w"> </span>--logprobs<span class="w"> </span>True<span class="w"> </span>--load_data_args<span class="w"> </span><span class="nb">false</span><span class="w"> </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Parameters need to be passed from the client side, <code class="docutils literal notranslate"><span class="pre">request_config</span> <span class="pre">=</span> <span class="pre">RequestConfig(...,</span> <span class="pre">logprobs=True,</span> <span class="pre">top_logprobs=2)</span></code>.</p>
</section>
<section id="q12-can-we-set-request-timeout-time-for-swift3-0-deployment-inference-what-happens-if-the-image-url-is-invalid">
<h3>Q12: Can we set request timeout time for Swift3.0 deployment inference? What happens if the image URL is invalid?<a class="headerlink" href="#q12-can-we-set-request-timeout-time-for-swift3-0-deployment-inference-what-happens-if-the-image-url-is-invalid" title="Link to this heading">ïƒ</a></h3>
<p>You can set the <code class="docutils literal notranslate"><span class="pre">SWIFT_TIMEOUT</span></code> environment variable. Alternatively, you can pass parameters in <code class="docutils literal notranslate"><span class="pre">InferClient</span></code>.</p>
</section>
<section id="q13-why-can-t-i-get-streaming-generation-with-swift-deployed-models-i-ve-set-stream-to-true-on-both-server-and-client-side-but-it-s-still-not-streaming">
<h3>Q13: Why can't I get streaming generation with Swift deployed models? I've set stream to True on both server and client side, but it's still not streaming<a class="headerlink" href="#q13-why-can-t-i-get-streaming-generation-with-swift-deployed-models-i-ve-set-stream-to-true-on-both-server-and-client-side-but-it-s-still-not-streaming" title="Link to this heading">ïƒ</a></h3>
<p>It's controlled by the client side. Please check <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client">examples/deploy/client</a>.</p>
</section>
<section id="q14-after-deploying-a-multimodal-model-with-swift-is-there-an-example-of-passing-pil-image-from-the-client">
<h3>Q14: After deploying a multimodal model with Swift, is there an example of passing PIL.Image from the client?<a class="headerlink" href="#q14-after-deploying-a-multimodal-model-with-swift-is-there-an-example-of-passing-pil-image-from-the-client" title="Link to this heading">ïƒ</a></h3>
<p>Check this <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/deploy/client/mllm/openai_client.py">client example</a>.</p>
</section>
<section id="q15-when-deploying-which-parameter-should-be-set-to-output-multiple-results-in-a-single-response">
<h3>Q15: When deploying, which parameter should be set to output multiple results in a single response?<a class="headerlink" href="#q15-when-deploying-which-parameter-should-be-set-to-output-multiple-results-in-a-single-response" title="Link to this heading">ïƒ</a></h3>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">n</span></code> in <code class="docutils literal notranslate"><span class="pre">RequestConfig</span></code>.</p>
</section>
<section id="q16-when-deploying-using-swift-deploy-with-the-parameter-infer-backend-vllm-the-performance-was-nearly-10-points-worse-compared-to-deploying-directly-with-vllm-vllm-serve-does-anyone-know-the-reason-for-this">
<h3>Q16: When deploying using swift deploy with the parameter --infer_backend vllm, the performance was nearly 10 points worse compared to deploying directly with vllm: vllm serve. Does anyone know the reason for this?<a class="headerlink" href="#q16-when-deploying-using-swift-deploy-with-the-parameter-infer-backend-vllm-the-performance-was-nearly-10-points-worse-compared-to-deploying-directly-with-vllm-vllm-serve-does-anyone-know-the-reason-for-this" title="Link to this heading">ïƒ</a></h3>
<p>It's likely that the template did not match.</p>
</section>
<section id="q17-how-can-i-disable-the-deep-thinking-mode-of-qwem3-in-the-deployment-command">
<h3>Q17: How can I disable the deep thinking mode of qwem3 in the deployment command?<a class="headerlink" href="#q17-how-can-i-disable-the-deep-thinking-mode-of-qwem3-in-the-deployment-command" title="Link to this heading">ïƒ</a></h3>
<p>Check this <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/4030">issue</a>.</p>
</section>
<section id="q18-when-i-use-ms-swift-for-vllm-deployment-inference-it-is-much-slower-compared-to-native-vllm-is-this-a-problem-with-the-swift-framework">
<h3>Q18: When I use ms-swift for vllm deployment inference, it is much slower compared to native vllm. Is this a problem with the swift framework?<a class="headerlink" href="#q18-when-i-use-ms-swift-for-vllm-deployment-inference-it-is-much-slower-compared-to-native-vllm-is-this-a-problem-with-the-swift-framework" title="Link to this heading">ïƒ</a></h3>
<p>The main branch should be using the V1 engine by default. Try adding <code class="docutils literal notranslate"><span class="pre">VLLM_USE_V1=1</span></code>. Also, make sure to align the image resolution.</p>
</section>
<section id="q19-if-a-swift-deployment-is-accelerated-with-vllm-can-i-specify-the-proportion-of-gpu-memory-to-be-used-on-different-cards-individually">
<h3>Q19: If a Swift deployment is accelerated with vLLM, can I specify the proportion of GPU memory to be used on different cards individually?<a class="headerlink" href="#q19-if-a-swift-deployment-is-accelerated-with-vllm-can-i-specify-the-proportion-of-gpu-memory-to-be-used-on-different-cards-individually" title="Link to this heading">ïƒ</a></h3>
<p>Heterogeneous configurations are not supported.</p>
</section>
<section id="q20-after-a-model-is-saved-it-seems-it-can-t-be-loaded-by-vllm-it-throws-an-error-about-a-missing-model-language-model-embed-tokens-weight-is-there-a-solution">
<h3>Q20: After a model is saved, it seems it can't be loaded by vLLM. It throws an error about a missing &quot;model.language_model.embed_tokens.weight&quot;. Is there a solution?<a class="headerlink" href="#q20-after-a-model-is-saved-it-seems-it-can-t-be-loaded-by-vllm-it-throws-an-error-about-a-missing-model-language-model-embed-tokens-weight-is-there-a-solution" title="Link to this heading">ïƒ</a></h3>
<p>The transformers versions used for training and inference must be consistent.</p>
</section>
<section id="q21-regarding-the-system-prompt-you-can-specify-it-via-the-system-parameter-prepend-it-to-each-data-entry-in-the-dataset-or-define-it-in-the-template-is-it-sufficient-to-use-just-one-of-these-methods-and-are-they-all-treated-the-same-way-by-the-model">
<h3>Q21: Regarding the system prompt, you can specify it via the --system parameter, prepend it to each data entry in the dataset, or define it in the template. Is it sufficient to use just one of these methods? And are they all treated the same way by the model?<a class="headerlink" href="#q21-regarding-the-system-prompt-you-can-specify-it-via-the-system-parameter-prepend-it-to-each-data-entry-in-the-dataset-or-define-it-in-the-template-is-it-sufficient-to-use-just-one-of-these-methods-and-are-they-all-treated-the-same-way-by-the-model" title="Link to this heading">ïƒ</a></h3>
<p>System prompt priority: The one in the dataset &gt; The one from the command line &gt; The default one in the template.</p>
</section>
<section id="q22-after-deploying-a-model-using-the-swift-transformers-engine-inference-is-not-parallelized-and-data-is-not-distributed-to-other-gpus-everything-is-running-on-the-first-gpu">
<h3>Q22: After deploying a model using the Swift Transformers engine, inference is not parallelized, and data is not distributed to other GPUs. Everything is running on the first GPU.<a class="headerlink" href="#q22-after-deploying-a-model-using-the-swift-transformers-engine-inference-is-not-parallelized-and-data-is-not-distributed-to-other-gpus-everything-is-running-on-the-first-gpu" title="Link to this heading">ïƒ</a></h3>
<p>Try using swift infer. The deploy command does not support DDP.</p>
</section>
<section id="q23-for-a-model-deployed-with-swift-deploy-how-can-i-disable-the-thinking-status-on-the-client-side-adding-it-to-the-extra-body-of-the-request-doesn-t-work">
<h3>Q23: For a model deployed with swift deploy, how can I disable the &quot;thinking&quot; status on the client side? Adding it to the extra_body of the request doesn't work.<a class="headerlink" href="#q23-for-a-model-deployed-with-swift-deploy-how-can-i-disable-the-thinking-status-on-the-client-side-adding-it-to-the-extra-body-of-the-request-doesn-t-work" title="Link to this heading">ïƒ</a></h3>
<p>Currently, the &quot;thinking&quot; status can only be disabled when launching swift deploy.</p>
</section>
</section>
<section id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">ïƒ</a></h2>
<section id="q1-what-evaluation-datasets-are-supported-by-swift">
<h3>Q1: What evaluation datasets are supported by Swift?<a class="headerlink" href="#q1-what-evaluation-datasets-are-supported-by-swift" title="Link to this heading">ïƒ</a></h3>
<p>Pure text evaluation:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;obqa&#39;, &#39;cmb&#39;, &#39;AX_b&#39;, &#39;siqa&#39;, &#39;nq&#39;, &#39;mbpp&#39;, &#39;winogrande&#39;, &#39;mmlu&#39;, &#39;BoolQ&#39;, &#39;cluewsc&#39;, &#39;ocnli&#39;, &#39;lambada&#39;,
&#39;CMRC&#39;, &#39;ceval&#39;, &#39;csl&#39;, &#39;cmnli&#39;, &#39;bbh&#39;, &#39;ReCoRD&#39;, &#39;math&#39;, &#39;humaneval&#39;, &#39;eprstmt&#39;, &#39;WSC&#39;, &#39;storycloze&#39;,
&#39;MultiRC&#39;, &#39;RTE&#39;, &#39;chid&#39;, &#39;gsm8k&#39;, &#39;AX_g&#39;, &#39;bustm&#39;, &#39;afqmc&#39;, &#39;piqa&#39;, &#39;lcsts&#39;, &#39;strategyqa&#39;, &#39;Xsum&#39;, &#39;agieval&#39;,
&#39;ocnli_fc&#39;, &#39;C3&#39;, &#39;tnews&#39;, &#39;race&#39;, &#39;triviaqa&#39;, &#39;CB&#39;, &#39;WiC&#39;, &#39;hellaswag&#39;, &#39;summedits&#39;, &#39;GaokaoBench&#39;,
&#39;ARC_e&#39;, &#39;COPA&#39;, &#39;ARC_c&#39;, &#39;DRCD&#39;
</pre></div>
</div>
<p>Multimodal evaluation:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;COCO_VAL&#39;, &#39;MME&#39;, &#39;HallusionBench&#39;, &#39;POPE&#39;, &#39;MMBench_DEV_EN&#39;, &#39;MMBench_TEST_EN&#39;, &#39;MMBench_DEV_CN&#39;, &#39;MMBench_TEST_CN&#39;,
&#39;MMBench&#39;, &#39;MMBench_CN&#39;, &#39;MMBench_DEV_EN_V11&#39;, &#39;MMBench_TEST_EN_V11&#39;, &#39;MMBench_DEV_CN_V11&#39;,
&#39;MMBench_TEST_CN_V11&#39;, &#39;MMBench_V11&#39;, &#39;MMBench_CN_V11&#39;, &#39;SEEDBench_IMG&#39;, &#39;SEEDBench2&#39;,
&#39;SEEDBench2_Plus&#39;, &#39;ScienceQA_VAL&#39;, &#39;ScienceQA_TEST&#39;, &#39;MMT-Bench_ALL_MI&#39;, &#39;MMT-Bench_ALL&#39;,
&#39;MMT-Bench_VAL_MI&#39;, &#39;MMT-Bench_VAL&#39;, &#39;AesBench_VAL&#39;, &#39;AesBench_TEST&#39;, &#39;CCBench&#39;, &#39;AI2D_TEST&#39;, &#39;MMStar&#39;,
&#39;RealWorldQA&#39;, &#39;MLLMGuard_DS&#39;, &#39;BLINK&#39;, &#39;OCRVQA_TEST&#39;, &#39;OCRVQA_TESTCORE&#39;, &#39;TextVQA_VAL&#39;, &#39;DocVQA_VAL&#39;,
&#39;DocVQA_TEST&#39;, &#39;InfoVQA_VAL&#39;, &#39;InfoVQA_TEST&#39;, &#39;ChartQA_TEST&#39;, &#39;MathVision&#39;, &#39;MathVision_MINI&#39;,
&#39;MMMU_DEV_VAL&#39;, &#39;MMMU_TEST&#39;, &#39;OCRBench&#39;, &#39;MathVista_MINI&#39;, &#39;LLaVABench&#39;, &#39;MMVet&#39;, &#39;MTVQA_TEST&#39;,
&#39;MMLongBench_DOC&#39;, &#39;VCR_EN_EASY_500&#39;, &#39;VCR_EN_EASY_100&#39;, &#39;VCR_EN_EASY_ALL&#39;, &#39;VCR_EN_HARD_500&#39;,
&#39;VCR_EN_HARD_100&#39;, &#39;VCR_EN_HARD_ALL&#39;, &#39;VCR_ZH_EASY_500&#39;, &#39;VCR_ZH_EASY_100&#39;, &#39;VCR_ZH_EASY_ALL&#39;,
&#39;VCR_ZH_HARD_500&#39;, &#39;VCR_ZH_HARD_100&#39;, &#39;VCR_ZH_HARD_ALL&#39;, &#39;MMDU&#39;, &#39;MMBench-Video&#39;, &#39;Video-MME&#39;
</pre></div>
</div>
<p>See the document <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Evaluation.html">Evaluation</a> for details.</p>
</section>
<section id="q2-how-to-use-a-custom-evaluation-dataset">
<h3>Q2: How to use a custom evaluation dataset?<a class="headerlink" href="#q2-how-to-use-a-custom-evaluation-dataset" title="Link to this heading">ïƒ</a></h3>
<p>Custom evaluation datasets, both plain text and multimodal, must match the data format (pattern) of an official dataset. See the document <a class="reference external" href="https://swift.readthedocs.io/en/latest/Instruction/Evaluation.html">Evaluation</a> for details.</p>
</section>
<section id="q3-error-with-mmengine-in-python3-11-environment-during-evaluation">
<h3>Q3: Error with mmengine in python3.11 environment during evaluation<a class="headerlink" href="#q3-error-with-mmengine-in-python3-11-environment-during-evaluation" title="Link to this heading">ïƒ</a></h3>
<p>Try using the Python 3.10 environment. Or first install all dependencies:
<code class="docutils literal notranslate"><span class="pre">pip3</span> <span class="pre">install</span> <span class="pre">evalscope[all]</span></code>,
then apply the patch:
<code class="docutils literal notranslate"><span class="pre">pip3</span> <span class="pre">installÂ https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/package/evalscope-0.5.3.post1-py3-none-any.whl</span></code>.</p>
</section>
<section id="q4-after-manually-downloading-the-official-evaluation-dataset-can-swift-eval-be-configured-for-local-path-evaluation">
<h3>Q4: After manually downloading the official evaluation dataset, can Swift eval be configured for local path evaluation?<a class="headerlink" href="#q4-after-manually-downloading-the-official-evaluation-dataset-can-swift-eval-be-configured-for-local-path-evaluation" title="Link to this heading">ïƒ</a></h3>
<p>First, download the evaluation dataset <a class="reference external" href="https://modelscope.cn/datasets/swift/evalscope_resource/files">eval.zip</a>, extract it, and place its contents in the <code class="docutils literal notranslate"><span class="pre">~/.cache/modelscope/media_resources/evalscope/data</span></code> folder. Then execute the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">eval</span></code> command to use the local data.</p>
</section>
<section id="q5-is-there-a-bug-with-custom-evaluation-i-modified-the-standard-example-to-english-but-it-doesn-t-work">
<h3>Q5: Is there a bug with custom evaluation? I modified the standard example to English, but it doesn't work?<a class="headerlink" href="#q5-is-there-a-bug-with-custom-evaluation-i-modified-the-standard-example-to-english-but-it-doesn-t-work" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>swift<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--model_type<span class="w"> </span><span class="s1">&#39;qwen2_5-1_5b-instruct&#39;</span><span class="w"> </span>--eval_dataset<span class="w"> </span>no<span class="w"> </span>--custom_eval_config<span class="w"> </span><span class="s1">&#39;/mnt/workspace/test_data/config_eval.json&#39;</span>
</pre></div>
</div>
<p>This relies on the nltk package, which needs to download a punkt_tab zip file. Some environments in China have unstable or failed downloads. The code has been modified to handle this issue; reference <a class="reference external" href="https://github.com/nltk/nltk/issues/3293">issue</a>.</p>
</section>
<section id="q6-the-model-after-eval-fine-tuning-keeps-stopping-at-a-fixed-percentage-but-the-vllm-service-seems-to-be-running-normally-the-larger-the-model-the-sooner-it-disconnects">
<h3>Q6: The model after eval fine-tuning keeps stopping at a fixed percentage, but the vllm service seems to be running normally. The larger the model, the sooner it disconnects.<a class="headerlink" href="#q6-the-model-after-eval-fine-tuning-keeps-stopping-at-a-fixed-percentage-but-the-vllm-service-seems-to-be-running-normally-the-larger-the-model-the-sooner-it-disconnects" title="Link to this heading">ïƒ</a></h3>
<p>Set the <code class="docutils literal notranslate"><span class="pre">SWIFT_TIMEOUT</span></code> environment variable to -1.</p>
</section>
<section id="q7-does-evalscope-support-multi-model-comparison">
<h3>Q7: Does evalscope support multi-model comparison?<a class="headerlink" href="#q7-does-evalscope-support-multi-model-comparison" title="Link to this heading">ïƒ</a></h3>
<p>See the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/arena.html">documentation</a> for details.</p>
</section>
<section id="q8-is-there-a-custom-evaluation-for-multimodal-datasets">
<h3>Q8: Is there a custom evaluation for multimodal datasets?<a class="headerlink" href="#q8-is-there-a-custom-evaluation-for-multimodal-datasets" title="Link to this heading">ïƒ</a></h3>
<p>Custom evaluation for multimodal datasets can be referenced in the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html">documentation</a>.</p>
</section>
<section id="q9-does-ms-swift-have-methods-to-test-qps-latency-and-tokens-s">
<h3>Q9: Does ms-swift have methods to test QPS, latency, and tokens/s?<a class="headerlink" href="#q9-does-ms-swift-have-methods-to-test-qps-latency-and-tokens-s" title="Link to this heading">ïƒ</a></h3>
<p>You can try using evalscope's <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html">Model Inference Stress Testing</a>.</p>
</section>
<section id="q10-can-i-control-the-number-of-dataset-entries-during-evaluation-it-takes-over-an-hour-to-evaluate-an-mmlu-which-is-too-slow">
<h3>Q10: Can I control the number of dataset entries during evaluation? It takes over an hour to evaluate an MMLU, which is too slow.<a class="headerlink" href="#q10-can-i-control-the-number-of-dataset-entries-during-evaluation-it-takes-over-an-hour-to-evaluate-an-mmlu-which-is-too-slow" title="Link to this heading">ïƒ</a></h3>
<p>Use the configuration parameter <code class="docutils literal notranslate"><span class="pre">--eval_limit</span></code>. This <code class="docutils literal notranslate"><span class="pre">--eval_limit</span></code> controls the number of entries in each subset. For example, if MMLU has over 50 subsets, and each limit is set to 10 entries, then that would be over 500 entries in total.</p>
</section>
<section id="q11-when-evaluating-isn-t-it-just-having-the-model-output-an-answer-once-and-checking-if-it-s-correct-is-there-a-way-to-record-or-see-the-complete-answer-each-time">
<h3>Q11: When evaluating, isn't it just having the model output an answer once and checking if it's correct? Is there a way to record or see the complete answer each time?<a class="headerlink" href="#q11-when-evaluating-isn-t-it-just-having-the-model-output-an-answer-once-and-checking-if-it-s-correct-is-there-a-way-to-record-or-see-the-complete-answer-each-time" title="Link to this heading">ïƒ</a></h3>
<p>For multiple-choice evaluations like ceval, the evaluation is done by calculating the logits for each option, without outputting the actual answer content. If you want to see the answer content, you can deploy the model as a service with a specified API URL for evaluation, which will evaluate based on parsing the model's output. See the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#model-api-service-evaluation">documentation</a> for details. Both methods can be made optional.</p>
</section>
<section id="q12-i-want-to-stress-test-my-model-using-evalscope-and-would-like-to-use-a-prompt-txt-file-format-what-should-the-format-of-this-file-look-like">
<h3>Q12: I want to stress test my model using evalscope and would like to use a prompt.txt file format. What should the format of this file look like?<a class="headerlink" href="#q12-i-want-to-stress-test-my-model-using-evalscope-and-would-like-to-use-a-prompt-txt-file-format-what-should-the-format-of-this-file-look-like" title="Link to this heading">ïƒ</a></h3>
<p>Configure line_by_line, see the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/parameters.html#dataset-configuration">documentation</a> for details.</p>
</section>
<section id="q13-how-should-i-use-the-parallel-and-number-parameters-when-conducting-model-inference-performance-testing-using-evalscope-perf">
<h3>Q13: How should I use the 'parallel' and 'number' parameters when conducting model inference performance testing using evalscope perf?<a class="headerlink" href="#q13-how-should-i-use-the-parallel-and-number-parameters-when-conducting-model-inference-performance-testing-using-evalscope-perf" title="Link to this heading">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">number</span></code> is the total number of requests, while <code class="docutils literal notranslate"><span class="pre">parallel</span></code> is the number of concurrent requests.</p>
</section>
<section id="q14-in-swift-eval-the-model-stops-generating-after-1024-tokens-how-can-i-modify-this-setting-max-new-tokens-5000-doesn-t-seem-to-work">
<h3>Q14: In swift eval, the model stops generating after 1024 tokens. How can I modify this? Setting --max_new_tokens 5000 doesn't seem to work.<a class="headerlink" href="#q14-in-swift-eval-the-model-stops-generating-after-1024-tokens-how-can-i-modify-this-setting-max-new-tokens-5000-doesn-t-seem-to-work" title="Link to this heading">ïƒ</a></h3>
<p>This parameter hasn't been exposed in swift yet. You can use evalscope to run it, and configure max_tokens in the model according to the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/vlmevalkit_backend.html#configure-model-evaluation-parameters">documentation</a>.</p>
</section>
<section id="q15-does-evalscope-currently-support-benchmarks-like-aime-and-math-500-for-deepseek-r1">
<h3>Q15: Does evalscope currently support benchmarks like AIME and MATH-500 for deepseek-r1?<a class="headerlink" href="#q15-does-evalscope-currently-support-benchmarks-like-aime-and-math-500-for-deepseek-r1" title="Link to this heading">ïƒ</a></h3>
<p>Yes, it does. Here are the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/best_practice/deepseek_r1_distill.html">best practices</a>.</p>
</section>
<section id="q16-i-m-getting-this-error-when-using-a-local-path-for-gpqa-evaluation-in-evalscope-valueerror-buildingconfig-gpqa-extended-not-found-available-default">
<h3>Q16: I'm getting this error when using a local path for gpqa evaluation in evalscope: ValueError: BuildingConfig 'gpqa_extended' not found. Available: ['default']<a class="headerlink" href="#q16-i-m-getting-this-error-when-using-a-local-path-for-gpqa-evaluation-in-evalscope-valueerror-buildingconfig-gpqa-extended-not-found-available-default" title="Link to this heading">ïƒ</a></h3>
<p>Parameter configuration:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>--datasets<span class="w"> </span>gpqa<span class="w"> </span>--dataset-args<span class="w"> </span><span class="s1">&#39;{&quot;gpqa&quot;: {&quot;local_path&quot;: &quot;/mnt/workspace/gpqa&quot;} }&#39;</span>
</pre></div>
</div>
<p>If you want to use datasets locally, it's recommended to clone the repository from modelscope and then specify the path.</p>
</section>
<section id="q17-when-evaluating-the-arc-dataset-with-evalscope-i-get-this-error-what-s-the-reason-i-m-using-the-local-data-path-method">
<h3>Q17: When evaluating the arc dataset with evalscope, I get this error. What's the reason? I'm using the local data path method.<a class="headerlink" href="#q17-when-evaluating-the-arc-dataset-with-evalscope-i-get-this-error-what-s-the-reason-i-m-using-the-local-data-path-method" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>KeyError: &#39;RequestId&#39;
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>--datasets<span class="w"> </span>arc<span class="w"> </span>--dataset-args<span class="w"> </span><span class="s1">&#39;{&quot;arc&quot;: {&quot;local_path&quot;: &quot;/mnt/workspace/arc&quot;}}&#39;</span>
</pre></div>
</div>
<p>According to the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#using-local-datasets-and-models">documentation</a>, the arc dataset needs to be downloaded using a Python script; directly cloning the repository won't work.</p>
</section>
<section id="q18-how-can-i-load-downloaded-datasets-locally-when-using-opencompass-backend-for-evaluation">
<h3>Q18: How can I load downloaded datasets locally when using opencompass backend for evaluation?<a class="headerlink" href="#q18-how-can-i-load-downloaded-datasets-locally-when-using-opencompass-backend-for-evaluation" title="Link to this heading">ïƒ</a></h3>
<p>The opencompass backend doesn't support setting <code class="docutils literal notranslate"><span class="pre">data_args</span></code>.</p>
</section>
<section id="q19-does-swift-eval-with-eval-backend-opencompass-not-support-custom-datasets">
<h3>Q19: Does swift eval with --eval_backend OpenCompass not support custom datasets?<a class="headerlink" href="#q19-does-swift-eval-with-eval-backend-opencompass-not-support-custom-datasets" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ValueError: eval_dataset: /mnt/workspace/data.jsonl is not supported.
eval_backend: OpenCompass supported datasets: [&#39;C3&#39;, &#39;summedits&#39;, &#39;WiC&#39;, &#39;csl&#39;, &#39;lambada&#39;, &#39;mbpp&#39;, &#39;hellaswag&#39;, &#39;ARC_e&#39;, &#39;math&#39;, &#39;nq&#39;, &#39;race&#39;, &#39;MultiRC&#39;, &#39;cmb&#39;, &#39;ceval&#39;, &#39;GaokaoBench&#39;, &#39;mmlu&#39;, &#39;winogrande&#39;, &#39;tnews&#39;, &#39;triviaqa&#39;, &#39;CB&#39;, &#39;cluewsc&#39;, &#39;humaneval&#39;, &#39;AX_g&#39;, &#39;DRCD&#39;, &#39;RTE&#39;, &#39;ocnli_fc&#39;, &#39;gsm8k&#39;, &#39;obqa&#39;, &#39;ReCoRD&#39;, &#39;Xsum&#39;, &#39;ocnli&#39;, &#39;WSC&#39;, &#39;siqa&#39;, &#39;agieval&#39;, &#39;piqa&#39;, &#39;cmnli&#39;, &#39;cmmlu&#39;, &#39;eprstmt&#39;, &#39;storycloze&#39;, &#39;AX_b&#39;, &#39;afqmc&#39;, &#39;strategyqa&#39;, &#39;bustm&#39;, &#39;BoolQ&#39;, &#39;COPA&#39;, &#39;ARC_c&#39;, &#39;PMMEval&#39;, &#39;chid&#39;, &#39;CMRC&#39;, &#39;lcsts&#39;]
</pre></div>
</div>
<p>OpenCompass doesn't support custom datasets; use native mode for custom datasets.</p>
</section>
<section id="q20-when-i-run-the-ragas-evaluation-task-from-the-evalscope-official-documentation-locally-on-a-single-a100-it-takes-10-minutes-to-run-the-two-examples-in-the-documentation-is-this-normal-are-there-ways-to-optimize-the-running-speed">
<h3>Q20: When I run the <a class="reference external" href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html">RAGAS evaluation task</a> from the evalscope official documentation locally on a single A100, it takes 10 minutes to run the two examples in the documentation. Is this normal? Are there ways to optimize the running speed?<a class="headerlink" href="#q20-when-i-run-the-ragas-evaluation-task-from-the-evalscope-official-documentation-locally-on-a-single-a100-it-takes-10-minutes-to-run-the-two-examples-in-the-documentation-is-this-normal-are-there-ways-to-optimize-the-running-speed" title="Link to this heading">ïƒ</a></h3>
<p>RAG evaluation itself is resource-intensive, and using a local critic LLM will indeed be slower as it can't handle batch requests. It's recommended to use frameworks like vllm to launch tasks.</p>
</section>
<section id="q21-i-m-using-evalscope-to-evaluate-rag-but-i-also-want-to-use-the-api-method-to-call-the-embedded-model-is-this-supported-i-don-t-see-it-mentioned-in-the-documentation">
<h3>Q21: I'm using evalscope to evaluate RAG, but I also want to use the API method to call the embedded model. Is this supported? I don't see it mentioned in the documentation.<a class="headerlink" href="#q21-i-m-using-evalscope-to-evaluate-rag-but-i-also-want-to-use-the-api-method-to-call-the-embedded-model-is-this-supported-i-don-t-see-it-mentioned-in-the-documentation" title="Link to this heading">ïƒ</a></h3>
<p>Currently, embedding models do not support API calls, but this will be supported in the future.</p>
</section>
<section id="q22-when-testing-a-locally-trained-model-using-evalscope-the-output-for-the-test-data-is-very-simple-but-the-data-was-constructed-in-an-inferential-way-during-model-training-leading-to-lower-test-results-how-can-evalscope-be-used-to-test-only-the-data-within-xxx-from-the-model-s-output">
<h3>Q22: When testing a locally trained model using evalscope, the output for the test data is very simple, but the data was constructed in an inferential way during model training, leading to lower test results. How can evalscope be used to test only the data within xxx from the model's output?<a class="headerlink" href="#q22-when-testing-a-locally-trained-model-using-evalscope-the-output-for-the-test-data-is-very-simple-but-the-data-was-constructed-in-an-inferential-way-during-model-training-leading-to-lower-test-results-how-can-evalscope-be-used-to-test-only-the-data-within-xxx-from-the-model-s-output" title="Link to this heading">ïƒ</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">{&quot;filters&quot;:</span> <span class="pre">{&quot;remove_until&quot;:</span> <span class="pre">&quot;&lt;/think&gt;&quot;}}</span></code> in dataset-args, and refer to this <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/get_started/parameters.html#dataset-parameters">documentation</a>. Setting this parameter will remove <code class="docutils literal notranslate"><span class="pre">&lt;think&gt;</span></code> when calculating metrics.</p>
</section>
<section id="q23-evalscope-can-natively-generate-reports-but-other-backends-like-opencompass-do-not-support-report-visualization-correct">
<h3>Q23: Evalscope can natively generate reports, but other backends like OpenCompass do not support report visualization, correct?<a class="headerlink" href="#q23-evalscope-can-natively-generate-reports-but-other-backends-like-opencompass-do-not-support-report-visualization-correct" title="Link to this heading">ïƒ</a></h3>
<p>Currently, only native visualization is supported; other backends are not yet supported.</p>
</section>
<section id="q24-could-you-explain-what-causes-the-following-error-when-using-ifeval-for-evaluation">
<h3>Q24: Could you explain what causes the following error when using ifeval for evaluation?<a class="headerlink" href="#q24-could-you-explain-what-causes-the-following-error-when-using-ifeval-for-evaluation" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[Errno 20] Not a directory: &#39;/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab&#39;
</pre></div>
</div>
<p>Unzip the file using <code class="docutils literal notranslate"><span class="pre">unzip</span> <span class="pre">/path/to/nltk_data/tokenizers/punkt_tab.zip</span></code>.</p>
</section>
<section id="q25-when-evaluating-with-eval-backend-opencompass-how-can-i-specify-the-path-to-offline-datasets">
<h3>Q25: When evaluating with eval_backend='OpenCompass', how can I specify the path to offline datasets?<a class="headerlink" href="#q25-when-evaluating-with-eval-backend-opencompass-how-can-i-specify-the-path-to-offline-datasets" title="Link to this heading">ïƒ</a></h3>
<p>Check the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/opencompass_backend.html#data-preparation">data preparation guide</a>, download and unzip the dataset. You don't need to specify <code class="docutils literal notranslate"><span class="pre">dataset-args</span></code>; just place the dataset folder (the data folder) in the current working directory.</p>
</section>
<section id="q26-what-causes-the-following-error-when-using-evalscope">
<h3>Q26: What causes the following error when using evalscope?<a class="headerlink" href="#q26-what-causes-the-following-error-when-using-evalscope" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>unzip: cannot find or open /root/nltk_data/tokenizers/punkt_tab.zip, /root/nltk_data/tokenizers/punkt_tab.zip.zip or /root/nltk_data/tokenizers/punkt_tab.zip.ZIP
</pre></div>
</div>
<p>This occurs during the download of nltk dependencies. Manually download <a class="reference external" href="https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/open_data/nltk_data/punkt_tab.zip">punkt_tab.zip</a> and unzip it under <code class="docutils literal notranslate"><span class="pre">~/nltk_data/tokenizers</span></code>.</p>
</section>
<section id="q27-why-is-there-no-issue-with-plain-text-but-when-testing-multi-modal-data-even-though-we-specify-the-path-it-still-fails-to-detect-the-dataset-and-attempts-to-download-it">
<h3>Q27: Why is there no issue with plain text, but when testing multi-modal data, even though we specify the path, it still fails to detect the dataset and attempts to download it?<a class="headerlink" href="#q27-why-is-there-no-issue-with-plain-text-but-when-testing-multi-modal-data-even-though-we-specify-the-path-it-still-fails-to-detect-the-dataset-and-attempts-to-download-it" title="Link to this heading">ïƒ</a></h3>
<p>The vlmevalkit process differs from native; it will automatically download data to <code class="docutils literal notranslate"><span class="pre">~/LMUData/</span></code>.</p>
</section>
<section id="q28-could-you-explain-how-the-score-in-evalscope-is-calculated-is-there-any-documentation-about-this-part">
<h3>Q28: Could you explain how the score in evalscope is calculated? Is there any documentation about this part?<a class="headerlink" href="#q28-could-you-explain-how-the-score-in-evalscope-is-calculated-is-there-any-documentation-about-this-part" title="Link to this heading">ïƒ</a></h3>
<p>Please refer to this <a class="reference external" href="https://github.com/modelscope/evalscope/issues/610">issue</a>.</p>
</section>
<section id="q29-when-using-swift-eval-for-benchmark-evaluation-can-i-specify-an-llm-as-a-judge-and-how-should-i-pass-in-the-parameters">
<h3>Q29: When using swift eval for benchmark evaluation, can I specify an llm as a judge and how should I pass in the parameters?<a class="headerlink" href="#q29-when-using-swift-eval-for-benchmark-evaluation-can-i-specify-an-llm-as-a-judge-and-how-should-i-pass-in-the-parameters" title="Link to this heading">ïƒ</a></h3>
<p>Yes, you can use swift to pass <code class="docutils literal notranslate"><span class="pre">judge-model-args</span></code> parameters from <code class="docutils literal notranslate"><span class="pre">extra_eval_args</span></code>, which include <code class="docutils literal notranslate"><span class="pre">api_key,</span> <span class="pre">api_url,</span> <span class="pre">and</span> <span class="pre">model_id</span></code>, as a JSON string.</p>
</section>
<section id="q30-how-can-i-remove-the-cot-chain-of-thought-from-the-model-s-response-before-evaluation-when-using-the-vlmevalkit-backend">
<h3>Q30: How can I remove the CoT (Chain-of-Thought) from the model's response before evaluation when using the vlmevalkit backend?<a class="headerlink" href="#q30-how-can-i-remove-the-cot-chain-of-thought-from-the-model-s-response-before-evaluation-when-using-the-vlmevalkit-backend" title="Link to this heading">ïƒ</a></h3>
<p>This post-processing feature is not yet supported.</p>
</section>
<section id="q31-for-embedding-evaluation-the-first-run-automatically-downloads-the-dataset-is-it-possible-to-download-it-manually-and-then-specify-a-local-directory">
<h3>Q31: For embedding evaluation, the first run automatically downloads the dataset. Is it possible to download it manually and then specify a local directory?<a class="headerlink" href="#q31-for-embedding-evaluation-the-first-run-automatically-downloads-the-dataset-is-it-possible-to-download-it-manually-and-then-specify-a-local-directory" title="Link to this heading">ïƒ</a></h3>
<p>Currently, for embedding evaluation, you can only specify a path for custom datasets.</p>
</section>
<section id="q32-how-can-i-define-custom-evaluation-metrics">
<h3>Q32: How can I define custom evaluation metrics?<a class="headerlink" href="#q32-how-can-i-define-custom-evaluation-metrics" title="Link to this heading">ïƒ</a></h3>
<p>Are you trying to use custom evaluation metrics on a custom dataset? Currently, there is no plugin system for this. You will need to modify the match method in evalscope/benchmarks/general_qa/general_qa_adapter.py yourself.</p>
</section>
<section id="q33-for-mmvet-evaluation-a-judge-model-needs-to-be-configured-i-followed-this-document-to-set-it-up-but-i-m-getting-an-error-does-the-judge-model-s-name-have-to-be-one-of-these-three">
<h3>Q33: For MMVet evaluation, a judge model needs to be configured. I followed this document to set it up, but I'm getting an error. Does the judge model's name have to be one of these three?<a class="headerlink" href="#q33-for-mmvet-evaluation-a-judge-model-needs-to-be-configured-i-followed-this-document-to-set-it-up-but-i-m-getting-an-error-does-the-judge-model-s-name-have-to-be-one-of-these-three" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
File &quot;/usr/local/lib/python3.10/dist-packages/vlmeval/run.py&quot;, line 484, in run_task eval_results = dataset.evaluate(result_file,**judge_kwargs )
File &quot;/usr/local/lib/python3.10/dist-packages/vlmeval/dataset/image_mcq.py&quot;, line 244, in evaluate assert model in [&#39;chatgpt-0125&#39;ï¼Œ &#39;exact_matching&#39;ï¼Œ&#39;gpt-4-0125&#39;] AssertionError
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">task_cfg_dict</span> <span class="o">=</span> <span class="n">TaskConfig</span><span class="p">(</span>
    <span class="n">work_dir</span><span class="o">=</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
    <span class="n">eval_backend</span><span class="o">=</span><span class="s1">&#39;VLMEvalKit&#39;</span><span class="p">,</span>
    <span class="n">eval_config</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;MMVet&#39;</span><span class="p">,</span> <span class="s1">&#39;DocVQA_VAL&#39;</span><span class="p">,</span> <span class="s1">&#39;MMBench_DEV_EN&#39;</span><span class="p">],</span>
        <span class="s1">&#39;limit&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;all&#39;</span><span class="p">,</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s1">&#39;api_base&#39;</span><span class="p">:</span> <span class="s1">&#39;http://127.0.0.1:8001/v1/chat/completions&#39;</span><span class="p">,</span>
            <span class="s1">&#39;key&#39;</span><span class="p">:</span> <span class="s1">&#39;EMPTY&#39;</span><span class="p">,</span>
            <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;CustomAPIModel&#39;</span><span class="p">,</span>
            <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;Qwen2.5-VL-72B-Instruct-AWQ&#39;</span><span class="p">,</span>
            <span class="s1">&#39;img_size&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;video_llm&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s1">&#39;max_tokens&#39;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,}</span>
            <span class="p">],</span>
        <span class="s1">&#39;reuse&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;nproc&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
        <span class="s1">&#39;judge&#39;</span><span class="p">:</span> <span class="s1">&#39;exact_matching&#39;</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For multiple-choice question answering, this is a requirement. Alternatively, you can specify exact_matching. Try separating the MMBench and MMVet evaluations and running them without a judge model.</p>
</section>
<section id="q34-what-could-be-the-reason-for-uneven-gpu-memory-allocation-across-multiple-cards-when-running-eval">
<h3>Q34: What could be the reason for uneven GPU memory allocation across multiple cards when running eval?<a class="headerlink" href="#q34-what-could-be-the-reason-for-uneven-gpu-memory-allocation-across-multiple-cards-when-running-eval" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">8</span>
<span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5,6,7<span class="se">\ </span><span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">802816</span><span class="se">\ </span>swift<span class="w"> </span>eval<span class="se">\</span>
--model<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$MODEL_PATH</span><span class="s2">â€ \$EXTRA_ARGS \</span>
<span class="s2">--eval_backend Native \ --infer_backend transformers\ --device_map auto \</span>
<span class="s2">--eval_limit&quot;</span><span class="nv">$EVAL_LIMIT</span><span class="s2">&quot;\ --eval_dataset general_qa\</span>
<span class="s2">--dataset_args &quot;</span><span class="o">{</span><span class="se">\&quot;</span>general_qa<span class="se">\&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="se">\&quot;</span>local_path<span class="se">\&quot;</span>:<span class="w"> </span><span class="se">\&quot;</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span><span class="se">\&quot;</span>,<span class="w"> </span><span class="se">\&quot;</span>subset_list<span class="se">\&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="se">\&quot;</span><span class="si">${</span><span class="nv">SUBSET_NAME</span><span class="si">}</span><span class="se">\&quot;</span><span class="o">]}}</span><span class="s2">&quot; \ --host 127.0.0.1\&gt; &quot;</span><span class="nv">$LOG_FILE</span><span class="s2">&quot; 2&gt;&amp;1</span>
</pre></div>
</div>
<p>swift eval does not support being launched in DDP mode.</p>
</section>
<section id="q35-when-using-evalscope-for-evaluation-how-can-i-control-the-input-to-have-a-fixed-token-length">
<h3>Q35: When using evalscope for evaluation, how can I control the input to have a fixed token length?<a class="headerlink" href="#q35-when-using-evalscope-for-evaluation-how-can-i-control-the-input-to-have-a-fixed-token-length" title="Link to this heading">ïƒ</a></h3>
<p>Controlling the length is only supported for the random dataset. Please refer to the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#using-the-random-dataset">documentation</a>.</p>
</section>
<section id="q36-why-can-t-evalscope-app-find-the-report-even-though-there-are-corresponding-records-in-the-outputs-directory">
<h3>Q36: Why can't evalscope app find the report, even though there are corresponding records in the outputs directory?<a class="headerlink" href="#q36-why-can-t-evalscope-app-find-the-report-even-though-there-are-corresponding-records-in-the-outputs-directory" title="Link to this heading">ïƒ</a></h3>
<p>It might be an inference performance stress test. For visualizing evalscope perf results, please refer to the <a class="reference external" href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#visualizing-test-results">documentation</a>.</p>
</section>
<section id="q37-where-can-i-see-what-extra-fields-besides-the-question-itself-are-included-in-the-query-sent-during-a-swift-evaluation">
<h3>Q37: Where can I see what extra fields, besides the question itself, are included in the query sent during a Swift evaluation?<a class="headerlink" href="#q37-where-can-i-see-what-extra-fields-besides-the-question-itself-are-included-in-the-query-sent-during-a-swift-evaluation" title="Link to this heading">ïƒ</a></h3>
<p>The simplest way is to check the input field in the output reviews file. It contains the content sent to the model, converted to Markdown format. Note that this is not available if you are using the opencompass backend; you need to use the native backend.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>