

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DAPO: An Open-Source LLM Reinforcement Learning System at Scale &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">DAPO: An Open-Source LLM Reinforcement Learning System at Scale</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/GRPO/AdvancedResearch/DAPO.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="dapo-an-open-source-llm-reinforcement-learning-system-at-scale">
<h1>DAPO: An Open-Source LLM Reinforcement Learning System at Scale<a class="headerlink" href="#dapo-an-open-source-llm-reinforcement-learning-system-at-scale" title="Link to this heading">ïƒ</a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/2503.14476">Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)</a> introduces several tricks based on GRPO, including:</p>
<ul class="simple">
<li><p><a class="reference external" href="#clip-higher">Clip Higher</a></p></li>
<li><p><a class="reference external" href="#dynamic-sampling">Dynamic Sampling</a></p></li>
<li><p><a class="reference external" href="#token-level-loss">Token level Loss</a></p></li>
<li><p><a class="reference external" href="#overlong-filtering">Overlong Filtering</a></p></li>
<li><p><a class="reference external" href="#soft-overlong-punishment">Soft Overlong Punishment</a></p></li>
</ul>
<section id="clip-higher">
<h2>Clip Higher<a class="headerlink" href="#clip-higher" title="Link to this heading">ïƒ</a></h2>
<p>PPO and GRPO use symmetric clipping ranges (e.g., Â±0.2) to limit the magnitude of policy updates. While this ensures stability, it also restricts the model's exploratory capabilities. Specifically, when certain tokens have extremely low probabilities under the old policy, even if the current gradient indicates they should be reinforced (A &gt; 0), the maximum increase is strictly limited.</p>
<p>DAPO employs an asymmetric clipping range, raising the upper clipping limit to encourage exploration:</p>
<ul class="simple">
<li><p>The upper bound (encouragement side) is relaxed to 0.28.</p></li>
<li><p>The lower bound (suppression side) remains unchanged at 0.2.</p></li>
</ul>
<p>In GRPO, the default symmetric clipping range is set using <code class="docutils literal notranslate"><span class="pre">epsilon</span></code>.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">epsilon_high</span></code> sets the upper clipping range, while <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> serves as the lower clipping range.</p></li>
</ul>
</section>
<section id="dynamic-sampling">
<h2>Dynamic Sampling<a class="headerlink" href="#dynamic-sampling" title="Link to this heading">ïƒ</a></h2>
<p>GRPO samples multiple responses per question to compute inter-group advantages:</p>
<p>$$
\hat{A}<em>{i,t} = \frac{R_i - \text{mean}({R_j}</em>{j=1}^G)}{\text{std}({R_j}_{j=1}^G)}
$$</p>
<p>However, when all generated outputs {o_i} receive the same reward, the inter-group advantage becomes zero, leading to vanishing gradients and reduced training efficiency.</p>
<p>DAPO addresses this issue with a dynamic sampling strategy:</p>
<ul class="simple">
<li><p>Skips data with zero inter-group reward standard deviation during sampling.</p></li>
<li><p>Continues generating samples until the batch is filled.</p></li>
</ul>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_sample</span> <span class="pre">true</span></code> enables dynamic sampling.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_resample_times</span></code> sets the maximum number of resampling attempts.</p></li>
</ul>
</section>
<section id="token-level-loss">
<h2>Token level Loss<a class="headerlink" href="#token-level-loss" title="Link to this heading">ïƒ</a></h2>
<p>GRPO normalizes losses at the sentence level, which introduces bias based on response length.</p>
<p>DAPO uses token-level normalization to avoid this bias in loss calculation.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loss_type</span> <span class="pre">bnpo/dapo</span></code> enables token-level normalization.</p></li>
</ul>
<blockquote>
<div><p>For the loss_type formula, please refer to the <a class="reference internal" href="../DeveloperGuide/loss_types.html"><span class="doc">documentation</span></a>.</p>
</div></blockquote>
</section>
<section id="overlong-filtering">
<h2>Overlong Filtering<a class="headerlink" href="#overlong-filtering" title="Link to this heading">ïƒ</a></h2>
<p>DAPO argues that forcibly truncated responses contain high reward noise, making it difficult for the model to distinguish between quality issues and length issues. To address this, DAPO filters out truncated data during training, excluding it from loss computation.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">overlong_filter</span></code> enables filtering of overly long samples.</p></li>
</ul>
</section>
<section id="soft-overlong-punishment">
<h2>Soft Overlong Punishment<a class="headerlink" href="#soft-overlong-punishment" title="Link to this heading">ïƒ</a></h2>
<p>Language models often struggle with controlling output length:</p>
<ul class="simple">
<li><p>Overly long outputs may be truncated, leading to incorrect judgments of valid content.</p></li>
<li><p>Unconstrained length generation affects practicality and computational efficiency.</p></li>
</ul>
<p>DAPO designs a three-stage length penalty function:</p>
<p>$$
R_{\text{length}}(L) =
\begin{cases}
0, &amp; L \leq L_{\text{max}} - L_{\text{cache}} \[10pt]
\dfrac{(L_{\text{max}} - L_{\text{cache}}) - L}{L_{\text{cache}}}, &amp; L_{\text{max}} - L_{\text{cache}} &lt; L \leq L_{\text{max}} \[10pt]
-1, &amp;  L &gt; L_{\text{max}}
\end{cases}
$$</p>
<p>When the length falls within the interval $(L_{\text{max}} - L_{\text{cache}} &lt; L \leq L_{\text{max}})$, a linearly increasing penalty is applied. For lengths $(L &gt; L_{\text{max}})$, the maximum penalty (-1) is imposed.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">reward_funcs</span> <span class="pre">soft_overlong</span></code> enables this reward function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">soft_max_length</span></code> sets L_max, which defaults to the model's maximum output length (<code class="docutils literal notranslate"><span class="pre">max_completion_length</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">soft_cache_length</span></code> sets L_cache.</p></li>
</ul>
</section>
<section id="parameter-settings">
<h2>Parameter Settings<a class="headerlink" href="#parameter-settings" title="Link to this heading">ïƒ</a></h2>
<p>In summary, the following parameters can be set based on GRPOTrainer to implement DAPO training.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--loss_type</code></td>
<td><code>str</code></td>
<td><code>bnpo</code>/<code>dapo</code></td>
</tr>
<tr>
<td><code>--epsilon_high</code></td>
<td><code>float</code></td>
<td><code>0.28</code></td>
</tr>
<tr>
<td><code>--dynamic_sample</code></td>
<td><code>bool</code></td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>--max_resample_times</code></td>
<td><code>int</code></td>
<td><code>3</code></td>
</tr>
<tr>
<td><code>--overlong_filter</code></td>
<td><code>bool</code></td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>--reward_funcs</code></td>
<td><code>str</code></td>
<td><code>soft_overlong</code></td>
</tr>
<tr>
<td><code>--soft_cache_length</code></td>
<td><code>int</code></td>
<td><code>4096</code></td>
</tr>
</tbody>
</table></section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>