

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reward Function &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Reward Function</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/GRPO/DeveloperGuide/reward_function.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reward-function">
<h1>Reward Function<a class="headerlink" href="#reward-function" title="Link to this heading">ïƒ</a></h1>
<section id="custom-reward-function">
<h2>Custom Reward Function<a class="headerlink" href="#custom-reward-function" title="Link to this heading">ïƒ</a></h2>
<p>The reward function takes as arguments (via kwargs) the model-generated completions, other columns from the dataset, and the training state, and calculates a reward score. The <a class="reference external" href="https://huggingface.co/docs/transformers/main/main_classes/callback#transformers.TrainerState">trainer state</a> includes information such as the current training step.</p>
<p>Note: The columns related to model input (such as query and response) are converted to the messages key. The original assistant response in the dataset will be discarded, so please use extra columns if you wish to retain it.
The relevant column names for processing can be found in the <a class="reference external" href="../../../Customization/Custom-dataset.md#Query-Response">document</a></p>
<p>Below is an example illustrating how to implement a simple length-based reward function. This function assigns a reward of 1.0 if the length of the generated completion exceeds 1024, and 0.0 otherwise.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">swift.rewards</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORM</span><span class="p">,</span> <span class="n">orms</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DummyLengthRewardFunction</span><span class="p">(</span><span class="n">ORM</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="n">completions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">completion</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1024</span> <span class="k">else</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">completion</span> <span class="ow">in</span> <span class="n">completions</span><span class="p">]</span>

<span class="n">orms</span><span class="p">[</span><span class="s1">&#39;dummy&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">DummyLengthRewardFunction</span>
</pre></div>
</div>
<p><strong>Accessing Other Columns in the Dataset</strong>
For example, if the reward function needs to access the solution column from the dataset, as well as the current training step and the total number of steps for calculation, there are two ways to retrieve these values:</p>
<p>Explicitly define the column name in the <strong>call</strong> parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="n">completions</span><span class="p">,</span> <span class="n">solution</span><span class="p">,</span> <span class="n">trainer_state</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">solution</span><span class="p">)</span>
        <span class="n">global_step</span> <span class="o">=</span> <span class="n">trainer_state</span><span class="o">.</span><span class="n">global_step</span>
        <span class="n">max_steps</span> <span class="o">=</span> <span class="n">trainer_state</span><span class="o">.</span><span class="n">max_steps</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Retrieve it from kwargs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="n">completions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">solution</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;solution&#39;</span><span class="p">)</span>
        <span class="n">trainer_state</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;trainer_state&#39;</span><span class="p">)</span>
        <span class="n">global_step</span> <span class="o">=</span> <span class="n">trainer_state</span><span class="o">.</span><span class="n">global_step</span>
        <span class="n">max_steps</span> <span class="o">=</span> <span class="n">trainer_state</span><span class="o">.</span><span class="n">max_steps</span>
        <span class="o">...</span>
</pre></div>
</div>
<p><strong>Using Custom Reward Functions</strong></p>
<p>You can add the reward function in <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/grpo/plugin/plugin.py">plugin program</a>, register it using the parameter <code class="docutils literal notranslate"><span class="pre">--external_plugins</span> <span class="pre">examples/train/grpo/plugin/plugin.py</span></code>, and specify it via the <code class="docutils literal notranslate"><span class="pre">reward_funcs</span></code> parameter.</p>
<p>For execution scripts, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/plugin/run_external_reward_func.sh">here</a>.</p>
</section>
<section id="async-reward-functions">
<h2>Async Reward Functions<a class="headerlink" href="#async-reward-functions" title="Link to this heading">ïƒ</a></h2>
<p><strong>Version requirement</strong>: ms-swift&gt;=3.12.1</p>
<p>For reward functions involving I/O operations (such as API calls, database queries, etc.), you can use asynchronous (async) reward functions to improve performance. Async reward functions are executed in parallel using <code class="docutils literal notranslate"><span class="pre">asyncio.gather</span></code>, which can significantly speed up reward computation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">swift.rewards</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncORM</span><span class="p">,</span> <span class="n">orms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AsyncAPIReward</span><span class="p">(</span><span class="n">AsyncORM</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">completions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">aiohttp</span>

        <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">score_single</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
            <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
                <span class="s1">&#39;https://api.example.com/score&#39;</span><span class="p">,</span>
                <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">text</span><span class="p">}</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">resp</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
                <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span>

        <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="o">.</span><span class="n">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="c1"># Use asyncio.gather to send all requests in parallel</span>
            <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">score_single</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">completions</span><span class="p">]</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>

<span class="n">orms</span><span class="p">[</span><span class="s1">&#39;async_api&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">AsyncAPIReward</span>
</pre></div>
</div>
<p>Swift supports using both synchronous and asynchronous reward functions simultaneously. The trainer automatically detects the type of reward function:</p>
<ul class="simple">
<li><p>Synchronous reward functions are executed sequentially</p></li>
<li><p>Asynchronous reward functions are executed in parallel using <code class="docutils literal notranslate"><span class="pre">asyncio.gather</span></code></p></li>
</ul>
<p>The <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/grpo/plugin/plugin.py">plugin</a> file provides an example of a generative reward model (async_genrm) that calls the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">deploy</span></code> service.</p>
</section>
<section id="built-in-reward-functions">
<h2>Built-in Reward Functions<a class="headerlink" href="#built-in-reward-functions" title="Link to this heading">ïƒ</a></h2>
<p>Swift includes five rule-based reward functions (code can be found in swift/rewards/orm.py).</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Reward Function</th>
<th>Paper</th>
</tr>
</thead>
<tbody>
<tr>
<td>accuracy</td>
<td><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL</a></td>
</tr>
<tr>
<td>format</td>
<td>Same as above</td>
</tr>
<tr>
<td>cosine</td>
<td><a href="https://arxiv.org/abs/2502.03373">Demystifying Long Chain-of-Thought Reasoning in LLMs</a></td>
</tr>
<tr>
<td>repetition</td>
<td>Same as above</td>
</tr>
<tr>
<td>soft_overlong</td>
<td><a href="https://arxiv.org/abs/2503.14476">Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)</a></td>
</tr>
</tbody>
</table><section id="accuracy">
<h3>1. <strong>accuracy</strong><a class="headerlink" href="#accuracy" title="Link to this heading">ïƒ</a></h3>
<p>This function compares the model's generated output with the solution column in the dataset to calculate an accuracy score. If the generated output matches the reference answer, the score is 1.0; otherwise, it is 0.0.</p>
<p>Note: This reward function uses the <code class="docutils literal notranslate"><span class="pre">math_verify</span></code> library to parse the generated output and the solution, which may only be applicable to specific mathematical datasets.</p>
</section>
<section id="format">
<h3>2. <strong>format</strong><a class="headerlink" href="#format" title="Link to this heading">ïƒ</a></h3>
<p>The paper uses the following system prompt to require the model to return responses in a fixed format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="n">conversation</span> <span class="n">between</span> <span class="n">User</span> <span class="ow">and</span> <span class="n">Assistant</span><span class="o">.</span> <span class="n">The</span> <span class="n">user</span> <span class="n">asks</span> <span class="n">a</span> <span class="n">question</span><span class="p">,</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">Assistant</span> <span class="n">solves</span> <span class="n">it</span><span class="o">.</span> <span class="n">The</span> <span class="n">assistant</span> <span class="n">first</span> <span class="n">thinks</span> <span class="n">about</span> <span class="n">the</span> <span class="n">reasoning</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">mind</span> <span class="ow">and</span> <span class="n">then</span> <span class="n">provides</span> <span class="n">the</span> <span class="n">user</span> <span class="k">with</span> <span class="n">the</span> <span class="n">answer</span><span class="o">.</span> <span class="n">The</span> <span class="n">reasoning</span> <span class="n">process</span> <span class="ow">and</span> <span class="n">answer</span> <span class="n">are</span> <span class="n">enclosed</span> <span class="n">within</span> <span class="o">&lt;</span><span class="n">think</span><span class="o">&gt;</span> <span class="o">&lt;/</span><span class="n">think</span><span class="o">&gt;</span> <span class="ow">and</span> <span class="o">&lt;</span><span class="n">answer</span><span class="o">&gt;</span> <span class="o">&lt;/</span><span class="n">answer</span><span class="o">&gt;</span> <span class="n">tags</span><span class="p">,</span> <span class="n">respectively</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">think</span><span class="o">&gt;</span> <span class="n">reasoning</span> <span class="n">process</span> <span class="n">here</span> <span class="o">&lt;/</span><span class="n">think</span><span class="o">&gt;&lt;</span><span class="n">answer</span><span class="o">&gt;</span> <span class="n">answer</span> <span class="n">here</span> <span class="o">&lt;/</span><span class="n">answer</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>This function checks whether the model generates text in the format <code class="docutils literal notranslate"><span class="pre">&lt;think&gt;think</span> <span class="pre">content&lt;/think&gt;&lt;answer&gt;answer</span> <span class="pre">content&lt;/answer&gt;</span></code>. If the generated text meets the format requirements, the score is 1.0; otherwise, it is 0.0.</p>
</section>
<section id="cosine">
<h3>3. <strong>cosine</strong><a class="headerlink" href="#cosine" title="Link to this heading">ïƒ</a></h3>
<p>The paper found that using only the accuracy reward function for training could lead to excessively long generated outputs, thereby affecting training effectiveness. The cosine reward function optimizes the training process by controlling the length of the model's outputs:</p>
<ul class="simple">
<li><p>For texts with correct answers, the reward decreases as the length increases, encouraging the model to generate concise responses.</p></li>
<li><p>For texts with incorrect answers, the reward increases as the length increases, encouraging the model to think more deeply.</p></li>
</ul>
<p>A cosine function is used to smoothly adjust the reward value, ensuring the changes remain within a reasonable range. The parameters of the cosine function include the length of the generated text, the maximum length limit, and the minimum and maximum reward values.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p>cosine_min_len_value_wrong (default: -0.5): The reward value for the minimum length when the answer is incorrect.</p></li>
<li><p>cosine_max_len_value_wrong (default: 0.0): The reward value for the maximum length when the answer is incorrect.</p></li>
<li><p>cosine_min_len_value_correct (default: 1.0): The reward value for the minimum length when the answer is correct.</p></li>
<li><p>cosine_max_len_value_correct (default: 0.5): The reward value for the maximum length when the answer is correct.</p></li>
<li><p>cosine_max_len (default equals the model's maximum generation length): The maximum length limit for the generated text.</p></li>
</ul>
</section>
<section id="repetition">
<h3>4. <strong>repetition</strong><a class="headerlink" href="#repetition" title="Link to this heading">ïƒ</a></h3>
<p>Penalizes repetitive content in the model's generated text by detecting repeated n-gram patterns and applying corresponding penalties.</p>
<p>The function splits the generated text into words and extracts n-grams of a specified size (default: 3-grams). By calculating the ratio of unique n-grams to the total number of n-grams, it determines the repetition rate. If the repetition rate is high, a larger negative reward (penalty) is applied. The penalty value is calculated based on the repetition rate and the maximum penalty value (default: -1.0).</p>
<p>Parameters:</p>
<ul class="simple">
<li><p>repetition_n_grams (default: 3): The size of n-grams used to detect repetition.</p></li>
<li><p>repetition_max_penalty (default: -1.0): The maximum penalty value, controlling the penalty strength.</p></li>
</ul>
</section>
<section id="soft-overlong-punishment">
<h3>5. <strong>soft overlong punishment</strong><a class="headerlink" href="#soft-overlong-punishment" title="Link to this heading">ïƒ</a></h3>
<p>Defines a length penalty interval. Within this interval, a linear penalty in the range [-1, 0] is applied.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p>soft_max_length: L_max in the paper, the model's maximum generation length, defaulting to max_completion_length.</p></li>
<li><p>soft_cache_length: L_cache in the paper, controlling the length penalty interval, which is [soft_max_length - soft_cache_length, soft_max_length].</p></li>
</ul>
</section>
</section>
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Link to this heading">ïƒ</a></h2>
<p>If a model needs to be loaded in the reward function, the training DeepSpeed plugin (transformers logic) will be used by default. Under Zero3, this may cause the model to fail to perform inference properly. Refer to this <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/4580">issue</a> to skip the DeepSpeed initialization environment.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>