

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reward Model &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Reward Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/GRPO/DeveloperGuide/reward_model.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reward-model">
<h1>Reward Model<a class="headerlink" href="#reward-model" title="Link to this heading">ïƒ</a></h1>
<p>By default, a reward model refers to a model with a classification head that outputs numeric values, usually called an Output Reward Model (ORM). These models score the outputs from other models and produce a scalar value representing the quality of the model response.</p>
<p>You can load reward models with a classification head using the <code class="docutils literal notranslate"><span class="pre">reward_models</span></code> parameter, or load reward models trained by <a class="reference external" href="../../RLHF.md#rm">reward modeling</a>, and then use the model's logits as rewards.</p>
<section id="custom-reward-models">
<h2>Custom Reward Models<a class="headerlink" href="#custom-reward-models" title="Link to this heading">ïƒ</a></h2>
<p>For generative reward models, there are two common ways to use them: one is by directly defining the reward model logic inside the Trainer via the <code class="docutils literal notranslate"><span class="pre">reward_model_plugin</span></code>, and then using TransformersEngine for inference; the other is to call an externally deployed model service.</p>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">reward_model_plugin</span></code>, the reward model will be embedded within the Trainer and does not require additional computational resources. The advantage of this approach is ease of integration, but generation speed is relatively slow, making it more suitable for small-parameter reward models.</p></li>
<li><p>When deploying reward models externally, you can use commands like <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">deploy</span></code> or <code class="docutils literal notranslate"><span class="pre">vllm</span> <span class="pre">serve</span></code> to deploy the model service on an independent device to greatly improve inference speed, which is more suitable for large models. However, this approach requires reserving extra hardware resources.</p></li>
</ul>
<section id="internal-plugin">
<h3>Internal Plugin<a class="headerlink" href="#internal-plugin" title="Link to this heading">ïƒ</a></h3>
<p>You can flexibly customize the reward model processing logic inside <code class="docutils literal notranslate"><span class="pre">reward_model_plugin</span></code>. This enables implementations such as generative reward models, including:</p>
<ul class="simple">
<li><p>Custom model system prompts: define specific instructions and context to guide the evaluation process.</p></li>
<li><p>Handling model interaction history: manage dialog context to allow meaningful and context-aware evaluation.</p></li>
<li><p>Defining custom evaluation metrics: set unique criteria and measures for response evaluation, beyond the default accuracy and relevance checks.</p></li>
</ul>
<p>With <code class="docutils literal notranslate"><span class="pre">reward_model_plugin</span></code>, developers can tailor the reward evaluation process for specific application needs. This flexibility allows for more fine-grained and effective reward-based training strategies.</p>
<p>The reward model is called via the plugin's <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method, which takes <code class="docutils literal notranslate"><span class="pre">inputs</span></code> as a parameter. <code class="docutils literal notranslate"><span class="pre">inputs</span></code> contains the messages of model input/output and other columns from the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">[</span>
<span class="sd">    {</span>
<span class="sd">        &#39;messages&#39;: [</span>
<span class="sd">                {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;system prompt&#39;},</span>
<span class="sd">                {&#39;role&#39;: &#39;query&#39;, &#39;content&#39;: &#39;query&#39;},</span>
<span class="sd">                {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;completions1&#39;},</span>
<span class="sd">            ],</span>
<span class="sd">        &#39;solution&#39;: &quot;abc&quot;,</span>
<span class="sd">    },</span>
<span class="sd">    {</span>
<span class="sd">        &#39;messages&#39;: [</span>
<span class="sd">                {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;system prompt&#39;},</span>
<span class="sd">                {&#39;role&#39;: &#39;query&#39;, &#39;content&#39;: &#39;query&#39;},</span>
<span class="sd">                {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;completions2&#39;},</span>
<span class="sd">            ],</span>
<span class="sd">        &#39;solution&#39;: &quot;abc&quot;,</span>
<span class="sd">    }</span>
<span class="sd">]</span>
<span class="sd">        &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>When using TransformersEngine in the plugin for reward model inference, you only need to construct messages and call the infer interface:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RMPlugin</span><span class="p">(</span><span class="n">DefaultRMPlugin</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">template</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">template</span><span class="p">)</span>
        <span class="c1"># initilize TransformersEngine to infer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">TransformersEngine</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">template</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">template</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">system_prompt</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">query</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;query&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">query</span><span class="p">}]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">infer</span><span class="p">([</span><span class="n">messages</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_config</span><span class="p">,</span> <span class="n">use_tqdm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="o">...</span>
        <span class="k">return</span> <span class="n">rewards</span>
</pre></div>
</div>
<p>We provide a simple example of a generative reward model (<code class="docutils literal notranslate"><span class="pre">GenRMPlugin</span></code>) in <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/swift/rewards/rm_plugin.py">rm_plugin.py</a>.</p>
<p>You can customize your reward model plugin in <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/train/grpo/plugin/plugin.py">plugin.py</a> and register it using the <code class="docutils literal notranslate"><span class="pre">external_plugins</span></code> parameter.</p>
<p>Note:</p>
<ol class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">GRPOTrainer</span></code>, the reward_model will be appended to reward_funcs one by one. Therefore, the order of <code class="docutils literal notranslate"><span class="pre">reward_weights</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">[reward_funcs,</span> <span class="pre">reward_model]</span></code>.</p></li>
<li><p>The default for <code class="docutils literal notranslate"><span class="pre">reward_model_plugin</span></code> is <code class="docutils literal notranslate"><span class="pre">default</span></code>, which uses ORM logic.</p></li>
<li><p>For models with a large number of parameters, TransformersEngine generation is slow. Please use <a class="reference external" href="#external-deployment">external deployment</a>.</p></li>
</ol>
<p>For models like BERT that cannot be loaded by <code class="docutils literal notranslate"><span class="pre">reward_model</span></code>, you can load them inside <code class="docutils literal notranslate"><span class="pre">reward_function</span></code>, see <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/4580">issue</a>.</p>
</section>
<section id="external-deployment">
<h3>External Deployment<a class="headerlink" href="#external-deployment" title="Link to this heading">ïƒ</a></h3>
<p>This approach does not require the <code class="docutils literal notranslate"><span class="pre">reward_model_plugin</span></code> and can be called directly in the reward function.</p>
<p>First, use the following command to start the model service:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: Do not overlap deployment devices with training devices</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3<span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>deploy<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-72B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">4</span>

<span class="c1"># [INFO:swift] model_list: [&#39;Qwen2.5-72B-Instruct&#39;]</span>
<span class="c1"># INFO:     Started server process [xxxxxx]</span>
<span class="c1"># INFO:     Waiting for application startup.</span>
<span class="c1"># INFO:     Application startup complete.</span>
<span class="c1"># INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)</span>
</pre></div>
</div>
<p>In the reward function, initialize the client using the OpenAI library and specify the address and port of the model service. Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RMReward</span><span class="p">(</span><span class="n">ORM</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
                <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;EMPTY&#39;</span><span class="p">,</span>
                <span class="n">base_url</span><span class="o">=</span><span class="s1">&#39;http://127.0.0.1:8000/v1&#39;</span><span class="p">,</span> <span class="c1"># 127.0.0.1 if deployed locally</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">verify_model_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">id</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Failed to connect to the model service. Please deploy the model &#39;</span>
                               <span class="s2">&quot;using &#39;swift deploy&#39; or &#39;vllm serve&#39;.&quot;</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">completions</span><span class="p">,</span> <span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">completion</span><span class="p">,</span> <span class="n">message</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">completions</span><span class="p">,</span> <span class="n">messages</span><span class="p">):</span>
            <span class="n">rm_prompt</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># Construct the prompt for the reward model</span>
            <span class="n">chat_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verify_model_name</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span>
                    <span class="p">},</span>
                    <span class="p">{</span>
                        <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">rm_prompt</span>
                    <span class="p">},</span>
                <span class="p">],</span>
            <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">chat_response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># Extract the reward value from the result</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rewards</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>