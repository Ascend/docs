

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GRPO &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">GRPO</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/GRPO/GetStarted/GRPO.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="grpo">
<h1>GRPO<a class="headerlink" href="#grpo" title="Link to this heading">ÔÉÅ</a></h1>
<p>GRPOTrainer underwent a code refactoring in ms-swift3.5. If you are using a swift version &lt; 3.5, please refer to the <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/v3.4.1/docs/source/Instruction/GRPO.md">stable documentation</a>.</p>
<p><a class="reference external" href="https://arxiv.org/abs/2402.03300">GRPO (Group Relative Policy Optimization)</a> leverages intra-group relative advantage calculations to replace the independent value model in the PPO algorithm and directly incorporates KL divergence penalties into the loss function to improve training stability.</p>
<section id="algorithm-overview">
<h2>Algorithm Overview<a class="headerlink" href="#algorithm-overview" title="Link to this heading">ÔÉÅ</a></h2>
<p>GRPO Objective Function is defined as
$
{\scriptstyle
\begin{aligned}
\mathcal{J}<em>{G R P O}(\theta) &amp; =\mathbb{E}</em>{\left[q \sim P(Q),\left{o_i\right}<em>{i=1}^G \sim \pi</em>{\theta_{o l d}}(O \mid q)\right]} \
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left{\min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)} \hat{A}<em>{i, t}, \operatorname{clip}\left(\frac{\pi</em>\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}<em>{i, t}\right]-\beta \mathbb{D}</em>{K L}\left[\pi_\theta| | \pi_{r e f}\right]\right}
\end{aligned}
}
$</p>
<p>The advantage function is defined as</p>
<p>$
\hat{A}<em>{i,t} = \frac{R_i - \text{mean}({R_j}</em>{j=1}^G)}{\text{std}({R_j}_{j=1}^G)}
$</p>
<details> <summary>GRPO Algorithm Pseudocode</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ========== 1. Rollout Generation Phase ==========</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Question: Which is bigger? 9.11 or 9.9?&quot;</span>

<span class="c1"># Generate multiple completions through parallel sampling</span>
<span class="n">completions</span> <span class="o">=</span> <span class="n">rollout_function</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">current_policy_model</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
    <span class="n">num_generations</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># Hyperparameter: number of samples per prompt</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span>     <span class="c1"># Hyperparameter: sampling diversity</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">completions = [</span>
<span class="sd">    (completion 1) &quot;The larger number is 9.11...&quot;,</span>
<span class="sd">    (completion 2) &quot;9.9 is bigger than...&quot;,</span>
<span class="sd">    ...</span>
<span class="sd">    (completion 8) &quot;After calculation, 9.11...&quot;</span>
<span class="sd">]</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># ========== 2. Reward Calculation Phase ==========</span>
<span class="c1"># Evaluate generated completions using reward model</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">reward_function</span><span class="p">(</span>
    <span class="n">completions</span><span class="o">=</span><span class="n">completions</span><span class="p">,</span>
    <span class="n">ground_truth</span><span class="o">=</span><span class="s2">&quot;9.11&quot;</span>  <span class="c1"># Expected correct answer</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">rewards = [</span>
<span class="sd">    (reward 1) 1.0,  # Correct answer</span>
<span class="sd">    (reward 2) 0.0,  # Incorrect</span>
<span class="sd">    ...</span>
<span class="sd">    (reward 8) 1.0   # Correct</span>
<span class="sd">]</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Normalize rewards to advantages</span>
<span class="n">rewards_mean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>  <span class="c1"># Œº = 0.5</span>
<span class="n">rewards_std</span> <span class="o">=</span> <span class="n">std</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>    <span class="c1"># œÉ = 0.25</span>
<span class="n">advantages</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards</span> <span class="o">-</span> <span class="n">rewards_mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">rewards_std</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># Standardization</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">advantages = [</span>
<span class="sd">    (advantage 1)  2.0,  # (1.0 - 0.5)/0.25</span>
<span class="sd">    (advantage 2) -2.0,</span>
<span class="sd">    ...</span>
<span class="sd">    (advantage 8)  2.0</span>
<span class="sd">]</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># ========== 3. Policy Optimization Phase ==========</span>
<span class="c1"># Get token-level log probabilities from different models</span>
<span class="n">current_logps</span> <span class="o">=</span> <span class="n">get_per_token_logps</span><span class="p">(</span><span class="n">current_policy_model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">completions</span><span class="p">)</span>  <span class="c1"># œÄ_Œ∏</span>
<span class="n">old_logps</span> <span class="o">=</span> <span class="n">get_per_token_logps</span><span class="p">(</span><span class="n">old_policy_model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">completions</span><span class="p">)</span>          <span class="c1"># œÄ_Œ∏_old</span>
<span class="n">ref_logps</span> <span class="o">=</span> <span class="n">get_per_token_logps</span><span class="p">(</span><span class="n">reference_model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">completions</span><span class="p">)</span>           <span class="c1"># œÄ_ref</span>

<span class="c1"># PPO Clipped Objective</span>
<span class="n">is_ratio</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">current_logps</span> <span class="o">-</span> <span class="n">old_logps</span><span class="p">)</span>  <span class="c1"># Importance sampling ratio: e^(œÄ_Œ∏ - œÄ_Œ∏_old)</span>
<span class="n">clipped_ratio</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">is_ratio</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">Œµ</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="n">Œµ</span><span class="p">)</span>   <span class="c1"># Œµ=0.2 typically</span>

<span class="c1"># Policy gradient term (dual form)</span>
<span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span>
    <span class="n">minimum</span><span class="p">(</span><span class="n">is_ratio</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">,</span>       <span class="c1"># Unclipped objective</span>
           <span class="n">clipped_ratio</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">)</span>  <span class="c1"># Clipped objective</span>
<span class="p">)</span>

<span class="c1"># KL Divergence Penalty (K3 estimator)</span>
<span class="c1"># KL(œÄ_Œ∏||œÄ_ref) ‚âà e^(logœÄ_ref - logœÄ_Œ∏) - (logœÄ_ref - logœÄ_Œ∏) - 1</span>
<span class="n">kl_penalty</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">mean</span><span class="p">(</span>
    <span class="n">exp</span><span class="p">(</span><span class="n">ref_logps</span> <span class="o">-</span> <span class="n">current_logps</span><span class="p">)</span> <span class="o">-</span>
    <span class="p">(</span><span class="n">ref_logps</span> <span class="o">-</span> <span class="n">current_logps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Total Loss = Policy Loss + KL Penalty</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="n">kl_penalty</span>

<span class="c1"># ========== 4. Update Rule ==========</span>
<span class="c1"># Apply gradient descent to minimize total_loss</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</details><p>For training script examples, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo">examples</a>.</p>
<p>For GRPO parameters, refer to the <a class="reference external" href="../../../Instruction/Command-line-parameters.md#grpo-arguments">documentation</a></p>
</section>
<section id="cluster-support">
<h2>Cluster Support<a class="headerlink" href="#cluster-support" title="Link to this heading">ÔÉÅ</a></h2>
<p><img alt="../../../../../../../../_images/grpo.png" src="../../../../../../../../_images/grpo.png" /></p>
<p>The GRPO training framework supports integration with high-performance inference engines (e.g., vLLM) to accelerate the sampling process, offering the following two deployment modes:</p>
<section id="colocate-internal-mode">
<h3>1. Colocate (Internal) Mode<a class="headerlink" href="#colocate-internal-mode" title="Link to this heading">ÔÉÅ</a></h3>
<p>Training and inference share GPU resources, with the inference service launched internally within the Trainer.</p>
<p>Startup parameters</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--use_vllm<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
--vllm_mode<span class="w"> </span>colocate
</pre></div>
</div>
<section id="memory-optimization-solutions-in-colocate-mode">
<h4>Memory Optimization Solutions in Colocate Mode<a class="headerlink" href="#memory-optimization-solutions-in-colocate-mode" title="Link to this heading">ÔÉÅ</a></h4>
<p>When running in Colocate mode, out-of-memory (OOM) issues may frequently occur. Below are several effective memory optimization methods and parameter configurations:</p>
<ol class="simple">
<li><p>Reduce the vllm_gpu_memory_utilization parameter.</p></li>
<li><p>During the training phase, release the GPU memory occupied by vLLM:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--sleep_level<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<ol class="simple">
<li><p>During the vLLM inference phase, release the GPU memory occupied by the model and optimizer:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--offload_optimizer<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
--offload_model<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<ol class="simple">
<li><p>Use Tensor Parallelism in vLLM:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--vllm_tensor_parallel_size<span class="w"> </span><span class="o">[</span>tp_size<span class="o">]</span>
</pre></div>
</div>
<ol class="simple">
<li><p>Gather model weights in batches (when synchronizing vLLM weights under zero3):</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--move_model_batches<span class="w"> </span><span class="o">[</span>ÊâπÊ¨°Êï∞Èáè<span class="o">]</span>
</pre></div>
</div>
<ol class="simple">
<li><p>Store Megatron exported HF format weights for vLLM updates in CPU main memory to reduce GPU memory usage:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--offload_bridge<span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
</section>
</section>
<section id="async-external-mode">
<h3>2. Async(External) Mode<a class="headerlink" href="#async-external-mode" title="Link to this heading">ÔÉÅ</a></h3>
<p>Training and inference resources are separated, with a dedicated inference server deployed.</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">rollout</span></code> command to deploy the vLLM server (currently only supports vLLM backend):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rollout<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--vllm_data_parallel_size<span class="w"> </span><span class="m">1</span>

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rollout<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--vllm_data_parallel_size<span class="w"> </span><span class="m">1</span>

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3<span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>rollout<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--vllm_data_parallel_size<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<p>For more rollout parameters, refer to the <a class="reference external" href="../../../Instruction/Command-line-parameters.md#vllm-arguments">vllm arguments</a> and <a class="reference external" href="../../../Instruction/Command-line-parameters.md#rollout-arguments">rollout arguments</a></p>
<p>Note: When set <code class="docutils literal notranslate"><span class="pre">use_async_engine</span></code>, enabling only DP (Data Parallelism) may cause errors. <a class="reference external" href="https://github.com/vllm-project/vllm/issues/18567">Related issue</a>. If errors occur, try enabling both TP (Tensor Parallelism) and DP or upgrading vLLM.</p>
<p>To configure the external vLLM server during training, use the following parameters:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--use_vllm<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
--vllm_mode<span class="w"> </span>server<span class="w"> </span><span class="se">\</span>
--vllm_server_host<span class="w"> </span>&lt;server_IP&gt;<span class="w"> </span><span class="se">\</span>
--vllm_server_port<span class="w"> </span>&lt;service_port&gt;<span class="w"> </span><span class="se">\</span>
--vllm_server_timeout<span class="w"> </span>&lt;timeout&gt;<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<section id="weight-sync-acceleration">
<h4>Weight-Sync Acceleration<a class="headerlink" href="#weight-sync-acceleration" title="Link to this heading">ÔÉÅ</a></h4>
<p>Swift 3.10 optimizes weight synchronization, and setting the following parameters can further improve the weight synchronization speed for LoRA training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># rollout(server mode)</span>
swift<span class="w"> </span>rollout<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_enable_lora<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_max_lora_rank<span class="w"> </span>xxx<span class="w"> </span><span class="c1"># match the lora_rank in the training script</span>
<span class="w">    </span>...

<span class="c1"># grpo(colocate mode)</span>
swift<span class="w"> </span>rlhf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--rlhf_type<span class="w"> </span>grpo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_mode<span class="w"> </span>colocate<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_enable_lora<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>...
</pre></div>
</div>
<p>Note: This optimization cannot be used in the following cases:</p>
<ul class="simple">
<li><p>Training the ViT layers of multimodal models (freeze_vit set to false)</p></li>
<li><p>MoE models</p></li>
</ul>
<p>For implementation details, please refer to the <a class="reference external" href="https://github.com/modelscope/ms-swift/pull/5773">PR</a></p>
</section>
</section>
</section>
<section id="logged-metrics">
<h2>logged metrics<a class="headerlink" href="#logged-metrics" title="Link to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>completions/mean_length: The average length of generated completions.</p></li>
<li><p>completions/min_length: The minimum length among generated completions.</p></li>
<li><p>completions/max_length: The maximum length among generated completions.</p></li>
<li><p>completions/clipped_ratio: The proportion of completions that were truncated due to length limits.</p></li>
<li><p>reward/{reward_func_name}/mean: The average reward value for a specific reward function.</p></li>
<li><p>reward/{reward_func_name}/std: The standard deviation of the reward for a specific reward function.</p></li>
</ul>
<blockquote>
<div><p>Note: These two metrics are calculated across all completions.</p>
</div></blockquote>
<ul class="simple">
<li><p>reward: The overall average reward after applying reward_weights.</p></li>
<li><p>reward_std: The standard deviation of the overall reward within each batch after applying reward_weights.</p></li>
</ul>
<blockquote>
<div><p>Note: These two metrics are first computed within each group and then averaged (for mean/std) across groups.</p>
</div></blockquote>
<ul class="simple">
<li><p>frac_reward_zero_std: The proportion of samples in a generation batch where the reward standard deviation is zero, meaning there is almost no diversity in answers for that prompt (i.e., the rewards of all completions are same).</p></li>
<li><p>kl: The average KL divergence between the model and the reference model on completions. This is logged only if beta is nonzero.</p></li>
<li><p>clip_ratio/region_mean: The average proportion of tokens clipped by the CLIP operator across different sentences.</p></li>
<li><p>clip_ratio/low_mean: The average proportion of tokens clipped by the lower CLIP bound across different sentences.</p></li>
<li><p>clip_ratio/low_min: The minimum proportion of tokens clipped by the lower CLIP bound across different sentences.</p></li>
<li><p>clip_ratio/high_mean: The average proportion of tokens clipped by the upper CLIP bound across different sentences.</p></li>
<li><p>clip_ratio/high_max: The maximum proportion of tokens clipped by the upper CLIP bound across different sentences.</p></li>
</ul>
<blockquote>
<div><p>Note: If <code class="docutils literal notranslate"><span class="pre">overlong_filter</span></code> is enabled, the kl and clip_ratio metrics will exclude overlength samples.</p>
</div></blockquote>
<p>If the <code class="docutils literal notranslate"><span class="pre">log_entropy</span></code> parameter is set, additional entropy-related metrics will be logged, including:</p>
<ul class="simple">
<li><p>entropy/mean: the average entropy across different sentences</p></li>
<li><p>entropy/max: the maximum entropy among different sentences</p></li>
<li><p>entropy/min: the minimum entropy among different sentences</p></li>
</ul>
<blockquote>
<div><p>Note: Here, sentence entropy refers to the mean entropy of tokens in each completion.</p>
</div></blockquote>
<p>If <code class="docutils literal notranslate"><span class="pre">top_entropy_quantile</span></code> is set to a value smaller than 1.0, the entropy threshold value will also be recorded:</p>
<ul class="simple">
<li><p>entropy/threshold: Tokens with entropy below this value will be excluded from the loss calculation.</p></li>
</ul>
<p>Training-inference consistency metrics, prefixed with rollout_correction (ms-swift&gt;=3.11), requires setting <code class="docutils literal notranslate"><span class="pre">log_rollout_offpolicy_metrics=true</span></code> or <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">kl</span></code> / <code class="docutils literal notranslate"><span class="pre">k3_kl</span></code>: KL divergence between training policy and rollout policy (direct estimator / K3 estimator)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training_ppl</span></code> / <code class="docutils literal notranslate"><span class="pre">rollout_ppl</span></code>: Perplexity of training policy and rollout policy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_ppl_diff</span></code>: Log PPL difference, reflects the degree of distribution shift</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ppl_ratio</span></code>: PPL ratio</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chi2_token</span></code> / <code class="docutils literal notranslate"><span class="pre">chi2_seq</span></code>: Token/Sequence-level œá¬≤ divergence</p></li>
</ul>
<p>IS correction metrics (requires setting <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code>):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">is_weight_mean</span></code>: Average importance sampling weight</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ess</span></code>: Effective Sample Size</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">clipped_frac</span></code>: Fraction of samples that were truncated or masked</p></li>
</ul>
<blockquote>
<div><p>For detailed explanation of training-inference consistency metrics, please refer to <a class="reference internal" href="../AdvancedResearch/training_inference_mismatch.html"><span class="doc">Training-Inference-Mismatch</span></a></p>
</div></blockquote>
<p>If <code class="docutils literal notranslate"><span class="pre">log_completions</span></code> is set, the training dynamics will be saved in the output directory, including:</p>
<ul class="simple">
<li><p>step: The training step at the time of logging.</p></li>
<li><p>prompt: The model input.</p></li>
<li><p>completion: The model's sampled answer.</p></li>
<li><p>{reward_func_name}: The specific reward(s).</p></li>
<li><p>entropy: The average token entropy (recorded if <code class="docutils literal notranslate"><span class="pre">log_entropy</span></code> is set).</p></li>
</ul>
<p>Setting <code class="docutils literal notranslate"><span class="pre">report_to</span> <span class="pre">wandb/swanlab</span></code> will send training dynamics table to the respective platform.</p>
<p>If you want to log extra columns in the Table, populate the <code class="docutils literal notranslate"><span class="pre">metrics_to_gather</span></code> dictionary inside <code class="docutils literal notranslate"><span class="pre">GRPOTrainer._generate_and_score_completions</span></code>.</p>
<p>The trainer automatically detects and logs the following keys:</p>
<ul class="simple">
<li><p>image: image inputs for vision models(wandb only).</p></li>
<li><p>solution: the solution column from the dataset.</p></li>
</ul>
</section>
<section id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Link to this heading">ÔÉÅ</a></h2>
<p><strong>1. Loss Equals Zero / Approaches Zero / Is Negative During Training</strong></p>
<p>This is normal behavior. For reference, see <a class="reference external" href="https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851">issue</a>.</p>
<hr class="docutils" />
<p><strong>2. num_generations / Batch Size Related</strong></p>
<p>In GRPO, the batch size is measured in terms of completions (i.e., model-generated outputs). For example, setting <code class="docutils literal notranslate"><span class="pre">per_device_train_batch_size=8</span></code> means that each GPU processes 8 completions for loss calculation during training.</p>
<p>During the training phase, the total effective batch size in a full gradient accumulation step equals:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">num_processes</span> <span class="o">*</span> <span class="n">per_device_train_batch_size</span> <span class="o">*</span> <span class="n">gradient_accumulation_steps</span>
</pre></div>
</div>
<p>During the sampling phase, the total batch size (completion-level) depends on the following:</p>
<ul class="simple">
<li><p>If generation_batch_size is set, the total equals generation_batch_size.</p></li>
<li><p>If steps_per_generation is set, the total equals per_device_train_batch_size * steps_per_generation * num_processes.</p></li>
<li><p>By default, steps_per_generation is set to gradient_accumulation_steps, and generation_batch_size equals the per_device_train_batch_size * steps_per_generation * num_processes = per_device_train_batch_size * gradient_accumulation_steps * num_processes = effective_batch_size.</p></li>
</ul>
<p>During evaluation, the number of completions equals:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_processes</span> <span class="o">*</span> <span class="n">per_device_eval_batch_size</span>
</pre></div>
</div>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">num_generations</span></code> must be divisible by the total batch size used in sampling and evaluation to ensure even distribution across devices.</p>
<p><strong>Example</strong></p>
<ul class="simple">
<li><p>num_processes = 8</p></li>
<li><p>per_device_train_batch_size = 4</p></li>
<li><p>gradient_accumulation_steps = 8</p></li>
<li><p>generation_batch_size = 512</p></li>
<li><p>num_generations = 64</p></li>
</ul>
<ol class="simple">
<li><p>Total prompts needed for sampling: 512 / 64 = 8</p></li>
<li><p>Generate 512 responses from the model per sampling step</p></li>
<li><p>Model update batch size: 8 * 4 * 8 = 256</p></li>
</ol>
<p><strong>3. Why did KL result in NaN?</strong></p>
<p>With <code class="docutils literal notranslate"><span class="pre">overlong_filter</span></code> enabled, all completions on a certain GPU were truncated.</p>
<p><strong>4. How is the training steps calculated?</strong></p>
<p>Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/issues/3912">issue</a>.</p>
<p><strong>5. Why is the clip ratio always 0?</strong></p>
<p>The core purpose of the clip mechanism is to limit the magnitude of policy updates to prevent policy performance collapse due to excessively large updates (i.e., a drastic decline in performance after policy updates). The specific formula for the clip operation is as follows:</p>
<p>$$
L_{\text{CLIP}}(\theta) = \mathbb{E}<em>{t} \left[ \min\left(r</em>{t}(\theta) \hat{A}<em>{t}, \text{clip}(r</em>{t}(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{t} \right) \right]
$$</p>
<p>Where: $r_{t}(\theta) = \frac{\pi_{\theta}(a_{t} \mid s_{t})}{\pi_{\text{old}}(a_{t} \mid s_{t})}$ is the importance sampling ratio, which measures the difference between the new and old policy. $\hat{A}<em>{t}$ is the advantage function, representing the relative return of the action. $\epsilon$ is used to limit the deviation range of $r</em>{t}(\theta)$.</p>
<p>In the on-policy training process, since each update uses data generated by the latest policy, the new and old policies are the same, i.e., $\pi_{\theta} = \pi_{\text{old}}$.</p>
<p>Thus, the importance sampling ratio is always 1, and the clip operation does not take effect.</p>
<p>The algorithm becomes off-policy (near-on-policy) under the following parameter settings:</p>
<ol class="simple">
<li><p>num_iterations &gt; 1, or</p></li>
<li><p>gradient_accumulation_steps % steps_per_generation != 0</p></li>
</ol>
<p>Refer to <a class="reference external" href="https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851">issue</a>.</p>
<p><strong>6. Why is there a validation process even when <code class="docutils literal notranslate"><span class="pre">val_dataset</span></code> is not set, and how can I disable it?</strong></p>
<p>When <code class="docutils literal notranslate"><span class="pre">val_dataset</span></code> is not explicitly passed, the <code class="docutils literal notranslate"><span class="pre">split_dataset_ratio</span></code> parameter is responsible for splitting part of the <code class="docutils literal notranslate"><span class="pre">dataset</span></code> into a validation dataset, which defaults to splitting 1% of the data. (In &quot;ms-swift&gt;=3.6&quot;, the default value of split_dataset_ratio will be changed from 0.01 to 0.)</p>
<p>To disable the validation process, set <code class="docutils literal notranslate"><span class="pre">--split_dataset_ratio</span> <span class="pre">0</span></code>.</p>
<p><strong>7. How to set the training <code class="docutils literal notranslate"><span class="pre">mini-batch</span> <span class="pre">size</span></code></strong></p>
<p>In GRPO training, we can configure mini-batch updates in the following two ways:</p>
<ul class="simple">
<li><p>Set <code class="docutils literal notranslate"><span class="pre">generation_batch_size</span></code> to be an integer multiple of the training global batch size (effective_batch_size).</p></li>
<li><p>Or set <code class="docutils literal notranslate"><span class="pre">steps_per_generation</span></code> to be an integer multiple of <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span></code>.</p></li>
</ul>
<p>Typical configuration example:</p>
<ul class="simple">
<li><p>When configured with:
steps_per_generation = 16, gradient_accumulation_steps = 8, mini_batch_size = steps_per_generation / gradient_accumulation_steps = 2. The results from 1 rollout will be split into 2 mini-batch updates.</p></li>
</ul>
<p><strong>8. Difference between swift deploy and swift rollout</strong></p>
<ul class="simple">
<li><p>swift deploy is primarily used for model deployment and inference. It supports various engines such as PT, vLLM, and SGLang, and is compatible with streaming inference as well as the OpenAI API format.</p></li>
<li><p>swift rollout, on the other hand, is dedicated to GRPO rollout acceleration. Currently, it only supports the vLLM engine and comes with built-in automatic weight synchronization.</p></li>
</ul>
<p><strong>9. How to disable the KL loss term</strong></p>
<p>Set the parameter <code class="docutils literal notranslate"><span class="pre">--beta</span> <span class="pre">0</span></code> to disable KL loss calculation. The reference model (ref model) will not be loaded in this case.</p>
</section>
<section id="rl-wechat-group">
<h2>RL WeChat Group<a class="headerlink" href="#rl-wechat-group" title="Link to this heading">ÔÉÅ</a></h2>
<img src="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/resources/wechat/grpo.png" width="250"></section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>