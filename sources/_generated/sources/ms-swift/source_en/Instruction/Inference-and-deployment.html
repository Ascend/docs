

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Inference and Deployment &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Inference and Deployment</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/Inference-and-deployment.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="inference-and-deployment">
<h1>Inference and Deployment<a class="headerlink" href="#inference-and-deployment" title="Link to this heading">ÔÉÅ</a></h1>
<p>Below are the inference engines supported by Swift along with their corresponding capabilities. The three inference acceleration engines provide inference acceleration for Swift's inference, deployment, and evaluation modules:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Inference Acceleration Engine</th>
<th>OpenAI API</th>
<th>Multimodal</th>
<th>Quantized Model</th>
<th>Multiple LoRAs</th>
<th>QLoRA</th>
<th>Batch Inference</th>
<th>Parallel Techniques</th>
</tr>
</thead>
<tbody>
<tr>
<td>transformers</td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/deploy/client/llm/chat/openai_client.py">‚úÖ</a></td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/app/mllm.sh">‚úÖ</a></td>
<td>‚úÖ</td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_lora.py">‚úÖ</a></td>
<td>‚úÖ</td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/batch_ddp.sh">‚úÖ</a></td>
<td>DDP/device_map</td>
</tr>
<tr>
<td><a href="https://github.com/vllm-project/vllm">vllm</a></td>
<td>‚úÖ</td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/vllm/mllm_tp.sh">‚úÖ</a></td>
<td>‚úÖ</td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/deploy/lora/server.sh">‚úÖ</a></td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>TP/PP/DP</td>
</tr>
<tr>
<td><a href="https://github.com/sgl-project/sglang">sglang</a></td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>TP/PP/DP/EP</td>
</tr>
<tr>
<td><a href="https://github.com/InternLM/lmdeploy">lmdeploy</a></td>
<td>‚úÖ</td>
<td><a href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/lmdeploy/mllm_tp.sh">‚úÖ</a></td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>TP/DP</td>
</tr>
</tbody>
</table><section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Link to this heading">ÔÉÅ</a></h2>
<p>ms-swift uses a layered design philosophy, allowing users to perform inference through the command-line interface, web UI, or directly using Python.</p>
<p>To view the inference of a model fine-tuned with LoRA, please refer to the <a class="reference external" href="./Pre-training-and-Fine-tuning.md#inference-fine-tuned-model">Pre-training and Fine-tuning documentation</a>.</p>
<section id="using-cli">
<h3>Using CLI<a class="headerlink" href="#using-cli" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Full Parameter Model:</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stream<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">2048</span>
</pre></div>
</div>
<p><strong>LoRA Model:</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>swift/test_lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stream<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">2048</span>
</pre></div>
</div>
<p><strong>Command-Line Inference Instructions</strong></p>
<p>The above commands are for interactive command-line interface inference. After running the script, you can simply enter your query in the terminal. You can also input the following special commands:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">multi-line</span></code>: Switch to multi-line mode, allowing line breaks in the input, ending with <code class="docutils literal notranslate"><span class="pre">#</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">single-line</span></code>: Switch to single-line mode, with line breaks indicating the end of input.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reset-system</span></code>: Reset the system and clear history.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">clear</span></code>: Clear the history.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">quit</span></code> or <code class="docutils literal notranslate"><span class="pre">exit</span></code>: Exit the conversation.</p></li>
</ul>
<p><strong>Multimodal Model</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span><span class="se">\</span>
<span class="nv">VIDEO_MAX_PIXELS</span><span class="o">=</span><span class="m">50176</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FPS_MAX_FRAMES</span><span class="o">=</span><span class="m">12</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-3B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stream<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>transformers<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">2048</span>
</pre></div>
</div>
<p>To perform inference with a multimodal model, you can add tags like <code class="docutils literal notranslate"><span class="pre">&lt;image&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;video&gt;</span></code>, or <code class="docutils literal notranslate"><span class="pre">&lt;audio&gt;</span></code> in your query (representing the location of image representations in <code class="docutils literal notranslate"><span class="pre">inputs_embeds</span></code>). For example, you can input <code class="docutils literal notranslate"><span class="pre">&lt;image&gt;&lt;image&gt;What</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">difference</span> <span class="pre">between</span> <span class="pre">these</span> <span class="pre">two</span> <span class="pre">images?</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;video&gt;Describe</span> <span class="pre">this</span> <span class="pre">video.</span></code> Then, follow the prompts to input the corresponding image/video/audio.</p>
<p>Here is an example of inference:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;&lt;&lt; &lt;image&gt;&lt;image&gt;What is the difference between these two images?
Input an image path or URL &lt;&lt;&lt; http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png
Input an image path or URL &lt;&lt;&lt; http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/animal.png
The first image depicts a cute, cartoon-style kitten with large, expressive eyes and a fluffy white and gray coat. The background is simple, featuring a gradient of colors that highlight the kitten&#39;s face.

The second image shows a group of four cartoon-style sheep standing on a grassy field with mountains in the background. The sheep have fluffy white wool, black legs, and black faces with white markings around their eyes and noses. The background includes green hills and a blue sky with clouds, giving it a pastoral and serene atmosphere.
--------------------------------------------------
&lt;&lt;&lt; clear
&lt;&lt;&lt; &lt;video&gt;Describe this video.
Input a video path or URL &lt;&lt;&lt; https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4
A baby wearing glasses is sitting on a bed and reading a book. The baby is holding the book with both hands and is looking down at it. The baby is wearing a light blue shirt and pink pants. The baby is sitting on a white pillow. The baby is looking at the book with interest. The baby is not moving much, just turning the pages of the book.
</pre></div>
</div>
<p><strong>Dataset Inference:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="mi">0</span> <span class="n">swift</span> <span class="n">infer</span> \
    <span class="o">--</span><span class="n">model</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">Qwen2</span><span class="mf">.5</span><span class="o">-</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">--</span><span class="n">stream</span> <span class="n">true</span> \
    <span class="o">--</span><span class="n">infer_backend</span> <span class="n">transformers</span> \
    <span class="o">--</span><span class="n">val_dataset</span> <span class="n">AI</span><span class="o">-</span><span class="n">ModelScope</span><span class="o">/</span><span class="n">alpaca</span><span class="o">-</span><span class="n">gpt4</span><span class="o">-</span><span class="n">data</span><span class="o">-</span><span class="n">zh</span> \
    <span class="o">--</span><span class="n">max_new_tokens</span> <span class="mi">2048</span>
</pre></div>
</div>
<p>The above example provides streaming inference for both full parameters and LoRA, and below are more inference techniques available in SWIFT:</p>
<ul class="simple">
<li><p>Interface Inference: You can change <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">infer</span></code> to <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">app</span></code>.</p></li>
<li><p>Batch Inference: For large models and multimodal models, you can specify <code class="docutils literal notranslate"><span class="pre">--max_batch_size</span></code> for batch inference by using <code class="docutils literal notranslate"><span class="pre">infer_backend=transformers</span></code>. For specific details, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/batch_ddp.sh">here</a>. Note that you cannot set <code class="docutils literal notranslate"><span class="pre">--stream</span> <span class="pre">true</span></code> when performing batch inference.</p></li>
<li><p>DDP/device_map Inference: <code class="docutils literal notranslate"><span class="pre">infer_backend=transformers</span></code> supports parallel inference using DDP/device_map technology. For further details, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/mllm_device_map.sh">here</a>.</p></li>
<li><p>Inference Acceleration: Swift supports using vllm/sglang/lmdeploy for inference acceleration across the inference, deployment, and evaluation modules by simply adding <code class="docutils literal notranslate"><span class="pre">--infer_backend</span> <span class="pre">vllm/sglang/lmdeploy</span></code>. You can refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/vllm/mllm_ddp.sh">here</a>.</p></li>
<li><p>Multimodal Models: We provide shell scripts for multi-GPU inference for multimodal models using <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/mllm_device_map.sh">transformers</a>, <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/vllm/mllm_tp.sh">vllm</a>, and <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/lmdeploy/mllm_tp.sh">lmdeploy</a>.</p></li>
<li><p>Quantized Models: You can directly select models that are quantized with GPTQ, AWQ, or BNB, for example: <code class="docutils literal notranslate"><span class="pre">--model</span> <span class="pre">Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4</span></code>.</p></li>
<li><p>More Model Types: We also provide inference scripts for <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/bert.sh">bert</a>, <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/reward_model.sh">reward_model</a>, and <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/transformers/prm.sh">prm</a>.</p></li>
</ul>
<p><strong>Tips:</strong></p>
<ul class="simple">
<li><p>SWIFT saves inference results, and you can specify the save path using <code class="docutils literal notranslate"><span class="pre">--result_path</span></code>.</p></li>
<li><p>To output log probabilities, simply specify <code class="docutils literal notranslate"><span class="pre">--logprobs</span> <span class="pre">true</span></code> during inference. SWIFT will save these results. Note that setting <code class="docutils literal notranslate"><span class="pre">--stream</span> <span class="pre">true</span></code> will prevent storage of results.</p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">infer_backend=transformers</span></code> supports inference for all models supported by SWIFT, while <code class="docutils literal notranslate"><span class="pre">infer_backend=vllm/lmdeploy</span></code> supports only a subset of models. Please refer to the documentation for <a class="reference external" href="https://docs.vllm.ai/en/latest/models/supported_models.html">vllm</a>, <a class="reference external" href="https://docs.sglang.ai/supported_models/generative_models.html">sglang</a> and <a class="reference external" href="https://lmdeploy.readthedocs.io/en/latest/supported_models/supported_models.html">lmdeploy</a>.</p></li>
<li><p>If you encounter OOM when using <code class="docutils literal notranslate"><span class="pre">--infer_backend</span> <span class="pre">vllm</span></code>, you can lower <code class="docutils literal notranslate"><span class="pre">--vllm_max_model_len</span></code>, <code class="docutils literal notranslate"><span class="pre">--vllm_max_num_seqs</span></code>, choose an appropriate <code class="docutils literal notranslate"><span class="pre">--vllm_gpu_memory_utilization</span></code>, or set <code class="docutils literal notranslate"><span class="pre">--vllm_enforce_eager</span> <span class="pre">true</span></code>. Alternatively, you can address this by using tensor parallelism with <code class="docutils literal notranslate"><span class="pre">--vllm_tensor_parallel_size</span></code>.</p></li>
<li><p>When inferring multimodal models using <code class="docutils literal notranslate"><span class="pre">--infer_backend</span> <span class="pre">vllm</span></code>, you need to input multiple images. You can set <code class="docutils literal notranslate"><span class="pre">--vllm_limit_mm_per_prompt</span></code> to resolve this, for example: <code class="docutils literal notranslate"><span class="pre">--vllm_limit_mm_per_prompt</span> <span class="pre">'{&quot;image&quot;:</span> <span class="pre">10,</span> <span class="pre">&quot;video&quot;:</span> <span class="pre">5}'</span></code>.</p></li>
<li><p>If you encounter OOM issues while inferring qwen2-vl/qwen2.5-vl, you can address this by setting <code class="docutils literal notranslate"><span class="pre">MAX_PIXELS</span></code>, <code class="docutils literal notranslate"><span class="pre">VIDEO_MAX_PIXELS</span></code>, and <code class="docutils literal notranslate"><span class="pre">FPS_MAX_FRAMES</span></code>. For more information, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/app/mllm.sh">here</a>.</p></li>
<li><p>SWIFT's built-in dialogue templates align with dialogue templates run using transformers. You can refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/tests/test_align/test_template/test_vision.py">here</a> for testing. If there are any misalignments, please feel free to submit an issue or PR for correction.</p></li>
</ul>
</section>
<section id="using-web-ui">
<h3>Using Web-UI<a class="headerlink" href="#using-web-ui" title="Link to this heading">ÔÉÅ</a></h3>
<p>If you want to perform inference through a graphical interface, you can refer to the <a class="reference internal" href="../GetStarted/Web-UI.html"><span class="doc">Web-UI documentation</span></a>.</p>
</section>
<section id="using-python">
<h3>Using Python<a class="headerlink" href="#using-python" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Text Model:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">swift.infer_engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">TransformersEngine</span><span class="p">,</span> <span class="n">RequestConfig</span><span class="p">,</span> <span class="n">InferRequest</span>
<span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;Qwen/Qwen2.5-0.5B-Instruct&#39;</span>

<span class="c1"># Load the inference engine</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">TransformersEngine</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">request_config</span> <span class="o">=</span> <span class="n">RequestConfig</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Using 2 infer_requests to demonstrate batch inference</span>
<span class="n">infer_requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Who are you?&#39;</span><span class="p">}]),</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Where is the capital of Zhejiang?&#39;</span><span class="p">},</span>
                           <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;The capital of Zhejiang Province, China, is Hangzhou.&#39;</span><span class="p">},</span>
                           <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;What are some fun places here?&#39;</span><span class="p">}]),</span>
<span class="p">]</span>
<span class="n">resp_list</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">infer_requests</span><span class="p">,</span> <span class="n">request_config</span><span class="p">)</span>
<span class="n">query0</span> <span class="o">=</span> <span class="n">infer_requests</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response0: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response1: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Multimodal Model:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MAX_PIXELS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1003520&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;VIDEO_MAX_PIXELS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;50176&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;FPS_MAX_FRAMES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;12&#39;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">swift.infer_engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">TransformersEngine</span><span class="p">,</span> <span class="n">RequestConfig</span><span class="p">,</span> <span class="n">InferRequest</span>
<span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;Qwen/Qwen2.5-VL-3B-Instruct&#39;</span>

<span class="c1"># Load the inference engine</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">TransformersEngine</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">request_config</span> <span class="o">=</span> <span class="n">RequestConfig</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Using 3 infer_requests to demonstrate batch inference</span>
<span class="n">infer_requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Who are you?&#39;</span><span class="p">}]),</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;&lt;image&gt;&lt;image&gt; What is the difference between these two images?&#39;</span><span class="p">}],</span>
                 <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png&#39;</span><span class="p">,</span>
                         <span class="s1">&#39;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/animal.png&#39;</span><span class="p">]),</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;&lt;video&gt; Describe the video&#39;</span><span class="p">}],</span>
                 <span class="n">videos</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">]),</span>
<span class="p">]</span>
<span class="n">resp_list</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">infer_requests</span><span class="p">,</span> <span class="n">request_config</span><span class="p">)</span>
<span class="n">query0</span> <span class="o">=</span> <span class="n">infer_requests</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response0: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response1: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response2: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>We also provide more demos for Python-based inference:</p>
<ul class="simple">
<li><p>For streaming inference using <code class="docutils literal notranslate"><span class="pre">VllmEngine</span></code>, <code class="docutils literal notranslate"><span class="pre">SglangEngine</span></code> and <code class="docutils literal notranslate"><span class="pre">LmdeployEngine</span></code> for inference acceleration, you can refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo.py">here</a>.</p></li>
<li><p>Multimodal Inference: In addition to the aforementioned multimodal input formats, Swift is compatible with OpenAI's multimodal input format; refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_mllm.py">here</a>.</p></li>
<li><p>Grounding Tasks: For performing grounding tasks with multimodal models, you can refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_grounding.py">here</a>.</p></li>
<li><p>Multiple LoRA Inference: Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_lora.py">here</a>.</p></li>
<li><p>Agent Inference: Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo_agent.py">here</a>.</p></li>
<li><p>Asynchronous Interface: For Python-based inference using <code class="docutils literal notranslate"><span class="pre">engine.infer_async</span></code>, refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/infer/demo.py">here</a>.</p></li>
</ul>
</section>
</section>
<section id="deployment">
<h2>Deployment<a class="headerlink" href="#deployment" title="Link to this heading">ÔÉÅ</a></h2>
<p>If you want to see the deployment of a model fine-tuned with LoRA, you can refer to the <a class="reference external" href="./Pre-training-and-Fine-tuning.md#deployment-fine-tuned-model">Pre-training and Fine-tuning documentation</a>.</p>
<p>This section primarily focuses on the deployment and invocation of multimodal models. For text-based large models, we provide a simple deployment and invocation example:</p>
<p><strong>Server Deployment:</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>swift<span class="w"> </span>deploy<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>vllm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--served_model_name<span class="w"> </span>Qwen2.5-7B-Instruct
</pre></div>
</div>
<p><strong>Client Invocation Test:</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/v1/chat/completions<span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">&quot;model&quot;: &quot;Qwen2.5-7B-Instruct&quot;,</span>
<span class="s1">&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What should I do if I can‚Äôt sleep at night?&quot;}],</span>
<span class="s1">&quot;max_tokens&quot;: 256,</span>
<span class="s1">&quot;temperature&quot;: 0</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<section id="server-side">
<h3>Server Side<a class="headerlink" href="#server-side" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># test env: pip install transformers==4.51.3 vllm==0.8.5.post1</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MAX_PIXELS</span><span class="o">=</span><span class="m">1003520</span><span class="w"> </span><span class="se">\</span>
<span class="nv">VIDEO_MAX_PIXELS</span><span class="o">=</span><span class="m">50176</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FPS_MAX_FRAMES</span><span class="o">=</span><span class="m">12</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>deploy<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-VL-3B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--infer_backend<span class="w"> </span>vllm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_gpu_memory_utilization<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_max_model_len<span class="w"> </span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_new_tokens<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_limit_mm_per_prompt<span class="w"> </span><span class="s1">&#39;{&quot;image&quot;: 5, &quot;video&quot;: 2}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--served_model_name<span class="w"> </span>Qwen2.5-VL-3B-Instruct
</pre></div>
</div>
</section>
<section id="client-side">
<h3>Client Side<a class="headerlink" href="#client-side" title="Link to this heading">ÔÉÅ</a></h3>
<p>We introduce three methods for invoking the client: using curl, the OpenAI library, and the Swift client.</p>
<p><strong>Method 1: curl</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/v1/chat/completions<span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">&quot;model&quot;: &quot;Qwen2.5-VL-3B-Instruct&quot;,</span>
<span class="s1">&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [</span>
<span class="s1">    {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png&quot;},</span>
<span class="s1">    {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/animal.png&quot;},</span>
<span class="s1">    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What is the difference between these two images?&quot;}</span>
<span class="s1">]}],</span>
<span class="s1">&quot;max_tokens&quot;: 256,</span>
<span class="s1">&quot;temperature&quot;: 0</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p><strong>Method 2: OpenAI Library</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;EMPTY&#39;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;http://127.0.0.1:8000/v1&#39;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">id</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;model: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;video&#39;</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">:</span> <span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;describe the video&#39;</span><span class="p">}</span>
<span class="p">]}]</span>

<span class="n">resp</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;query: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Using base64</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">)</span>
<span class="n">base64_encoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;video&#39;</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;data:video/mp4;base64,</span><span class="si">{</span><span class="n">base64_encoded</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;describe the video&#39;</span><span class="p">}</span>
<span class="p">]}]</span>

<span class="n">gen</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;query: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\n</span><span class="s1">response: &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">chunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Method 3: Swift Client</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">swift</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferRequest</span><span class="p">,</span> <span class="n">InferClient</span><span class="p">,</span> <span class="n">RequestConfig</span><span class="p">,</span> <span class="n">InferStats</span>

<span class="n">engine</span> <span class="o">=</span> <span class="n">InferClient</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;127.0.0.1&#39;</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">8000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;models: </span><span class="si">{</span><span class="n">engine</span><span class="o">.</span><span class="n">models</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">InferStats</span><span class="p">()</span>
<span class="n">request_config</span> <span class="o">=</span> <span class="n">RequestConfig</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Using 3 infer_requests to demonstrate batch inference</span>
<span class="c1"># Supports local paths, base64, and URLs</span>
<span class="n">infer_requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Who are you?&#39;</span><span class="p">}]),</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;&lt;image&gt;&lt;image&gt; What is the difference between these two images?&#39;</span><span class="p">}],</span>
                 <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png&#39;</span><span class="p">,</span>
                         <span class="s1">&#39;http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/animal.png&#39;</span><span class="p">]),</span>
    <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;&lt;video&gt; Describe the video&#39;</span><span class="p">}],</span>
                 <span class="n">videos</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">]),</span>
<span class="p">]</span>

<span class="n">resp_list</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">infer_requests</span><span class="p">,</span> <span class="n">request_config</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response0: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response1: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response2: </span><span class="si">{</span><span class="n">resp_list</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
<span class="n">metric</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># Using base64</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/baby.mp4&#39;</span><span class="p">)</span>
<span class="n">base64_encoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;video&#39;</span><span class="p">,</span> <span class="s1">&#39;video&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;data:video/mp4;base64,</span><span class="si">{</span><span class="n">base64_encoded</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;describe the video&#39;</span><span class="p">}</span>
<span class="p">]}]</span>
<span class="n">infer_request</span> <span class="o">=</span> <span class="n">InferRequest</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
<span class="n">request_config</span> <span class="o">=</span> <span class="n">RequestConfig</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">gen_list</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">infer</span><span class="p">([</span><span class="n">infer_request</span><span class="p">],</span> <span class="n">request_config</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;response0: &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">gen_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">chunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
<p>We also provide more deployment demos:</p>
<ul class="simple">
<li><p>Multiple LoRA deployment and invocation: Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/lora">this link</a>.</p></li>
<li><p>Deployment and invocation of the Base model: Refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/client/llm/base">this link</a>.</p></li>
<li><p>More model types: We provide deployment scripts for <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/bert">bert</a> and <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/deploy/reward_model">reward_model</a>.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>