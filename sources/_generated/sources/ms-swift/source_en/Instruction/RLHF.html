

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>RLHF &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">RLHF</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/RLHF.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="rlhf">
<h1>RLHF<a class="headerlink" href="#rlhf" title="Link to this heading">ïƒ</a></h1>
<p>This document provides training scripts for various human preference alignment algorithms. If you want to learn more about the algorithms and how to choose them, please refer to the <a class="reference external" href="https://github.com/modelscope/modelscope-classroom/blob/main/LLM-tutorial/M.%E4%BA%BA%E7%B1%BB%E5%81%8F%E5%A5%BD%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83.md">documentation</a>.</p>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">ïƒ</a></h2>
<p>The data required by the PPO and GRPO algorithm consists solely of model inputs, which include the system prompt (optional) and the query. In the case of the GRPO algorithm, the reward function may require additional data columns. For example, to calculate accuracy, a <code class="docutils literal notranslate"><span class="pre">solution</span></code> column is needed as a reference answer.</p>
<p>For RM and DPO-type algorithms such as ORPO, CPO, and SimPO, $(x,y_w,y_l)$ formatted data is required, where $x$ is the model input, $y_w$ is the preferred answer that aligns with human preferences, and $y_l$ is the rejected answer that does not align with human preferences, as shown in <img alt="dpo_data" src="../../../../../../_images/dpo_data.png" />.</p>
<p>In contrast, the KTO algorithm has a special data format that only requires $(x,y,\text{label})$, where $x$ is the model input, $y$ is the model output, and the label indicates whether the answer aligns with human preferences, as shown in <img alt="kto_data" src="../../../../../../_images/kto_data.png" />.</p>
<p>For RLHF training of text models or multimodal large models using a custom dataset, you can refer to the <a class="reference external" href="../Customization/Custom-dataset.md#rlhf">custom dataset documentation</a>.</p>
</section>
<section id="grpo">
<h2>GRPO<a class="headerlink" href="#grpo" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2402.03300">Paper on arXiv</a></p>
<p>Reference the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo">here</a>.</p>
</section>
<section id="dpo">
<h2>DPO<a class="headerlink" href="#dpo" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2305.18290">Paper on arXiv</a></p>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>beta: KL regularization coefficient. A larger value imposes a stronger penalty for deviating from the reference model. Default is 0.1.</p></li>
<li><p>loss_type: Variant of the DPO algorithm. You can find the available options in the <a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions">documentation</a>. Default is 'sigmoid'.</p></li>
<li><p>(Optional) loss_weights: Weights for mixing multiple loss functions.</p></li>
<li><p>(Optional) ld_alpha: From the <a class="reference external" href="https://arxiv.org/abs/2409.06411">LD-DPO paper</a>. Applies a weight Î± &lt; 1 to the log-probabilities of tokens that lie beyond the shared prefix of the chosen and rejected responses, thereby mitigating length bias.</p></li>
<li><p>(Optional) discopop_tau: Temperature parameter Ï„ from the <a class="reference external" href="https://arxiv.org/abs/2406.08414">DiscoPOP paper</a> used to scale the log-ratio before the sigmoid modulation. Default 0.05; only active when loss_type is discopop.</p></li>
</ul>
<p>It is recommended to perform SFT training on the preferred responses in your preference dataset before starting DPO training. This helps ensure that the data distribution better matches the requirements of the DPO algorithm.</p>
<p>If you want to mix multiple losses (such as for <a class="reference external" href="https://arxiv.org/abs/2411.10442">MPO</a> training), you can specify multiple loss_type values and set their weights via loss_weights.</p>
<p>By setting the hyperparameter <code class="docutils literal notranslate"><span class="pre">rpo_alpha</span></code>, a certain proportion of SFT loss can be mixed into the loss to improve training stability.</p>
<p>Training script references:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/dpo">DPO script</a></p></li>
<li><p><a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/mpo.sh">MPO script</a></p></li>
</ul>
</section>
<section id="rm">
<h2>RM<a class="headerlink" href="#rm" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2203.02155">Paper on arXiv</a></p>
<p>Reward Modeling stage in RLHF.</p>
<p>Use the base model or instruct model trained with SFT as the foundation model. Add a value head and train it using the preference dataset to create the reward model.</p>
<p>The weights of the added value head will be saved in <code class="docutils literal notranslate"><span class="pre">value_head.safetensors</span></code> or <code class="docutils literal notranslate"><span class="pre">value_head.bin</span></code>.</p>
<p>The loss function for reward modeling is as follows:</p>
<p>$
\text{loss} = -\log \sigma \left( r^{(c)} - r^{(r)} - m \right) + \lambda \left( r^{(c)} + r^{(r)} \right)^2
$</p>
<ul class="simple">
<li><p>$r^{(c)}$: The score assigned by the model to the chosen response.</p></li>
<li><p>$r^{(r)}$: The score assigned by the model to the rejected response.</p></li>
<li><p>$\lambda$: L2 regularization coefficient that encourages the model outputs to be close to zero. It is set by the parameter <code class="docutils literal notranslate"><span class="pre">center_rewards_coefficient</span></code>, as described in <a class="reference external" href="https://arxiv.org/pdf/2307.09288">the paper</a>, and defaults to 0.</p></li>
<li><p>$m$: Margin term that encourages the model to distinguish between samples of different difficulty levels. The dataset needs to provide a <code class="docutils literal notranslate"><span class="pre">margin</span></code> column for this; by default, it is 0. This term is also introduced in <a class="reference external" href="https://arxiv.org/pdf/2307.09288">the paper</a>.</p></li>
</ul>
<p>Reference the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/rm.sh">here</a>.</p>
</section>
<section id="ppo">
<h2>PPO<a class="headerlink" href="#ppo" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2203.02155">Paper on arXiv</a></p>
<p>PPO (proximal policy optimization) stage in RLHF involves four models:</p>
<ul class="simple">
<li><p>model: The training model, either the base model or the instruct model trained with SFT.</p></li>
<li><p>ref_model: The reference model, which defaults to the model.</p></li>
<li><p>reward_model: The reward model obtained from the RM stage.</p></li>
<li><p>value_model: The value model initialized by the reward model, updated synchronously during training.</p></li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>local_rollout_forward_batch_size: Batch size for each data sample, default is 64.</p></li>
<li><p>whiten_rewards: Normalize rewards, default is False.</p></li>
<li><p>kl_coef: Coefficient for the KL divergence term, default is 0.05.</p></li>
<li><p>cliprange: Clip range in the PPO policy loss function, default is 0.2.</p></li>
<li><p>vf_coef: Coefficient for the value loss function, default is 0.1.</p></li>
<li><p>cliprange_value: Clip range in the PPO value loss function, default is 0.2.</p></li>
<li><p>gamma: Discount factor for cumulative rewards, default is 1.0.</p></li>
<li><p>lam: Lambda coefficient in <a class="reference external" href="https://arxiv.org/abs/1506.02438">GAE</a>, default is 0.95.</p></li>
<li><p>num_sample_generations: Number of debugging samples generated during training, default is 10.</p></li>
</ul>
<p>Note: When training the base model, perform SFT first and then proceed to RLHF. Specify the chat template, and it is recommended to use <code class="docutils literal notranslate"><span class="pre">full</span></code> for <code class="docutils literal notranslate"><span class="pre">tuner_type</span></code>.</p>
<p>Refer to the <a class="reference external" href="https://huggingface.co/docs/trl/ppov2_trainer#explanation-of-the-logged-metrics">documentation</a> for metric explanations during training.</p>
</section>
<section id="kto">
<h2>KTO<a class="headerlink" href="#kto" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2402.01306">Paper on arXiv</a></p>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>beta: KL regularization coefficient. A larger value leads to a greater penalty for deviation from the reference model. Default is 0.1.</p></li>
<li><p>desirable_weight: The $\lambda_D$ term in the loss function represents the loss weight for the preferred response samples, with a default value of 1.0.</p></li>
<li><p>undesirable_weight: The $\lambda_U$ term in the loss function represents the loss weight for rejected samples, with a default value of 1.0.</p></li>
</ul>
<p>Let $n_D$ and $n_U$ represent the number of preferred and rejected samples in the dataset, respectively. For hyperparameters $\lambda_D$ and $\lambda_U$, the authors recommend setting $\frac{\lambda_D n_D}{\lambda_U n_U} \in [1, \frac{4}{3}]$.</p>
<p>Training script:
Train using data in the $(x,y,\text{label})$ format.</p>
<p>Reference the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/kto.sh">here</a>.</p>
</section>
<section id="cpo">
<h2>CPO<a class="headerlink" href="#cpo" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2401.08417">Paper on arXiv</a></p>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>beta: Coefficient before the implicit reward, default is 0.1.</p></li>
<li><p>cpo_alpha: Coefficient for NLL loss, default is 1.0.</p></li>
</ul>
<p>Reference the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/cpo.sh">here</a>.</p>
</section>
<section id="orpo">
<h2>ORPO<a class="headerlink" href="#orpo" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2403.07691">Paper on arXiv</a></p>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>lambda: Odds Ratio loss coefficient.</p></li>
</ul>
<p>Note: ORPO uses the parameter <code class="docutils literal notranslate"><span class="pre">--beta</span></code> to pass the hyperparameter <code class="docutils literal notranslate"><span class="pre">lambda</span></code>.</p>
<p>Reference the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/orpo.sh">here</a>.</p>
</section>
<section id="simpo">
<h2>SimPO<a class="headerlink" href="#simpo" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2405.14734">Paper on arXiv</a></p>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>beta: Coefficient before the implicit reward, default is 2.0.</p></li>
<li><p>simpo_gamma: Reward margin term, default is 1.0.</p></li>
<li><p>cpo_alpha: The mixed CPO NLL loss for improving training stability; defaults to 1.0, set to 0.0 to use the original SimPO algorithm.</p></li>
</ul>
<p>Reference the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf/simpo.sh">here</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>