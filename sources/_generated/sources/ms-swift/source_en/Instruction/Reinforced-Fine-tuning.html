

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reinforced Fine-Tuning &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Reinforced Fine-Tuning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/Reinforced-Fine-tuning.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reinforced-fine-tuning">
<h1>Reinforced Fine-Tuning<a class="headerlink" href="#reinforced-fine-tuning" title="Link to this heading">ïƒ</a></h1>
<p>Reinforced fine-tuning is one of the most important functionalities in current model training, with various implementations. SWIFT has already supported the atomic capabilities required for reinforced fine-tuning, such as sampling, reinforcement learning, and fine-tuning. Currently, we provide a specific example of rejection sampling fine-tuning, which can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rft/rft.py">here</a>.</p>
<section id="concept-of-reinforced-fine-tuning">
<h2>Concept of Reinforced Fine-Tuning<a class="headerlink" href="#concept-of-reinforced-fine-tuning" title="Link to this heading">ïƒ</a></h2>
<p>The concept of reinforced fine-tuning has been proposed since 2022 (or even earlier). Its general workflow typically includes the following steps:</p>
<ol class="simple">
<li><p>Generate data using a specific model or augment the original dataset.</p></li>
<li><p>Train the target model using the generated data.</p></li>
<li><p>Repeat the above process if necessary.</p></li>
</ol>
<p><strong>Step 1:</strong></p>
<ul class="simple">
<li><p>If the data-generating model is a larger model, such as GPT, Qwen-Max, DeepSeek-V3/R1, etc., this process can be understood as distillation.</p></li>
<li><p>If the data-generating model is the same model being trained, this can be considered self-improvement fine-tuning.</p></li>
<li><p>If the sampling process involves sampling a batch, fitting the data with KL divergence and rewards, and iterating continuously, it can be classified as on-policy algorithms like PPO or GRPO.</p></li>
<li><p>Sampling algorithms include Monte Carlo sampling, do_sample, group beam search, DVTS, etc.</p></li>
<li><p>The sampling process can incorporate ORM (Outcome Reward Model), PRM (Process Reward Model), diversity filtering, language filtering, etc.</p></li>
</ul>
<p><strong>Step 2:</strong></p>
<ul class="simple">
<li><p>If SFT (Supervised Fine-Tuning) is used, it is referred to as rejection sampling fine-tuning.</p></li>
<li><p>If reinforcement learning is used, it is called reinforcement learning fine-tuning.</p></li>
</ul>
<p><strong>Step 3:</strong></p>
<ul class="simple">
<li><p>If distillation is performed using a larger model (e.g., Monte Carlo sampling distillation with a larger model), the process usually does not involve iterations.</p></li>
<li><p>If the same model is used for sampling or algorithms like PPO are applied, iterations are typically included.</p></li>
</ul>
<p>In general, the common approaches to reinforced fine-tuning include:</p>
<ol class="simple">
<li><p><strong>Distillation</strong>: Sampling high-quality data in bulk from a larger model using methods like Monte Carlo or do_sample, and training a smaller model on this data.</p></li>
<li><p><strong>Self-improvement</strong>: Sampling a portion of high-quality data from the same model, filtering it, and training the model iteratively.</p></li>
<li><p><strong>On-policy RL</strong>: Using methods like PPO or GRPO for iterative training.</p></li>
</ol>
<p>The sampling process is usually much more time-consuming than the training process. If data is distilled using GPT or other large models, token costs must be considered. Thus, reinforced fine-tuning is generally a supplementary mechanism for fine-tuning, except for special cases like DeepSeek-R1.</p>
<p>DeepSeek-R1 uses the GRPO algorithm to enable the emergence of CoT (Chain-of-Thought) capabilities from scratch in a base model. This method requires large-scale cluster support and sufficiently large models for capability emergence. This is not discussed in detail here, but more information can be found in the <a class="reference external" href="https://zhuanlan.zhihu.com/p/19714987272">paper analysis</a>.</p>
<p>Some related papers on reinforced fine-tuning:</p>
<ul class="simple">
<li><p>Rejection Sampling Fine-Tuning: https://arxiv.org/pdf/2308.01825</p></li>
<li><p>ReST: https://arxiv.org/pdf/2308.08998</p></li>
<li><p>B-STAR: https://arxiv.org/pdf/2412.17256</p></li>
<li><p>DeepSeekMath: https://arxiv.org/pdf/2402.03300</p></li>
<li><p>Qwen-Math-PRM: https://arxiv.org/pdf/2501.07301</p></li>
<li><p>DeepSeek-R1: https://github.com/deepseek-ai/DeepSeek-R1/tree/main</p></li>
</ul>
</section>
<section id="when-to-use-reinforced-fine-tuning">
<h2>When to Use Reinforced Fine-Tuning<a class="headerlink" href="#when-to-use-reinforced-fine-tuning" title="Link to this heading">ïƒ</a></h2>
<p>Since LLaMA3, we have observed a very noticeable yet rarely mentioned phenomenon: when training an Instruct model using a CoT-enabled training dataset and evaluating it on the corresponding test set, the test set performance tends to degrade. For example, training <code class="docutils literal notranslate"><span class="pre">llama3.1-8b-instruct</span></code> on the GSM8K training set and evaluating the generated checkpoint on the test set reveals performance degradation.</p>
<p>This phenomenon mainly arises from the issue of knowledge forgetting disaster in models. During fine-tuning by model manufacturers, a significant amount of CoT data is often included. When solving mathematical tasks, the model's capability often originates not from the math dataset itself but potentially from datasets like ARC. This inference is supported by <a class="reference external" href="https://zhuanlan.zhihu.com/p/19269451950">some works</a>. Continued training on general tasks disrupts the model's existing capabilities, leading to performance degradation.</p>
<p>However, it is always correct to prioritize fine-tuning. Fine-tuning allows the model to quickly adapt to the dataset distribution at a low cost. Reinforced fine-tuning should be used under the following conditions:</p>
<ol class="simple">
<li><p>The model has already been fine-tuned but does not meet the requirements.</p></li>
<li><p>Stronger CoT capabilities are needed.</p></li>
<li><p>Base model training for general capabilities is necessary, and the original dataset no longer improves performance.</p></li>
<li><p>The output results for corresponding queries can be relatively accurately evaluated, such as tasks with clear results (math, code) or clear processes (translation, style fitting).</p></li>
</ol>
<p>Reinforced fine-tuning heavily depends on the accuracy of reward evaluations. If the evaluations are inaccurate, the training may oscillate without progress or even degrade the model performance.</p>
</section>
<section id="swift-implementation">
<h2>SWIFT Implementation<a class="headerlink" href="#swift-implementation" title="Link to this heading">ïƒ</a></h2>
<p>SWIFT supports the <code class="docutils literal notranslate"><span class="pre">sample</span></code> command, which is used for model sampling. Currently supported sampling methods include:</p>
<ul class="simple">
<li><p><strong>sample</strong>: Use <code class="docutils literal notranslate"><span class="pre">generate</span></code> do rollout.</p></li>
</ul>
<p>We have provided a general <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/rft/rft.py">RFT script</a>. This script supports self-improvement training and allows dynamic adjustments of sampling temperature, PRM thresholds, and other hyperparameters. The training method is flexible (e.g., fine-tuning, DPO) and supports iterative retraining of the original model or continued training from the previous iteration, even loading all training states from the previous iteration. Developers can incorporate additional data filtering (e.g., ensuring rows with the same ID come from the same query), including diversity checks, language filtering, etc.</p>
</section>
<section id="experimental-results">
<h2>Experimental Results<a class="headerlink" href="#experimental-results" title="Link to this heading">ïƒ</a></h2>
<p>We used the RFT script to train and evaluate the <code class="docutils literal notranslate"><span class="pre">competition_math</span></code> dataset in the math domain. The results are as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>MATH Score</th>
<th>Training Method</th>
<th>Iterations</th>
<th>Post-Training MATH Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA3.1_8b</td>
<td>12.0</td>
<td>SFT</td>
<td>3</td>
<td>25.2 (LLaMA3.1_8b_sft)</td>
</tr>
<tr>
<td>LLaMA3.1_8b_sft</td>
<td>25.2</td>
<td>RFT</td>
<td>2</td>
<td>32.4</td>
</tr>
<tr>
<td>LLaMA3.1_8b_instruct</td>
<td>52.2</td>
<td>SFT</td>
<td>2</td>
<td>39.0</td>
</tr>
<tr>
<td>LLaMA3.1_8b_instruct</td>
<td>52.2</td>
<td>RFT</td>
<td>3</td>
<td>58</td>
</tr>
<tr>
<td>Qwen2.5_math_7b_instruct</td>
<td>79.6</td>
<td>RFT</td>
<td>2</td>
<td>83.2</td>
</tr>
</tbody>
</table><p>As shown, applying SFT to the <code class="docutils literal notranslate"><span class="pre">competition_math</span></code> dataset resulted in significant performance degradation for the instruct model. However, RFT improved the model's capabilities, even for the state-of-the-art <code class="docutils literal notranslate"><span class="pre">Qwen2.5_math_7b_instruct</span></code> math model.</p>
<p>Specifically, we tested the GSM8K metric for <code class="docutils literal notranslate"><span class="pre">Qwen2.5_math_7b_instruct</span></code>:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>GSM8K Score</th>
<th>Post-RFT GSM8K Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen2.5_math_7b_instruct</td>
<td>92.8</td>
<td>91.6</td>
</tr>
</tbody>
</table><p>As shown, RFT training did not significantly change the GSM8K score, avoiding the previously mentioned performance degradation phenomenon.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>