

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Tuners &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Using Tuners</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Instruction/Use-tuners.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-tuners">
<h1>Using Tuners<a class="headerlink" href="#using-tuners" title="Link to this heading">ïƒ</a></h1>
<p>Tuners refer to additional structural components attached to a model, aimed at reducing the number of training parameters or enhancing training accuracy. The tuners currently supported by SWIFT include:</p>
<ul class="simple">
<li><p>LoRA: <a class="reference external" href="https://arxiv.org/abs/2106.09685">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></p></li>
<li><p>LoRA+: <a class="reference external" href="https://arxiv.org/pdf/2402.12354.pdf">LoRA+: Efficient Low Rank Adaptation of Large Models</a></p></li>
<li><p>LLaMA PRO: <a class="reference external" href="https://arxiv.org/pdf/2401.02415.pdf">LLAMA PRO: Progressive LLaMA with Block Expansion</a></p></li>
<li><p>GaLore/Q-GaLore: <a class="reference external" href="https://arxiv.org/abs/2403.03507">GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></p></li>
<li><p>Liger Kernel: <a class="reference external" href="https://arxiv.org/abs/2410.10989">Liger Kernel: Efficient Triton Kernels for LLM Training</a></p></li>
<li><p>LISA: <a class="reference external" href="https://arxiv.org/abs/2403.17919">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a></p></li>
<li><p>UnSloth: https://github.com/unslothai/unsloth</p></li>
<li><p>SCEdit: <a class="reference external" href="https://arxiv.org/abs/2312.11392">SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing</a>  &lt; <a class="reference external" href="https://arxiv.org/abs/2312.11392">arXiv</a>  |  <a class="reference external" href="https://scedit.github.io/">Project Page</a> &gt;</p></li>
<li><p>NEFTune: <a class="reference external" href="https://arxiv.org/abs/2310.05914">Noisy Embeddings Improve Instruction Finetuning</a></p></li>
<li><p>LongLoRA: <a class="reference external" href="https://arxiv.org/abs/2309.12307">Efficient Fine-tuning of Long-Context Large Language Models</a></p></li>
<li><p>Adapter: <a class="reference external" href="http://arxiv.org/abs/1902.00751">Parameter-Efficient Transfer Learning for NLP</a></p></li>
<li><p>Vision Prompt Tuning: <a class="reference external" href="https://arxiv.org/abs/2203.12119">Visual Prompt Tuning</a></p></li>
<li><p>Side: <a class="reference external" href="https://arxiv.org/abs/1912.13503">Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks</a></p></li>
<li><p>Res-Tuning: <a class="reference external" href="https://arxiv.org/abs/2310.19859">Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone</a>  &lt; <a class="reference external" href="https://arxiv.org/abs/2310.19859">arXiv</a>  |  <a class="reference external" href="https://res-tuning.github.io/">Project Page</a> &gt;</p></li>
<li><p>Tuners provided by <a class="reference external" href="https://github.com/huggingface/peft">PEFT</a>, such as AdaLoRA, DoRA, Fourierft, etc.</p></li>
</ul>
<section id="interface-list">
<h2>Interface List<a class="headerlink" href="#interface-list" title="Link to this heading">ïƒ</a></h2>
<section id="swift-class-static-interfaces">
<h3>Swift Class Static Interfaces<a class="headerlink" href="#swift-class-static-interfaces" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Swift.prepare_model(model,</span> <span class="pre">config,</span> <span class="pre">**kwargs)</span></code></p>
<ul>
<li><p>Function: Loads a tuner into a model. If it is a subclass of <code class="docutils literal notranslate"><span class="pre">PeftConfig</span></code>, it uses the corresponding interface from the Peft library to load the tuner. When using <code class="docutils literal notranslate"><span class="pre">SwiftConfig</span></code>, this interface can accept <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code> instances and can be called repeatedly, functioning similarly to passing a dictionary of configs.</p>
<ul>
<li><p>This interface supports the parallel loading of multiple tuners of different types for concurrent use.</p></li>
</ul>
</li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: An instance of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> or <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code>, the model to be loaded.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config</span></code>: An instance of <code class="docutils literal notranslate"><span class="pre">SwiftConfig</span></code> or <code class="docutils literal notranslate"><span class="pre">PeftConfig</span></code>, or a dictionary of custom tuner names paired with their respective configs.</p></li>
</ul>
</li>
<li><p>Return Value: An instance of <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code> or <code class="docutils literal notranslate"><span class="pre">PeftModel</span></code>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Swift.merge_and_unload(model)</span></code></p>
<ul>
<li><p>Function: Merges LoRA weights back into the original model and completely unloads the LoRA component.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: An instance of <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code> or <code class="docutils literal notranslate"><span class="pre">PeftModel</span></code> that has had LoRA loaded.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Swift.merge(model)</span></code></p>
<ul>
<li><p>Function: Merges LoRA weights back into the original model without unloading the LoRA component.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: An instance of <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code> or <code class="docutils literal notranslate"><span class="pre">PeftModel</span></code> that has had LoRA loaded.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Swift.unmerge(model)</span></code></p>
<ul>
<li><p>Function: Splits LoRA weights back from the original model weights into the LoRA structure.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: An instance of <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code> or <code class="docutils literal notranslate"><span class="pre">PeftModel</span></code> that has had LoRA loaded.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Swift.save_to_peft_format(ckpt_dir,</span> <span class="pre">output_dir)</span></code></p>
<ul>
<li><p>Function: Converts stored LoRA checkpoints to a PEFT-compatible format. Key changes include:</p>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">default</span></code> will be split from the corresponding <code class="docutils literal notranslate"><span class="pre">default</span></code> folder into the root directory of <code class="docutils literal notranslate"><span class="pre">output_dir</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">{tuner_name}.</span></code> field will be removed from weight keys, e.g., <code class="docutils literal notranslate"><span class="pre">model.layer.0.self.in_proj.lora_A.default.weight</span></code> becomes <code class="docutils literal notranslate"><span class="pre">model.layer.0.self.in_proj.lora_A.weight</span></code>.</p></li>
<li><p>Weight keys will have a <code class="docutils literal notranslate"><span class="pre">basemodel.model</span></code> prefix added.</p></li>
<li><p>Note: Only LoRA can be converted; other types of tuners will raise conversion errors due to PEFT not supporting them. Additionally, due to the presence of extra parameters in LoRAConfig, such as <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, conversion to Peft format is not supported when these parameters are set. In such cases, you can manually delete the corresponding fields in adapter_config.json.</p></li>
</ul>
</li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ckpt_dir</span></code>: Original weights directory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_dir</span></code>: The target directory for the weights.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Swift.from_pretrained(model,</span> <span class="pre">model_id,</span> <span class="pre">adapter_name,</span> <span class="pre">revision,</span> <span class="pre">**kwargs)</span></code></p>
<ul>
<li><p>Function: Load the tuner onto the model from the stored weights directory. If <code class="docutils literal notranslate"><span class="pre">adapter_name</span></code> is not provided, all tuners from the <code class="docutils literal notranslate"><span class="pre">model_id</span></code> directory will be loaded. This interface can also be called repeatedly, similar to <code class="docutils literal notranslate"><span class="pre">prepare_model</span></code>.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: An instance of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> or <code class="docutils literal notranslate"><span class="pre">SwiftModel</span></code> to which the tuner will be loaded.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_id</span></code>: A string indicating the tuner checkpoint to be loaded, which can be an ID from the model hub or a local directory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">adapter_name</span></code>: Can be of type <code class="docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code>. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all tuners in the specified directory will be loaded. If it is a <code class="docutils literal notranslate"><span class="pre">str</span></code> or <code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, only specific tuners will be loaded. If it is a <code class="docutils literal notranslate"><span class="pre">Dict</span></code>, the key represents the tuner to load, which will be renamed to the corresponding value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">revision</span></code>: If <code class="docutils literal notranslate"><span class="pre">model_id</span></code> is an ID from the model hub, <code class="docutils literal notranslate"><span class="pre">revision</span></code> can specify the corresponding version number.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="swiftmodel-interfaces">
<h3>SwiftModel Interfaces<a class="headerlink" href="#swiftmodel-interfaces" title="Link to this heading">ïƒ</a></h3>
<p>Below is a list of interfaces that users may call. Other internal or less recommended interfaces can be viewed by running the <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">docs</span></code> command to access the API Doc.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.create_optimizer_param_groups(self,</span> <span class="pre">**defaults)</span></code></p>
<ul class="simple">
<li><p>Function: Creates parameter groups based on the loaded tuners; currently, this only applies to the <code class="docutils literal notranslate"><span class="pre">LoRA+</span></code> algorithm.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">defaults</span></code>: Default parameters for the <code class="docutils literal notranslate"><span class="pre">optimizer_groups</span></code>, such as <code class="docutils literal notranslate"><span class="pre">lr</span></code> and <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>.</p></li>
</ul>
</li>
<li><p>Return Value:</p>
<ul>
<li><p>The created <code class="docutils literal notranslate"><span class="pre">optimizer_groups</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.add_weighted_adapter(self,</span> <span class="pre">...)</span></code></p>
<ul class="simple">
<li><p>Function: Merges existing LoRA tuners into one.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p>This interface is a passthrough to <code class="docutils literal notranslate"><span class="pre">PeftModel.add_weighted_adapter</span></code>, and parameters can be referenced in the <a class="reference external" href="https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter">add_weighted_adapter documentation</a>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.save_pretrained(self,</span> <span class="pre">save_directory,</span> <span class="pre">safe_serialization,</span> <span class="pre">adapter_name)</span></code></p>
<ul class="simple">
<li><p>Function: Saves tuner weights.</p></li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">save_directory</span></code>: The directory for saving.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">safe_serialization</span></code>: Whether to use safe tensors, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">adapter_name</span></code>: Stored adapter tuner, if not provided, defaults to storing all tuners.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.set_active_adapters(self,</span> <span class="pre">adapter_names,</span> <span class="pre">offload=None)</span></code></p>
<ul class="simple">
<li><p>Function: Sets the currently active adapters; adapters not in the list will be deactivated.</p>
<ul>
<li><p>In inference, the environment variable <code class="docutils literal notranslate"><span class="pre">USE_UNIQUE_THREAD=0/1</span></code>, default is <code class="docutils literal notranslate"><span class="pre">1</span></code>. If set to <code class="docutils literal notranslate"><span class="pre">0</span></code>, then <code class="docutils literal notranslate"><span class="pre">set_active_adapters</span></code> only takes effect in the current thread, at which point it defaults to using the tuners activated in this thread, with tuners in different threads not interfering with each other.</p></li>
</ul>
</li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">adapter_names</span></code>: The names of the active tuners.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">offload</span></code>: How to handle deactivated adapters; default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning they remain in GPU memory. Can also use <code class="docutils literal notranslate"><span class="pre">cpu</span></code> or <code class="docutils literal notranslate"><span class="pre">meta</span></code> to offload to CPU or meta device to reduce memory consumption. In <code class="docutils literal notranslate"><span class="pre">USE_UNIQUE_THREAD=0</span></code>, do not pass the <code class="docutils literal notranslate"><span class="pre">offload</span></code> value to avoid affecting other threads.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.activate_adapter(self,</span> <span class="pre">adapter_name)</span></code></p>
<ul class="simple">
<li><p>Function: Activates a tuner.</p>
<ul>
<li><p>In inference, the environment variable <code class="docutils literal notranslate"><span class="pre">USE_UNIQUE_THREAD=0/1</span></code>, default is <code class="docutils literal notranslate"><span class="pre">1</span></code>. If set to <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">activate_adapter</span></code> will only be effective for the current thread, at which point it defaults to using the tuners activated in this thread, with tuners in different threads not interfering with each other.</p></li>
</ul>
</li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">adapter_name</span></code>: The name of the tuner to be activated.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.deactivate_adapter(self,</span> <span class="pre">adapter_name,</span> <span class="pre">offload)</span></code></p>
<ul class="simple">
<li><p>Function: Deactivates a tuner.</p>
<ul>
<li><p>During <code class="docutils literal notranslate"><span class="pre">inference</span></code>, do not call this interface when the <code class="docutils literal notranslate"><span class="pre">USE_UNIQUE_THREAD=0</span></code>.</p></li>
</ul>
</li>
<li><p>Parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">adapter_name</span></code>: The name of the tuner to be deactivated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">offload</span></code>: How to handle deactivated adapters; defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning they remain in GPU memory. Can also use <code class="docutils literal notranslate"><span class="pre">cpu</span></code> or <code class="docutils literal notranslate"><span class="pre">meta</span></code> to offload to CPU or meta device to reduce memory consumption.</p></li>
</ul>
</li>
<li><p>Return Value: None.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwiftModel.get_trainable_parameters(self)</span></code></p>
<ul>
<li><p>Function: Returns information about the trainable parameters.</p></li>
<li><p>Parameters: None.</p></li>
<li><p>Return Value: Information about trainable parameters in the following format:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>trainable params: 100M || all params: 1000M || trainable%: 10.00% || cuda memory: 10GiB.
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>