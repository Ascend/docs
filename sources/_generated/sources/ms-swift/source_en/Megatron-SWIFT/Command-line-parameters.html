

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Command Line Arguments &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../../../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Command Line Arguments</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Megatron-SWIFT/Command-line-parameters.md.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="command-line-arguments">
<h1>Command Line Arguments<a class="headerlink" href="#command-line-arguments" title="Link to this heading">ïƒ</a></h1>
<section id="megatron-parameters">
<h2>Megatron Parameters<a class="headerlink" href="#megatron-parameters" title="Link to this heading">ïƒ</a></h2>
<p><strong>Training Parameters</strong>:</p>
<ul class="simple">
<li><p>ğŸ”¥micro_batch_size: Batch size per device, default is 1.</p></li>
<li><p>ğŸ”¥global_batch_size: Total batch size, equivalent to <code class="docutils literal notranslate"><span class="pre">micro_batch_size</span> <span class="pre">*</span> <span class="pre">data</span> <span class="pre">parallel</span> <span class="pre">size</span> <span class="pre">*</span> <span class="pre">gradient</span> <span class="pre">accumulation</span> <span class="pre">steps</span></code>. Default is 16.</p>
<ul>
<li><p>Here, <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Parallelism</span> <span class="pre">size</span> <span class="pre">(DP)</span> <span class="pre">=</span> <span class="pre">Total</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">GPUs</span> <span class="pre">/</span> <span class="pre">(TP</span> <span class="pre">Ã—</span> <span class="pre">PP</span> <span class="pre">Ã—</span> <span class="pre">CP)</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥recompute_granularity: Granularity of activation recomputation, options are 'full', 'selective' and 'none'. 'full' means recomputing the entire transformer layer, while 'selective' means only recomputing the core attention part of the transformer layer. 'selective' is generally recommended. Default is 'selective'.</p>
<ul>
<li><p>When you set it to 'selective', you can specify <code class="docutils literal notranslate"><span class="pre">--recompute_modules</span></code> to choose which parts to recompute.</p></li>
</ul>
</li>
<li><p>ğŸ”¥recompute_method: This parameter takes effect only when recompute_granularity is set to 'full', options are 'uniform', 'block'. Default is None.</p></li>
<li><p>ğŸ”¥recompute_num_layers: This parameter takes effect only when recompute_granularity is set to 'full'. Default is None. If <code class="docutils literal notranslate"><span class="pre">recompute_method</span></code> is set to uniform, this parameter specifies the number of transformer layers in each uniformly divided recomputation unit. For example, you can specify <code class="docutils literal notranslate"><span class="pre">--recompute_granularity</span> <span class="pre">full</span> <span class="pre">--recompute_method</span> <span class="pre">uniform</span> <span class="pre">--recompute_num_layers</span> <span class="pre">4</span></code>. The larger the recompute_num_layers, the smaller the memory usage but higher computation cost. Note: The number of model layers in the current process must be divisible by <code class="docutils literal notranslate"><span class="pre">recompute_num_layers</span></code>. Default is None.</p></li>
<li><p>ğŸ”¥recompute_modules: Options include &quot;core_attn&quot;, &quot;moe_act&quot;, &quot;layernorm&quot;, &quot;mla_up_proj&quot;, &quot;mlp&quot;, and &quot;moe&quot;. The default value is <code class="docutils literal notranslate"><span class="pre">[&quot;core_attn&quot;]</span></code>. This parameter takes effect when <code class="docutils literal notranslate"><span class="pre">--recompute_granularity</span> <span class="pre">selective</span></code> is set. For example, during MoE training, you can reduce memory usage by specifying <code class="docutils literal notranslate"><span class="pre">--recompute_granularity</span> <span class="pre">selective</span> <span class="pre">--recompute_modules</span> <span class="pre">core_attn</span> <span class="pre">moe</span></code>. Among these, &quot;core_attn&quot;, &quot;mlp&quot;, and &quot;moe&quot; use normal checkpointing, while &quot;moe_act&quot;, &quot;layernorm&quot;, and &quot;mla_up_proj&quot; use output-discarding checkpointing.</p>
<ul>
<li><p>&quot;core_attn&quot;: Recomputes the core attention part of the Transformer layer.</p></li>
<li><p>&quot;mlp&quot;: Recomputes the dense MLP layer.</p></li>
<li><p>&quot;moe&quot;: Recomputes the MoE layer.</p></li>
<li><p>&quot;moe_act&quot;: Recomputes the MLP activation function part in the MoE module.</p></li>
<li><p>&quot;layernorm&quot;: Recomputes the input_layernorm and pre_mlp_layernorm.</p></li>
<li><p>&quot;mla_up_proj&quot;: Recomputes the MLA up-projection and RoPE application parts.</p></li>
</ul>
</li>
<li><p>deterministic_mode: Deterministic mode, which may lead to slower training speed, default is False.</p></li>
<li><p>ğŸ”¥train_iters: Total number of training iterations, default is None.</p>
<ul>
<li><p>Tip: You can set <code class="docutils literal notranslate"><span class="pre">--num_train_epochs</span></code> to specify the number of training epochs. When using a non-streaming dataset, <code class="docutils literal notranslate"><span class="pre">train_iters</span></code> will be automatically calculated based on the dataset size (compatible with packing).</p></li>
</ul>
</li>
<li><p>ğŸ”¥num_train_epochs: Specifies the number of training epochs. When using non-streaming datasets, this parameter will automatically calculate train_iters for you without the need to manually pass <code class="docutils literal notranslate"><span class="pre">train_iters</span></code>. When using streaming datasets, this parameter will force exit the training when <code class="docutils literal notranslate"><span class="pre">num_train_epochs</span></code> is reached, and perform validation and saving of weights. Defaults to None.</p></li>
<li><p>masked_softmax_fusion: Defaults to True. Used to enable the fusion of scaling, masking, and softmax for query_key_value.</p></li>
<li><p>bias_dropout_fusion: Defaults to True. Used to enable the fusion of bias and dropout.</p></li>
<li><p>bias_activation_fusion: If True, fuses bias addition and activation function when possible. Defaults to True.</p></li>
<li><p>apply_rope_fusion: Defaults to False. Used to enable RoPE fusion. This parameter is passed through from megatron-core. Note: RoPE fusion is not supported in all cases, for example: MLA, mrope, etc. are not supported.</p></li>
<li><p>gradient_accumulation_fusion: Defaults to True. Used to enable gradient accumulation fusion.</p></li>
<li><p>ğŸ”¥cross_entropy_loss_fusion: Enables cross-entropy loss computation fusion. Defaults to True.</p></li>
<li><p>cross_entropy_fusion_impl: Implementation of cross-entropy loss fusion. Options include 'native' and 'te'. Defaults to 'native'.</p></li>
<li><p>calculate_per_token_loss: Scales the cross-entropy loss according to the number of non-padding tokens in the global batch. Defaults to None. When <code class="docutils literal notranslate"><span class="pre">task_type</span></code> is 'causal_lm' and during pretraining/fine-tuning, it defaults to True; otherwise, it defaults to False.</p></li>
<li><p>ğŸ”¥attention_backend: The attention backend to use (flash, fused, unfused, local, auto). Default is flash.</p>
<ul>
<li><p>Some models may not support flash attention; you need to manually set <code class="docutils literal notranslate"><span class="pre">--attention_backend</span> <span class="pre">unfused/fused</span> <span class="pre">--padding_free</span> <span class="pre">false</span></code>, for example: Llama4, GPT-OSS.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">flash_attention_3</span></code> is installed, specifying <code class="docutils literal notranslate"><span class="pre">--attention_backend</span> <span class="pre">flash</span></code> will prioritize using FA3. Refer to the training script <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/train/flash_attention_3">here</a>.</p></li>
</ul>
</li>
<li><p>optimizer: Optimizer type, options are 'adam', 'sgd'. Default is adam.</p>
<ul>
<li><p>Note: This 'adam' is actually 'adamw'. See <a class="reference external" href="https://github.com/NVIDIA/TransformerEngine/blob/d8f1e68f7c414f3e7985a8b41de4443b2f819af3/transformer_engine/pytorch/optimizers/fused_adam.py#L69-L70">here</a> for reference.</p></li>
</ul>
</li>
<li><p>ğŸ”¥optimizer_cpu_offload: Offloads optimizer states to the CPU. For example, set: <code class="docutils literal notranslate"><span class="pre">--use_precision_aware_optimizer</span> <span class="pre">true</span> <span class="pre">--optimizer_cpu_offload</span> <span class="pre">true</span> <span class="pre">--optimizer_offload_fraction</span> <span class="pre">0.7</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<ul>
<li><p>This parameter can significantly reduce GPU memory usage (at the cost of increased CPU memory consumption). When the <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> is large, its impact on training speed is minimal.</p></li>
</ul>
</li>
<li><p>ğŸ”¥optimizer_offload_fraction: The fraction of the optimizer state to offload to CPU. Default is <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p>use_precision_aware_optimizer: Use the precision-aware optimizer in TransformerEngine, which allows setting the main parameters and optimizer states to lower precision, such as fp16 and fp8.</p></li>
<li><p>main_grads_dtype: The dtype of main gradients when use_precision_aware_optimizer is enabled. Options are 'fp32' and 'bf16'. Default is 'fp32'.</p></li>
<li><p>main_params_dtype: The dtype of main parameters when use_precision_aware_optimizer is enabled. Options are 'fp32' and 'fp16'. Default is 'fp32'.</p></li>
<li><p>exp_avg_dtype: The dtype of exp_avg (i.e., the first moment in the Adam optimizer) when use_precision_aware_optimizer is enabled. This dtype is used for storing the optimizer state in memory during training, but does not affect the precision in kernel computation. Options are 'fp32', 'fp16', 'bf16', and 'fp8'. Default is 'fp32'.</p></li>
<li><p>exp_avg_sq_dtype: The dtype of exp_avg_sq (i.e., the second moment in the Adam optimizer) when use_precision_aware_optimizer is enabled. This dtype is used for storing the optimizer state in memory during training, but does not affect the precision in kernel computation. Options are 'fp32', 'fp16', 'bf16', and 'fp8'. Default is 'fp32'.</p></li>
<li><p>manual_gc: Disables the default garbage collector and manually triggers garbage collection. Default is False.</p></li>
<li><p>manual_gc_steps: Interval (in steps) to manually trigger garbage collection. Defaults to 0.</p></li>
<li><p>manual_gc_eval: When using manual garbage collection (<code class="docutils literal notranslate"><span class="pre">--manual_gc</span> <span class="pre">true</span></code>), disables garbage collection at the beginning and end of each evaluation run. Defaults to True.</p></li>
</ul>
<p><strong>Data Parameters</strong>:</p>
<ul class="simple">
<li><p>seed: Random seed for python, numpy, pytorch, and cuda, default is 42.</p></li>
<li><p>dataset_shuffle: Whether to shuffle the dataset. Defaults to True.</p>
<ul>
<li><p>Note: <strong>Megatron-SWIFT's shuffling includes two parts</strong>: dataset shuffling, controlled by <code class="docutils literal notranslate"><span class="pre">dataset_shuffle</span></code>; and shuffling in train_dataloader, controlled by <code class="docutils literal notranslate"><span class="pre">train_dataloader_shuffle</span></code>.</p></li>
</ul>
</li>
<li><p>train_dataloader_shuffle: Whether to use shuffling for train_dataloader. Defaults to True. val_dataset is not shuffled.</p></li>
<li><p>ğŸ”¥dataloader_num_workers: Number of workers for the dataloader. Defaults to 4.</p>
<ul>
<li><p>Note: If <code class="docutils literal notranslate"><span class="pre">--streaming</span> <span class="pre">true</span></code> is set, it will be set to 1.</p></li>
</ul>
</li>
<li><p>dataloader_pin_memory: Defaults to True.</p></li>
<li><p>dataloader_persistent_workers: Defaults to True.</p></li>
<li><p>dataloader_prefetch_factor: Defaults to 2.</p></li>
<li><p>data_sharding: Takes effect on train_dataloader when <code class="docutils literal notranslate"><span class="pre">--train_dataloader_shuffle</span> <span class="pre">true</span></code>. Defaults to False. This parameter controls the scope of dataset shuffling. If set to True, the dataset is first sharded, then each shard is shuffled (slightly saves memory); if set to False, the dataset is shuffled first, then sharded (better shuffling effect).</p></li>
<li><p>ğŸ”¥group_by_length: Whether to group samples with roughly similar lengths together in the training dataset (with randomness), to minimize padding and ensure load balancing across nodes and processes for improved efficiency. Defaults to False. For the specific algorithm, refer to <code class="docutils literal notranslate"><span class="pre">transformers.trainer_pt_utils.get_length_grouped_indices</span></code>.</p></li>
<li><p>te_rng_tracker: Use the Transformer Engine version of the random number generator. Defaults to False.</p></li>
<li><p>data_parallel_random_init: Enable different random initializations across data parallel ranks. Defaults to False.</p></li>
<li><p>padding_free: Flatten the data in a batch to avoid data padding, thereby reducing memory usage and accelerating training. Defaults to True.</p>
<ul>
<li><p>If you want to customize attention_mask, you can set <code class="docutils literal notranslate"><span class="pre">--padding_free</span> <span class="pre">false</span></code>.</p></li>
<li><p>Note: <strong>Megatron-SWIFT training features prioritize support for padding_free format</strong>. Unless there are special circumstances, please do not modify this value.</p></li>
</ul>
</li>
<li><p>mlp_padding_free: Defaults to False. Used for padding_free optimization of mlp when padding_free is set to false. This can improve training speed and reduce memory usage while customizing attention_mask.</p></li>
</ul>
<p><strong>Learning Rate Parameters</strong>:</p>
<ul class="simple">
<li><p>lr_warmup_init: The initial value for learning rate warmup. The learning rate scheduler starts warming up from this value. Defaults to 0.</p></li>
<li><p>ğŸ”¥lr: The initial learning rate. The actual learning rate for each iteration will be determined based on the learning rate warmup and decay strategies. The default value is None; <strong>for full-parameter training, the default is 1e-5, while for LoRA training, the default is 1e-4</strong>.</p></li>
<li><p>lr_decay_style: Learning rate decay strategy. Defaults to 'cosine'. Typically set to 'constant', 'linear', 'cosine', 'inverse-square-root', or 'WSD'.</p></li>
<li><p>ğŸ”¥lr_decay_iters: Number of iterations for learning rate decay. Default is None, meaning it will be set to <code class="docutils literal notranslate"><span class="pre">--train_iters</span></code>.</p></li>
<li><p>lr_warmup_iters: Number of iterations for linear learning rate warm-up, default is 0.</p></li>
<li><p>ğŸ”¥lr_warmup_fraction: The fraction of the linear learning rate warmup phase, defaults to None.</p></li>
<li><p>ğŸ”¥min_lr: Minimum value of the learning rate, clipping any learning rate below this threshold to this value, default is 0.</p></li>
<li><p>lr_wsd_decay_style: The decay method for the WSD annealing phase. Defaults to 'exponential'.</p></li>
<li><p>lr_wsd_decay_iters: The number of iterations for learning rate decay. Defaults to None.</p></li>
</ul>
<p><strong>Regularization Parameters</strong>:</p>
<ul class="simple">
<li><p>ğŸ”¥weight_decay: Default is 0.1.</p></li>
<li><p>weight_decay_incr_style: The increment function for weight decay. Options are 'constant', 'linear', 'cosine'. Defaults to 'constant'.</p></li>
<li><p>start_weight_decay: The initial weight decay coefficient for L2 regularization.</p></li>
<li><p>end_weight_decay: The weight decay coefficient for L2 regularization at the end of training.</p></li>
<li><p>ğŸ”¥clip_grad: L2 gradient clipping, default is 1.0.</p>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">grad_norm</span></code> printed in logs is the value before clipping.</p></li>
</ul>
</li>
<li><p>adam_beta1: Default is 0.9.</p></li>
<li><p>adam_beta2: Default is 0.95.</p></li>
<li><p>adam_eps: Default is 1e-8.</p></li>
<li><p>sgd_momentum: Takes effect when <code class="docutils literal notranslate"><span class="pre">--optimizer</span> <span class="pre">sgd</span></code> is set. Defaults to 0.9.</p></li>
</ul>
<p><strong>Checkpoint Parameters</strong>:</p>
<ul class="simple">
<li><p>ğŸ”¥output_dir: Output directory for checkpoints, default is None. During training, if this parameter is not set, it defaults to <code class="docutils literal notranslate"><span class="pre">f'megatron_output/{model_suffix}'</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">'megatron_output/Qwen2.5-7B-Instruct'</span></code>.</p>
<ul>
<li><p>Note: <strong>When training on multiple machines, ensure that the save paths on each node point to the same location</strong>. Otherwise, you will need to manually consolidate these weights after training.</p></li>
</ul>
</li>
<li><p>ğŸ”¥save_steps: Interval (in steps) for saving checkpoints. Defaults to 500.</p>
<ul>
<li><p>Note: Weights will always be saved at the end of training.</p></li>
</ul>
</li>
<li><p>ğŸ”¥no_save_optim: Do not save optimizer, default is False. When performing full-parameter training, this can significantly reduce storage time.</p></li>
<li><p>ğŸ”¥no_save_rng: Do not save RNG, default is False.</p></li>
<li><p>ğŸ”¥mcore_model: The checkpoint directory to load (mcore storage format). Defaults to None. For information about resuming training from checkpoints, please refer to the description of the <code class="docutils literal notranslate"><span class="pre">--finetune</span></code> parameter.</p>
<ul>
<li><p>megatron-swift recommends directly loading and storing safetensors weights, refer to <a class="reference internal" href="Mcore-Bridge.html"><span class="doc">mcore-bridge documentation</span></a>.</p></li>
<li><p>Difference between <code class="docutils literal notranslate"><span class="pre">--model</span></code> and <code class="docutils literal notranslate"><span class="pre">--mcore_model</span></code>: <code class="docutils literal notranslate"><span class="pre">--model/--adapters/--ref_model/--ref_adapters</span></code> are followed by safetensors weight directories, while <code class="docutils literal notranslate"><span class="pre">--mcore_model/--mcore_adapter/--mcore_ref_model/--mcore_ref_adapter</span></code> are followed by mcore weight directories. <code class="docutils literal notranslate"><span class="pre">--model/--adapters</span></code> do not support loading checkpoint resumption states. Therefore, if you set <code class="docutils literal notranslate"><span class="pre">--no_save_optim</span> <span class="pre">false</span></code>, mcore weight format will be additionally stored for checkpoint resumption, and you need to use <code class="docutils literal notranslate"><span class="pre">--mcore_model/--mcore_adapter</span></code> to load the checkpoint resumption state.</p></li>
</ul>
</li>
<li><p>ğŸ”¥no_load_optim: Do not load optimizer, default is False.</p>
<ul>
<li><p>Note: When resuming training from a checkpoint, setting <code class="docutils literal notranslate"><span class="pre">--no_load_optim</span> <span class="pre">false</span></code> (i.e., loading the optimizer state) typically consumes significantly more GPU memory than setting <code class="docutils literal notranslate"><span class="pre">--no_load_optim</span> <span class="pre">true</span></code> (i.e., skipping the optimizer state).</p></li>
</ul>
</li>
<li><p>ğŸ”¥no_load_rng: Do not load RNG, default is False.</p></li>
<li><p>ğŸ”¥finetune: Load and fine-tune the model. Does not load the optimizer and random seed state from the checkpoint, and sets the iteration count to 0. Defaults to True.</p>
<ul>
<li><p>Note: For <strong>checkpoint resumption</strong>, you need to set <code class="docutils literal notranslate"><span class="pre">--mcore_model</span></code> (LoRA training requires additionally setting <code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span></code>). If you set <code class="docutils literal notranslate"><span class="pre">--finetune</span> <span class="pre">true</span></code>, it will not load the optimizer state and random seed state, will set the iteration count to 0, and will not skip the dataset; if you set <code class="docutils literal notranslate"><span class="pre">--finetune</span> <span class="pre">false</span></code>, it will read the iteration count and skip the previously trained dataset amount, and the loading of optimizer state and random seed state is controlled by <code class="docutils literal notranslate"><span class="pre">--no_load_optim</span></code> and <code class="docutils literal notranslate"><span class="pre">--no_load_rng</span></code>.</p></li>
<li><p>Streaming datasets (<code class="docutils literal notranslate"><span class="pre">--streaming</span></code>) are currently not supported for skipping datasets.</p></li>
</ul>
</li>
<li><p>perform_initialization: Initialize the weights. Defaults to False.</p></li>
<li><p>use_cpu_initialization: Initialize weights on the CPU. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. This option is used during weight conversion between Hugging Face (HF) and MCore formats. The value typically does not need to be modified.</p></li>
<li><p>ğŸ”¥async_save: Use asynchronous checkpoint saving. Currently only applicable to the <code class="docutils literal notranslate"><span class="pre">torch_dist</span></code> distributed checkpoint format. Defaults to False.</p></li>
<li><p>ğŸ”¥save_total_limit: Maximum number of checkpoints to save. Expired checkpoints will be deleted. Default is None, which saves all checkpoints. This parameter must be set to a number <code class="docutils literal notranslate"><span class="pre">&gt;=2</span></code>. If set to 2, it will save the best checkpoint and the last checkpoint. This parameter is currently not compatible with <code class="docutils literal notranslate"><span class="pre">async_save</span></code>.</p></li>
<li><p>metric_for_best_model: Default is None. For GRPO, defaults to 'reward'; for other cases, defaults to 'loss'.</p></li>
<li><p>greater_is_better: Default is None. When <code class="docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> contains 'loss', it is set to False; otherwise, it is set to True.</p></li>
<li><p>use_persistent_ckpt_worker: Enable a persistent checkpoint worker process for async save. Defaults to False.</p></li>
<li><p>dist_ckpt_save_pre_mcore_014: Save in the format prior to Megatron-Core 0.14. Defaults to False.</p></li>
<li><p>dist_ckpt_optim_fully_reshardable: Make optimizer distributed checkpoint fully reshardable (TP/PP/EP/DP) as opposed to plain DP reshardability. Defaults to False.</p></li>
<li><p>distrib_optim_fully_reshardable_mem_efficient: During distributed optimizer checkpoint save and load, tries to use as little memory as possible by using Gloo (instead of NCCL) and only one rank for saving. Turn on only if experiencing host or device memory issues. Has effect only when <code class="docutils literal notranslate"><span class="pre">--dist-ckpt-optim-fully-reshardable</span></code> flag is set. Defaults to False.</p></li>
</ul>
<p><strong>Distributed Parameters</strong>:
For guidance on selecting parallelization strategies, please refer to the <a class="reference external" href="./Quick-start.md#training-tips">Training Tips documentation</a>.</p>
<ul class="simple">
<li><p>ddp_backend: Distributed backend. Options are 'nccl' or 'gloo'. Defaults to nccl.</p></li>
<li><p>ddp_timeout: Defaults to 18000000, in seconds.</p></li>
<li><p>ğŸ”¥use_distributed_optimizer: Use a distributed optimizer (i.e., ZeRO-1). Default is True.</p></li>
<li><p>ğŸ”¥tensor_model_parallel_size: TP (Tensor Parallelism) size, default is 1.</p></li>
<li><p>ğŸ”¥pipeline_model_parallel_size: PP (Pipeline Parallelism) size, default is 1.</p></li>
<li><p>ğŸ”¥decoder_first_pipeline_num_layers: The number of Transformer layers in the first pipeline stage of the decoder. Default is None, which means the Transformer layers are evenly distributed across all pipeline stages.</p>
<ul>
<li><p>This parameter is typically used when <strong>the total number of Transformer layers is not divisible by the pipeline parallelism (PP) size</strong>, or when the first pipeline stage (PP stage 0) of a multimodal model consumes excessive GPU memory.</p></li>
</ul>
</li>
<li><p>ğŸ”¥decoder_last_pipeline_num_layers: The number of Transformer layers in the last pipeline stage of the decoder. Default is None, which means the Transformer layers are evenly distributed across all pipeline stages.</p></li>
<li><p>account_for_embedding_in_pipeline_split: If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input embedding layer will be treated as a standard Transformer layer in the context of partitioning and placement for pipeline parallelism. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>account_for_loss_in_pipeline_split: If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss layer will be treated as a standard Transformer layer in the context of partitioning and placement for pipeline parallelism. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>overlap_p2p_comm: Overlap pipeline parallel communication with forward and backward blocks in 1F1B. Defaults to True.</p></li>
<li><p>align_param_gather: When set to True, all PP stages will launch parameter all-gather operations simultaneously. Otherwise, each PP stage will launch independently as needed. Defaults to True.</p></li>
<li><p>ğŸ”¥sequence_parallel: Enables sequence parallel optimization; this option takes effect only when <code class="docutils literal notranslate"><span class="pre">tensor_model_parallel_size</span></code> is set. Default is False.</p></li>
<li><p>ğŸ”¥context_parallel_size: CP (Context Parallelism) size, default is 1.</p></li>
<li><p>tp_comm_overlap: Overlap tensor parallel communication with GEMM (General Matrix Multiplication) kernels (to reduce communication time). Default is False.</p></li>
<li><p>ğŸ”¥overlap_grad_reduce: Overlap grad reduction operations in DDP (to reduce DP communication time). Default is False.</p></li>
<li><p>ğŸ”¥overlap_param_gather: Overlap all-gather of parameters in the distributed optimizer (to reduce DP communication time). Default is False.</p></li>
<li><p>virtual_pipeline_model_parallel_size: The number of virtual pipeline stages per pipeline parallel rank. Defaults to None. VPP parallelism is used to reduce computation bubbles in PP parallelism and improve GPU utilization, but will slightly increase communication overhead.</p></li>
<li><p>microbatch_group_size_per_vp_stage: The number of consecutive microbatches processed by each virtual pipeline stage. Defaults to None, which equals <code class="docutils literal notranslate"><span class="pre">pipeline_model_parallel_size</span></code>.</p></li>
<li><p>ğŸ”¥pipeline_model_parallel_layout: A string describing a custom pipeline (pp/vpp) model parallel layout. For example: &quot;E|(t|)*3,m|m||L&quot;. Here, E, L, t, and m denote the embedding layer, loss layer, Transformer decoder layer, and MTP layer, respectively. Stages are separated by &quot;|&quot;. Repeated stages or layers can be expressed using multiplication. Commas are only for cosmetic readability and have no syntactic meaning. The default value is None, indicating that this argument is not used to set the layout.</p>
<ul>
<li><p>This parameter is typically used on heterogeneous GPU clusters.</p></li>
</ul>
</li>
<li><p>ğŸ”¥expert_model_parallel_size: The degree of expert parallelism, default is 1.</p></li>
<li><p>ğŸ”¥expert_tensor_parallel_size: expert tensor-parallel size. Default is 1.</p></li>
</ul>
<p><strong>Logging Parameters</strong>:</p>
<ul class="simple">
<li><p>report_to: Enabled logging backends. Defaults to <code class="docutils literal notranslate"><span class="pre">['tensorboard']</span></code>. Options are 'tensorboard', 'wandb', and 'swanlab'. Login for 'wandb' and 'swanlab' can use <code class="docutils literal notranslate"><span class="pre">WANDB_API_KEY</span></code> and <code class="docutils literal notranslate"><span class="pre">SWANLAB_API_KEY</span></code> environment variables.</p></li>
<li><p>ğŸ”¥logging_steps: Interval (in steps) for logging. Defaults to 5.</p></li>
<li><p>tensorboard_dir: Directory where tensorboard logs are written. Defaults to None, which means logs are stored in the <code class="docutils literal notranslate"><span class="pre">f'{save}/runs'</span></code> directory.</p></li>
<li><p>tensorboard_queue_size: Size of the TensorBoard queue for buffering pending events and summaries. When the number of pending items reaches this value, the next call to an &quot;add&quot; method will trigger a flush to disk. The default is 50.</p></li>
<li><p>wandb_project: Wandb project name. Defaults to 'megatron-swift'.</p></li>
<li><p>wandb_exp_name: Wandb experiment name. Defaults to the value of <code class="docutils literal notranslate"><span class="pre">--output_dir</span></code>.</p></li>
<li><p>swanlab_project: Swanlab project name. Defaults to 'megatron-swift'.</p></li>
<li><p>swanlab_exp_name: Swanlab experiment name. Defaults to the value of <code class="docutils literal notranslate"><span class="pre">--output_dir</span></code>.</p></li>
</ul>
<p><strong>Evaluation Parameters</strong>:</p>
<ul class="simple">
<li><p>ğŸ”¥eval_iters: Number of iterations for evaluation. Defaults to <code class="docutils literal notranslate"><span class="pre">-1</span></code>, in which case an appropriate value is automatically determined based on the size of the validation dataset. <strong>If the validation dataset size is smaller than the global batch size, evaluation will not be performed.</strong> When using streaming datasets, this value must be set manually.</p></li>
<li><p>ğŸ”¥eval_steps: Interval (in steps) for evaluation, i.e., how many steps to train before performing evaluation. Defaults to None, which is set to <code class="docutils literal notranslate"><span class="pre">save_steps</span></code>.</p></li>
</ul>
<p><strong>FP8 Parameters</strong>:</p>
<ul class="simple">
<li><p>fp8_format: The FP8 format scheme used for FP8 tensors in the forward and backward pass. Options are 'e4m3' and 'hybrid'. Default is None.</p></li>
<li><p>fp8_recipe: The FP8 recipe (algorithm scheme) used for FP8 tensors in the forward and backward pass. Options are 'tensorwise', 'delayed', 'mxfp8', and 'blockwise'. Default is 'delayed'. Note that blockwise fp8 requires CUDA version 12.9 or higher.</p></li>
<li><p>fp8_amax_history_len: Number of steps for which amax history is recorded per tensor. Default is 1024.</p></li>
<li><p>fp8_amax_compute_algo: Algorithm for computing amax from history. Options are 'most_recent' and 'max'. Default is 'max'.</p></li>
<li><p>fp8_param_gather: Keep the compute parameter in FP8 (do not use any other intermediate dtype) and perform the parameter all-gather in FP8 format. Default is False.</p>
<ul>
<li><p>Tips: Set this to True if you want to export weights in FP8 format; otherwise, set it to False.</p></li>
</ul>
</li>
</ul>
<p><strong>Mixed Precision Parameters</strong>:</p>
<ul class="simple">
<li><p>fp16: FP16 mode. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, and will be automatically set based on the model's <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code>â€”specifically, <code class="docutils literal notranslate"><span class="pre">fp16</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> is <code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float32</span></code>. The <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> is by default read from <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p></li>
<li><p>bf16: BF16 mode. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, and will be automatically set based on the model's <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code>â€”specifically, <code class="docutils literal notranslate"><span class="pre">bf16</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> is <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></li>
<li><p>apply_query_key_layer_scaling: Scales <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">*</span> <span class="pre">K^T</span></code> by <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">layer</span> <span class="pre">number</span></code> (e.g., divide by layer_num for layer_num-th layer). This is helpful for FP16 training. Default is None, meaning that if <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> is used, it will be set to True.</p></li>
<li><p>ğŸ”¥attention_softmax_in_fp32: Uses FP32 for computations in attention_mask and softmax. Default is True.</p></li>
<li><p>accumulate_allreduce_grads_in_fp32: Perform gradient accumulation and allreduce operations in fp32 precision. If <code class="docutils literal notranslate"><span class="pre">--bf16</span></code> is enabled and <code class="docutils literal notranslate"><span class="pre">main_params_dtype</span></code> is 'fp32', this is set to True. Otherwise, it defaults to False.</p></li>
</ul>
<p><strong>MoE Parameters</strong>:</p>
<ul class="simple">
<li><p>moe_router_load_balancing_type: Determines the load balancing strategy for the router. Options include &quot;aux_loss&quot;, &quot;seq_aux_loss&quot;, &quot;global_aux_loss&quot;, &quot;sinkhorn&quot;, and &quot;none&quot;. Note that &quot;global_aux_loss&quot; requires &quot;megatron-core&gt;=0.15&quot;. Default value is None. Read from config.json.</p></li>
<li><p>ğŸ”¥moe_router_dtype: Data type used for routing computation and expert output weighted averaging. Options are 'none', 'fp32', and 'fp64', which enhances numerical stability, especially when the number of experts is large. When used together with <code class="docutils literal notranslate"><span class="pre">moe_permute_fusion</span></code>, the performance impact is negligible. Default is 'fp32'. 'none' means no change to data type.</p></li>
<li><p>moe_token_dispatcher_type: The type of token dispatcher to use. Options include 'allgather', 'alltoall', 'flex', and 'alltoall_seq'. Default is 'alltoall'.</p></li>
<li><p>moe_enable_deepep: Enable DeepEP for efficient token dispatching and combine in MoE models. Only works with flex token dispatcher by setting <code class="docutils literal notranslate"><span class="pre">--moe_token_dispatcher_type</span> <span class="pre">flex</span></code>.</p></li>
<li><p>ğŸ”¥moe_grouped_gemm: When each rank contains multiple experts, multiple local GEMM kernels can be launched in parallel streams to improve utilization and performance by using GroupedLinear from TransformerEngine. Default is True.</p></li>
<li><p>ğŸ”¥moe_permute_fusion: Fuses token permutation operations during token dispatch. Default is False.</p></li>
<li><p>ğŸ”¥moe_aux_loss_coeff: Defaults to 0, meaning the auxiliary loss is not used. <strong>Generally, a higher value leads to worse training performance but more balanced MoE expert utilization.</strong> Please choose an appropriate value based on experimental results.</p></li>
<li><p>moe_z_loss_coeff: Scaling coefficient for z-loss. Default is None.</p></li>
<li><p>ğŸ”¥moe_shared_expert_overlap: Enables overlap between shared expert computation and the dispatcher. If not enabled, shared expert computation will be performed after routing experts. Only effective when <code class="docutils literal notranslate"><span class="pre">moe_shared_expert_intermediate_size</span></code> is set. Default is False.</p></li>
<li><p>ğŸ”¥moe_expert_capacity_factor: Capacity factor for each expert. <code class="docutils literal notranslate"><span class="pre">None</span></code> means no tokens will be dropped. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. When <code class="docutils literal notranslate"><span class="pre">--moe_expert_capacity_factor</span></code> is set, tokens exceeding an expertâ€™s capacity will be dropped based on their selection probability. This can <strong>balance the training load and improve training speed</strong> (for example, set it to 1. or 2.).</p></li>
<li><p>moe_pad_expert_input_to_capacity: Pad the input of each expert so that its length aligns with the expert capacity length. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. This option only takes effect if <code class="docutils literal notranslate"><span class="pre">--moe_expert_capacity_factor</span></code> is set.</p></li>
<li><p>moe_token_drop_policy: Options are 'probs' and 'position'. Default is 'probs'.</p></li>
</ul>
<p><strong>MTP Parameters</strong></p>
<ul class="simple">
<li><p>mtp_num_layers: Number of Multi-Token Prediction (MTP) layers. MTP extends the prediction scope at each position to multiple future tokens. This MTP implementation uses D sequential modules to sequentially predict D additional tokens. Default is None. (requires &quot;megatron-core&gt;=0.14&quot;)</p>
<ul>
<li><p>Note: The value of mtp_num_layers will not be automatically retrieved from config.json and must be set manually. You can refer to the <code class="docutils literal notranslate"><span class="pre">num_nextn_predict_layers</span></code> field in config.json to fill in this value. When using mcore-bridge, MTP weights will be loaded from safetensors files first. If not found, random initialization will be performed. (To use blockwise fp8 + mtp, please use mcore&gt;=0.15)</p></li>
</ul>
</li>
<li><p>mtp_loss_scaling_factor: Scaling factor of Multi-Token Prediction (MTP) loss. We compute the average of MTP losses across all depths, then multiply it by this scaling factor to obtain the overall MTP loss, which serves as an additional training objective. Default is 0.1.</p></li>
</ul>
<p><strong>Tuner Parameters</strong>:</p>
<ul class="simple">
<li><p>tuner_type: Options are <code class="docutils literal notranslate"><span class="pre">'lora'</span></code> and <code class="docutils literal notranslate"><span class="pre">'full'</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'full'</span></code>. (<strong>In ms-swift 3.x, the parameter name is <code class="docutils literal notranslate"><span class="pre">train_type</span></code></strong>)</p></li>
<li><p>ğŸ”¥freeze_llm: This argument only takes effect for multimodal models and can be used in both full-parameter and LoRA training, but with different behaviors. In full-parameter training, setting <code class="docutils literal notranslate"><span class="pre">freeze_llm=True</span></code> freezes the LLM component's weights. In LoRA training with <code class="docutils literal notranslate"><span class="pre">target_modules=['all-linear']</span></code>, setting <code class="docutils literal notranslate"><span class="pre">freeze_llm=True</span></code> prevents LoRA modules from being added to the LLM part. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>ğŸ”¥freeze_vit: This argument only applies to multimodal models and behaves differently depending on the training mode. In full-parameter training, setting <code class="docutils literal notranslate"><span class="pre">freeze_vit=True</span></code> freezes the ViT (vision transformer) component's weights. In LoRA training with <code class="docutils literal notranslate"><span class="pre">target_modules=['all-linear']</span></code>, setting <code class="docutils literal notranslate"><span class="pre">freeze_vit=True</span></code> prevents LoRA modules from being added to the ViT part. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<ul>
<li><p>Note: <strong>Here, &quot;vit&quot; refers not only to <code class="docutils literal notranslate"><span class="pre">vision_tower</span></code>, but also to <code class="docutils literal notranslate"><span class="pre">audio_tower</span></code></strong>. For Omni models, if you want to apply LoRA only to <code class="docutils literal notranslate"><span class="pre">vision_tower</span></code> and not <code class="docutils literal notranslate"><span class="pre">audio_tower</span></code>, you can modify <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/a5d4c0a2ce0658cef8332d6c0fa619a52afa26ff/swift/llm/model/model_arch.py#L544-L554">this code</a>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥freeze_aligner: This argument only affects multimodal models. In full-parameter training, setting <code class="docutils literal notranslate"><span class="pre">freeze_aligner=True</span></code> freezes the aligner (also known as projector) weights. In LoRA training with <code class="docutils literal notranslate"><span class="pre">target_modules=['all-linear']</span></code>, setting <code class="docutils literal notranslate"><span class="pre">freeze_aligner=True</span></code> prevents LoRA modules from being added to the aligner component. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
<p>Full-parameter Training:</p>
<ul class="simple">
<li><p>freeze_parameters: Prefixes of parameters to be frozen. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>freeze_parameters_regex: Regex expression for parameters to be frozen. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>freeze_parameters_ratio: The proportion of parameters to freeze from bottom to top. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>. Setting this to <code class="docutils literal notranslate"><span class="pre">1</span></code> will freeze all parameters; you can set trainable parameters separately using <code class="docutils literal notranslate"><span class="pre">trainable_parameters</span></code>. Except for values 0 or 1, this parameter is incompatible with pipeline parallelism (PP).</p></li>
<li><p>trainable_parameters: Prefixes of additional trainable parameters. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>trainable_parameters_regex: Regex expression to match additional trainable parameters. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
<p>LoRA Training:</p>
<ul class="simple">
<li><p>mcore_adapter: The path to the adapter weights for loading, used for resuming LoRA training from a checkpoint. The default is None. The method for resuming LoRA training from a checkpoint is the same as for full-parameter training. Please pay attention to the meaning of the <code class="docutils literal notranslate"><span class="pre">--finetune</span></code> parameter.</p></li>
<li><p>ğŸ”¥target_modules: Specifies the suffixes of modules to apply LoRA to. For example, you can set it as <code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">linear_qkv</span> <span class="pre">linear_proj</span></code>. The default is <code class="docutils literal notranslate"><span class="pre">['all-linear']</span></code>, which means all linear layers will be set as target modules.</p>
<ul>
<li><p>Note: The behavior of <code class="docutils literal notranslate"><span class="pre">'all-linear'</span></code> differs between LLMs and multimodal LLMs. For standard LLMs, it automatically finds all linear layers except <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> and attaches tuners. <strong>For multimodal LLMs, tuners are by default only attached to the LLM component; this behavior can be controlled via <code class="docutils literal notranslate"><span class="pre">freeze_llm</span></code>, <code class="docutils literal notranslate"><span class="pre">freeze_vit</span></code>, and <code class="docutils literal notranslate"><span class="pre">freeze_aligner</span></code></strong>.</p></li>
<li><p>Note: If you want to set all router layers as target modules, you can specify <code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">all-router</span> <span class="pre">...</span></code>. For example: <code class="docutils literal notranslate"><span class="pre">--target_modules</span> <span class="pre">all-router</span> <span class="pre">all-linear</span></code>.</p></li>
<li><p>The suffix names of Linear layers differ between transformers and Megatron. In Megatron, <code class="docutils literal notranslate"><span class="pre">linear_proj</span></code> represents <code class="docutils literal notranslate"><span class="pre">o_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">linear_qkv</span></code> represents the concatenation of <code class="docutils literal notranslate"><span class="pre">q_proj,</span> <span class="pre">k_proj,</span> <span class="pre">v_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">linear_fc1</span></code> represents the concatenation of <code class="docutils literal notranslate"><span class="pre">gate_proj</span></code> and <code class="docutils literal notranslate"><span class="pre">up_proj</span></code>, and <code class="docutils literal notranslate"><span class="pre">linear_fc2</span></code> represents <code class="docutils literal notranslate"><span class="pre">down_proj</span></code>.</p></li>
</ul>
</li>
<li><p>ğŸ”¥target_regex: Regex expression to specify LoRA modules. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>. If this value is provided, the <code class="docutils literal notranslate"><span class="pre">target_modules</span></code> parameter will be ignored.</p></li>
<li><p>ğŸ”¥modules_to_save: After attaching a tuner, explicitly specifies additional original model modules to participate in training and storage. The default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. For example, setting <code class="docutils literal notranslate"><span class="pre">--modules_to_save</span> <span class="pre">word_embeddings</span> <span class="pre">output_layer</span></code> will unfreeze the <code class="docutils literal notranslate"><span class="pre">word_embeddings</span></code> and <code class="docutils literal notranslate"><span class="pre">output_layer</span></code> layers during LoRA training, and the weights of these modules will be saved in the final checkpoint.</p></li>
<li><p>ğŸ”¥lora_rank: Default is <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p></li>
<li><p>ğŸ”¥lora_alpha: Default is <code class="docutils literal notranslate"><span class="pre">32</span></code>.</p></li>
<li><p>lora_dropout: Default is <code class="docutils literal notranslate"><span class="pre">0.05</span></code>.</p></li>
<li><p>lora_bias: Default is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>. Available options: <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'all'</span></code>. If you want all biases to be set as trainable, set this to <code class="docutils literal notranslate"><span class="pre">'all'</span></code>.</p></li>
<li><p>use_rslora: Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Whether to use <code class="docutils literal notranslate"><span class="pre">RS-LoRA</span></code>.</p></li>
</ul>
<p><strong>Mcore-Bridge Parameters</strong>:</p>
<ul class="simple">
<li><p>model: The model_id or model_path of safetensors weights. Default is None. Supports resume training from checkpoint using <code class="docutils literal notranslate"><span class="pre">--no_load_optim</span> <span class="pre">false</span> <span class="pre">--no_load_rng</span> <span class="pre">false</span></code>.</p></li>
<li><p>model_type: Model type. For details, refer to <a class="reference internal" href="../Instruction/Command-line-parameters.html"><span class="doc">ms-swift command-line parameters documentation</span></a>.</p></li>
<li><p>ğŸ”¥save_safetensors: Defaults to True, whether to directly save as safetensors weights. If <code class="docutils literal notranslate"><span class="pre">--no_save_optim</span> <span class="pre">false</span></code> is set, additional mcore format weights and optimizer weights will be saved (also saved in <code class="docutils literal notranslate"><span class="pre">output_dir</span></code>). When resuming from checkpoint, use <code class="docutils literal notranslate"><span class="pre">--mcore_model/--mcore_adapter/--no_load_optim/--no_load_rng</span></code> parameters to load mcore format weights.</p></li>
<li><p>adapters: adapter_id or adapter_path of LoRA incremental weights in safetensors format. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>ref_model: model_id or model_path of ref_model safetensors weights. Required when using DPO/GRPO/KTO algorithms with full-parameter training. Default is None, set to <code class="docutils literal notranslate"><span class="pre">--model</span></code>.</p></li>
<li><p>ref_adapters: List of adapter_id or adapter_path of ref_adapters safetensors weights (currently only supports length of 1). Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>use_hf: Controls whether to use ModelScope or HuggingFace for model download, dataset download, and model push. Default is False, using ModelScope.</p></li>
<li><p>hub_token: Hub token. ModelScope hub token can be found <a class="reference external" href="https://modelscope.cn/my/myaccesstoken">here</a>. Default is None.</p></li>
<li><p>merge_lora: Whether to store merged weights. Defaults to None. If <code class="docutils literal notranslate"><span class="pre">save_safetensors</span></code> is set to True, this parameter defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>; otherwise, it defaults to False. That is, by default, LoRA will be merged when storing in safetensors format; LoRA will not be merged when storing in torch_dist format.</p></li>
<li><p>max_shard_size: Maximum file size for safetensors format storage, defaults to '5GB'.</p></li>
<li><p>ğŸ”¥offload_bridge: Use CPU main memory to store HF format weights exported by Megatron for vLLM updates, to reduce GPU memory usage. Defaults to False. (Takes effect in GRPO/GKD algorithms)</p></li>
</ul>
<p><strong>Multimodal Parameters</strong>:</p>
<ul class="simple">
<li><p>vit_gradient_checkpointing: Whether to enable gradient checkpointing for the ViT (Vision Transformer) component during multimodal model training. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>. (<strong>The ViT implementation in Megatron-SWIFT uses the Hugging Face <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library.</strong>)</p></li>
<li><p>attn_impl: When training a multimodal model, sets the <code class="docutils literal notranslate"><span class="pre">attn_impl</span></code> implementation used for the ViT part. Defaults to <code class="docutils literal notranslate"><span class="pre">'flash_attn'</span></code>.</p></li>
<li><p>vit_lr: Specifies the learning rate for the ViT module when training multimodal models. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, same as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. Typically used together with <code class="docutils literal notranslate"><span class="pre">--freeze_vit</span></code> and <code class="docutils literal notranslate"><span class="pre">--freeze_aligner</span></code>.</p>
<ul>
<li><p>Note: The &quot;learning rate&quot; printed in the logs is the learning rate of the LLM.</p></li>
</ul>
</li>
<li><p>aligner_lr: Specifies the learning rate for the aligner module in multimodal models. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, same as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p></li>
<li><p>gradient_checkpointing_kwargs: Arguments passed to <code class="docutils literal notranslate"><span class="pre">torch.utils.checkpoint</span></code>. For example: set <code class="docutils literal notranslate"><span class="pre">--gradient_checkpointing_kwargs</span> <span class="pre">'{&quot;use_reentrant&quot;:</span> <span class="pre">false}'</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>. This parameter only takes effect when <code class="docutils literal notranslate"><span class="pre">vit_gradient_checkpointing</span></code> is enabled.</p></li>
</ul>
<p><strong>Other Parameters</strong>:</p>
<ul class="simple">
<li><p>check_model: Check local model files for corruption or modifications and provide prompts. Defaults to True. <strong>If in an offline environment, please set to False</strong>.</p></li>
<li><p>rope_scaling: Parameters related to rope_scaling. Defaults to None. For format reference, see <a class="reference external" href="https://modelscope.cn/models/LLM-Research/Meta-Llama-3.1-8B-Instruct/file/view/master?fileName=config.json&amp;status=1">llama3.1 config.json</a>, pass as a JSON string.</p>
<ul>
<li><p><strong>Currently the rope_scaling module uses transformers implementation and supports all rope_scaling supported by transformers.</strong></p></li>
</ul>
</li>
<li><p>apply_wd_to_qk_layernorm: Used for Qwen3-Next/Qwen3.5 full-parameter training to apply weight decay to qk layernorm. Defaults to False.</p></li>
<li><p>enable_dft_loss: Whether to use <a class="reference external" href="https://arxiv.org/abs/2508.05629">DFT</a> (Dynamic Fine-Tuning) loss in SFT training. Defaults to False.</p></li>
<li><p>enable_channel_loss: Enable channel loss. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. You need to prepare a &quot;channel&quot; field in the dataset, and ms-swift will group and calculate loss based on this field (if the &quot;channel&quot; field is not prepared, it will be classified under the default <code class="docutils literal notranslate"><span class="pre">None</span></code> channel). For dataset format, refer to <a class="reference external" href="../Customization/Custom-dataset.md#channel-loss">channel loss</a>. Channel loss is compatible with techniques such as packing/padding_free/loss_scale.</p></li>
<li><p>ğŸ”¥task_type: Defaults to 'causal_lm'. Options are 'causal_lm', 'seq_cls', 'embedding', and 'generative_reranker'.</p></li>
<li><p>num_labels: This parameter needs to be specified for classification models (i.e., <code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">seq_cls</span></code>). Represents the number of labels. Defaults to None.</p></li>
<li><p>problem_type: This parameter needs to be specified for classification models (i.e., <code class="docutils literal notranslate"><span class="pre">--task_type</span> <span class="pre">seq_cls</span></code>). Options are 'regression', 'single_label_classification', 'multi_label_classification'. Defaults to None. If the model is reward_model or num_labels is 1, this parameter is 'regression'; otherwise, it is 'single_label_classification'.</p></li>
<li><p>ğŸ”¥save_strategy: Saving strategy, options are 'steps' and 'epoch'. Defaults to 'steps'. When set to 'epoch', <code class="docutils literal notranslate"><span class="pre">save_steps</span></code> and <code class="docutils literal notranslate"><span class="pre">eval_steps</span></code> are automatically calculated to save at each epoch, so any user-provided values for these arguments are ignored.</p></li>
<li><p>callbacks: Custom trainer callbacks. Defaults to <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
</ul>
</section>
<section id="training-parameters">
<h2>Training Parameters<a class="headerlink" href="#training-parameters" title="Link to this heading">ïƒ</a></h2>
<p>Megatron training parameters are inherited from Megatron parameters and basic parameters (<strong>sharing dataset, template, etc. with ms-swift, and also supporting model-specific parameters from ms-swift</strong>). For details on basic parameters, please refer to <a class="reference external" href="../Instruction/Command-line-parameters.md#base-arguments">here</a>. Additionally, the following parameters are included:</p>
<ul class="simple">
<li><p>add_version: Adds a directory <code class="docutils literal notranslate"><span class="pre">&lt;version&gt;-&lt;timestamp&gt;</span></code> to <code class="docutils literal notranslate"><span class="pre">save</span></code> to prevent overwriting weights, default is True.</p></li>
<li><p>ğŸ”¥create_checkpoint_symlink: Creates additional checkpoint symlinks to facilitate writing automated training scripts. The symlink paths for <code class="docutils literal notranslate"><span class="pre">best_model</span></code> and <code class="docutils literal notranslate"><span class="pre">last_model</span></code> are <code class="docutils literal notranslate"><span class="pre">f'{output_dir}/best'</span></code> and <code class="docutils literal notranslate"><span class="pre">f'{output_dir}/last'</span></code> respectively.</p></li>
<li><p>ğŸ”¥packing: Use the <code class="docutils literal notranslate"><span class="pre">padding_free</span></code> method to pack data samples of different lengths into samples of <strong>approximately</strong> uniform length (packing ensures that complete sequences are not split), achieving load balancing across nodes and processes during training (preventing long texts from slowing down short text training), thereby improving GPU utilization and maintaining stable memory usage. When using <code class="docutils literal notranslate"><span class="pre">--attention_backend</span> <span class="pre">flash</span></code>, it ensures that different sequences within packed samples remain independent and invisible to each other (except for Qwen3-Next, which contains linear-attention). This parameter defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. All training tasks in Megatron-SWIFT support this parameter. Note: <strong>packing will reduce the number of dataset samples, please adjust gradient accumulation steps and learning rate accordingly</strong>.</p></li>
<li><p>packing_length: the length to use for packing. Defaults to None, in which case it is set to max_length.</p></li>
<li><p>packing_num_proc: Number of processes for packing, default is 1. Note that different values of <code class="docutils literal notranslate"><span class="pre">packing_num_proc</span></code> will result in different packed datasets. (This parameter does not take effect during streaming packing). Usually there is no need to modify this value, as packing speed is much faster than tokenization speed.</p></li>
<li><p>streaming: Stream data loading and processing, default is False. (The shuffling of streaming datasets is not thorough, which may lead to severe loss fluctuations.)</p>
<ul>
<li><p>Note: Since the length of a streaming dataset cannot be determined, the <code class="docutils literal notranslate"><span class="pre">--train_iters</span></code> parameter must be set. Also set the <code class="docutils literal notranslate"><span class="pre">num_train_epochs</span></code> parameter to ensure training exits after the specified number of epochs, and to validate and save the model weights accordingly.</p></li>
<li><p>Note: Streaming datasets can skip preprocessing wait time by overlapping preprocessing with training. Preprocessing for streaming datasets is performed only on rank 0 and then synchronized to other processes via data distribution. <strong>This is generally less efficient than the data sharding approach used in non-streaming datasets.</strong> When the training world_size is large, preprocessing and data distribution can become a training bottleneck.</p></li>
</ul>
</li>
<li><p>lazy_tokenize: Whether to use lazy tokenization. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, all dataset samples will be tokenized (and for multimodal models, images will be loaded from disk) before training begins. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>: in LLM training, it defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>; in MLLM training, it defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> to save memory.</p></li>
<li><p>new_special_tokens: List of additional special tokens to be added. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>. Example usage can be found <a class="reference external" href="https://github.com/modelscope/ms-swift/blob/main/examples/megatron/lora/new_special_tokens.sh">here</a>.</p>
<ul>
<li><p>Note: You can also pass a <code class="docutils literal notranslate"><span class="pre">.txt</span></code> file path where each line contains one special token.</p></li>
</ul>
</li>
</ul>
</section>
<section id="rlhf-parameters">
<h2>RLHF Parameters<a class="headerlink" href="#rlhf-parameters" title="Link to this heading">ïƒ</a></h2>
<p>In addition to inheriting the training parameters, the following parameters are also supported:</p>
<ul class="simple">
<li><p>ğŸ”¥rlhf_type: Default is 'dpo'. Currently, 'dpo', 'grpo', 'kto', 'rm', and 'gkd' are available.</p></li>
<li><p>loss_scale: Overrides the <code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> in <a class="reference internal" href="../Instruction/Command-line-parameters.html"><span class="doc">basic parameters</span></a>. Default is 'last_round'.</p></li>
<li><p>calculate_per_token_loss: Overrides the Megatron parameter. Default is False.</p></li>
</ul>
<section id="dpo-parameters">
<h3>DPO Parameters<a class="headerlink" href="#dpo-parameters" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>mcore_ref_model: Loading path for ref_model. Required when using DPO/GRPO/KTO algorithms with full parameter training. Defaults to None, which sets it to <code class="docutils literal notranslate"><span class="pre">mcore_model</span></code>.</p></li>
<li><p>mcore_ref_adapter: Weight loading path for ref_adapter. Defaults to None. If you want to use LoRA weights generated from SFT for DPO, set <code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span> <span class="pre">sft_ckpt</span> <span class="pre">--mcore_ref_adapter</span> <span class="pre">sft_ckpt</span> <span class="pre">--finetune</span> <span class="pre">true</span></code> during training. For checkpoint resumption in this scenario, set <code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span> <span class="pre">rlhf_ckpt</span> <span class="pre">--mcore_ref_adapter</span> <span class="pre">sft_ckpt</span> <span class="pre">--finetune</span> <span class="pre">false</span></code>.</p></li>
<li><p>beta: Has the same meaning as in <a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#trl.DPOConfig">TRL</a>. It controls the degree of deviation from the reference model. A higher beta value indicates less deviation from the reference model. For the IPO loss function (<code class="docutils literal notranslate"><span class="pre">loss_type=&quot;ipo&quot;</span></code>), beta is the regularization parameter as mentioned in the <a class="reference external" href="https://huggingface.co/papers/2310.12036">paper</a>. Default is 0.1.</p></li>
<li><p>ğŸ”¥rpo_alpha: A parameter from the <a class="reference external" href="https://huggingface.co/papers/2404.19733">RPO paper</a> that controls the weight of the NLL term (i.e., the SFT loss) in the loss function, where <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">dpo_loss</span> <span class="pre">+</span> <span class="pre">rpo_alpha</span> <span class="pre">*</span> <span class="pre">sft_loss</span></code>. The paper recommends setting it to <code class="docutils literal notranslate"><span class="pre">1.</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning the SFT loss is not included by default.</p></li>
<li><p>reference_free: Whether to ignore the provided reference model and implicitly use a reference model that assigns equal probability to all responses. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>label_smoothing: Default is 0.</p></li>
<li><p>f_divergence_type: Default is <code class="docutils literal notranslate"><span class="pre">reverse_kl</span></code>. See the <a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer">TRL documentation</a> for possible values.</p></li>
<li><p>loss_type: Default is <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code>. See the <a class="reference external" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions">TRL documentation</a> for possible values.</p></li>
</ul>
</section>
<section id="kto-parameters">
<h3>KTO Parameters<a class="headerlink" href="#kto-parameters" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>mcore_ref_model: same meaning as in DPO.</p></li>
<li><p>mcore_ref_adapter: same meaning as in DPO.</p></li>
<li><p>beta: parameter controlling the deviation from the ref_model. Higher <code class="docutils literal notranslate"><span class="pre">beta</span></code> means less deviation from the ref_model. Default is <code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p></li>
<li><p>loss_type: default is <code class="docutils literal notranslate"><span class="pre">'kto'</span></code>. See possible values in the TRL docs: https://huggingface.co/docs/trl/main/en/kto_trainer#trl.KTOConfig.loss_type.</p></li>
<li><p>desirable_weight: factor to weight desirable losses to counter imbalance between desirable and undesirable pairs. Default is <code class="docutils literal notranslate"><span class="pre">1.</span></code>.</p></li>
<li><p>undesirable_weight: factor to weight undesirable losses to counter imbalance between desirable and undesirable pairs. Default is <code class="docutils literal notranslate"><span class="pre">1.</span></code>.</p></li>
</ul>
</section>
<section id="rm-parameters">
<h3>RM Parameters<a class="headerlink" href="#rm-parameters" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>center_rewards_coefficient: A coefficient used in reward model (RM) training to incentivize the model to output rewards with zero mean. See this <a class="reference external" href="https://huggingface.co/papers/2312.09244">paper</a> for details. Recommended value: 0.01.</p></li>
</ul>
</section>
<section id="grpo-parameters">
<h3>GRPO Parameters<a class="headerlink" href="#grpo-parameters" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>mcore_ref_model: Same meaning as in DPO.</p></li>
<li><p>mcore_ref_adapter: Same meaning as in DPO.</p></li>
<li><p>beta: KL regularization coefficient, default is 0.04. When set to 0, the ref model is not loaded.</p></li>
<li><p>micro_batch_size: Batch size per device, default is 1.</p></li>
<li><p>global_batch_size: Total batch size, equivalent to <code class="docutils literal notranslate"><span class="pre">micro_batch_size</span> <span class="pre">*</span> <span class="pre">data</span> <span class="pre">parallel</span> <span class="pre">size</span> <span class="pre">*</span> <span class="pre">gradient</span> <span class="pre">accumulation</span> <span class="pre">steps</span></code>. Default is 16.</p></li>
<li><p>steps_per_generation: Number of optimization steps per generation round, i.e., the ratio of sampling batch size to global_batch_size. Default is 1.</p></li>
<li><p>generation_batch_size: Sampling batch size, must be a multiple of global_batch_size. Default equals global_batch_size * steps_per_generation.</p></li>
<li><p>num_generations: Number of samples per prompt, the G value in the paper, default is 8.</p></li>
<li><p>num_generations_eval: Number of generations to sample during evaluation. This allows using fewer generations during evaluation to save computation. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the value of <code class="docutils literal notranslate"><span class="pre">num_generations</span></code>. Default is None.</p></li>
<li><p>reward_funcs: GRPO algorithm reward functions. Options include <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>, <code class="docutils literal notranslate"><span class="pre">format</span></code>, <code class="docutils literal notranslate"><span class="pre">cosine</span></code>, <code class="docutils literal notranslate"><span class="pre">repetition</span></code>, and <code class="docutils literal notranslate"><span class="pre">soft_overlong</span></code>. See swift/rewards/orm.py. You can also customize your own reward functions in the plugin. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p>reward_weights: Weights for each reward function. Must match the total number of reward functions and reward models. Default is None, meaning all rewards have equal weights of <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<ul>
<li><p>Tip: If GRPO training includes <code class="docutils literal notranslate"><span class="pre">--reward_model</span></code>, it is added at the end of the reward functions.</p></li>
</ul>
</li>
<li><p>truncation_strategy: The method to handle inputs exceeding <code class="docutils literal notranslate"><span class="pre">max_length</span></code>. Supported values are <code class="docutils literal notranslate"><span class="pre">delete</span></code> and <code class="docutils literal notranslate"><span class="pre">left</span></code>, representing deletion and left-side truncation respectively. The default is <code class="docutils literal notranslate"><span class="pre">left</span></code>. Note that for multi-modal models, left-side truncation may remove multi-modal tokens and cause a shape mismatch error during model forward. With the delete strategy, over-long or encoding-failed samples are discarded, and new samples are resampled from the original dataset to maintain the intended batch size.</p></li>
<li><p>loss_type: Loss normalization type. Options are <code class="docutils literal notranslate"><span class="pre">['grpo',</span> <span class="pre">'bnpo',</span> <span class="pre">'dr_grpo']</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">'grpo'</span></code>. See this <a class="reference external" href="https://github.com/huggingface/trl/pull/3256#discussion_r2033213348">PR</a> for details.</p></li>
<li><p>log_completions: Whether to log model-generated content during training. Default is False.</p></li>
<li><p>vllm_mode: vLLM integration mode. Options are <code class="docutils literal notranslate"><span class="pre">server</span></code> and <code class="docutils literal notranslate"><span class="pre">colocate</span></code>. Server mode uses the vLLM server launched by <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">rollout</span></code> for sampling, while colocate mode deploys vLLM within the program. When using server mode:</p></li>
<li><p>vllm_mode server parameters:</p>
<ul>
<li><p>vllm_server_host: vLLM server host address. Default is None.</p></li>
<li><p>vllm_server_port: vLLM server port. Default is 8000.</p></li>
<li><p>vllm_server_base_url: Base URL of the vLLM server (e.g., http://local_host:8000). Default is None. When set, host and port settings are ignored.</p></li>
<li><p>vllm_server_timeout: Timeout for connecting to the vLLM server. Default is 240s.</p></li>
<li><p>vllm_server_pass_dataset: Pass additional dataset information to the vLLM server for multi-round training.</p></li>
<li><p>async_generate: Asynchronous rollout to improve training speed. Note: When enabled, sampling uses the model from the previous round update, and multi-round scenarios are not supported. Default is <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>SWIFT_UPDATE_WEIGHTS_BUCKET_SIZE: Environment variable for controlling the bucket size during weight synchronization. Applicable to full-parameter training in Server Mode. Unit is MB, default value is 512 MB.</p></li>
</ul>
</li>
<li><p>vllm_mode colocate parameters (for more parameter support, refer to <a class="reference external" href="#vllm-parameters">vLLM parameters</a>):</p>
<ul>
<li><p>vllm_gpu_memory_utilization: vLLM passthrough parameter. Default is 0.9.</p></li>
<li><p>vllm_max_model_len: vLLM passthrough parameter. Default is None.</p></li>
<li><p>vllm_enforce_eager: vLLM passthrough parameter. Default is False.</p></li>
<li><p>vllm_limit_mm_per_prompt: vLLM passthrough parameter. Default is None.</p></li>
<li><p>vllm_enable_prefix_caching: vLLM passthrough parameter. Default is True.</p></li>
<li><p>vllm_tensor_parallel_size: Tensor parallel size. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>vllm_enable_lora: Support loading LoRA adapters in the vLLM Engine. Default is False. Used to accelerate weight synchronization in LoRA training. See <a class="reference external" href="../Instruction/GRPO/GetStarted/GRPO.md#weight-synchronization-acceleration">documentation</a> for details.</p></li>
<li><p>sleep_level: Release vLLM GPU memory during training. Options are <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2]</span></code>. Default is 0, meaning no release.</p></li>
<li><p>offload_optimizer: Whether to offload optimizer parameters during vLLM inference. Default is False.</p></li>
<li><p>offload_model: Whether to offload the model during vLLM inference. Default is False.</p></li>
</ul>
</li>
<li><p>num_iterations: Number of updates per data sample, the $\mu$ value in the <a class="reference external" href="https://arxiv.org/abs/2402.03300">GRPO paper</a>. Default is 1.</p></li>
<li><p>epsilon: Clip coefficient. Default is 0.2.</p></li>
<li><p>epsilon_high: Upper clip coefficient. Default is None. When set, together with epsilon, forms the clipping range <code class="docutils literal notranslate"><span class="pre">[epsilon,</span> <span class="pre">epsilon_high]</span></code>.</p></li>
<li><p>dynamic_sample: Filter out data with zero reward standard deviation within groups and sample additional new data. Default is False.</p></li>
<li><p>max_resample_times: Limit the number of resampling times under dynamic_sample setting. Default is 3.</p></li>
<li><p>overlong_filter: Skip overlong truncated samples, which do not participate in loss calculation. Default is False.</p></li>
<li><p>delta: Bilateral GRPO upper bound clipping value from the <a class="reference external" href="https://huggingface.co/papers/2505.07291">INTELLECT-2 tech report</a>. If set, it is recommended to be greater than 1 + epsilon. Default is None.</p></li>
<li><p>importance_sampling_level: Controls importance sampling ratio calculation. Options are <code class="docutils literal notranslate"><span class="pre">token</span></code> and <code class="docutils literal notranslate"><span class="pre">sequence</span></code>. In <code class="docutils literal notranslate"><span class="pre">token</span></code> mode, the original log probability ratio for each token is preserved. In <code class="docutils literal notranslate"><span class="pre">sequence</span></code> mode, the log probability ratios of all valid tokens in the sequence are averaged. The <a class="reference external" href="https://arxiv.org/abs/2507.18071">GSPO paper</a> uses sequence-level calculation to stabilize training. Default is <code class="docutils literal notranslate"><span class="pre">token</span></code>.</p></li>
<li><p>scale_rewards: Specifies the reward scaling strategy. Options include <code class="docutils literal notranslate"><span class="pre">group</span></code> (scale by within-group standard deviation), <code class="docutils literal notranslate"><span class="pre">batch</span></code> (scale by batch-wide standard deviation), <code class="docutils literal notranslate"><span class="pre">none</span></code> (no scaling), and <code class="docutils literal notranslate"><span class="pre">gdpo</span></code> (normalize each reward function separately within groups before weighted aggregation, see <a class="reference external" href="https://arxiv.org/abs/2601.05242">GDPO paper</a>). In ms-swift &lt; 3.10, this parameter is boolean, where <code class="docutils literal notranslate"><span class="pre">true</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">group</span></code> and <code class="docutils literal notranslate"><span class="pre">false</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">none</span></code>. The default value is bound to <code class="docutils literal notranslate"><span class="pre">advantage_estimator</span></code>: <code class="docutils literal notranslate"><span class="pre">grpo</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">group</span></code>, <code class="docutils literal notranslate"><span class="pre">rloo</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">none</span></code>, and <code class="docutils literal notranslate"><span class="pre">reinforce_plus_plus</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">batch</span></code>.</p>
<ul>
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">gdpo</span></code> mode does not support <code class="docutils literal notranslate"><span class="pre">kl_in_reward=True</span></code>. If both are set, <code class="docutils literal notranslate"><span class="pre">kl_in_reward</span></code> will be automatically set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>GDPO is designed for multi-reward optimization: When using multiple reward functions, GDPO normalizes each reward function separately within groups (subtract mean, divide by std), then performs weighted aggregation using <code class="docutils literal notranslate"><span class="pre">reward_weights</span></code>, and finally applies batch-level normalization. This approach better preserves the relative differences between rewards and prevents different reward combinations from collapsing into identical advantage values.</p></li>
</ul>
</li>
<li><p>rollout_importance_sampling_mode: Training-inference mismatch correction mode. Options are <code class="docutils literal notranslate"><span class="pre">token_truncate</span></code>, <code class="docutils literal notranslate"><span class="pre">token_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">sequence_truncate</span></code>, <code class="docutils literal notranslate"><span class="pre">sequence_mask</span></code>. Default is None (disabled). For details, refer to the <a class="reference internal" href="../Instruction/GRPO/AdvancedResearch/training_inference_mismatch.html"><span class="doc">documentation</span></a>.</p></li>
<li><p>rollout_importance_sampling_threshold: Threshold for importance sampling weights, used for truncating or masking extreme weights. Default is 2.0.</p></li>
<li><p>log_rollout_offpolicy_metrics: Whether to log training-inference mismatch diagnostic metrics (KL, PPL, Ï‡Â², etc.) when <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code> is not set. When <code class="docutils literal notranslate"><span class="pre">rollout_importance_sampling_mode</span></code> is set, metrics are always logged. Default is False.</p></li>
<li><p>off_policy_sequence_mask_delta: Off-Policy Sequence Masking threshold from <a class="reference external" href="https://arxiv.org/abs/2512.02556">DeepSeek-V3.2 paper</a>. When set, computes <code class="docutils literal notranslate"><span class="pre">mean(old_policy_logps</span> <span class="pre">-</span> <span class="pre">policy_logps)</span></code> for each sequence. If this value exceeds the threshold AND the sequence has negative advantage, the sequence is masked out from loss computation. For details, refer to the <a class="reference external" href="../Instruction/GRPO/AdvancedResearch/training_inference_mismatch.md#off-policy-sequence-masking">documentation</a>.</p></li>
</ul>
<p>Built-in reward function parameters refer to the <a class="reference external" href="../Instruction/Command-line-parameters.md#reward-function-parameters">documentation</a>.</p>
</section>
<section id="gkd-parameters">
<h3>GKD Parameters<a class="headerlink" href="#gkd-parameters" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>teacher_model: Path or model ID of the teacher model. Required.</p></li>
<li><p>teacher_model_type: Teacher model type. Default is None, auto-detected.</p></li>
<li><p>teacher_model_revision: Teacher model version. Default is None.</p></li>
<li><p>beta: JSD divergence interpolation coefficient. 0.0 means Forward KL, 0.5 means symmetric JSD, 1.0 means Reverse KL. Default is 0.5.</p></li>
<li><p>lmbda: On-Policy learning probability. 0.0 means pure Off-Policy, 1.0 means pure On-Policy. Default is 0.5.</p></li>
<li><p>seq_kd: Whether to use teacher-generated responses (Sequential KD), not yet supported. Default is False.</p></li>
<li><p>temperature: Temperature for sampling and loss computation. Default is 0.9.</p></li>
<li><p>offload_teacher_model: Whether to offload teacher model to CPU to save GPU memory. Default is False.</p></li>
<li><p>sft_alpha: Mixing coefficient for SFT loss, <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">jsd_loss</span> <span class="pre">+</span> <span class="pre">sft_alpha</span> <span class="pre">*</span> <span class="pre">sft_loss</span></code>. Takes effect when using dataset responses (Off-Policy). Default is 0.</p></li>
<li><p>max_completion_length: Maximum tokens for generation. Default is 512.</p></li>
<li><p>vllm_mode: Same as GRPO parameter, used for On-Policy generation. Colocate mode deploys vLLM within the program.</p>
<ul>
<li><p>Note: On-Policy generation requires vLLM (<code class="docutils literal notranslate"><span class="pre">--use_vllm</span> <span class="pre">true</span> <span class="pre">--vllm_mode</span> <span class="pre">colocate/server</span></code>).</p></li>
<li><p>When <code class="docutils literal notranslate"><span class="pre">lmbda</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> but vLLM is not enabled, it will automatically fall back to Off-Policy mode.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="export-parameters">
<h2>Export Parameters<a class="headerlink" href="#export-parameters" title="Link to this heading">ïƒ</a></h2>
<p>This section introduces the parameters for <code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code>. To use the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code> command for exporting, please refer to the <a class="reference external" href="../Instruction/Command-line-parameters.md#export-arguments">ms-swift Command Line Parameters Documentation</a>. Compared to <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code> supports distributed and multi-node exporting. Megatron export parameters inherit from Megatron parameters and basic parameters.</p>
<ul class="simple">
<li><p>ğŸ”¥to_mcore: Convert HF format weights to Megatron format. Defaults to False.</p></li>
<li><p>ğŸ”¥to_hf: Convert Megatron format weights to HF format. Defaults to False.</p></li>
<li><p>ğŸ”¥merge_lora: Defaults to None. If <code class="docutils literal notranslate"><span class="pre">to_hf</span></code> is set to True, this parameter defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>, otherwise False. In other words, by default, LoRA will be merged when saving in safetensors format; when saving in torch_dist format, LoRA will not be merged. The merged weights are stored in the <code class="docutils literal notranslate"><span class="pre">--save</span></code> directory.</p>
<ul>
<li><p>Note: Transformers 5.0 has refactored the model architecture for MoE models. This new structure does not support MoE LoRA inference and may cause inference errors. <strong>It is recommended to merge LoRA weights for MoE models</strong> (vLLM is not affected).</p></li>
<li><p>Note: The expert structure differs between Transformers and Megatron models. For example, the expert layers in Transformers' Qwen3-VL-MoE are implemented as Parameters rather than Linear layers. As a result, some models cannot convert LoRA delta weights (though Qwen3-VL-MoE supports conversion if LoRA is trained only on linear_proj and linear_qkv). However, most models support LoRA conversion, such as Qwen3-MoE, Qwen3-Omni-MoE, and GLM4.5-V.</p></li>
</ul>
</li>
<li><p>ğŸ”¥test_convert_precision: Test the precision error of HF and Megatron format weight conversion. Defaults to False.</p></li>
<li><p>test_convert_dtype: The dtype used for conversion precision testing, defaults to 'float32'.</p></li>
<li><p>exist_ok: If <code class="docutils literal notranslate"><span class="pre">args.save</span></code> exists, do not throw an exception and perform overwriting. Defaults to False.</p></li>
<li><p>device_map: Takes effect when <code class="docutils literal notranslate"><span class="pre">--test_convert_precision</span> <span class="pre">true</span></code> is set and controls where the HF model is loaded. The default is <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>. You can set it to <code class="docutils literal notranslate"><span class="pre">'cpu'</span></code> to save GPU memory.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>