

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LoRA Training &mdash; ÊòáËÖæÂºÄÊ∫ê  ÊñáÊ°£</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../../../../../_static/statistics.js?v=da671b53"></script>
      <script src="../../../../../../_static/translations.js?v=beaddf03"></script>
      <script src="../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Á¥¢Âºï" href="../../../../../../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            ÊòáËÖæÂºÄÊ∫ê
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="ÊêúÁ¥¢ÊñáÊ°£" aria-label="ÊêúÁ¥¢ÊñáÊ°£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="ÂØºËà™ËèúÂçï">
              <p class="caption" role="heading"><span class="caption-text">üèÅ ÂºÄÂßã‰ΩøÁî®</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ascend/quick_install.html">Âø´ÈÄüÂÆâË£ÖÊòáËÖæÁéØÂ¢É</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üèóÔ∏è  Âü∫Á°ÄËÆæÊñΩ‰∏éÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üß† ËÆ≠ÁªÉ‰∏éÂæÆË∞ÉÊ°ÜÊû∂</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Êé®ÁêÜ‰∏éÊúçÂä°</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üé® Â§öÊ®°ÊÄÅ„ÄÅÂ∫îÁî®‰∏éËØÑÊµã</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ÁßªÂä®ÁâàÂØºËà™ËèúÂçï" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">ÊòáËÖæÂºÄÊ∫ê</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="È°µÈù¢ÂØºËà™">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">LoRA Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/sources/_generated/sources/ms-swift/source_en/Megatron-SWIFT/LoRA-Training.md.txt" rel="nofollow"> Êü•ÁúãÈ°µÈù¢Ê∫êÁ†Å</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lora-training">
<h1>LoRA Training<a class="headerlink" href="#lora-training" title="Link to this heading">ÔÉÅ</a></h1>
<p>Best practice reference for single-node 8xH20 LoRA training with Qwen3-235B-A22B-Instruct-250718: https://github.com/modelscope/ms-swift/pull/5033.</p>
<p>For environment setup, please refer to the <a class="reference internal" href="Quick-start.html"><span class="doc">Quick Start Guide</span></a> of Megatron-SWIFT.</p>
<section id="traditional-method">
<h2>Traditional Method<a class="headerlink" href="#traditional-method" title="Link to this heading">ÔÉÅ</a></h2>
<section id="converting-hf-to-mcore">
<h3>Converting HF to Mcore<a class="headerlink" href="#converting-hf-to-mcore" title="Link to this heading">ÔÉÅ</a></h3>
<p>Below, we introduce weight conversion using the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code> and <code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code> commands respectively. Compared to <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code> supports multi-node and LoRA incremental weight conversion, but is also more complex, requiring additional specification of parallelism parameters during export, such as <code class="docutils literal notranslate"><span class="pre">--tensor_model_parallel_size</span></code> and <code class="docutils literal notranslate"><span class="pre">--export_model_parallel_size</span></code>. For details, refer to the <a class="reference internal" href="Mcore-Bridge.html"><span class="doc">Mcore-Bridge Documentation</span></a>. To use the <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code> command, refer to the <a class="reference internal" href="Quick-start.html"><span class="doc">Quick Start Documentation</span></a>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code> uses a single process, places HF weights on the GPU, and uses device_map for parallelization; mcore weights are placed on the CPU without enabling parallelization. This approach is very easy to debug and test the precision alignment between HF and mcore.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code> uses torchrun to launch multiple processes, places mcore weights on the GPU, supports enabling various parallelization methods, fp8, mtp, etc., with comprehensive functionality. If precision alignment testing is needed, the last rank will load HF weights and place them on the CPU.</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># megatron export</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
megatron<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--to_mcore<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>Qwen2.5-7B-Instruct-mcore<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--test_convert_precision<span class="w"> </span><span class="nb">true</span>

<span class="c1"># swift export</span>
<span class="c1"># CUDA_VISIBLE_DEVICES=0 \</span>
<span class="c1"># swift export \</span>
<span class="c1">#     --model Qwen/Qwen2.5-7B-Instruct \</span>
<span class="c1">#     --to_mcore true \</span>
<span class="c1">#     --torch_dtype bfloat16 \</span>
<span class="c1">#     --output_dir Qwen2.5-7B-Instruct-mcore \</span>
<span class="c1">#     --test_convert_precision true</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>LoRA Training<a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h3>
<p>Training Script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># full: 2 * 70GiB 0.61s/it</span>
<span class="c1"># lora: 2 * 14GiB 0.45s/it</span>
<span class="nv">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="s1">&#39;expandable_segments:True&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
megatron<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mcore_model<span class="w"> </span>Qwen2.5-7B-Instruct-mcore<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_safetensors<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-zh#500&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-en#500&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;swift/self-cognition#500&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_alpha<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--sequence_parallel<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--micro_batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--global_batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_granularity<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_method<span class="w"> </span>uniform<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_num_layers<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--finetune<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cross_entropy_loss_fusion<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_warmup_fraction<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--min_lr<span class="w"> </span>1e-5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--system<span class="w"> </span><span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_optim<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_rng<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_num_proc<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_author<span class="w"> </span>swift<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name<span class="w"> </span>swift-robot
</pre></div>
</div>
<ul class="simple">
<li><p>For LoRA training scripts of MoE models, please refer to <a class="reference external" href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/lora">here</a>.</p></li>
</ul>
</section>
<section id="converting-mcore-to-hf">
<h3>Converting MCore to HF<a class="headerlink" href="#converting-mcore-to-hf" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># megatron export</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
megatron<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mcore_adapter<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--to_hf<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--merge_lora<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--test_convert_precision<span class="w"> </span><span class="nb">true</span>

<span class="c1"># swift export</span>
<span class="c1"># CUDA_VISIBLE_DEVICES=0 \</span>
<span class="c1"># swift export \</span>
<span class="c1">#     --mcore_adapter megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx \</span>
<span class="c1">#     --to_hf true \</span>
<span class="c1">#     --torch_dtype bfloat16 \</span>
<span class="c1">#     --output_dir megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx-hf \</span>
<span class="c1">#     --test_convert_precision true</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Note: The <code class="docutils literal notranslate"><span class="pre">--mcore_adapter</span></code> folder contains an <code class="docutils literal notranslate"><span class="pre">args.json</span></code> file. The conversion process will read the <code class="docutils literal notranslate"><span class="pre">--model/--mcore_model</span></code> and LoRA-related parameter information from this file. <code class="docutils literal notranslate"><span class="pre">swift</span> <span class="pre">export</span></code> does not currently support conversion of LoRA incremental weights. With <code class="docutils literal notranslate"><span class="pre">megatron</span> <span class="pre">export</span></code>, you can use the <code class="docutils literal notranslate"><span class="pre">--merge_lora</span></code> parameter to control whether to merge weights.</p></li>
</ul>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># If using full weights, replace `--adapters` with `--model`</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stream<span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
</section>
<section id="merge-lora">
<h3>Merge-LoRA<a class="headerlink" href="#merge-lora" title="Link to this heading">ÔÉÅ</a></h3>
<p>If you only want to merge the LoRA weights without converting them to Hugging Face format, for subsequent DPO training, you can use the following script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># megatron export</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
megatron<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mcore_adapter<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--to_mcore<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--merge_lora<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch_dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx-mcore<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--test_convert_precision<span class="w"> </span><span class="nb">true</span>

<span class="c1"># swift export</span>
<span class="c1"># CUDA_VISIBLE_DEVICES=0 \</span>
<span class="c1"># swift export \</span>
<span class="c1">#     --mcore_adapter megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx \</span>
<span class="c1">#     --to_mcore true \</span>
<span class="c1">#     --torch_dtype bfloat16 \</span>
<span class="c1">#     --output_dir megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx-mcore \</span>
<span class="c1">#     --test_convert_precision true</span>
</pre></div>
</div>
</section>
</section>
<section id="mcore-bridge-recommended">
<h2>Mcore-Bridge [Recommended]<a class="headerlink" href="#mcore-bridge-recommended" title="Link to this heading">ÔÉÅ</a></h2>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># full: 2 * 70GiB 0.61s/it</span>
<span class="c1"># lora: 2 * 14GiB 0.45s/it</span>
<span class="nv">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="s1">&#39;expandable_segments:True&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span><span class="se">\</span>
megatron<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_safetensors<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--merge_lora<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-zh#500&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;AI-ModelScope/alpaca-gpt4-data-en#500&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">              </span><span class="s1">&#39;swift/self-cognition#500&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tuner_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_rank<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_alpha<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target_modules<span class="w"> </span>all-linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor_model_parallel_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--sequence_parallel<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--micro_batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--global_batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_granularity<span class="w"> </span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_method<span class="w"> </span>uniform<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--recompute_num_layers<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--finetune<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cross_entropy_loss_fusion<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_warmup_fraction<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--min_lr<span class="w"> </span>1e-5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--system<span class="w"> </span><span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataloader_num_workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_optim<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_save_rng<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_num_proc<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_author<span class="w"> </span>swift<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name<span class="w"> </span>swift-robot
</pre></div>
</div>
</section>
<section id="id2">
<h3>Inference<a class="headerlink" href="#id2" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># If using full weights, replace `--adapters` with `--model`</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
swift<span class="w"> </span>infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--adapters<span class="w"> </span>megatron_output/Qwen2.5-7B-Instruct/vx-xxx/checkpoint-xxx-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stream<span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ÁâàÊùÉÊâÄÊúâ 2024, Ascend„ÄÇ</p>
  </div>

  Âà©Áî® <a href="https://www.sphinx-doc.org/">Sphinx</a> ÊûÑÂª∫Ôºå‰ΩøÁî®ÁöÑ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">‰∏ªÈ¢ò</a>
    Áî± <a href="https://readthedocs.org">Read the Docs</a> ÂºÄÂèë.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>