

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>å¿«é€Ÿå¼€å§‹ &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../_static/statistics.js?v=da671b53"></script>
      <script src="../../_static/translations.js?v=beaddf03"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../search.html" />
    <link rel="next" title="LMDeploy" href="../lm_deploy/index.html" />
    <link rel="prev" title="å®‰è£…æŒ‡å—" href="install.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernels/index.html">kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Llama.cpp</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="install.html">å®‰è£…æŒ‡å—</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">å¿«é€Ÿå¼€å§‹</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">æ¨¡å‹æ–‡ä»¶å‡†å¤‡åŠé‡åŒ–</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">æ¨ç†</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">ä½¿ç”¨å•å¡æ¨ç†</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">ä½¿ç”¨å¤šå¡æ¨ç†</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Llama.cpp</a></li>
      <li class="breadcrumb-item active">å¿«é€Ÿå¼€å§‹</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/sources/llama_cpp/quick_start.rst.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>å¿«é€Ÿå¼€å§‹<a class="headerlink" href="#id1" title="Link to this heading">ïƒ</a></h1>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>é˜…è¯»æœ¬ç¯‡å‰ï¼Œè¯·ç¡®ä¿å·²æŒ‰ç…§ <a class="reference internal" href="install.html"><span class="doc">å®‰è£…æŒ‡å—</span></a> å‡†å¤‡å¥½æ˜‡è…¾ç¯å¢ƒåŠ llama.cpp ï¼</p>
</div>
<p>æœ¬æ•™ç¨‹èšç„¦å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰çš„æ¨ç†è¿‡ç¨‹ï¼Œä»¥ Qwen2.5-7B æ¨¡å‹ä¸ºä¾‹ï¼Œè®²è¿°å¦‚ä½•ä½¿ç”¨ llama.cpp åœ¨æ˜‡è…¾ NPU ä¸Šè¿›è¡Œæ¨ç†ã€‚</p>
<section id="id2">
<h2>æ¨¡å‹æ–‡ä»¶å‡†å¤‡åŠé‡åŒ–<a class="headerlink" href="#id2" title="Link to this heading">ïƒ</a></h2>
<p>llama.cpp çš„æ¨ç†éœ€è¦ä½¿ç”¨ gguf æ ¼å¼æ–‡ä»¶ï¼Œllama.cpp æä¾›äº†ä¸¤ç§æ–¹å¼è½¬æ¢ Hugging Face æ¨¡å‹æ–‡ä»¶ï¼š</p>
<ul>
<li><p>ä½¿ç”¨ <a class="reference external" href="https://huggingface.co/spaces/ggml-org/gguf-my-repo">GGUF-my-repo</a> å°†æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚</p></li>
<li><p>ä½¿ç”¨é¡¹ç›®ä¸­çš„ <cite>convert_hf_to_gguf.py</cite> æ–‡ä»¶å°† Hugging Face æ¨¡å‹è½¬æ¢ä¸º gguf æ ¼å¼:</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>python<span class="w"> </span>convert_hf_to_gguf.py<span class="w"> </span>path/to/model
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>è¯¦æƒ…è¯·å‚è€ƒ <a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/README.md#prepare-and-quantize">Prepare and Quantize</a> ã€‚</p>
<p>æ³¨æ„ï¼šç›®å‰ä»…æ”¯æŒ FP16 ç²¾åº¦åŠ Q4_0/Q8_0 é‡åŒ–æ¨¡å‹ã€‚</p>
</section>
<section id="id3">
<h2>æ¨ç†<a class="headerlink" href="#id3" title="Link to this heading">ïƒ</a></h2>
<p>æœ‰ä¸¤ç§è®¾å¤‡é€‰æ‹©æ¨¡å¼:</p>
<ul class="simple">
<li><p>å•è®¾å¤‡ï¼šä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ä¸€ä¸ªè®¾å¤‡ç›®æ ‡ã€‚</p></li>
<li><p>å¤šè®¾å¤‡ï¼šè‡ªåŠ¨é€‰æ‹©å…·æœ‰ç›¸åŒåç«¯çš„è®¾å¤‡ã€‚</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>è®¾å¤‡é€‰æ‹©</p></th>
<th class="head"><p>å‚æ•°</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>å•è®¾å¤‡</p></td>
<td><p>--split-mode none --main-gpu DEVICE_ID</p></td>
</tr>
<tr class="row-odd"><td><p>å¤šè®¾å¤‡</p></td>
<td><p>--split-mode layer (default)</p></td>
</tr>
</tbody>
</table>
<section id="id4">
<h3>ä½¿ç”¨å•å¡æ¨ç†<a class="headerlink" href="#id4" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>./build/bin/llama-cli<span class="w"> </span>-m<span class="w"> </span>path_to_model<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;Building a website can be done in 10 simple steps:&quot;</span><span class="w"> </span>-n<span class="w"> </span><span class="m">400</span><span class="w"> </span>-e<span class="w"> </span>-ngl<span class="w"> </span><span class="m">33</span><span class="w"> </span>-sm<span class="w"> </span>none<span class="w"> </span>-mg<span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
</section>
<section id="id5">
<h3>ä½¿ç”¨å¤šå¡æ¨ç†<a class="headerlink" href="#id5" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>./build/bin/llama-cli<span class="w"> </span>-m<span class="w"> </span>path_to_model<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;Building a website can be done in 10 simple steps:&quot;</span><span class="w"> </span>-n<span class="w"> </span><span class="m">400</span><span class="w"> </span>-e<span class="w"> </span>-ngl<span class="w"> </span><span class="m">33</span><span class="w"> </span>-sm<span class="w"> </span>layer
</pre></div>
</div>
<p>ä»¥ä¸‹ä¸ºæ­£å¸¸æ¨ç†ç»“æœï¼š</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span>Log<span class="w"> </span>start
<span class="linenos">  2</span>main:<span class="w"> </span><span class="nv">build</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3520</span><span class="w"> </span><span class="o">(</span>8e707118<span class="o">)</span>
<span class="linenos">  3</span>main:<span class="w"> </span>built<span class="w"> </span>with<span class="w"> </span>cc<span class="w"> </span><span class="o">(</span>Ubuntu<span class="w"> </span><span class="m">9</span>.4.0-1ubuntu1~20.04.2<span class="o">)</span><span class="w"> </span><span class="m">9</span>.4.0<span class="w"> </span><span class="k">for</span><span class="w"> </span>aarch64-linux-gnu
<span class="linenos">  4</span>main:<span class="w"> </span><span class="nv">seed</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="m">1728907816</span>
<span class="linenos">  5</span>llama_model_loader:<span class="w"> </span>loaded<span class="w"> </span>meta<span class="w"> </span>data<span class="w"> </span>with<span class="w"> </span><span class="m">22</span><span class="w"> </span>key-value<span class="w"> </span>pairs<span class="w"> </span>and<span class="w"> </span><span class="m">291</span><span class="w"> </span>tensors<span class="w"> </span>from<span class="w"> </span>/home/jiahao/models/llama3-8b-instruct-fp16.gguf<span class="w"> </span><span class="o">(</span>version<span class="w"> </span>GGUF<span class="w"> </span>V3<span class="w"> </span><span class="o">(</span>latest<span class="o">))</span>
<span class="linenos">  6</span>llama_model_loader:<span class="w"> </span>Dumping<span class="w"> </span>metadata<span class="w"> </span>keys/values.<span class="w"> </span>Note:<span class="w"> </span>KV<span class="w"> </span>overrides<span class="w"> </span><span class="k">do</span><span class="w"> </span>not<span class="w"> </span>apply<span class="w"> </span><span class="k">in</span><span class="w"> </span>this<span class="w"> </span>output.
<span class="linenos">  7</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">0</span>:<span class="w">                       </span>general.architecture<span class="w"> </span><span class="nv">str</span><span class="w">              </span><span class="o">=</span><span class="w"> </span>llama
<span class="linenos">  8</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">1</span>:<span class="w">                               </span>general.name<span class="w"> </span><span class="nv">str</span><span class="w">              </span><span class="o">=</span><span class="w"> </span>Meta-Llama-3-8B-Instruct
<span class="linenos">  9</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">2</span>:<span class="w">                          </span>llama.block_count<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">32</span>
<span class="linenos"> 10</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">3</span>:<span class="w">                       </span>llama.context_length<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">8192</span>
<span class="linenos"> 11</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">4</span>:<span class="w">                     </span>llama.embedding_length<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">4096</span>
<span class="linenos"> 12</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">5</span>:<span class="w">                  </span>llama.feed_forward_length<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">14336</span>
<span class="linenos"> 13</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">6</span>:<span class="w">                 </span>llama.attention.head_count<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">32</span>
<span class="linenos"> 14</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">7</span>:<span class="w">              </span>llama.attention.head_count_kv<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">8</span>
<span class="linenos"> 15</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">8</span>:<span class="w">                       </span>llama.rope.freq_base<span class="w"> </span><span class="nv">f32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">500000</span>.000000
<span class="linenos"> 16</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">   </span><span class="m">9</span>:<span class="w">     </span>llama.attention.layer_norm_rms_epsilon<span class="w"> </span><span class="nv">f32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.000010
<span class="linenos"> 17</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">10</span>:<span class="w">                          </span>general.file_type<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>
<span class="linenos"> 18</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">11</span>:<span class="w">                           </span>llama.vocab_size<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">128256</span>
<span class="linenos"> 19</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">12</span>:<span class="w">                 </span>llama.rope.dimension_count<span class="w"> </span><span class="nv">u32</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="m">128</span>
<span class="linenos"> 20</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">13</span>:<span class="w">                       </span>tokenizer.ggml.model<span class="w"> </span><span class="nv">str</span><span class="w">              </span><span class="o">=</span><span class="w"> </span>gpt2
<span class="linenos"> 21</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">14</span>:<span class="w">                         </span>tokenizer.ggml.pre<span class="w"> </span><span class="nv">str</span><span class="w">              </span><span class="o">=</span><span class="w"> </span>llama-bpe
<span class="linenos"> 22</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">15</span>:<span class="w">                      </span>tokenizer.ggml.tokens<span class="w"> </span>arr<span class="o">[</span>str,128256<span class="o">]</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;!&quot;</span>,<span class="w"> </span><span class="s2">&quot;\&quot;&quot;</span>,<span class="w"> </span><span class="s2">&quot;#&quot;</span>,<span class="w"> </span><span class="s2">&quot;</span>$<span class="s2">&quot;</span>,<span class="w"> </span><span class="s2">&quot;%&quot;</span>,<span class="w"> </span><span class="s2">&quot;&amp;&quot;</span>,<span class="w"> </span><span class="s2">&quot;&#39;&quot;</span>,<span class="w"> </span>...
<span class="linenos"> 23</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">16</span>:<span class="w">                  </span>tokenizer.ggml.token_type<span class="w"> </span>arr<span class="o">[</span>i32,128256<span class="o">]</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span>,<span class="w"> </span>...
<span class="linenos"> 24</span>llama_model_loader:<span class="w"> </span>-<span class="w"> </span>kv<span class="w">  </span><span class="m">17</span>:<span class="w">                      </span>tokenizer.ggml.merges<span class="w"> </span>arr<span class="o">[</span>str,280147<span class="o">]</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Ä  Ä &quot;</span>,<span class="w"> </span><span class="s2">&quot;Ä  Ä Ä Ä &quot;</span>,<span class="w"> </span><span class="s2">&quot;Ä Ä  Ä Ä &quot;</span>,<span class="w"> </span><span class="s2">&quot;...</span>
<span class="linenos"> 25</span><span class="s2">llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000</span>
<span class="linenos"> 26</span><span class="s2">llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009</span>
<span class="linenos"> 27</span><span class="s2">llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...</span>
<span class="linenos"> 28</span><span class="s2">llama_model_loader: - kv  21:               general.quantization_version u32              = 2</span>
<span class="linenos"> 29</span><span class="s2">llama_model_loader: - type  f32:   65 tensors</span>
<span class="linenos"> 30</span><span class="s2">llama_model_loader: - type  f16:  226 tensors</span>
<span class="linenos"> 31</span><span class="s2">llm_load_vocab: special tokens cache size = 256</span>
<span class="linenos"> 32</span><span class="s2">llm_load_vocab: token to piece cache size = 0.8000 MB</span>
<span class="linenos"> 33</span><span class="s2">llm_load_print_meta: format           = GGUF V3 (latest)</span>
<span class="linenos"> 34</span><span class="s2">llm_load_print_meta: arch             = llama</span>
<span class="linenos"> 35</span><span class="s2">llm_load_print_meta: vocab type       = BPE</span>
<span class="linenos"> 36</span><span class="s2">llm_load_print_meta: n_vocab          = 128256</span>
<span class="linenos"> 37</span><span class="s2">llm_load_print_meta: n_merges         = 280147</span>
<span class="linenos"> 38</span><span class="s2">llm_load_print_meta: vocab_only       = 0</span>
<span class="linenos"> 39</span><span class="s2">llm_load_print_meta: n_ctx_train      = 8192</span>
<span class="linenos"> 40</span><span class="s2">llm_load_print_meta: n_embd           = 4096</span>
<span class="linenos"> 41</span><span class="s2">llm_load_print_meta: n_layer          = 32</span>
<span class="linenos"> 42</span><span class="s2">llm_load_print_meta: n_head           = 32</span>
<span class="linenos"> 43</span><span class="s2">llm_load_print_meta: n_head_kv        = 8</span>
<span class="linenos"> 44</span><span class="s2">llm_load_print_meta: n_rot            = 128</span>
<span class="linenos"> 45</span><span class="s2">llm_load_print_meta: n_swa            = 0</span>
<span class="linenos"> 46</span><span class="s2">llm_load_print_meta: n_embd_head_k    = 128</span>
<span class="linenos"> 47</span><span class="s2">llm_load_print_meta: n_embd_head_v    = 128</span>
<span class="linenos"> 48</span><span class="s2">llm_load_print_meta: n_gqa            = 4</span>
<span class="linenos"> 49</span><span class="s2">llm_load_print_meta: n_embd_k_gqa     = 1024</span>
<span class="linenos"> 50</span><span class="s2">llm_load_print_meta: n_embd_v_gqa     = 1024</span>
<span class="linenos"> 51</span><span class="s2">llm_load_print_meta: f_norm_eps       = 0.0e+00</span>
<span class="linenos"> 52</span><span class="s2">llm_load_print_meta: f_norm_rms_eps   = 1.0e-05</span>
<span class="linenos"> 53</span><span class="s2">llm_load_print_meta: f_clamp_kqv      = 0.0e+00</span>
<span class="linenos"> 54</span><span class="s2">llm_load_print_meta: f_max_alibi_bias = 0.0e+00</span>
<span class="linenos"> 55</span><span class="s2">llm_load_print_meta: f_logit_scale    = 0.0e+00</span>
<span class="linenos"> 56</span><span class="s2">llm_load_print_meta: n_ff             = 14336</span>
<span class="linenos"> 57</span><span class="s2">llm_load_print_meta: n_expert         = 0</span>
<span class="linenos"> 58</span><span class="s2">llm_load_print_meta: n_expert_used    = 0</span>
<span class="linenos"> 59</span><span class="s2">llm_load_print_meta: causal attn      = 1</span>
<span class="linenos"> 60</span><span class="s2">llm_load_print_meta: pooling type     = 0</span>
<span class="linenos"> 61</span><span class="s2">llm_load_print_meta: rope type        = 0</span>
<span class="linenos"> 62</span><span class="s2">llm_load_print_meta: rope scaling     = linear</span>
<span class="linenos"> 63</span><span class="s2">llm_load_print_meta: freq_base_train  = 500000.0</span>
<span class="linenos"> 64</span><span class="s2">llm_load_print_meta: freq_scale_train = 1</span>
<span class="linenos"> 65</span><span class="s2">llm_load_print_meta: n_ctx_orig_yarn  = 8192</span>
<span class="linenos"> 66</span><span class="s2">llm_load_print_meta: rope_finetuned   = unknown</span>
<span class="linenos"> 67</span><span class="s2">llm_load_print_meta: ssm_d_conv       = 0</span>
<span class="linenos"> 68</span><span class="s2">llm_load_print_meta: ssm_d_inner      = 0</span>
<span class="linenos"> 69</span><span class="s2">llm_load_print_meta: ssm_d_state      = 0</span>
<span class="linenos"> 70</span><span class="s2">llm_load_print_meta: ssm_dt_rank      = 0</span>
<span class="linenos"> 71</span><span class="s2">llm_load_print_meta: model type       = 8B</span>
<span class="linenos"> 72</span><span class="s2">llm_load_print_meta: model ftype      = F16</span>
<span class="linenos"> 73</span><span class="s2">llm_load_print_meta: model params     = 8.03 B</span>
<span class="linenos"> 74</span><span class="s2">llm_load_print_meta: model size       = 14.96 GiB (16.00 BPW)</span>
<span class="linenos"> 75</span><span class="s2">llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct</span>
<span class="linenos"> 76</span><span class="s2">llm_load_print_meta: BOS token        = 128000 &#39;&lt;|begin_of_text|&gt;&#39;</span>
<span class="linenos"> 77</span><span class="s2">llm_load_print_meta: EOS token        = 128009 &#39;&lt;|eot_id|&gt;&#39;</span>
<span class="linenos"> 78</span><span class="s2">llm_load_print_meta: LF token         = 128 &#39;Ã„&#39;</span>
<span class="linenos"> 79</span><span class="s2">llm_load_print_meta: EOT token        = 128009 &#39;&lt;|eot_id|&gt;&#39;</span>
<span class="linenos"> 80</span><span class="s2">llm_load_print_meta: max token length = 256</span>
<span class="linenos"> 81</span><span class="s2">llm_load_tensors: ggml ctx size =    0.27 MiB</span>
<span class="linenos"> 82</span><span class="s2">llm_load_tensors:        CPU buffer size = 15317.02 MiB</span>
<span class="linenos"> 83</span><span class="s2">llm_load_tensors:       CANN buffer size = 13313.00 MiB</span>
<span class="linenos"> 84</span><span class="s2">.........................................................................................</span>
<span class="linenos"> 85</span><span class="s2">llama_new_context_with_model: n_ctx      = 8192</span>
<span class="linenos"> 86</span><span class="s2">llama_new_context_with_model: n_batch    = 2048</span>
<span class="linenos"> 87</span><span class="s2">llama_new_context_with_model: n_ubatch   = 512</span>
<span class="linenos"> 88</span><span class="s2">llama_new_context_with_model: flash_attn = 0</span>
<span class="linenos"> 89</span><span class="s2">llama_new_context_with_model: freq_base  = 500000.0</span>
<span class="linenos"> 90</span><span class="s2">llama_new_context_with_model: freq_scale = 1</span>
<span class="linenos"> 91</span><span class="s2">llama_kv_cache_init:       CANN KV buffer size =  1024.00 MiB</span>
<span class="linenos"> 92</span><span class="s2">llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB</span>
<span class="linenos"> 93</span><span class="s2">llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB</span>
<span class="linenos"> 94</span><span class="s2">llama_new_context_with_model:       CANN compute buffer size =  1260.50 MiB</span>
<span class="linenos"> 95</span><span class="s2">llama_new_context_with_model:        CPU compute buffer size =    24.01 MiB</span>
<span class="linenos"> 96</span><span class="s2">llama_new_context_with_model: graph nodes  = 1030</span>
<span class="linenos"> 97</span><span class="s2">llama_new_context_with_model: graph splits = 4</span>
<span class="linenos"> 98</span>
<span class="linenos"> 99</span><span class="s2">system_info: n_threads = 192 / 192 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |</span>
<span class="linenos">100</span><span class="s2">sampling:</span>
<span class="linenos">101</span><span class="s2">    repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000</span>
<span class="linenos">102</span><span class="s2">    top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800</span>
<span class="linenos">103</span><span class="s2">    mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000</span>
<span class="linenos">104</span><span class="s2">sampling order:</span>
<span class="linenos">105</span><span class="s2">CFG -&gt; Penalties -&gt; top_k -&gt; tfs_z -&gt; typical_p -&gt; top_p -&gt; min_p -&gt; temperature</span>
<span class="linenos">106</span><span class="s2">generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1</span>
<span class="linenos">107</span>
<span class="linenos">108</span>
<span class="linenos">109</span><span class="s2">Building a website can be done in 10 simple steps: 1. Define your website&#39;s purpose and target audience 2. Choose a domain name and register it with a registrar 3. Select a web hosting service and set up your hosting account 4. Design your website&#39;s layout and structure 5. Create content for your website, including text, images, and other media 6. Build a responsive website design that adapts to different devices and screen sizes 7. Choose a Content Management System (CMS) and install it on your website 8. Customize your website&#39;s design and layout using a CMS</span>
<span class="linenos">110</span>
<span class="linenos">111</span><span class="s2">llama_print_timings:        load time =    9074.69 ms</span>
<span class="linenos">112</span><span class="s2">llama_print_timings:      sample time =      31.97 ms /   112 runs   (    0.29 ms per token,  3503.28 tokens per second)</span>
<span class="linenos">113</span><span class="s2">llama_print_timings: prompt eval time =     238.53 ms /    13 tokens (   18.35 ms per token,    54.50 tokens per second)</span>
<span class="linenos">114</span><span class="s2">llama_print_timings:        eval time =   13152.29 ms /   111 runs   (  118.49 ms per token,     8.44 tokens per second)</span>
<span class="linenos">115</span><span class="s2">llama_print_timings:       total time =   13623.53 ms /   124 tokens</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="é¡µè„š">
        <a href="install.html" class="btn btn-neutral float-left" title="å®‰è£…æŒ‡å—" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> ä¸Šä¸€é¡µ</a>
        <a href="../lm_deploy/index.html" class="btn btn-neutral float-right" title="LMDeploy" accesskey="n" rel="next">ä¸‹ä¸€é¡µ <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>