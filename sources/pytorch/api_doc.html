

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>API æ–‡æ¡£ &mdash; æ˜‡è…¾å¼€æº  æ–‡æ¡£</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=f2aa3e58" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../_static/statistics.js?v=da671b53"></script>
      <script src="../../_static/translations.js?v=beaddf03"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="ç´¢å¼•" href="../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../search.html" />
    <link rel="next" title="FAQ" href="faq.html" />
    <link rel="prev" title="åŠŸèƒ½æ ·ä¾‹" href="examples.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            æ˜‡è…¾å¼€æº
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="æœç´¢æ–‡æ¡£" aria-label="æœç´¢æ–‡æ¡£" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="å¯¼èˆªèœå•">
              <p class="caption" role="heading"><span class="caption-text">ğŸ å¼€å§‹ä½¿ç”¨</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ascend/quick_install.html">å¿«é€Ÿå®‰è£…æ˜‡è…¾ç¯å¢ƒ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ—ï¸  åŸºç¡€è®¾æ–½ä¸æ¡†æ¶</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernels/index.html">kernels</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">PyTorch</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="install.html">å®‰è£…æŒ‡å—</a></li>
<li class="toctree-l2"><a class="reference internal" href="quick_start.html">å¿«é€Ÿå¼€å§‹</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html">åŠŸèƒ½æ ·ä¾‹</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">API æ–‡æ¡£</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu._npu_dropout"><code class="docutils literal notranslate"><span class="pre">torch_npu._npu_dropout()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.copy_memory_"><code class="docutils literal notranslate"><span class="pre">torch_npu.copy_memory_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.empty_with_format"><code class="docutils literal notranslate"><span class="pre">torch_npu.empty_with_format()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.fast_gelu"><code class="docutils literal notranslate"><span class="pre">torch_npu.fast_gelu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_alloc_float_status"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_alloc_float_status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_anchor_response_flags"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_anchor_response_flags()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_apply_adam"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_apply_adam()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_batch_nms"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_batch_nms()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bert_apply_adam"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bert_apply_adam()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bmmV2"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bmmV2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bounding_box_decode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bounding_box_decode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bounding_box_encode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bounding_box_encode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_broadcast"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_broadcast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ciou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ciou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_clear_float_status"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_clear_float_status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_confusion_transpose"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_confusion_transpose()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_conv2d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_conv2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_conv3d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_conv3d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_conv_transpose2d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_conv_transpose2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_convolution"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_convolution()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_convolution_transpose"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_convolution_transpose()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_deformable_conv2d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_deformable_conv2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_diou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_diou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_dropout_with_add_softmax"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_dropout_with_add_softmax()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_dtype_cast"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_dtype_cast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_format_cast"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_format_cast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_format_cast_"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_format_cast_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_fused_attention_score"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_fused_attention_score()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_fusion_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_fusion_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_geglu"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_geglu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_get_float_status"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_get_float_status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_giou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_giou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_grid_assign_positive"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_grid_assign_positive()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_gru"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_gru()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ifmr"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ifmr()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_indexing"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_indexing()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_iou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_iou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_layer_norm_eval"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_layer_norm_eval()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_linear"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_linear()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_lstm"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_lstm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_masked_fill_range"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_masked_fill_range()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_max"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_max()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_min"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_min()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_mish"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_mish()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_multi_head_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_multi_head_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_nms_rotated"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_nms_rotated()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_nms_v4"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_nms_v4()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_nms_with_mask"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_nms_with_mask()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_normalize_batch"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_normalize_batch()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_one_hot"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_one_hot()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_pad"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_pad()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ps_roi_pooling"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ps_roi_pooling()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ptiou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ptiou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_random_choice_with_mask"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_random_choice_with_mask()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_reshape"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_reshape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rms_norm"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rms_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_roi_align"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_roi_align()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotary_mul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotary_mul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_box_decode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_box_decode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_box_encode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_box_encode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_iou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_iou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_overlaps"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_overlaps()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scaled_masked_softmax"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scaled_masked_softmax()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scatter"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scatter()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_sign_bits_pack"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_sign_bits_pack()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_sign_bits_unpack"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_sign_bits_unpack()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_silu"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_silu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_slice"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_slice()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_softmax_cross_entropy_with_logits"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_softmax_cross_entropy_with_logits()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_sort_v2"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_sort_v2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_stride_add"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_stride_add()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_transpose"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_transpose()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_yolo_boxes_encode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_yolo_boxes_encode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.one_"><code class="docutils literal notranslate"><span class="pre">torch_npu.one_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_swiglu"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_swiglu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_trans_quant_param"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_trans_quant_param()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_quant_matmul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_quant_matmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_weight_quant_batchmatmul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_weight_quant_batchmatmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_convert_weight_to_int4pack"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_convert_weight_to_int4pack()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_grouped_matmul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_grouped_matmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_quant_scatter"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_quant_scatter()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_quant_scatter_"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_quant_scatter_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scatter_nd_update"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scatter_nd_update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scatter_nd_update_"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scatter_nd_update_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_anti_quant"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_anti_quant()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_mm_all_reduce_base"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_mm_all_reduce_base()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ffn"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ffn()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_incre_flash_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_incre_flash_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_prompt_flash_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_prompt_flash_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_fused_infer_attention_score"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_fused_infer_attention_score()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../transformers/index.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ§  è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../LLaMA-Factory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ms-swift/index.html">ms-swift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../roll/index.html">ROLL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchtitan/index.html">TorchTitan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VeOmni/index.html">VeOmni</a></li>
<li class="toctree-l1"><a class="reference internal" href="../verl/index.html">verl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸš€ æ¨ç†ä¸æœåŠ¡</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sglang/index.html">SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchchat/index.html">Torchchat</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸ¨ å¤šæ¨¡æ€ã€åº”ç”¨ä¸è¯„æµ‹</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whisper_cpp/index.html">Whisper.cpp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="ç§»åŠ¨ç‰ˆå¯¼èˆªèœå•" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">æ˜‡è…¾å¼€æº</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="é¡µé¢å¯¼èˆª">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">PyTorch</a></li>
      <li class="breadcrumb-item active">API æ–‡æ¡£</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/sources/pytorch/api_doc.rst.txt" rel="nofollow"> æŸ¥çœ‹é¡µé¢æºç </a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="api">
<span id="pytorch-api"></span><h1>API æ–‡æ¡£<a class="headerlink" href="#api" title="Link to this heading">ïƒ</a></h1>
<p>PyTorch-NPU é™¤äº†æä¾›äº† PyTorch å®˜æ–¹ç®—å­å®ç°ä¹‹å¤–ï¼Œä¹Ÿæä¾›äº†å¤§é‡é«˜æ€§èƒ½çš„è‡ªå®šä¹‰ç®—å­ï¼Œè¯¦ç»†çš„ç®—å­ä¿¡æ¯ä»¥åŠæè¿°å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>åœ¨è¿è¡Œä¸‹è¿°ç¤ºä¾‹ä¹‹å‰ï¼Œéœ€è¦å¯¼å…¥torch_npuæ‰©å±•åŒ… <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch_npu</span></code></p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu._npu_dropout">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">_npu_dropout</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu._npu_dropout" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu._npu_dropout(self, p) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä¸ä½¿ç”¨ç§å­(seed)è¿›è¡Œdropoutç»“æœè®¡æ•°ã€‚ä¸torch.dropoutç›¸ä¼¼ï¼Œä¼˜åŒ–NPUè®¾å¤‡å®ç°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>p (Float) - ä¸¢å¼ƒæ¦‚ç‡ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prob</span> <span class="o">=</span> <span class="mf">0.3</span><span class="o">&gt;&gt;&gt;</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">_npu_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">2.8571</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">188</span><span class="p">,</span> <span class="mi">186</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">157</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">159</span><span class="p">,</span>  <span class="mi">77</span><span class="p">,</span> <span class="mi">223</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span>  <span class="mi">79</span><span class="p">,</span> <span class="mi">247</span><span class="p">,</span> <span class="mi">151</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">253</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.copy_memory_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">copy_memory_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.copy_memory_" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.copy_memory_(dst, src, non_blocking=False) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä»srcæ‹·è´å…ƒç´ åˆ°selfå¼ é‡ï¼Œå¹¶è¿”å›selfã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>dst (Tensor) - æ‹·è´æºå¼ é‡ã€‚</p>
<p>src (Tensor) - è¿”å›å¼ é‡æ‰€éœ€æ•°æ®ç±»å‹ã€‚</p>
<p>non_blocking (Bool,é»˜è®¤å€¼ä¸ºFalse) - å¦‚æœè®¾ç½®ä¸ºTrueä¸”æ­¤æ‹·è´ä½äºCPUå’ŒNPUä¹‹é—´ï¼Œåˆ™æ‹·è´å¯èƒ½ç›¸å¯¹äºä¸»æœºå¼‚æ­¥å‘ç”Ÿã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ­¤å‚æ•°æ²¡æœ‰æ•ˆæœã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>copy_memory_ä»…æ”¯æŒNPUå¼ é‡ã€‚copy_memory_çš„è¾“å…¥å¼ é‡åº”å…·æœ‰ç›¸åŒçš„dtypeå’Œè®¾å¤‡indexã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">copy_memory_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.empty_with_format">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">empty_with_format</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.empty_with_format" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.empty_with_format(size, dtype, layout, device, pin_memory, acl_format)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è¿”å›ä¸€ä¸ªå¡«å……æœªåˆå§‹åŒ–æ•°æ®çš„å¼ é‡ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>size (ListInt) - å®šä¹‰è¾“å‡ºå¼ é‡shapeçš„æ•´æ•°åºåˆ—ã€‚å¯ä»¥æ˜¯å‚æ•°æ•°é‡(å¯å˜å€¼)ï¼Œä¹Ÿå¯ä»¥æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ç­‰é›†åˆã€‚</p>
<p>dtype (torch.dtype, å¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºNone) - è¿”å›å¼ é‡æ‰€éœ€æ•°æ®ç±»å‹ã€‚å¦‚æœå€¼ä¸ºNoneï¼Œè¯·ä½¿ç”¨å…¨å±€é»˜è®¤å€¼(è¯·å‚è§torch.set_default_tensor_type()).</p>
<p>layout (torch.layout, å¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºtorch.strided) - è¿”å›å¼ é‡æ‰€éœ€å¸ƒå±€ã€‚</p>
<p>device (torch.device, å¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºNone) - è¿”å›å¼ é‡çš„æ‰€éœ€è®¾å¤‡ã€‚</p>
<p>pin_memory (Bool, å¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºFalse) - å¦‚æœè®¾ç½®æ­¤å‚æ•°ï¼Œè¿”å›å¼ é‡å°†åˆ†é…åœ¨å›ºå®šå†…å­˜ä¸­ã€‚</p>
<p>acl_format (Intï¼Œé»˜è®¤å€¼ä¸º2) - è¿”å›å¼ é‡æ‰€éœ€å†…å­˜æ ¼å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">empty_with_format</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.fast_gelu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">fast_gelu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.fast_gelu" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.fast_gelu(self) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>geluçš„npuå®ç°ã€‚æ”¯æŒFakeTensoræ¨¡å¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ã€float32ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; ç¤ºä¾‹ä¸€ï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; x = torch.rand(2).npu()
&gt;&gt;&gt; x
&gt;&gt;&gt; tensor([0.5991, 0.4094], device=&#39;npu:0&#39;)
&gt;&gt;&gt; torch_npu.fast_gelu(x)
&gt;&gt;&gt; tensor([0.4403, 0.2733], device=&#39;npu:0&#39;)
&gt;&gt;&gt; ç¤ºä¾‹äºŒï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; //FakeTensoræ¨¡å¼
&gt;&gt;&gt; from torch._subclasses.fake_tensor import FakeTensorMode
&gt;&gt;&gt; with FakeTensorMode():
&gt;&gt;&gt; ...     x = torch.rand(2).npu()
&gt;&gt;&gt; ...     torch_npu.fast_gelu(x)
&gt;&gt;&gt; FakeTensor(..., device=&#39;npu:0&#39;, size=(2,))
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_alloc_float_status">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_alloc_float_status</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_alloc_float_status" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_alloc_float_status(self) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ç”Ÿæˆä¸€ä¸ªåŒ…å«8ä¸ª0çš„ä¸€ç»´å¼ é‡ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - ä»»ä½•å¼ é‡ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_alloc_float_status</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">2.2324</span><span class="p">,</span>  <span class="mf">0.2478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1056</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1273</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2573</span><span class="p">,</span>  <span class="mf">1.0558</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_anchor_response_flags">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_anchor_response_flags</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_anchor_response_flags" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_anchor_response_flags(self, featmap_size, stride, num_base_anchors) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨å•ä¸ªç‰¹å¾å›¾ä¸­ç”Ÿæˆé”šç‚¹çš„è´£ä»»æ ‡å¿—ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - çœŸå€¼æ¡†ï¼Œshapeä¸º[batch, 4]çš„2Då¼ é‡ã€‚</p>
<p>featmap_size (ListInt of length 2) - ç‰¹å¾å›¾å¤§å°ã€‚</p>
<p>strides (ListInt of length 2) - å½“å‰æ°´å¹³çš„æ­¥é•¿ã€‚</p>
<p>num_base_anchors (Int) - base anchorsçš„æ•°é‡ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_anchor_response_flags</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">32400</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_apply_adam">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_apply_adam</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_apply_adam" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_apply_adam(beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, out = (var, m, v))</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>adamç»“æœè®¡æ•°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>beta1_power (Scalar) - beta1çš„å¹‚ã€‚</p>
<p>beta2_power (Scalar) - beta2çš„å¹‚ã€‚</p>
<p>lr (Scalar) - å­¦ä¹ ç‡ã€‚</p>
<p>beta1 (Scalar) - ä¸€é˜¶çŸ©ä¼°è®¡å€¼çš„æŒ‡æ•°è¡°å‡ç‡ã€‚</p>
<p>beta2 (Scalar) - äºŒé˜¶çŸ©ä¼°è®¡å€¼çš„æŒ‡æ•°è¡°å‡ç‡ã€‚</p>
<p>epsilon (Scalar) - æ·»åŠ åˆ°åˆ†æ¯ä¸­ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§çš„é¡¹æ•°ã€‚</p>
<p>grad (Tensor) - æ¢¯åº¦ã€‚</p>
<p>use_locking (Boolï¼Œå¯é€‰) - è®¾ç½®ä¸ºTrueæ—¶ä½¿ç”¨lockè¿›è¡Œæ›´æ–°æ“ä½œã€‚</p>
<p>use_nesterov (Boolï¼Œå¯é€‰) - è®¾ç½®ä¸ºTrueæ—¶é‡‡ç”¨nesterovæ›´æ–°ã€‚</p>
<p>var (Tensor) - å¾…ä¼˜åŒ–å˜é‡ã€‚</p>
<p>m (Tensor) - å˜é‡å¹³å‡å€¼ã€‚</p>
<p>v (Tensor) - å˜é‡æ–¹å·®ã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_batch_nms">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_batch_nms</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_batch_nms" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_batch_nms(self, scores, score_threshold, iou_threshold, max_size_per_class, max_total_size, change_coordinate_frame=False, transpose_box=False) -&gt; (Tensor, Tensor, Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ ¹æ®batchåˆ†ç±»è®¡ç®—è¾“å…¥æ¡†è¯„åˆ†ï¼Œé€šè¿‡è¯„åˆ†æ’åºï¼Œåˆ é™¤è¯„åˆ†é«˜äºé˜ˆå€¼(iou_threshold)çš„æ¡†ï¼Œæ”¯æŒå¤šæ‰¹å¤šç±»å¤„ç†ã€‚é€šè¿‡NonMaxSuppression(nms)æ“ä½œå¯æœ‰æ•ˆåˆ é™¤å†—ä½™çš„è¾“å…¥æ¡†ï¼Œæé«˜æ£€æµ‹ç²¾åº¦ã€‚NonMaxSuppressionï¼šæŠ‘åˆ¶ä¸æ˜¯æå¤§å€¼çš„å…ƒç´ ï¼Œæœç´¢å±€éƒ¨çš„æå¤§å€¼ï¼Œå¸¸ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æ£€æµ‹ç±»æ¨¡å‹ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - å¿…å¡«å€¼ï¼Œè¾“å…¥æ¡†çš„tensorï¼ŒåŒ…å«batchå¤§å°ï¼Œæ•°æ®ç±»å‹Float16ï¼Œè¾“å…¥ç¤ºä¾‹ï¼š[batch_size, num_anchors, q, 4]ï¼Œå…¶ä¸­q=1æˆ–q=num_classesã€‚</p>
<p>scores (Tensor) - å¿…å¡«å€¼ï¼Œè¾“å…¥tensorï¼Œæ•°æ®ç±»å‹Float16ï¼Œè¾“å…¥ç¤ºä¾‹ï¼š[batch_size, num_anchors, num_classes]ã€‚</p>
<p>score_threshold (Float32) - å¿…å¡«å€¼ï¼ŒæŒ‡å®šè¯„åˆ†è¿‡æ»¤å™¨çš„iou_thresholdï¼Œç”¨äºç­›é€‰æ¡†ï¼Œå»é™¤å¾—åˆ†è¾ƒä½çš„æ¡†ï¼Œæ•°æ®ç±»å‹Float32ã€‚</p>
<p>iou_threshold (Float32) - å¿…å¡«å€¼ï¼ŒæŒ‡å®šnmsçš„iou_thresholdï¼Œç”¨äºè®¾å®šé˜ˆå€¼ï¼Œå»é™¤é«˜äºé˜ˆå€¼çš„çš„æ¡†ï¼Œæ•°æ®ç±»å‹Float32ã€‚</p>
<p>max_size_per_class (Int) - å¿…å¡«å€¼ï¼ŒæŒ‡å®šæ¯ä¸ªç±»åˆ«çš„æœ€å¤§å¯é€‰çš„æ¡†æ•°ï¼Œæ•°æ®ç±»å‹Intã€‚</p>
<p>max_total_size (Int) - å¿…å¡«å€¼ï¼ŒæŒ‡å®šæ¯ä¸ªbatchæœ€å¤§å¯é€‰çš„æ¡†æ•°ï¼Œæ•°æ®ç±»å‹Intã€‚</p>
<p>change_coordinate_frame (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) -å¯é€‰å€¼ï¼Œ æ˜¯å¦æ­£åˆ™åŒ–è¾“å‡ºæ¡†åæ ‡çŸ©é˜µï¼Œæ•°æ®ç±»å‹Boolã€‚</p>
<p>transpose_box (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - å¯é€‰å€¼ï¼Œç¡®å®šæ˜¯å¦åœ¨æ­¤opä¹‹å‰æ’å…¥è½¬ç½®ï¼Œæ•°æ®ç±»å‹Boolã€‚Trueè¡¨ç¤ºboxesä½¿ç”¨4,Næ’å¸ƒã€‚ Falseè¡¨ç¤ºboxesä½¿ç”¨è¿‡N,4æ’å¸ƒã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>nmsed_boxes (Tensor) - shapeä¸º(batch, max_total_size, 4)çš„3Då¼ é‡ï¼ŒæŒ‡å®šæ¯æ‰¹æ¬¡è¾“å‡ºçš„nmsæ¡†ï¼Œæ•°æ®ç±»å‹Float16ã€‚</p>
<p>nmsed_scores (Tensor) - shapeä¸º(batch, max_total_size)çš„2Då¼ é‡ï¼ŒæŒ‡å®šæ¯æ‰¹æ¬¡è¾“å‡ºçš„nmsåˆ†æ•°ï¼Œæ•°æ®ç±»å‹Float16ã€‚</p>
<p>nmsed_classes (Tensor) - shapeä¸º(batch, max_total_size)çš„2Då¼ é‡ï¼ŒæŒ‡å®šæ¯æ‰¹æ¬¡è¾“å‡ºçš„nmsç±»ï¼Œæ•°æ®ç±»å‹Float16ã€‚</p>
<p>nmsed_num (Tensor) - shapeä¸º(batch)çš„1Då¼ é‡ï¼ŒæŒ‡å®šnmsed_boxesçš„æœ‰æ•ˆæ•°é‡ï¼Œæ•°æ®ç±»å‹Int32ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_boxes</span><span class="p">,</span> <span class="n">nmsed_scores</span><span class="p">,</span> <span class="n">nmsed_classes</span><span class="p">,</span> <span class="n">nmsed_num</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_batch_nms</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_boxes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_scores</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_classes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_num</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bert_apply_adam">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bert_apply_adam</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bert_apply_adam" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bert_apply_adam(lr, beta1, beta2, epsilon, grad, max_grad_norm, global_grad_norm, weight_decay, step_size=None, adam_mode=0, <a href="#id1"><span class="problematic" id="id2">*</span></a>, out=(var,m,v))</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>adamç»“æœè®¡æ•°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>å‚æ•°:</p>
<p>var (Tensor) - float16æˆ–float32ç±»å‹å¼ é‡ã€‚</p>
<p>m (Tensor) - æ•°æ®ç±»å‹å’Œshapeä¸exp_avgç›¸åŒã€‚</p>
<p>v (Tensor) - æ•°æ®ç±»å‹å’Œshapeä¸exp_avgç›¸åŒã€‚</p>
<p>lr (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>beta1 (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>beta2 (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>epsilon (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>grad (Tensor) - æ•°æ®ç±»å‹å’Œshapeä¸exp_avgç›¸åŒã€‚</p>
<p>max_grad_norm (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>global_grad_norm (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>weight_decay (Scalar) - æ•°æ®ç±»å‹ä¸exp_avgç›¸åŒã€‚</p>
<p>step_size (Tensorï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºNone) - shapeä¸º(1, )ï¼Œæ•°æ®ç±»å‹ä¸exp_avgä¸€è‡´ã€‚</p>
<p>adam_mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - é€‰æ‹©adamæ¨¡å¼ã€‚0è¡¨ç¤ºâ€œadamâ€ï¼Œ1è¡¨ç¤ºâ€œmbert_adamâ€ã€‚</p>
<p>å…³é”®å­—å‚æ•°:</p>
<p>out (Tensorï¼Œå¯é€‰) - è¾“å‡ºå¼ é‡ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">var_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">32.</span><span class="p">,</span> <span class="mf">21.</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_grad_norm</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-06</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">global_grad_norm</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_out</span><span class="p">,</span> <span class="n">m_out</span><span class="p">,</span> <span class="n">v_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_bert_apply_adam</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">max_grad_norm</span><span class="p">,</span> <span class="n">global_grad_norm</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="p">(</span><span class="n">var_in</span><span class="p">,</span> <span class="n">m_in</span><span class="p">,</span> <span class="n">v_in</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span> <span class="mf">14.7733</span><span class="p">,</span> <span class="o">-</span><span class="mf">30.1218</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.3647</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">16.6840</span><span class="p">,</span>   <span class="mf">7.1518</span><span class="p">,</span>   <span class="mf">8.4872</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bmmV2">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bmmV2</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bmmV2" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bmmV2(self, mat2, output_sizes) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å°†çŸ©é˜µâ€œaâ€ä¹˜ä»¥çŸ©é˜µâ€œbâ€ï¼Œç”Ÿæˆâ€œa*bâ€ã€‚æ”¯æŒFakeTensoræ¨¡å¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - 2Dæˆ–æ›´é«˜ç»´åº¦çŸ©é˜µå¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16ã€float32ã€int32ã€‚æ ¼å¼ï¼š[ND, NHWC, FRACTAL_NZ]ã€‚</p>
<p>mat2 (Tensor) - 2Dæˆ–æ›´é«˜ç»´åº¦çŸ©é˜µå¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16ã€float32ã€int32ã€‚æ ¼å¼ï¼š[ND, NHWC, FRACTAL_NZ]ã€‚</p>
<p>output_sizes (ListIntï¼Œé»˜è®¤å€¼ä¸º[]) - è¾“å‡ºçš„shapeï¼Œç”¨äºmatmulçš„åå‘ä¼ æ’­ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; ç¤ºä¾‹ä¸€ï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; mat1 = torch.randn(10, 3, 4).npu()
&gt;&gt;&gt; mat2 = torch.randn(10, 4, 5).npu()
&gt;&gt;&gt; res = torch_npu.npu_bmmV2(mat1, mat2, [])
&gt;&gt;&gt; res.shape
&gt;&gt;&gt; torch.Size([10, 3, 5])
&gt;&gt;&gt; ç¤ºä¾‹äºŒï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; //FakeTensoræ¨¡å¼
&gt;&gt;&gt; from torch._subclasses.fake_tensor import FakeTensorMode
&gt;&gt;&gt; with FakeTensorMode():
&gt;&gt;&gt; ...     mat1 = torch.randn(10, 3, 4).npu()
&gt;&gt;&gt; ...     mat2 = torch.randn(10, 4, 5).npu()
&gt;&gt;&gt; ...     result = torch_npu.npu_bmmV2(mat1, mat2, [])
&gt;&gt;&gt; ...
&gt;&gt;&gt; result
&gt;&gt;&gt; FakeTensor(..., device=&#39;npu:0&#39;, size=(10, 3, 5))
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bounding_box_decode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bounding_box_decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bounding_box_decode" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bounding_box_decode(rois, deltas, means0, means1, means2, means3, stds0, stds1, stds2, stds3, max_shape, wh_ratio_clip) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ ¹æ®roiså’Œdeltasç”Ÿæˆæ ‡æ³¨æ¡†ã€‚è‡ªå®šä¹‰FasterRcnnç®—å­ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>rois (Tensor) - åŒºåŸŸå€™é€‰ç½‘ç»œ(RPN)ç”Ÿæˆçš„region of interests(ROI)ã€‚shapeä¸º(N,4)æ•°æ®ç±»å‹ä¸ºfloat32æˆ–float16çš„2Då¼ é‡ã€‚â€œNâ€è¡¨ç¤ºROIçš„æ•°é‡ï¼Œ â€œ4â€è¡¨ç¤ºâ€œx0â€ã€â€œx1â€ã€â€œy0â€å’Œâ€œy1â€ã€‚</p>
<p>deltas (Tensor) - RPNç”Ÿæˆçš„ROIå’ŒçœŸå€¼æ¡†ä¹‹é—´çš„ç»å¯¹å˜åŒ–ã€‚shapeä¸º(N,4)æ•°æ®ç±»å‹ä¸ºfloat32æˆ–float16çš„2Då¼ é‡ã€‚â€œNâ€è¡¨ç¤ºé”™è¯¯æ•°ï¼Œâ€œ4â€è¡¨ç¤ºâ€œdxâ€ã€â€œdyâ€ã€â€œdwâ€å’Œâ€œdhâ€ã€‚</p>
<p>means0 (Float) - indexã€‚</p>
<p>means1 (Float) - indexã€‚</p>
<p>means2 (Float) - indexã€‚</p>
<p>means3 (Floatï¼Œé»˜è®¤å€¼ä¸º[0,0,0,0]) - indexã€‚&quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p>stds0 (Float) - indexã€‚</p>
<p>stds1 (Float) - indexã€‚</p>
<p>stds2 (Float) - indexã€‚</p>
<p>stds3 (Float, é»˜è®¤å€¼ï¼š[1.0,1.0,1.0,1.0]) - indexã€‚&quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p>max_shape (ListInt of length 2) - shape[h, w]ï¼ŒæŒ‡å®šä¼ è¾“åˆ°ç½‘ç»œçš„å›¾åƒå¤§å°ã€‚ç”¨äºç¡®ä¿è½¬æ¢åçš„bbox shapeä¸è¶…è¿‡â€œmax_shapeâ€ã€‚</p>
<p>wh_ratio_clip (Float) -â€œdwâ€å’Œâ€œdhâ€çš„å€¼åœ¨(-wh_ratio_clip, wh_ratio_clip)èŒƒå›´å†…ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rois</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deltas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_bounding_box_decode</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.5000</span><span class="p">,</span> <span class="mf">6.5000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bounding_box_encode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bounding_box_encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bounding_box_encode" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bounding_box_encode(anchor_box, ground_truth_box, means0, means1, means2, means3, stds0, stds1, stds2, stds3) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—æ ‡æ³¨æ¡†å’Œground truthçœŸå€¼æ¡†ä¹‹é—´çš„åæ ‡å˜åŒ–ã€‚è‡ªå®šä¹‰FasterRcnnç®—å­ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>anchor_box (Tensor) - è¾“å…¥å¼ é‡ã€‚é”šç‚¹æ¡†ã€‚shapeä¸º(N,4)æ•°æ®ç±»å‹ä¸ºfloat32çš„2Då¼ é‡ã€‚â€œNâ€è¡¨ç¤ºæ ‡æ³¨æ¡†çš„æ•°é‡ï¼Œâ€œ4â€è¡¨ç¤ºâ€œx0â€ã€â€œx1â€ã€â€œy0â€å’Œâ€œy1â€ã€‚</p>
<p>ground_truth_box (Tensor) - è¾“å…¥å¼ é‡ã€‚çœŸå€¼æ¡†ã€‚shapeä¸º(N,4)æ•°æ®ç±»å‹ä¸ºfloat32çš„2Då¼ é‡ã€‚â€œNâ€è¡¨ç¤ºæ ‡æ³¨æ¡†çš„æ•°é‡ï¼Œâ€œ4â€è¡¨ç¤ºâ€œx0â€ã€â€œx1â€ã€â€œy0â€å’Œâ€œy1â€ã€‚</p>
<p>means0 (Float) - indexã€‚</p>
<p>means1 (Float) - indexã€‚</p>
<p>means2 (Float) - indexã€‚</p>
<p>means3 (Float, é»˜è®¤å€¼ä¸º[0,0,0,0]) - indexã€‚ &quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p>stds0 (Float) - indexã€‚</p>
<p>stds1 (Float) - indexã€‚</p>
<p>stds2 (Float) - indexã€‚</p>
<p>stds3 (Float, é»˜è®¤å€¼ï¼š[1.0,1.0,1.0,1.0]) -indexã€‚ &quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_box</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ground_truth_box</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_bounding_box_encode</span><span class="p">(</span><span class="n">anchor_box</span><span class="p">,</span> <span class="n">ground_truth_box</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputtensor</span><span class="p">([[</span><span class="mf">13.3281</span><span class="p">,</span> <span class="mf">13.3281</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">13.3281</span><span class="p">,</span>  <span class="mf">6.6641</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4922</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_broadcast">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_broadcast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_broadcast" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_broadcast(self, size) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è¿”å›selfå¼ é‡çš„æ–°è§†å›¾ï¼Œå…¶å•ç»´åº¦æ‰©å±•ï¼Œç»“æœè¿ç»­ã€‚</p>
<p>å¼ é‡ä¹Ÿå¯ä»¥æ‰©å±•æ›´å¤šç»´åº¦ï¼Œæ–°çš„ç»´åº¦æ·»åŠ åœ¨æœ€å‰é¢ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>size (ListInt) - å¯¹åº”æ‰©å±•å°ºå¯¸ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">npu_broadcast</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ciou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ciou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ciou" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åº”ç”¨åŸºäºNPUçš„CIoUæ“ä½œã€‚åœ¨DIoUçš„åŸºç¡€ä¸Šå¢åŠ äº†penalty itemï¼Œå¹¶propose CIoUã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>boxes1 (Tensor)ï¼šæ ¼å¼ä¸ºxywhã€shapeä¸º(4, n)çš„é¢„æµ‹æ£€æµ‹æ¡†ã€‚</p>
<p>boxes2 (Tensor)ï¼šç›¸åº”çš„gtæ£€æµ‹æ¡†ï¼Œshapeä¸º(4, n)ã€‚</p>
<p>trans (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse)ï¼šæ˜¯å¦æœ‰åç§»ã€‚</p>
<p>is_cross (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue)ï¼šbox1å’Œbox2ä¹‹é—´æ˜¯å¦æœ‰äº¤å‰æ“ä½œã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0)ï¼šé€‰æ‹©CIoUçš„è®¡ç®—æ–¹å¼ã€‚0è¡¨ç¤ºIoUï¼Œ1è¡¨ç¤ºIoFã€‚</p>
<p>atan_sub_flag (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse)ï¼šæ˜¯å¦å°†æ­£å‘çš„ç¬¬äºŒä¸ªå€¼ä¼ é€’ç»™åå‘ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>torch.Tensorï¼šmaskæ“ä½œçš„ç»“æœã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒCIoUå‘ååªæ”¯æŒå½“å‰ç‰ˆæœ¬ä¸­çš„trans==Trueã€is_cross==Falseã€mode==0('iou')ã€‚å¦‚æœéœ€è¦åå‘ä¼ æ’­ï¼Œç¡®ä¿å‚æ•°æ­£ç¡®ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">npu_ciou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">ciou</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_clear_float_status">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_clear_float_status</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_clear_float_status" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_clear_float_status(self) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨æ¯ä¸ªæ ¸ä¸­è®¾ç½®åœ°å€0x40000çš„å€¼ä¸º0ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ•°æ®ç±»å‹ä¸ºfloat32çš„å¼ é‡ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_clear_float_status</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_confusion_transpose">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_confusion_transpose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_confusion_transpose" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_confusion_transpose(self, perm, shape, transpose_first) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ··æ·†reshapeå’Œtransposeè¿ç®—ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ã€float32ã€int8ã€int16ã€int32ã€int64ã€uint8ã€uint16ã€uint32ã€uint64ã€‚</p>
<p>perm (ListInt) - selfå¼ é‡çš„ç»´åº¦æ’åˆ—ã€‚</p>
<p>shape (ListInt) - è¾“å…¥shapeã€‚</p>
<p>transpose_first (Bool) - å¦‚æœå€¼ä¸ºTrueï¼Œé¦–å…ˆæ‰§è¡Œtransposeï¼Œå¦åˆ™å…ˆæ‰§è¡Œreshapeã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_confusion_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">18</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y2</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_confusion_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y2</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_conv2d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_conv2d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_conv2d" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_conv2d(input, weight, bias, stride, padding, dilation, groups) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨ç”±å¤šä¸ªè¾“å…¥å¹³é¢ç»„æˆçš„è¾“å…¥å›¾åƒä¸Šåº”ç”¨ä¸€ä¸ª2Då·ç§¯ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - shapeçš„è¾“å…¥å¼ é‡ï¼Œå€¼ä¸º (minibatch, in_channels, iH, iW)ã€‚</p>
<p>weight (Tensor) - shapeè¿‡æ»¤å™¨ï¼Œå€¼ä¸º (out_channels, in_channels/groups, kH, kW)ã€‚</p>
<p>bias (Tensor, å¯é€‰) - shapeåå·® (out_channels)ã€‚</p>
<p>stride (ListInt) - å·ç§¯æ ¸æ­¥é•¿ã€‚</p>
<p>padding (ListInt) - è¾“å…¥ä¸¤ä¾§çš„éšå¼å¡«å……ã€‚</p>
<p>dilation (ListInt) - å†…æ ¸å…ƒç´ é—´è·ã€‚</p>
<p>groups (Int) - å¯¹è¾“å…¥è¿›è¡Œåˆ†ç»„ã€‚In_channelså¯è¢«ç»„æ•°æ•´é™¤ã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_conv3d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_conv3d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_conv3d" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_conv3d(input, weight, bias, stride, padding, dilation, groups) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨ç”±å¤šä¸ªè¾“å…¥å¹³é¢ç»„æˆçš„è¾“å…¥å›¾åƒä¸Šåº”ç”¨ä¸€ä¸ª3Då·ç§¯ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - shapeçš„è¾“å…¥å¼ é‡ï¼Œå€¼ä¸º (minibatch, in_channels, iT, iH, iW)ã€‚</p>
<p>weight (Tensor) - shapeè¿‡æ»¤å™¨ï¼Œå€¼ä¸º (out_channels, in_channels/groups, kT, kH, kW)ã€‚</p>
<p>bias (Tensor, å¯é€‰) - shapeåå·® (out_channels)ã€‚</p>
<p>stride (ListInt) - å·ç§¯æ ¸æ­¥é•¿ã€‚</p>
<p>padding (ListInt) - è¾“å…¥ä¸¤ä¾§çš„éšå¼å¡«å……ã€‚</p>
<p>dilation (ListInt) - å†…æ ¸å…ƒç´ é—´è·ã€‚</p>
<p>groups (Int) - å¯¹è¾“å…¥è¿›è¡Œåˆ†ç»„ã€‚In_channelså¯è¢«ç»„æ•°æ•´é™¤ã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_conv_transpose2d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_conv_transpose2d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_conv_transpose2d" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_conv_transpose2d(input, weight, bias, padding, output_padding, stride, dilation, groups) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨ç”±å¤šä¸ªè¾“å…¥å¹³é¢ç»„æˆçš„è¾“å…¥å›¾åƒä¸Šåº”ç”¨ä¸€ä¸ª2Dè½¬ç½®å·ç§¯ç®—å­ï¼Œæœ‰æ—¶è¿™ä¸ªè¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸ºâ€œåå·ç§¯â€ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - shapeçš„è¾“å…¥å¼ é‡ï¼Œå€¼ä¸º (minibatch, in_channels, iH, iW)ã€‚</p>
<p>weight (Tensor) - shapeè¿‡æ»¤å™¨ï¼Œå€¼ä¸º (in_channels, out_channels/groups, kH, kW)ã€‚</p>
<p>bias (Tensor, å¯é€‰) - shapeåå·® (out_channels)ã€‚</p>
<p>padding (ListInt) - (dilation * (kernel_size - 1) - padding) ç”¨é›¶æ¥å¡«å……è¾“å…¥æ¯ä¸ªç»´åº¦çš„ä¸¤ä¾§ã€‚</p>
<p>output_padding (ListInt) - æ·»åŠ åˆ°è¾“å‡ºshapeæ¯ä¸ªç»´åº¦ä¸€ä¾§çš„é™„åŠ å°ºå¯¸ã€‚</p>
<p>stride (ListInt) - å·ç§¯æ ¸æ­¥é•¿ã€‚</p>
<p>dilation (ListInt) - å†…æ ¸å…ƒç´ é—´è·ã€‚</p>
<p>groups (Int) - å¯¹è¾“å…¥è¿›è¡Œåˆ†ç»„ã€‚In_channelså¯è¢«ç»„æ•°æ•´é™¤ã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_convolution">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_convolution</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_convolution" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_convolution(input, weight, bias, stride, padding, dilation, groups) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨ç”±å¤šä¸ªè¾“å…¥å¹³é¢ç»„æˆçš„è¾“å…¥å›¾åƒä¸Šåº”ç”¨ä¸€ä¸ª2Dæˆ–3Då·ç§¯ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - shapeçš„è¾“å…¥å¼ é‡ï¼Œå€¼ä¸º (minibatch, in_channels, iH, iW) æˆ– (minibatch, in_channels, iT, iH, iW)ã€‚</p>
<p>weight (Tensor) - shapeè¿‡æ»¤å™¨ï¼Œå€¼ä¸º (out_channels, in_channels/groups, kH, kW) æˆ– (out_channels, in_channels/groups, kT, kH, kW)ã€‚</p>
<p>bias (Tensor, å¯é€‰) - shapeåå·® (out_channels)ã€‚</p>
<p>stride (ListInt) - å·ç§¯æ ¸æ­¥é•¿ã€‚</p>
<p>padding (ListInt) - è¾“å…¥ä¸¤ä¾§çš„éšå¼å¡«å……ã€‚</p>
<p>dilation (ListInt) - å†…æ ¸å…ƒç´ é—´è·ã€‚</p>
<p>groups (Int) - å¯¹è¾“å…¥è¿›è¡Œåˆ†ç»„ã€‚In_channelså¯è¢«ç»„æ•°æ•´é™¤ã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_convolution_transpose">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_convolution_transpose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_convolution_transpose" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_convolution_transpose(input, weight, bias, padding, output_padding, stride, dilation, groups) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åœ¨ç”±å¤šä¸ªè¾“å…¥å¹³é¢ç»„æˆçš„è¾“å…¥å›¾åƒä¸Šåº”ç”¨ä¸€ä¸ª2Dæˆ–3Dè½¬ç½®å·ç§¯ç®—å­ï¼Œæœ‰æ—¶è¿™ä¸ªè¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸ºâ€œåå·ç§¯â€ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - shapeçš„è¾“å…¥å¼ é‡ï¼Œå€¼ä¸º (minibatch, in_channels, iH, iW) æˆ– (minibatch, in_channels, iT, iH, iW)ã€‚</p>
<p>weight (Tensor) - shapeè¿‡æ»¤å™¨ï¼Œå€¼ä¸º (in_channels, out_channels/groups, kH, kW) æˆ– (in_channels, out_channels/groups, kT, kH, kW)ã€‚</p>
<p>bias (Tensor, å¯é€‰) - shapeåå·® (out_channels)ã€‚</p>
<p>padding (ListInt) - (dilation * (kernel_size - 1) - padding) ç”¨é›¶æ¥å¡«å……è¾“å…¥æ¯ä¸ªç»´åº¦çš„ä¸¤ä¾§ã€‚</p>
<p>output_padding (ListInt) - æ·»åŠ åˆ°è¾“å‡ºshapeæ¯ä¸ªç»´åº¦ä¸€ä¾§çš„é™„åŠ å°ºå¯¸ã€‚</p>
<p>stride (ListInt) - å·ç§¯æ ¸æ­¥é•¿ã€‚</p>
<p>dilation (ListInt) - å†…æ ¸å…ƒç´ é—´è·ã€‚</p>
<p>groups (Int) - å¯¹è¾“å…¥è¿›è¡Œåˆ†ç»„ã€‚In_channelså¯è¢«ç»„æ•°æ•´é™¤ã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_deformable_conv2d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_deformable_conv2d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_deformable_conv2d" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_deformable_conv2d(self, weight, offset, bias, kernel_size, stride, padding, dilation=[1,1,1,1], groups=1, deformable_groups=1, modulated=True) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä½¿ç”¨é¢„æœŸè¾“å…¥è®¡ç®—å˜å½¢å·ç§¯è¾“å‡º(deformed convolution output)ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å›¾åƒçš„4Då¼ é‡ã€‚æ ¼å¼ä¸ºâ€œNHWCâ€ï¼Œæ•°æ®æŒ‰ä»¥ä¸‹é¡ºåºå­˜å‚¨ï¼š[batch, in_height, in_width, in_channels]ã€‚</p>
<p>weight (Tensor) - å¯å­¦ä¹ è¿‡æ»¤å™¨çš„4Då¼ é‡ã€‚æ•°æ®ç±»å‹éœ€ä¸selfç›¸åŒã€‚æ ¼å¼ä¸ºâ€œHWCNâ€ï¼Œæ•°æ®æŒ‰ä»¥ä¸‹é¡ºåºå­˜å‚¨ï¼š[filter_height, filter_width, in_channels / groups, out_channels]ã€‚</p>
<p>offset (Tensor) - x-yåæ ‡åç§»å’Œæ©ç çš„4Då¼ é‡ã€‚æ ¼å¼ä¸ºâ€œNHWCâ€ï¼Œæ•°æ®æŒ‰ä»¥ä¸‹é¡ºåºå­˜å‚¨ï¼š[batch, out_height, out_width, deformable_groups * filter_height * filter_width * 3]ã€‚</p>
<p>bias (Tensorï¼Œå¯é€‰) - è¿‡æ»¤å™¨è¾“å‡ºé™„åŠ åç½®(additive bias)çš„1Då¼ é‡ï¼Œæ•°æ®æŒ‰[out_channels]çš„é¡ºåºå­˜å‚¨ã€‚</p>
<p>kernel_size (ListInt of length 2) - å†…æ ¸å¤§å°ï¼Œ2ä¸ªæ•´æ•°çš„å…ƒç»„/åˆ—è¡¨ã€‚</p>
<p>stride (ListInt) - 4ä¸ªæ•´æ•°çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥ç»´åº¦çš„æ»‘åŠ¨çª—å£æ­¥é•¿ã€‚ç»´åº¦é¡ºåºæ ¹æ®selfçš„æ•°æ®æ ¼å¼è§£é‡Šã€‚Nç»´å’ŒCç»´å¿…é¡»è®¾ç½®ä¸º1ã€‚</p>
<p>padding (ListInt) - 4ä¸ªæ•´æ•°çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºè¦æ·»åŠ åˆ°è¾“å…¥æ¯ä¾§(é¡¶éƒ¨ã€åº•éƒ¨ã€å·¦ä¾§ã€å³ä¾§)çš„åƒç´ æ•°ã€‚</p>
<p>dilations (ListIntï¼Œé»˜è®¤å€¼ä¸º[1, 1, 1, 1]) - 4ä¸ªæ•´æ•°çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºè¾“å…¥æ¯ä¸ªç»´åº¦çš„è†¨èƒ€ç³»æ•°(dilation factor)ã€‚ç»´åº¦é¡ºåºæ ¹æ®selfçš„æ•°æ®æ ¼å¼è§£é‡Šã€‚Nç»´å’ŒCç»´å¿…é¡»è®¾ç½®ä¸º1ã€‚</p>
<p>groups (Intï¼Œé»˜è®¤å€¼ä¸º1) - int32ç±»å‹å•æ•´æ•°ï¼Œè¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ã€‚In_channelså’Œout_channelséœ€éƒ½å¯è¢«â€œgroupsâ€æ•°æ•´é™¤ã€‚</p>
<p>deformable_groups (Intï¼Œé»˜è®¤å€¼ä¸º1) - int32ç±»å‹å•æ•´æ•°ï¼Œè¡¨ç¤ºå¯å˜å½¢ç»„åˆ†åŒºçš„æ•°é‡ã€‚In_channelséœ€å¯è¢«â€œdeformable_groupsâ€æ•°æ•´é™¤ã€‚</p>
<p>modulated (Boolï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºTrue) - æŒ‡å®šDeformableConv2Dç‰ˆæœ¬ã€‚Trueè¡¨ç¤ºv2ç‰ˆæœ¬, Falseè¡¨ç¤ºv1ç‰ˆæœ¬ï¼Œç›®å‰ä»…æ”¯æŒv2ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_deformable_conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">stride</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_diou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_diou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_diou" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åº”ç”¨åŸºäºNPUçš„DIoUæ“ä½œã€‚è€ƒè™‘åˆ°ç›®æ ‡ä¹‹é—´è·ç¦»ï¼Œä»¥åŠè·ç¦»å’ŒèŒƒå›´çš„é‡å ç‡ï¼Œä¸åŒç›®æ ‡æˆ–è¾¹ç•Œéœ€è¶‹äºç¨³å®šã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>boxes1 (Tensor) - æ ¼å¼ä¸ºxywhã€shapeä¸º(4, n)çš„é¢„æµ‹æ£€æµ‹æ¡†ã€‚</p>
<p>boxes2 (Tensor) - ç›¸åº”çš„gtæ£€æµ‹æ¡†ï¼Œshapeä¸º(4, n)ã€‚</p>
<p>trans (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - æ˜¯å¦æœ‰åç§»ã€‚</p>
<p>is_cross (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - box1å’Œbox2ä¹‹é—´æ˜¯å¦æœ‰äº¤å‰æ“ä½œã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - é€‰æ‹©DIoUçš„è®¡ç®—æ–¹å¼ã€‚0è¡¨ç¤ºIoUï¼Œ1è¡¨ç¤ºIoFã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>torch.Tensor (Tensor) - maskæ“ä½œçš„ç»“æœã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒDIoUå‘ååªæ”¯æŒå½“å‰ç‰ˆæœ¬ä¸­çš„trans==Trueã€is_cross==Falseã€mode==0('iou')ã€‚å¦‚æœéœ€è¦åå‘ä¼ æ’­ï¼Œç¡®ä¿å‚æ•°æ­£ç¡®ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">npu_diou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">diou</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_dropout_with_add_softmax">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_dropout_with_add_softmax</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_dropout_with_add_softmax" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -&gt; (Tensor, Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å®ç°axpy_v2ã€softmax_v2ã€drop_out_domask_v3åŠŸèƒ½ã€‚å³ï¼š</p>
<p>y=x1+ self <a href="#id3"><span class="problematic" id="id4">*</span></a>alpha</p>
<p>Softmax(xi)= exp(xi)/âˆ‘jexp(xj)</p>
<p>output = æ ¹æ®maskèˆå¼ƒxä¸­çš„å…ƒç´ ï¼Œç•™ä¸‹æ¥çš„å…ƒç´ ä¹˜(1/prob)</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>Tensor selfï¼š4ç»´å¼ é‡ï¼Œshapeä¸º(N, C, H, W)ã€‚</p>
<p>Tensor x1ï¼š4ç»´å¼ é‡ï¼Œshapeä¸º(N, C, H, W)ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>selfå’Œx1çš„shapeç›¸åŒï¼›</p>
<p>Hå’ŒWæ˜¯[128, 256, 384, 512]å…¶ä¸­ä¹‹ä¸€ï¼›</p>
<p>(N * C)%32ç»“æœä¸º0ï¼›</p>
<p>dimä¸º-1ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">7.2556e-02</span><span class="p">,</span> <span class="mf">3.0909e-01</span><span class="p">,</span> <span class="mf">7.9734e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.1179e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.2624e-03</span><span class="p">,</span> <span class="mf">8.5186e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">8.9196e-02</span><span class="p">,</span> <span class="mf">3.3319e-01</span><span class="p">,</span> <span class="mf">4.0780e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.9144e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2701e-01</span><span class="p">,</span> <span class="mf">6.4018e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.7275e-01</span><span class="p">,</span> <span class="mf">7.4895e-01</span><span class="p">,</span> <span class="mf">4.6215e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.3753e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.6048e-02</span><span class="p">,</span> <span class="mf">8.1877e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.9366e-01</span><span class="p">,</span> <span class="mf">5.1516e-01</span><span class="p">,</span> <span class="mf">5.6594e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.6457e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.0640e-01</span><span class="p">,</span> <span class="mf">3.4322e-03</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.5743e-02</span><span class="p">,</span> <span class="mf">1.2893e-01</span><span class="p">,</span> <span class="mf">5.8990e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.1721e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.7816e-02</span><span class="p">,</span> <span class="mf">6.8886e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.2980e-01</span><span class="p">,</span> <span class="mf">5.5447e-01</span><span class="p">,</span> <span class="mf">3.1894e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2638e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.9324e-01</span><span class="p">,</span> <span class="mf">4.6225e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">6.2426e-01</span><span class="p">,</span> <span class="mf">4.5948e-01</span><span class="p">,</span> <span class="mf">1.0837e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.9386e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.6932e-01</span><span class="p">,</span> <span class="mf">1.2406e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.1823e-01</span><span class="p">,</span> <span class="mf">6.2311e-01</span><span class="p">,</span> <span class="mf">5.1474e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.1042e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.5943e-01</span><span class="p">,</span> <span class="mf">3.1797e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.2891e-01</span><span class="p">,</span> <span class="mf">2.0183e-01</span><span class="p">,</span> <span class="mf">2.1452e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.1638e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.4109e-01</span><span class="p">,</span> <span class="mf">9.4484e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.7783e-02</span><span class="p">,</span> <span class="mf">1.3218e-01</span><span class="p">,</span> <span class="mf">3.1192e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.4931e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.8809e-01</span><span class="p">,</span> <span class="mf">9.6085e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.3197e-01</span><span class="p">,</span> <span class="mf">9.1186e-02</span><span class="p">,</span> <span class="mf">2.4839e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.1156e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.4952e-01</span><span class="p">,</span> <span class="mf">8.5996e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.7941e-01</span><span class="p">,</span> <span class="mf">5.1532e-01</span><span class="p">,</span> <span class="mf">7.8133e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.5526e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.3576e-01</span><span class="p">,</span> <span class="mf">6.0538e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">2.6743e-01</span><span class="p">,</span> <span class="mf">7.4942e-01</span><span class="p">,</span> <span class="mf">1.9146e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.9179e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.3319e-01</span><span class="p">,</span> <span class="mf">9.9269e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.5163e-01</span><span class="p">,</span> <span class="mf">3.7388e-01</span><span class="p">,</span> <span class="mf">8.0604e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.1193e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.7922e-01</span><span class="p">,</span> <span class="mf">8.6578e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">8.2558e-01</span><span class="p">,</span> <span class="mf">9.5139e-01</span><span class="p">,</span> <span class="mf">2.1313e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.1722e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.8402e-01</span><span class="p">,</span> <span class="mf">8.8888e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.8222e-01</span><span class="p">,</span> <span class="mf">2.7645e-01</span><span class="p">,</span> <span class="mf">6.7305e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.8003e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0917e-01</span><span class="p">,</span> <span class="mf">7.6655e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.1234e-01</span><span class="p">,</span> <span class="mf">7.8519e-01</span><span class="p">,</span> <span class="mf">8.8509e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.2574e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.6134e-01</span><span class="p">,</span> <span class="mf">2.2267e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.9233e-01</span><span class="p">,</span> <span class="mf">8.8407e-01</span><span class="p">,</span> <span class="mf">7.4390e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.2253e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.5150e-02</span><span class="p">,</span> <span class="mf">4.4108e-02</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">4.3370e-01</span><span class="p">,</span> <span class="mf">2.1176e-01</span><span class="p">,</span> <span class="mf">4.7512e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.7611e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.2619e-01</span><span class="p">,</span> <span class="mf">1.1523e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.1469e-01</span><span class="p">,</span> <span class="mf">7.4528e-01</span><span class="p">,</span> <span class="mf">7.9559e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.7112e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.8391e-01</span><span class="p">,</span> <span class="mf">8.9883e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">8.6677e-02</span><span class="p">,</span> <span class="mf">3.5051e-02</span><span class="p">,</span> <span class="mf">1.6875e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.9833e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.7967e-01</span><span class="p">,</span> <span class="mf">4.7062e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.1648e-01</span><span class="p">,</span> <span class="mf">1.8378e-01</span><span class="p">,</span> <span class="mf">5.3054e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.4282e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.1972e-01</span><span class="p">,</span> <span class="mf">7.0031e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.9876e-01</span><span class="p">,</span> <span class="mf">6.7868e-01</span><span class="p">,</span> <span class="mf">6.4128e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.9516e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.2571e-01</span><span class="p">,</span> <span class="mf">5.8792e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.6723e-01</span><span class="p">,</span> <span class="mf">6.9527e-01</span><span class="p">,</span> <span class="mf">9.3573e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.3490e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.6129e-01</span><span class="p">,</span> <span class="mf">2.4517e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">5.0158e-01</span><span class="p">,</span> <span class="mf">8.2565e-01</span><span class="p">,</span> <span class="mf">7.5532e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.9342e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3244e-01</span><span class="p">,</span> <span class="mf">5.3913e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.3347e-01</span><span class="p">,</span> <span class="mf">9.7822e-02</span><span class="p">,</span> <span class="mf">1.5009e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.5090e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.1813e-01</span><span class="p">,</span> <span class="mf">7.9857e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.2416e-02</span><span class="p">,</span> <span class="mf">5.9086e-01</span><span class="p">,</span> <span class="mf">1.2243e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.8511e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.4803e-01</span><span class="p">,</span> <span class="mf">5.3717e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.4899e-01</span><span class="p">,</span> <span class="mf">1.5467e-02</span><span class="p">,</span> <span class="mf">4.9711e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.2938e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6099e-01</span><span class="p">,</span> <span class="mf">3.1928e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.9111e-01</span><span class="p">,</span> <span class="mf">1.2422e-01</span><span class="p">,</span> <span class="mf">6.1795e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.4212e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1346e-01</span><span class="p">,</span> <span class="mf">1.0957e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.6311e-02</span><span class="p">,</span> <span class="mf">8.9652e-01</span><span class="p">,</span> <span class="mf">7.7428e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2212e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.9290e-01</span><span class="p">,</span> <span class="mf">4.5609e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">2.2052e-01</span><span class="p">,</span> <span class="mf">4.4260e-01</span><span class="p">,</span> <span class="mf">8.8627e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2381e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.7046e-01</span><span class="p">,</span> <span class="mf">9.2057e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.5775e-01</span><span class="p">,</span> <span class="mf">8.8951e-01</span><span class="p">,</span> <span class="mf">7.9238e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.9209e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.6636e-01</span><span class="p">,</span> <span class="mf">8.1876e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.4709e-01</span><span class="p">,</span> <span class="mf">7.8678e-01</span><span class="p">,</span> <span class="mf">1.4396e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.9073e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.9021e-01</span><span class="p">,</span> <span class="mf">8.5285e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.4238e-01</span><span class="p">,</span> <span class="mf">9.8432e-01</span><span class="p">,</span> <span class="mf">2.7802e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.1720e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6290e-01</span><span class="p">,</span> <span class="mf">8.2036e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.0184e-01</span><span class="p">,</span> <span class="mf">1.0635e-01</span><span class="p">,</span> <span class="mf">1.9612e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.7101e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.6679e-01</span><span class="p">,</span> <span class="mf">7.0811e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.8240e-01</span><span class="p">,</span> <span class="mf">3.1642e-01</span><span class="p">,</span> <span class="mf">9.6549e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.1130e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6725e-01</span><span class="p">,</span> <span class="mf">3.5238e-01</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">2.4353e-01</span><span class="p">,</span> <span class="mf">8.5665e-01</span><span class="p">,</span> <span class="mf">5.3571e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.9101e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0872e-01</span><span class="p">,</span> <span class="mf">6.3873e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.4489e-01</span><span class="p">,</span> <span class="mf">8.7982e-01</span><span class="p">,</span> <span class="mf">3.3114e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.5155e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.4987e-01</span><span class="p">,</span> <span class="mf">8.7096e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.5837e-02</span><span class="p">,</span> <span class="mf">2.2677e-02</span><span class="p">,</span> <span class="mf">7.2063e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.3542e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.3041e-01</span><span class="p">,</span> <span class="mf">8.9596e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.1450e-01</span><span class="p">,</span> <span class="mf">7.9412e-01</span><span class="p">,</span> <span class="mf">8.9288e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.3639e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6086e-01</span><span class="p">,</span> <span class="mf">4.8770e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.7557e-01</span><span class="p">,</span> <span class="mf">1.4793e-01</span><span class="p">,</span> <span class="mf">4.9800e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.9479e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6052e-01</span><span class="p">,</span> <span class="mf">9.8271e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.4438e-01</span><span class="p">,</span> <span class="mf">7.5646e-01</span><span class="p">,</span> <span class="mf">2.7942e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.0381e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.3703e-01</span><span class="p">,</span> <span class="mf">1.4037e-02</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">4.0232e-01</span><span class="p">,</span> <span class="mf">9.4407e-01</span><span class="p">,</span> <span class="mf">6.4969e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.4524e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2647e-01</span><span class="p">,</span> <span class="mf">5.4792e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.1801e-01</span><span class="p">,</span> <span class="mf">1.8281e-01</span><span class="p">,</span> <span class="mf">6.1723e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.9393e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.5877e-01</span><span class="p">,</span> <span class="mf">8.9990e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.6244e-01</span><span class="p">,</span> <span class="mf">6.9614e-01</span><span class="p">,</span> <span class="mf">3.6008e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.0258e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.1919e-01</span><span class="p">,</span> <span class="mf">4.6943e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.4710e-01</span><span class="p">,</span> <span class="mf">5.8911e-01</span><span class="p">,</span> <span class="mf">1.5292e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.6590e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0754e-01</span><span class="p">,</span> <span class="mf">3.6944e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.0501e-01</span><span class="p">,</span> <span class="mf">2.7943e-01</span><span class="p">,</span> <span class="mf">3.7068e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.5053e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.3413e-01</span><span class="p">,</span> <span class="mf">7.9626e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.5200e-01</span><span class="p">,</span> <span class="mf">7.8327e-01</span><span class="p">,</span> <span class="mf">3.4033e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.0892e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0480e-01</span><span class="p">,</span> <span class="mf">3.8717e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">7.5938e-01</span><span class="p">,</span> <span class="mf">2.9089e-01</span><span class="p">,</span> <span class="mf">5.9916e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.2526e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.9670e-01</span><span class="p">,</span> <span class="mf">3.3548e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.0733e-01</span><span class="p">,</span> <span class="mf">8.1400e-01</span><span class="p">,</span> <span class="mf">4.9259e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.6607e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.5331e-01</span><span class="p">,</span> <span class="mf">7.3150e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.2770e-01</span><span class="p">,</span> <span class="mf">7.8141e-01</span><span class="p">,</span> <span class="mf">4.1904e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.8917e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.1405e-01</span><span class="p">,</span> <span class="mf">9.9596e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.8669e-01</span><span class="p">,</span> <span class="mf">9.9948e-01</span><span class="p">,</span> <span class="mf">1.2023e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.0420e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.8522e-01</span><span class="p">,</span> <span class="mf">6.6192e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.9718e-01</span><span class="p">,</span> <span class="mf">7.5792e-01</span><span class="p">,</span> <span class="mf">6.6748e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.7302e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3443e-01</span><span class="p">,</span> <span class="mf">3.6536e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.7033e-01</span><span class="p">,</span> <span class="mf">6.0550e-01</span><span class="p">,</span> <span class="mf">8.2024e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.9711e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.9410e-01</span><span class="p">,</span> <span class="mf">6.6304e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.0284e-01</span><span class="p">,</span> <span class="mf">6.5712e-01</span><span class="p">,</span> <span class="mf">6.0831e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.2622e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.0355e-01</span><span class="p">,</span> <span class="mf">9.4250e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.9053e-01</span><span class="p">,</span> <span class="mf">2.0148e-01</span><span class="p">,</span> <span class="mf">2.4974e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2521e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.9919e-01</span><span class="p">,</span> <span class="mf">4.4700e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.6515e-01</span><span class="p">,</span> <span class="mf">8.7755e-01</span><span class="p">,</span> <span class="mf">1.3500e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.2136e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.0848e-01</span><span class="p">,</span> <span class="mf">5.6432e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.3618e-01</span><span class="p">,</span> <span class="mf">1.8585e-01</span><span class="p">,</span> <span class="mf">5.3475e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.9333e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.1018e-01</span><span class="p">,</span> <span class="mf">9.5052e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.1400e-01</span><span class="p">,</span> <span class="mf">1.7407e-01</span><span class="p">,</span> <span class="mf">5.8925e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.5722e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.9850e-01</span><span class="p">,</span> <span class="mf">3.9298e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.3625e-01</span><span class="p">,</span> <span class="mf">1.7168e-01</span><span class="p">,</span> <span class="mf">2.9183e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.9674e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.1718e-01</span><span class="p">,</span> <span class="mf">5.2626e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.8651e-01</span><span class="p">,</span> <span class="mf">2.5385e-01</span><span class="p">,</span> <span class="mf">2.0384e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.4462e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.4150e-01</span><span class="p">,</span> <span class="mf">4.7431e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.4992e-01</span><span class="p">,</span> <span class="mf">1.1788e-01</span><span class="p">,</span> <span class="mf">1.9730e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.3722e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8943e-01</span><span class="p">,</span> <span class="mf">9.9097e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.4493e-02</span><span class="p">,</span> <span class="mf">6.4856e-01</span><span class="p">,</span> <span class="mf">8.3344e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.6623e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.5456e-01</span><span class="p">,</span> <span class="mf">7.8423e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.1458e-01</span><span class="p">,</span> <span class="mf">4.4260e-01</span><span class="p">,</span> <span class="mf">7.4133e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.5126e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.7251e-01</span><span class="p">,</span> <span class="mf">6.9784e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.2419e-01</span><span class="p">,</span> <span class="mf">3.4159e-01</span><span class="p">,</span> <span class="mf">2.3232e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.2850e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2644e-02</span><span class="p">,</span> <span class="mf">4.8390e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.0171e-01</span><span class="p">,</span> <span class="mf">8.7662e-01</span><span class="p">,</span> <span class="mf">2.0457e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.6868e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.6592e-01</span><span class="p">,</span> <span class="mf">3.1254e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.8866e-01</span><span class="p">,</span> <span class="mf">1.5755e-01</span><span class="p">,</span> <span class="mf">3.1025e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.5044e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8293e-01</span><span class="p">,</span> <span class="mf">9.8030e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.7703e-01</span><span class="p">,</span> <span class="mf">5.3198e-01</span><span class="p">,</span> <span class="mf">1.8633e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.7398e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.3618e-01</span><span class="p">,</span> <span class="mf">8.7283e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.7119e-01</span><span class="p">,</span> <span class="mf">4.3620e-01</span><span class="p">,</span> <span class="mf">8.2536e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.5390e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6144e-01</span><span class="p">,</span> <span class="mf">4.4044e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.3243e-01</span><span class="p">,</span> <span class="mf">6.2002e-02</span><span class="p">,</span> <span class="mf">7.5278e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.5907e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.2472e-01</span><span class="p">,</span> <span class="mf">1.7624e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.7985e-01</span><span class="p">,</span> <span class="mf">7.9769e-01</span><span class="p">,</span> <span class="mf">8.1433e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.3780e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2877e-02</span><span class="p">,</span> <span class="mf">4.8816e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.5100e-01</span><span class="p">,</span> <span class="mf">9.9698e-02</span><span class="p">,</span> <span class="mf">7.0776e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.8046e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2372e-01</span><span class="p">,</span> <span class="mf">8.6304e-01</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_dropout_with_add_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0639</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0632</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0794</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1571</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1270</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1030</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.2134</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0342</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0633</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1578</span><span class="p">,</span> <span class="mf">0.1334</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.2316</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0237</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.2128</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1421</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0499</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0218</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1461</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1130</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1976</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_dtype_cast">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_dtype_cast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_dtype_cast" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_dtype_cast(input, dtype) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ‰§è¡Œå¼ é‡æ•°æ®ç±»å‹(dtype)è½¬æ¢ã€‚æ”¯æŒFakeTensoræ¨¡å¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>dtype (torch.dtype) - è¿”å›å¼ é‡çš„ç›®æ ‡æ•°æ®ç±»å‹ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; ç¤ºä¾‹ä¸€ï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; torch_npu.npu_dtype_cast(torch.tensor([0, 0.5, -1.]).npu(), dtype=torch.int)
&gt;&gt;&gt; tensor([ 0,  0, -1], device=&#39;npu:0&#39;, dtype=torch.int32)
&gt;&gt;&gt; ç¤ºä¾‹äºŒï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; //FakeTensoræ¨¡å¼
&gt;&gt;&gt; from torch._subclasses.fake_tensor import FakeTensorMode
&gt;&gt;&gt; with FakeTensorMode():
&gt;&gt;&gt; ...     x = torch.rand(2, dtype=torch.float32).npu()
&gt;&gt;&gt; ...     res = torch_npu.npu_dtype_cast(x, torch.float16)
&gt;&gt;&gt; ...
&gt;&gt;&gt; res
&gt;&gt;&gt; FakeTensor(..., device=&#39;npu:0&#39;, size=(2,), dtype=torch.float16)
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_format_cast">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_format_cast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_format_cast" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_format_cast(self, acl_format) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä¿®æ”¹NPUå¼ é‡çš„æ ¼å¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>acl_format (Int) - ç›®æ ‡æ ¼å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="mi">29</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">29</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_format_cast_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_format_cast_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_format_cast_" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_format_cast_(self, src) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åŸåœ°ä¿®æ”¹selfå¼ é‡æ ¼å¼ï¼Œä¸srcæ ¼å¼ä¿æŒä¸€è‡´ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>src (Tensorï¼Œint) - ç›®æ ‡æ ¼å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">npu_format_cast_</span><span class="p">(</span><span class="mi">29</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">29</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_fused_attention_score">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_fused_attention_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_fused_attention_score" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å®ç°â€œTransformer attention scoreâ€çš„èåˆè®¡ç®—é€»è¾‘ï¼Œä¸»è¦å°†matmulã€transposeã€addã€softmaxã€dropoutã€batchmatmulã€permuteç­‰è®¡ç®—è¿›è¡Œäº†èåˆã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>query_layerï¼šTensorç±»å‹ï¼Œä»…æ”¯æŒfloat16ã€‚</p>
<p>key_layerï¼šTensorç±»å‹ï¼Œä»…æ”¯æŒfloat16ã€‚</p>
<p>value_layerï¼šTensorç±»å‹ï¼Œä»…æ”¯æŒfloat16 ã€‚</p>
<p>attention_maskï¼šTensorç±»å‹ï¼Œä»…æ”¯æŒfloat16 ã€‚</p>
<p>scaleï¼šç¼©æ”¾ç³»æ•°ï¼Œæµ®ç‚¹æ•°æ ‡é‡ ã€‚</p>
<p>keep_probï¼šä¸åšdropoutçš„æ¦‚ç‡ï¼Œ0-1ä¹‹é—´ï¼Œæµ®ç‚¹æ•°ã€‚</p>
<p>query_transposeï¼šqueryæ˜¯å¦åšè½¬ç½®ï¼Œboolç±»å‹ï¼Œé»˜è®¤ä¸ºFalse ã€‚</p>
<p>key_transposeï¼škeyæ˜¯å¦åšè½¬ç½®ï¼Œboolç±»å‹ï¼Œé»˜è®¤ä¸ºFalse ã€‚</p>
<p>bmm_score_transpose_aï¼šbmmè®¡ç®—ä¸­å·¦çŸ©é˜µæ˜¯å¦åšè½¬ç½®ï¼Œboolç±»å‹ï¼Œé»˜è®¤ä¸ºFalseã€‚</p>
<p>bmm_score_transpose_bï¼šbmmè®¡ç®—ä¸­å³çŸ©é˜µæ˜¯å¦åšè½¬ç½®ï¼Œboolç±»å‹ï¼Œé»˜è®¤ä¸ºFalseã€‚</p>
<p>value_transposeï¼švalueæ˜¯å¦åšè½¬ç½®ï¼Œboolç±»å‹ï¼Œé»˜è®¤ä¸ºFalseã€‚</p>
<p>dx_transposeï¼šåå‘è®¡ç®—æ—¶dxæ˜¯å¦åšè½¬ç½®ï¼Œboolç±»å‹ï¼Œé»˜è®¤ä¸ºFalseã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>è¾“å…¥tensorçš„æ ¼å¼ç¼–å·å¿…é¡»å‡ä¸º29ï¼Œæ•°æ®ç±»å‹ä¸ºFP16ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span> <span class="p">,</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.125</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_attention_score</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context_layer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5063</span><span class="p">,</span> <span class="mf">0.4900</span><span class="p">,</span> <span class="mf">0.4951</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5493</span><span class="p">,</span> <span class="mf">0.5249</span><span class="p">,</span> <span class="mf">0.5400</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4844</span><span class="p">,</span> <span class="mf">0.4724</span><span class="p">,</span> <span class="mf">0.4927</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5176</span><span class="p">,</span> <span class="mf">0.4702</span><span class="p">,</span> <span class="mf">0.4790</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4683</span><span class="p">,</span> <span class="mf">0.4771</span><span class="p">,</span> <span class="mf">0.5054</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4917</span><span class="p">,</span> <span class="mf">0.4614</span><span class="p">,</span> <span class="mf">0.4739</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5137</span><span class="p">,</span> <span class="mf">0.5010</span><span class="p">,</span> <span class="mf">0.5078</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4656</span><span class="p">,</span> <span class="mf">0.4592</span><span class="p">,</span> <span class="mf">0.5034</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5425</span><span class="p">,</span> <span class="mf">0.5732</span><span class="p">,</span> <span class="mf">0.5347</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5054</span><span class="p">,</span> <span class="mf">0.5024</span><span class="p">,</span> <span class="mf">0.4924</span><span class="p">],</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_fusion_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_fusion_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_fusion_attention" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_fusion_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=1, int[]? prefix=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False ) -&gt; (Tensor, Tensor, Tensor, Tensor, int, int, int)</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>queryï¼šDeviceä¾§çš„Tensorï¼Œå…¬å¼ä¸­è¾“å…¥Qï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOATã€FLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>keyï¼šDeviceä¾§çš„Tensorï¼Œå…¬å¼ä¸­è¾“å…¥Kï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOATã€FLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>valueï¼šDeviceä¾§çš„Tensorï¼Œå…¬å¼ä¸­è¾“å…¥Vï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOATã€FLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>pseï¼šDeviceä¾§çš„Tensorï¼Œå…¬å¼ä¸­è¾“å…¥pseï¼Œå¯é€‰å‚æ•°ï¼Œè¡¨ç¤ºä½ç½®ç¼–ç ã€‚æ•°æ®ç±»å‹æ”¯æŒFLOATã€FLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚å››ç»´è¾“å…¥ï¼Œå‚æ•°æ¯ä¸ªbatchä¸ç›¸åŒï¼ŒBNSSæ ¼å¼ï¼›æ¯ä¸ªbatchç›¸åŒï¼Œ1NSSæ ¼å¼ã€‚alibiä½ç½®ç¼–ç ,å¦‚æœSå¤§äº1024ä¸”ä¸‹ä¸‰è§’æ©ç åœºæ™¯,åªè¾“å…¥ä¸‹ä¸‰è§’å€’æ•°1024è¡Œè¿›è¡Œå†…å­˜ä¼˜åŒ–ï¼Œå‚æ•°æ¯ä¸ªbatchä¸ç›¸åŒï¼Œè¾“å…¥BNHS ,æ¯ä¸ªbatchç›¸åŒï¼Œè¾“å…¥1NHS(H=1024)ã€‚</p>
<p>dropMaskï¼šDeviceä¾§çš„Tensorï¼Œå¯é€‰å±æ€§ï¼Œæ•°æ®ç±»å‹æ”¯æŒUINT8(æ ‡è¯†8ä¸ª1bit BOOL)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>paddingMaskï¼šDeviceä¾§çš„Tensorï¼Œæš‚ä¸æ”¯æŒè¯¥ä¼ å‚ã€‚</p>
<p>attenMaskï¼šDeviceä¾§çš„Tensorï¼Œå¯é€‰å±æ€§ï¼Œä»£è¡¨ä¸‹ä¸‰è§’å…¨ä¸º0ä¸Šä¸‰è§’å…¨ä¸ºè´Ÿæ— ç©·çš„å€’ä¸‰è§’maskçŸ©é˜µï¼Œæ•°æ®ç±»å‹æ”¯æŒBOOL(8bitçš„BOOL)ã€UINT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>prefixï¼šDeviceä¾§çš„Tensorï¼Œå¯é€‰å±æ€§ï¼Œä»£è¡¨prefixç¨€ç–è®¡ç®—åœºæ™¯æ¯ä¸ªBatchçš„Nå€¼ã€‚æ•°æ®ç±»å‹æ”¯æŒINT64ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>scaleï¼šHostä¾§çš„doubleï¼Œå…¬å¼ä¸­då¼€æ ¹å·çš„å€’æ•°ï¼Œä»£è¡¨ç¼©æ”¾ç³»æ•°ï¼Œä½œä¸ºè®¡ç®—æµä¸­Mulsçš„scalarå€¼ï¼Œæ•°æ®ç±»å‹æ”¯æŒDOUBLEã€‚</p>
<p>keepProbï¼šHostä¾§çš„doubleï¼Œå¯é€‰å‚æ•°ï¼Œä»£è¡¨dropMaskä¸­1çš„æ¯”ä¾‹ï¼Œæ•°æ®ç±»å‹æ”¯æŒDOUBLEã€‚</p>
<p>preTokensï¼šHostä¾§çš„int64_tï¼Œç”¨äºç¨€ç–è®¡ç®—çš„å‚æ•°ï¼Œå¯é€‰å‚æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p>nextTokensï¼šHostä¾§çš„int64_tï¼Œç”¨äºç¨€ç–è®¡ç®—çš„å‚æ•°ï¼Œå¯é€‰å‚æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p>headNumï¼šHostä¾§çš„int64_tï¼Œä»£è¡¨headä¸ªæ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p>inputLayoutï¼šHostä¾§çš„stringï¼Œä»£è¡¨è¾“å…¥queryã€keyã€valueçš„æ•°æ®æ’å¸ƒæ ¼å¼ï¼Œæ”¯æŒBSHã€SBHã€BSNDã€BNSDã€‚</p>
<p>innerPreciseï¼šHostä¾§çš„int32_tï¼Œæ•°æ®ç±»å‹æ”¯æŒINT32ï¼Œä¿ç•™å‚æ•°ï¼Œæš‚æœªä½¿ç”¨ã€‚</p>
<p>sparseModeï¼šHostä¾§çš„intï¼Œè¡¨ç¤ºsparseçš„æ¨¡å¼ã€‚æ•°æ®ç±»å‹æ”¯æŒï¼šINT64ã€‚</p>
<p>sparseModeä¸º0æ—¶ï¼Œä»£è¡¨defaultMaskæ¨¡å¼ï¼Œå¦‚æœattenmaskæœªä¼ å…¥åˆ™ä¸åšmaskæ“ä½œï¼Œå¿½ç•¥preTokenså’ŒnextTokens(å†…éƒ¨èµ‹å€¼ä¸ºINT_MAX)ï¼›å¦‚æœä¼ å…¥ï¼Œåˆ™éœ€è¦ä¼ å…¥å®Œæ•´çš„attenmaskçŸ©é˜µ(S1 * S2)ï¼Œè¡¨ç¤ºpreTokenså’ŒnextTokensä¹‹é—´çš„éƒ¨åˆ†éœ€è¦è®¡ç®—ã€‚</p>
<p>sparseModeä¸ºä¸º1æ—¶ï¼Œä»£è¡¨allMaskï¼Œå³ä¼ å…¥å®Œæ•´çš„attenmaskçŸ©é˜µã€‚ã€‚</p>
<p>sparseModeä¸º2æ—¶ï¼Œä»£è¡¨leftUpCausalæ¨¡å¼çš„maskï¼Œå¯¹åº”ä»¥å·¦é¡¶ç‚¹ä¸ºåˆ’åˆ†çš„ä¸‹ä¸‰è§’åœºæ™¯ï¼Œéœ€è¦ä¼ å…¥ä¼˜åŒ–åçš„attenmaskçŸ©é˜µ(2048*2048)ã€‚</p>
<p>sparseModeä¸º3æ—¶ï¼Œä»£è¡¨rightDownCausalæ¨¡å¼çš„maskï¼Œå¯¹åº”ä»¥å³ä¸‹é¡¶ç‚¹ä¸ºåˆ’åˆ†çš„ä¸‹ä¸‰è§’åœºæ™¯ï¼Œéœ€è¦ä¼ å…¥ä¼˜åŒ–åçš„attenmaskçŸ©é˜µ(2048*2048)ã€‚</p>
<p>sparseModeä¸ºä¸º4æ—¶ï¼Œä»£è¡¨bandåœºæ™¯ï¼Œå³è®¡ç®—preTokenså’ŒnextTokensä¹‹é—´çš„éƒ¨åˆ†ã€‚</p>
<p>sparseModeä¸ºä¸º5æ—¶ï¼Œä»£è¡¨prefixåœºæ™¯ï¼Œå³åœ¨rightDownCasualçš„åŸºç¡€ä¸Šï¼Œå·¦ä¾§åŠ ä¸Šä¸€ä¸ªé•¿ä¸ºS1ï¼Œå®½ä¸ºNçš„çŸ©é˜µï¼ŒNçš„å€¼ç”±æ–°å¢çš„è¾“å…¥prefixè·å–ï¼Œä¸”æ¯ä¸ªBatchè½´çš„Nå€¼ä¸ä¸€æ ·ã€‚</p>
<p>sparseModeä¸ºä¸º6ã€7ã€8æ—¶ï¼Œåˆ†åˆ«ä»£è¡¨globalã€dilatedã€block_localï¼Œå‡æš‚ä¸æ”¯æŒã€‚</p>
<p>gen_mask_parallelï¼šdebugå‚æ•°ï¼ŒDSAç”Ÿæˆdropoutéšæœºæ•°å‘é‡maskçš„æ§åˆ¶å¼€å…³ï¼Œé»˜è®¤å€¼ä¸ºTrueï¼šåŒAICOREè®¡ç®—å¹¶è¡Œï¼ŒFalseï¼šåŒAICOREè®¡ç®—ä¸²è¡Œ</p>
<p>syncï¼šdebugå‚æ•°ï¼ŒDSAç”Ÿæˆdropoutéšæœºæ•°å‘é‡maskçš„æ§åˆ¶å¼€å…³ï¼Œé»˜è®¤å€¼ä¸ºFalseï¼šdropout maskå¼‚æ­¥ç”Ÿæˆï¼ŒTrueï¼šdropout maskåŒæ­¥ç”Ÿæˆ</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>å…±7ä¸ªè¾“å‡º</p>
<p>(Tensor, Tensor, Tensor, Tensor, int, int, int)</p>
<p>ç¬¬1ä¸ªè¾“å‡ºä¸ºTensorï¼Œè®¡ç®—å…¬å¼çš„æœ€ç»ˆè¾“å‡ºyã€‚</p>
<p>ç¬¬2ä¸ªè¾“å‡ºä¸ºTensorï¼ŒSoftmax è®¡ç®—çš„Maxä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</p>
<p>ç¬¬3ä¸ªè¾“å‡ºä¸ºTensorï¼ŒSoftmaxè®¡ç®—çš„Sumä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</p>
<p>ç¬¬4ä¸ªè¾“å‡ºä¸ºTensorï¼Œä¿ç•™å‚æ•°ï¼Œæš‚æœªä½¿ç”¨ã€‚</p>
<p>ç¬¬5ä¸ªè¾“å‡ºä¸ºintï¼ŒDSAç”Ÿæˆdropoutmaskä¸­ï¼ŒPhiloxç®—æ³•çš„seedã€‚</p>
<p>ç¬¬6ä¸ªè¾“å‡ºä¸ºintï¼ŒDSAç”Ÿæˆdropoutmaskä¸­ï¼ŒPhiloxç®—æ³•çš„offsetã€‚</p>
<p>ç¬¬7ä¸ªè¾“å‡ºä¸ºintï¼ŒDSAç”Ÿæˆdropoutmaskçš„é•¿åº¦ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>è¾“å…¥queryã€keyã€valueçš„Bï¼šbatchsizeå¿…é¡»ç›¸ç­‰ã€‚</p>
<p>è¾“å…¥queryçš„Nå’Œkey/valueçš„N å¿…é¡»æˆæ¯”ä¾‹å…³ç³»ï¼Œå³Nq/Nkvå¿…é¡»æ˜¯é0æ•´æ•°ï¼Œå½“Nq/Nkv &gt; 1æ—¶ï¼Œå³ä¸ºGQAï¼Œå½“Nkv=1æ—¶ï¼Œå³ä¸ºMQAã€‚</p>
<p>è¾“å…¥key/valueçš„shapeå¿…é¡»ä¸€è‡´ã€‚</p>
<p>è¾“å…¥queryã€keyã€valueçš„Sï¼šsequence lengthï¼Œå–å€¼èŒƒå›´1~32Kï¼Œä¸”ä¸º16çš„å€æ•°ã€‚</p>
<p>è¾“å…¥queryã€keyã€valueçš„Dï¼šhead dimï¼Œå–å€¼èŒƒå›´64ã€80ã€96ã€120ã€128ã€256ã€‚</p>
<p>å½“pre_tockens&lt;Sq çš„æ—¶å€™, ä½¿èƒ½band sparseè®¡ç®—ï¼Œpre_tockensä¸èƒ½å°äº0ã€‚</p>
<p>å½“next_tockens&lt;Skvçš„æ—¶å€™ï¼Œä½¿èƒ½bandsparseè®¡ç®—ï¼Œnext_tokensä¸èƒ½å°äº0ã€‚</p>
<p>å½“pre_tokens &gt;= Sqï¼ŒåŒæ—¶next_tokens=0æ—¶ï¼Œä½¿èƒ½causalè®¡ç®—ã€‚</p>
<p>åœ¨ä½¿èƒ½band sparseã€causalè®¡ç®—æ—¶ï¼Œå¿…é¡»è¾“å…¥atten_maskã€‚</p>
<p>å½“æ‰€æœ‰çš„attenmaskçš„shapeå°äº2048ä¸”ç›¸åŒçš„æ—¶å€™ï¼Œå»ºè®®ä½¿ç”¨defaultæ¨¡å¼ï¼Œå³sparse_modeé…ç½®ä¸º0ï¼Œæ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ï¼›sparse_modeé…ç½®ä¸º2æˆ–3æ—¶ï¼Œç¦æ­¢é…ç½®preTokensã€nextTokensã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>æ”¯æŒçš„å‹å·</p>
<p>Atlas è®­ç»ƒç³»åˆ—äº§å“</p>
<p>Atlas A2è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">unittest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_npu.testing.testcase</span><span class="w"> </span><span class="kn">import</span> <span class="n">TestCase</span><span class="p">,</span> <span class="n">run_tests</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_npu.testing.common_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_npu_device</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">DEVICE_NAME</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">TestNPUFlashAttention</span><span class="p">(</span><span class="n">TestCase</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">supported_op_exec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mf">0.08838</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax_res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">softmax_res</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">custom_op_exec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.08838</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fusion_attention</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">head_num</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">trans_BNSD2BSH</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_geglu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_geglu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_geglu" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu. npu_geglu(Tensor self, int dim=-1, int approximate=1) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å¯¹è¾“å…¥Tensorå®ŒæˆGeGluè¿ç®—ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>Tensor selfï¼šå¾…è¿›è¡ŒGeGluè®¡ç®—çš„å…¥å‚ï¼Œnpu deviceä¾§çš„aclTensorï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€FLOAT16ã€BFLOAT16(Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>int dimï¼šå¯é€‰å…¥å‚ï¼Œè®¾å®šçš„sliceè½´ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p>int approximateï¼šå¯é€‰å…¥å‚ï¼ŒGeGluè®¡ç®—ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ç´¢å¼•ï¼Œ0è¡¨ç¤ºä½¿ç”¨noneï¼Œ1è¡¨ç¤ºä½¿ç”¨tanhï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p>outï¼šGeGluè®¡ç®—çš„å‡ºå‚ï¼Œnpu deviceä¾§çš„aclTensorï¼Œæ•°æ®ç±»å‹å¿…é¡»å’Œselfä¸€è‡´ï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>outGeluï¼šGeGluè®¡ç®—çš„å‡ºå‚ï¼Œnpu deviceä¾§çš„aclTensorï¼Œæ•°æ®ç±»å‹å¿…é¡»å’Œselfä¸€è‡´ï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>outã€outGeluåœ¨dimç»´çš„sizeç­‰äºselfåœ¨dimç»´sizeçš„1/2ã€‚</p>
<p>å½“self.dim()==0æ—¶ï¼Œdimçš„å–å€¼åœ¨[-1, 0]èŒƒå›´å†…ï¼›å½“self.dim()&gt;0æ—¶ï¼Œå–å€¼åœ¨[-self.dim(), self.dim()-1]èŒƒå›´å†…ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span><span class="mi">9216</span><span class="p">,</span><span class="mi">2560</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_npu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_npu</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.8750</span><span class="p">,</span>  <span class="mf">0.4766</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3535</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4619</span><span class="p">,</span>  <span class="mf">0.3542</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8389</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.9424</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0291</span><span class="p">,</span>  <span class="mf">0.9482</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5640</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2959</span><span class="p">,</span>  <span class="mf">1.7666</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.4958</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6787</span><span class="p">,</span>  <span class="mf">0.0179</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4365</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8311</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7676</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.1611</span><span class="p">,</span>  <span class="mf">1.4766</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1934</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5913</span><span class="p">,</span>  <span class="mf">1.1553</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4626</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.4873</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8105</span><span class="p">,</span>  <span class="mf">0.5723</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.3193</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1558</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6191</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.6816</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2080</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6953</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3096</span><span class="p">,</span>  <span class="mf">0.4158</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2168</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.4287</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9863</span><span class="p">,</span>  <span class="mf">1.4053</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7676</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6709</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1582</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.3281</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9043</span><span class="p">,</span>  <span class="mf">0.7725</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5596</span><span class="p">,</span>  <span class="mf">0.1632</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0732</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0254</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6650</span><span class="p">,</span>  <span class="mf">0.1318</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8159</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7134</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4536</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0327</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6206</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1492</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2559</span><span class="p">,</span>  <span class="mf">0.3777</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2822</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.1904</span><span class="p">,</span>  <span class="mf">1.1260</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3369</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4814</span><span class="p">,</span>  <span class="mf">0.4463</span><span class="p">,</span>  <span class="mf">1.0205</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1192</span><span class="p">,</span>  <span class="mf">1.7783</span><span class="p">,</span>  <span class="mf">0.1040</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0010</span><span class="p">,</span>  <span class="mf">1.5342</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5728</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.3296</span><span class="p">,</span>  <span class="mf">0.5703</span><span class="p">,</span>  <span class="mf">0.6338</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2131</span><span class="p">,</span>  <span class="mf">1.1113</span><span class="p">,</span>  <span class="mf">0.9854</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.4336</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7568</span><span class="p">,</span>  <span class="mf">1.8164</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2012</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8721</span><span class="p">,</span>  <span class="mf">0.6904</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6934</span><span class="p">,</span>  <span class="mf">0.3743</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9448</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9946</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6494</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3564</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1855</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9663</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8252</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2285</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5684</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4277</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1260</span><span class="p">,</span>  <span class="mf">1.2871</span><span class="p">,</span>  <span class="mf">1.2754</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5171</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1064</span><span class="p">,</span>  <span class="mf">0.9624</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4639</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0661</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7178</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.2656</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9023</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1641</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.8350</span><span class="p">,</span>  <span class="mf">1.0625</span><span class="p">,</span>  <span class="mf">1.6172</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4160</span><span class="p">,</span>  <span class="mf">1.2490</span><span class="p">,</span>  <span class="mf">1.9775</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.5615</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9990</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5996</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9404</span><span class="p">,</span>  <span class="mf">0.5068</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9829</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0771</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5537</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5654</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4678</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5215</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7920</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.3389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3228</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1514</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.8882</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9971</span><span class="p">,</span>  <span class="mf">1.2432</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5439</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8154</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9238</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2556</span><span class="p">,</span>  <span class="mf">0.2131</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7471</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.1074</span><span class="p">,</span>  <span class="mf">1.0391</span><span class="p">,</span>  <span class="mf">0.1556</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.1689</span><span class="p">,</span>  <span class="mf">0.6470</span><span class="p">,</span>  <span class="mf">0.2463</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.2617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8911</span><span class="p">,</span>  <span class="mf">1.9160</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3027</span><span class="p">,</span>  <span class="mf">1.7764</span><span class="p">,</span>  <span class="mf">0.3381</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4160</span><span class="p">,</span>  <span class="mf">1.6201</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5396</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.8271</span><span class="p">,</span>  <span class="mf">1.3086</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8770</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.8252</span><span class="p">,</span>  <span class="mf">1.3779</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3535</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5215</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4727</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0420</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4600</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7754</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4697</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3838</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.8506</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3945</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0142</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3447</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6587</span><span class="p">,</span>  <span class="mf">0.5728</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1523</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8027</span><span class="p">,</span>  <span class="mf">0.4731</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5464</span><span class="p">,</span>  <span class="mf">1.4014</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8594</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.1467</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5752</span><span class="p">,</span>  <span class="mf">0.3298</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9902</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8281</span><span class="p">,</span>  <span class="mf">1.8506</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2473</span><span class="p">,</span>  <span class="mf">1.0693</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8184</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.9277</span><span class="p">,</span>  <span class="mf">1.6543</span><span class="p">,</span>  <span class="mf">1.0088</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7939</span><span class="p">,</span>  <span class="mf">1.3486</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1543</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4053</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0055</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3672</span><span class="p">,</span>  <span class="mf">0.3274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3369</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4951</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9580</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7847</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.3525</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4780</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1610</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9209</span><span class="p">,</span>  <span class="mf">1.5498</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.4905</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7832</span><span class="p">,</span>  <span class="mf">0.4243</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9492</span><span class="p">,</span>  <span class="mf">0.3335</span><span class="p">,</span>  <span class="mf">0.9565</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_npu</span><span class="p">,</span> <span class="n">y_gelu_npu</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_geglu</span><span class="p">(</span><span class="n">x_npu</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_npu</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">9.2590e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2054e-01</span><span class="p">,</span>  <span class="mf">1.6980e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8542e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.5254e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9519e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.2405e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4902e+00</span><span class="p">,</span>  <span class="mf">8.0750e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.4570e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5029e+00</span><span class="p">,</span>  <span class="mf">2.8442e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">9.0271e-02</span><span class="p">,</span>  <span class="mf">4.3335e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7402e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.3574e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">5.5762e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3123e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0004e-01</span><span class="p">,</span>  <span class="mf">1.5312e+00</span><span class="p">,</span>  <span class="mf">1.4189e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6172e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6113e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1887e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">5.9845e-02</span><span class="p">,</span>  <span class="mf">2.0911e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.4735e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.1422e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6289e+00</span><span class="p">,</span>  <span class="mf">2.5977e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.3649e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3329e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9031e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.5977e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.2178e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3242e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">3.1816e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6719e+00</span><span class="p">,</span>  <span class="mf">1.4038e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.6660e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.7820e-02</span><span class="p">,</span>  <span class="mf">2.3999e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">2.9297e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7754e+00</span><span class="p">,</span>  <span class="mf">2.6703e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3318e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.2109e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9072e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1316e-01</span><span class="p">,</span>  <span class="mf">5.8887e-01</span><span class="p">,</span>  <span class="mf">8.2959e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.1273e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.1481e-01</span><span class="p">,</span>  <span class="mf">4.2419e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.6831e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7288e-02</span><span class="p">,</span>  <span class="mf">2.6343e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.3750e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.2324e+00</span><span class="p">,</span>  <span class="mf">1.2894e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.0630e-01</span><span class="p">,</span>  <span class="mf">5.9619e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4210e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2598e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.5552e-02</span><span class="p">,</span>  <span class="mf">1.1115e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6143e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6150e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9774e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">8.6426e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.1879e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9795e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">4.3152e-02</span><span class="p">,</span>  <span class="mf">1.9250e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7485e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.8632e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4551e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1289e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">4.7951e-03</span><span class="p">,</span>  <span class="mf">2.0691e-01</span><span class="p">,</span>  <span class="mf">4.4458e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">4.7485e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">4.8889e-02</span><span class="p">,</span>  <span class="mf">1.5684e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">8.9404e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0420e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9248e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.6205e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.5449e+00</span><span class="p">,</span>  <span class="mf">8.2397e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.9385e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8838e+00</span><span class="p">,</span>  <span class="mf">6.0010e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.5059e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1829e-02</span><span class="p">,</span>  <span class="mf">1.0547e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">5.1086e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0760e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1228e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.2468e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.7900e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5278e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.7078e-01</span><span class="p">,</span>  <span class="mf">1.6846e-01</span><span class="p">,</span>  <span class="mf">2.5528e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.3708e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4954e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8418e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">6.3574e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0156e+00</span><span class="p">,</span>  <span class="mf">9.3994e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.2402e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.2218e-03</span><span class="p">,</span>  <span class="mf">8.7402e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.5010e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8518e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0930e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.1511e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">3.8300e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6150e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.8442e-01</span><span class="p">,</span>  <span class="mf">4.4373e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0022e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.2468e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.2524e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2115e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.4760e-02</span><span class="p">,</span>  <span class="mf">1.9812e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.1431e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1650e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.4011e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0919e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5283e-01</span><span class="p">,</span>  <span class="mf">1.8535e+00</span><span class="p">,</span>  <span class="mf">4.4360e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">6.4844e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.8784e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5938e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">9.9915e-02</span><span class="p">,</span>  <span class="mf">4.6436e-01</span><span class="p">,</span>  <span class="mf">6.6528e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2817e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5686e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4962e-02</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">2.3279e-01</span><span class="p">,</span>  <span class="mf">4.5630e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4834e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.9013e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">4.7974e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7617e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0760e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0371e+00</span><span class="p">,</span>  <span class="mf">3.7915e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">6.4551e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6953e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0910e-03</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">4.9683e-01</span><span class="p">,</span>  <span class="mf">1.2402e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0429e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.4294e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">8.2959e-01</span><span class="p">,</span>  <span class="mf">1.2012e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.6956e-01</span><span class="p">,</span>  <span class="mf">5.3027e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6418e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1094e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">9.8267e-02</span><span class="p">,</span>  <span class="mf">2.3364e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">4.1687e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1365e-01</span><span class="p">,</span>  <span class="mf">1.2598e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6299e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.5967e+00</span><span class="p">,</span>  <span class="mf">9.3445e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">9.7656e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5410e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9395e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6565e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">8.2153e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.0068e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.6345e-01</span><span class="p">,</span>  <span class="mf">2.5806e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.1951e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.5857e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.0303e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9080e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.9666e-01</span><span class="p">,</span>  <span class="mf">1.8262e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1951e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0138e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.0911e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.0638e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">6.9141e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5234e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2734e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0510e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6504e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7070e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.5406e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1342e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.0862e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.2041e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.7271e-02</span><span class="p">,</span>  <span class="mf">8.0518e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5161e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8848e-02</span><span class="p">,</span>  <span class="mf">7.0801e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.0166e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">3.3661e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4319e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">3.0899e-02</span><span class="p">,</span>  <span class="mf">1.4490e-01</span><span class="p">,</span>  <span class="mf">1.9763e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1116e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8955e-01</span><span class="p">,</span>  <span class="mf">1.8347e-01</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_gelu_npu</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.5771e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4331e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0846e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1133e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3818e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5076e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.8600e-02</span><span class="p">,</span>  <span class="mf">1.6904e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9336e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.6890e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6768e+00</span><span class="p">,</span>  <span class="mf">2.5146e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">7.5342e-01</span><span class="p">,</span>  <span class="mf">6.0742e-01</span><span class="p">,</span>  <span class="mf">1.0820e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.5063e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.1572e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.4482e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5796e-01</span><span class="p">,</span>  <span class="mf">8.4082e-01</span><span class="p">,</span>  <span class="mf">9.2627e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6064e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.1096e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6370e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.4814e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6418e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1982e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5186e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3330e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4111e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">8.4778e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1023e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0669e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.9521e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.5654e-01</span><span class="p">,</span>  <span class="mf">1.5635e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.7881e+00</span><span class="p">,</span>  <span class="mf">1.8359e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6663e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4609e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6760e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6528e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.9434e+00</span><span class="p">,</span>  <span class="mf">1.7168e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1615e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.8816e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.4043e-01</span><span class="p">,</span>  <span class="mf">1.2344e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6064e-01</span><span class="p">,</span>  <span class="mf">5.7031e-01</span><span class="p">,</span>  <span class="mf">1.6475e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0809e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6785e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6345e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6797e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6326e-02</span><span class="p">,</span>  <span class="mf">2.6904e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">6.9458e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3174e+00</span><span class="p">,</span>  <span class="mf">1.3486e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0645e-01</span><span class="p">,</span>  <span class="mf">3.0249e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.9411e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3928e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.0974e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1533e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.7012e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0254e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.2825e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8492e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.1926e-01</span><span class="p">,</span>  <span class="mf">1.7490e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">6.6650e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0370e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3788e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0706e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6980e-01</span><span class="p">,</span>  <span class="mf">1.4209e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">5.2986e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1133e-01</span><span class="p">,</span>  <span class="mf">2.5439e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9459e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.8909e-02</span><span class="p">,</span>  <span class="mf">1.2119e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">6.1035e-01</span><span class="p">,</span>  <span class="mf">6.8506e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5039e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.8136e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.8232e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7383e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.4434e+00</span><span class="p">,</span>  <span class="mf">1.6787e+00</span><span class="p">,</span>  <span class="mf">1.2422e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.5488e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">5.0720e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8787e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4600e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2213e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6711e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.7280e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3125e+00</span><span class="p">,</span>  <span class="mf">2.2375e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.4985e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2659e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6722e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4685e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4856e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6406e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">4.8730e-01</span><span class="p">,</span>  <span class="mf">1.6680e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.7098e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4189e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.1983e-03</span><span class="p">,</span>  <span class="mf">7.8857e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1328e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6931e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1163e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6467e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.5309e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5173e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6858e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9111e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4709e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1970e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.4248e-01</span><span class="p">,</span>  <span class="mf">5.0830e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">2.1936e-01</span><span class="p">,</span>  <span class="mf">7.7197e-01</span><span class="p">,</span>  <span class="mf">4.8737e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">8.7842e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6406e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1716e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.2720e-01</span><span class="p">,</span>  <span class="mf">1.9404e+00</span><span class="p">,</span>  <span class="mf">1.0391e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.3877e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6199e-01</span><span class="p">,</span>  <span class="mf">1.5781e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6968e-01</span><span class="p">,</span>  <span class="mf">1.0664e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6431e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.5439e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5332e-01</span><span class="p">,</span>  <span class="mf">2.1790e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">3.0981e-01</span><span class="p">,</span>  <span class="mf">6.0010e-01</span><span class="p">,</span>  <span class="mf">7.9346e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">4.0169e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.8447e-01</span><span class="p">,</span>  <span class="mf">1.7109e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6699e-01</span><span class="p">,</span>  <span class="mf">1.7646e+00</span><span class="p">,</span>  <span class="mf">5.9326e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.3813e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5845e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7699e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.7573e-01</span><span class="p">,</span>  <span class="mf">9.4580e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.5276e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.4805e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.3350e-01</span><span class="p">,</span>  <span class="mf">1.2573e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5369e-01</span><span class="p">,</span>  <span class="mf">1.2021e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6626e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1108e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6084e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4807e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">4.6234e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.4331e-02</span><span class="p">,</span>  <span class="mf">8.9844e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.2871e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.9834e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6992e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">6.4941e-02</span><span class="p">,</span>  <span class="mf">1.1465e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5161e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5076e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">8.6487e-02</span><span class="p">,</span>  <span class="mf">1.0137e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.1731e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4404e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9050e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2128e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.0919e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6943e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.5186e-01</span><span class="p">,</span>  <span class="mf">1.1396e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.5735e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.4829e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6455e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9355e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">6.4404e-01</span><span class="p">,</span>  <span class="mf">1.5625e+00</span><span class="p">,</span>  <span class="mf">1.7725e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.5176e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.7920e+00</span><span class="p">,</span>  <span class="mf">6.6504e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.9083e-03</span><span class="p">,</span>  <span class="mf">3.8452e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9011e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5405e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6003e-01</span><span class="p">,</span>  <span class="mf">1.3975e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0437e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.6182e-02</span><span class="p">,</span>  <span class="mf">5.5713e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0645e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.3818e-01</span><span class="p">,</span>  <span class="mf">5.1562e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0229e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0529e-01</span><span class="p">,</span>  <span class="mf">2.6562e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6702e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.0830e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6833e-01</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_get_float_status">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_get_float_status</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_get_float_status" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_get_float_status(self) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—npu_get_float_statusç®—å­å‡½æ•°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ•°æ®å†…å­˜åœ°å€å¼ é‡ï¼Œæ•°æ®ç±»å‹ä¸ºfloat32ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_get_float_status</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_giou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_giou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_giou" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_giou(self, gtboxes, trans=False, is_cross=False, mode=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>é¦–å…ˆè®¡ç®—ä¸¤ä¸ªæ¡†çš„æœ€å°å°é—­é¢ç§¯å’ŒIoUï¼Œç„¶åè®¡ç®—å°é—­åŒºåŸŸä¸­ä¸å±äºä¸¤ä¸ªæ¡†çš„å°é—­é¢ç§¯çš„æ¯”ä¾‹ï¼Œæœ€åä»IoUä¸­å‡å»è¿™ä¸ªæ¯”ä¾‹ï¼Œå¾—åˆ°GIoUã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ ‡æ³¨æ¡†ï¼Œshapeä¸º(N, 4) æ•°æ®ç±»å‹ä¸ºfloat16æˆ–float32çš„2Då¼ é‡ã€‚â€œNâ€è¡¨ç¤ºæ ‡æ³¨æ¡†çš„æ•°é‡ï¼Œå€¼â€œ4â€è¡¨ç¤º[x1, y1, x2, y2]æˆ–[x, y, w, h]ã€‚</p>
<p>gtboxes (Tensor) - çœŸå€¼æ¡†ï¼Œshapeä¸º(M, 4) æ•°æ®ç±»å‹ä¸ºfloat16æˆ–float32çš„2Då¼ é‡ã€‚â€œMâ€è¡¨ç¤ºçœŸå€¼æ¡†çš„æ•°é‡ï¼Œå€¼â€œ4â€è¡¨ç¤º[x1, y1, x2, y2]æˆ–[x, y, w, h]ã€‚</p>
<p>trans (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - å€¼ä¸ºTrueä»£è¡¨â€œxywhâ€ï¼Œå€¼ä¸ºFalseä»£è¡¨â€œxyxyâ€ã€‚</p>
<p>is_cross (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - æ§åˆ¶è¾“å‡ºshapeæ˜¯[M, N]è¿˜æ˜¯[1,N]ã€‚å¦‚æœå€¼ä¸ºTrueï¼Œåˆ™è¾“å‡ºshapeä¸º[M,N]ã€‚å¦‚æœä¸ºFalseï¼Œåˆ™è¾“å‡ºshapeä¸º[1,N]ã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - è®¡ç®—æ¨¡å¼ï¼Œå–å€¼ä¸º0æˆ–1ã€‚0è¡¨ç¤ºIoUï¼Œ1è¡¨ç¤ºIoFã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_giou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_cross</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_grid_assign_positive">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_grid_assign_positive</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_grid_assign_positive" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_grid_assign_positive(self, overlaps, box_responsible_flags, max_overlaps, argmax_overlaps, gt_max_overlaps, gt_argmax_overlaps, num_gts, pos_iou_thr, min_pos_iou, gt_max_assign_all) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ‰§è¡Œposition-sensitiveçš„å€™é€‰åŒºåŸŸæ± åŒ–æ¢¯åº¦è®¡ç®—ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - float16æˆ–float32ç±»å‹çš„å¼ é‡, shapeä¸º(n, )ã€‚</p>
<p>overlaps (Tensor) - æ•°æ®ç±»å‹ä¸assigned_gt_indsç›¸åŒï¼Œè¡¨ç¤ºgt_bboxeså’Œbboxesä¹‹é—´çš„IoUï¼Œshapeä¸º(k,n)ã€‚</p>
<p>box_responsible_flags (Tensor) - æ”¯æŒuint8æ•°æ®ç±»å‹ã€‚è¡¨ç¤ºæ¡†æ˜¯å¦responsibleçš„æ ‡å¿—ã€‚</p>
<p>max_overlaps (Tensor) - æ•°æ®ç±»å‹ä¸assigned_gt_inds. overlaps.max(axis=0)ç›¸åŒã€‚</p>
<p>argmax_overlaps (Tensor) - æ”¯æŒuint32æ•°æ®ç±»å‹ï¼Œoverlaps.argmax(axis=0)ã€‚</p>
<p>gt_max_overlaps (Tensor) - æ•°æ®ç±»å‹ä¸assigned_gt_inds. overlaps.max(axis=1)ç›¸åŒã€‚</p>
<p>gt_argmax_overlaps (Tensor) - æ”¯æŒuint32æ•°æ®ç±»å‹ï¼Œ overlaps.argmax(axis=1)ã€‚</p>
<p>num_gts (Tensor) - æ”¯æŒuint32æ•°æ®ç±»å‹ï¼Œreal k ï¼Œshapeä¸º (1, )ã€‚</p>
<p>pos_iou_thr (Float) - æ­£æ£€æµ‹æ¡†çš„IoUé˜ˆå€¼ã€‚</p>
<p>min_pos_iou (Float) - æ£€æµ‹æ¡†è¢«è§†ä¸ºæ­£æ£€æµ‹æ¡†çš„æœ€å°IoU</p>
<p>gt_max_assign_all (Bool) - æ˜¯å¦å°†ä¸æŸä¸ªgtæœ‰ç›¸åŒæœ€é«˜é‡å çš„æ‰€æœ‰æ£€æµ‹æ¡†åˆ†é…ç»™è¯¥gtã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">assigned_gt_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">overlaps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box_responsible_flags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_overlap</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">argmax_overlap</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_max_overlaps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_argmax_overlaps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grid_assign_positive</span><span class="p">(</span><span class="n">assigned_gt_inds</span><span class="p">,</span> <span class="n">overlaps</span><span class="p">,</span> <span class="n">box_responsible_flags</span><span class="p">,</span> <span class="n">max_overlap</span><span class="p">,</span> <span class="n">argmax_overlap</span><span class="p">,</span> <span class="n">gt_max_overlaps</span><span class="p">,</span> <span class="n">gt_argmax_overlaps</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_gru">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_gru</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_gru" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_gru(input, hx, weight_input, weight_hidden, bias_input, bias_hidden, seq_length, has_biases, num_layers, dropout, train, bidirectional, batch_first) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—DynamicGRUV2ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>hx (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>weight_input (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ï¼›æ ¼å¼ï¼šFRACTAL_Zã€‚</p>
<p>weight_hidden (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ï¼›æ ¼å¼ï¼šFRACTAL_Zã€‚</p>
<p>bias_input (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šNDã€‚</p>
<p>bias_hidden (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šNDã€‚</p>
<p>seq_length (Tensor) - æ•°æ®ç±»å‹ï¼šint32ï¼›æ ¼å¼ï¼šNDã€‚</p>
<p>has_biases (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue)</p>
<p>num_layers (Int)</p>
<p>dropout (Float)</p>
<p>train (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue) - æ ‡è¯†è®­ç»ƒæ˜¯å¦åœ¨opè¿›è¡Œçš„boolå‚æ•°ã€‚</p>
<p>bidirectional (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue)</p>
<p>batch_first (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue)</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>y (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>output_h (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>update (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>reset (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>new (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>hidden_new (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ifmr">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ifmr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ifmr" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä½¿ç”¨â€œbegin,end,stridesâ€æ•°ç»„å¯¹ifmrç»“æœè¿›è¡Œè®¡æ•°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>data (Tensor) - ç‰¹å¾å›¾å¼ é‡ã€‚</p>
<p>data_min (Tensor) - ç‰¹å¾å›¾æœ€å°å€¼çš„å¼ é‡ã€‚</p>
<p>data_max (Tensor) - ç‰¹å¾å›¾æœ€å¤§å€¼çš„å¼ é‡ã€‚</p>
<p>cumsum (Tensor) - cumsum binæ•°æ®å¼ é‡ã€‚</p>
<p>min_percentile (Float) - æœ€å°åˆå§‹åŒ–ç™¾åˆ†ä½æ•°ã€‚</p>
<p>max_percentile (Float) - æœ€å¤§åˆå§‹åŒ–ç™¾åˆ†ä½æ•°ã€‚</p>
<p>search_start (Float) - æœç´¢èµ·ç‚¹ã€‚</p>
<p>search_end (Float) - æœç´¢ç»ˆç‚¹ã€‚</p>
<p>search_step (Float) - æœç´¢æ­¥é•¿ã€‚</p>
<p>with_offset (Bool) - æ˜¯å¦ä½¿ç”¨offsetã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>scale (Tensor) - æœ€ä¼˜å°ºåº¦ã€‚</p>
<p>offset (Tensor) - æœ€ä¼˜offsetã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.4508</span><span class="p">,</span> <span class="mf">0.6513</span><span class="p">,</span> <span class="mf">0.4734</span><span class="p">,</span> <span class="mf">0.1924</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0402</span><span class="p">,</span> <span class="mf">0.5502</span><span class="p">,</span> <span class="mf">0.0694</span><span class="p">,</span> <span class="mf">0.9032</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4844</span><span class="p">,</span> <span class="mf">0.5361</span><span class="p">,</span> <span class="mf">0.9369</span><span class="p">,</span> <span class="mf">0.7874</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.5157</span><span class="p">,</span> <span class="mf">0.1863</span><span class="p">,</span> <span class="mf">0.4574</span><span class="p">,</span> <span class="mf">0.8033</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5986</span><span class="p">,</span> <span class="mf">0.8090</span><span class="p">,</span> <span class="mf">0.7605</span><span class="p">,</span> <span class="mf">0.8252</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4264</span><span class="p">,</span> <span class="mf">0.8952</span><span class="p">,</span> <span class="mf">0.2279</span><span class="p">,</span> <span class="mf">0.9746</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.0803</span><span class="p">,</span> <span class="mf">0.7114</span><span class="p">,</span> <span class="mf">0.8773</span><span class="p">,</span> <span class="mf">0.2341</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6497</span><span class="p">,</span> <span class="mf">0.0423</span><span class="p">,</span> <span class="mf">0.8407</span><span class="p">,</span> <span class="mf">0.9515</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1821</span><span class="p">,</span> <span class="mf">0.5931</span><span class="p">,</span> <span class="mf">0.7160</span><span class="p">,</span> <span class="mf">0.4968</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.7977</span><span class="p">,</span> <span class="mf">0.0899</span><span class="p">,</span> <span class="mf">0.9572</span><span class="p">,</span> <span class="mf">0.0146</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.2804</span><span class="p">,</span> <span class="mf">0.8569</span><span class="p">,</span> <span class="mf">0.2292</span><span class="p">,</span> <span class="mf">0.1118</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5747</span><span class="p">,</span> <span class="mf">0.4064</span><span class="p">,</span> <span class="mf">0.8370</span><span class="p">,</span> <span class="mf">0.1611</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0146</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.9746</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bins</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">min</span><span class="o">=</span><span class="n">min_value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">max</span><span class="o">=</span><span class="n">max_value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>  <span class="o">&gt;&gt;&gt;</span> <span class="n">cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cdf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">17</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">38</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ifmr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_value</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_value</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cdf</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_percentile</span><span class="o">=</span><span class="mf">0.999999</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_percentile</span><span class="o">=</span><span class="mf">0.999999</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">search_start</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">search_end</span><span class="o">=</span><span class="mf">1.3</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">search_step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">with_offset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">0.0080</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_indexing">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_indexing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_indexing" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_indexing(self, begin, end, strides, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä½¿ç”¨â€œbegin,end,stridesâ€æ•°ç»„å¯¹indexç»“æœè¿›è¡Œè®¡æ•°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>begin (ListInt) - å¾…é€‰æ‹©çš„ç¬¬ä¸€ä¸ªå€¼çš„indexã€‚</p>
<p>end (ListInt) - å¾…é€‰æ‹©çš„æœ€åä¸€ä¸ªå€¼çš„indexã€‚</p>
<p>strides (ListInt) - indexå¢é‡ã€‚</p>
<p>begin_mask (Intï¼Œé»˜è®¤å€¼ä¸º0) - ä½æ©ç (bitmask)ï¼Œå…¶ä¸­ä½â€œiâ€ä¸ºâ€œ1â€æ„å‘³ç€å¿½ç•¥å¼€å§‹å€¼ï¼Œå°½å¯èƒ½ä½¿ç”¨æœ€å¤§é—´éš”ã€‚</p>
<p>end_mask (Intï¼Œé»˜è®¤å€¼ä¸º0) - ç±»ä¼¼äºâ€œbegin_maskâ€ã€‚</p>
<p>ellipsis_mask (Intï¼Œé»˜è®¤å€¼ä¸º0) - ä½æ©ç ï¼Œå…¶ä¸­ä½â€œiâ€ä¸ºâ€œ1â€æ„å‘³ç€ç¬¬â€œiâ€ä¸ªä½ç½®å®é™…ä¸Šæ˜¯çœç•¥å·ã€‚</p>
<p>new_axis_mask (Intï¼Œé»˜è®¤å€¼ä¸º0) - ä½æ©ç ï¼Œå…¶ä¸­ä½â€œiâ€ä¸ºâ€œ1â€æ„å‘³ç€åœ¨ç¬¬â€œiâ€ä½åˆ›å»ºæ–°çš„1D shapeã€‚</p>
<p>shrink_axis_mask (Intï¼Œé»˜è®¤å€¼ä¸º0) - ä½æ©ç ï¼Œå…¶ä¸­ä½â€œiâ€æ„å‘³ç€ç¬¬â€œiâ€ä½åº”ç¼©å°ç»´æ•°ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_indexing</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_iou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_iou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_iou" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_iou(bboxes, gtboxes, mode=0) -&gt; Tensor</p>
<p>torch_npu.npu_ptiou(bboxes, gtboxes, mode=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ ¹æ®ground-truthå’Œé¢„æµ‹åŒºåŸŸè®¡ç®—äº¤å¹¶æ¯”(IoU)æˆ–å‰æ™¯äº¤å‰æ¯”(IoF)ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>bboxes (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>gtboxes (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - 0ä¸ºIoUæ¨¡å¼ï¼Œ1ä¸ºIoFæ¨¡å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">42</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gtboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_iou</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">gtboxes</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4985</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.9961</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_layer_norm_eval">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_layer_norm_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_layer_norm_eval" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_layer_norm_eval(input, normalized_shape, weight=None, bias=None, eps=1e-05) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å¯¹å±‚å½’ä¸€åŒ–ç»“æœè¿›è¡Œè®¡æ•°ã€‚ä¸torch.nn.functional.layer_normç›¸åŒ, ä¼˜åŒ–NPUè®¾å¤‡å®ç°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>normalized_shape (ListInt) - sizeä¸ºé¢„æœŸè¾“å…¥çš„è¾“å…¥shapeã€‚</p>
<p>weight (Tensor, å¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºNone) - gammaå¼ é‡ã€‚</p>
<p>bias (Tensor, å¯é€‰é»˜è®¤å€¼ä¸ºNone) - betaå¼ é‡ã€‚</p>
<p>eps (Floatï¼Œé»˜è®¤å€¼ä¸º1e-5) - ä¸ºä¿è¯æ•°å€¼ç¨³å®šæ€§æ·»åŠ åˆ°åˆ†æ¯ä¸­çš„Îµå€¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1863</span><span class="p">,</span> <span class="mf">0.3755</span><span class="p">,</span> <span class="mf">0.1115</span><span class="p">,</span> <span class="mf">0.7308</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6004</span><span class="p">,</span> <span class="mf">0.6832</span><span class="p">,</span> <span class="mf">0.8951</span><span class="p">,</span> <span class="mf">0.2087</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.8548</span><span class="p">,</span> <span class="mf">0.0176</span><span class="p">,</span> <span class="mf">0.8498</span><span class="p">,</span> <span class="mf">0.3703</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5609</span><span class="p">,</span> <span class="mf">0.0114</span><span class="p">,</span> <span class="mf">0.5021</span><span class="p">,</span> <span class="mf">0.1242</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3966</span><span class="p">,</span> <span class="mf">0.3022</span><span class="p">,</span> <span class="mf">0.2323</span><span class="p">,</span> <span class="mf">0.3914</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1554</span><span class="p">,</span> <span class="mf">0.0149</span><span class="p">,</span> <span class="mf">0.1718</span><span class="p">,</span> <span class="mf">0.4972</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">normalized_shape</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">6.1223e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.3159e-20</span><span class="p">,</span>  <span class="mf">9.1834e-41</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">normalized_shape</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.6033e-39</span><span class="p">,</span> <span class="mf">6.1224e-41</span><span class="p">,</span> <span class="mf">6.1757e-39</span><span class="p">,</span> <span class="mf">6.1224e-41</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_layer_norm_eval</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">6.7474e-41</span><span class="p">,</span>  <span class="mf">8.3182e-20</span><span class="p">,</span>  <span class="mf">2.0687e-40</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">8.2494e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.9784e-20</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.2186e-41</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6695e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.7173e-20</span><span class="p">,</span>  <span class="mf">2.1353e-41</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3497e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1281e-20</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9827e-42</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">3.5663e-41</span><span class="p">,</span>  <span class="mf">1.2002e-19</span><span class="p">,</span>  <span class="mf">1.4314e-40</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.2792e-42</span><span class="p">,</span>  <span class="mf">1.7902e-20</span><span class="p">,</span>  <span class="mf">2.1050e-40</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_linear">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_linear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_linear" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_linear(input, weight, bias=None) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å°†çŸ©é˜µâ€œaâ€ä¹˜ä»¥çŸ©é˜µâ€œbâ€ï¼Œç”Ÿæˆâ€œa*bâ€ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - 2DçŸ©é˜µå¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat32ã€float16ã€int32ã€int8ã€‚æ ¼å¼ï¼š[ND, NHWC, FRACTAL_NZ]ã€‚</p>
<p>weight (Tensor) - 2DçŸ©é˜µå¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat32ã€float16ã€int32ã€int8ã€‚æ ¼å¼ï¼š[ND, NHWC, FRACTAL_NZ]ã€‚</p>
<p>bias (Tensorï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºNone) - 1Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat32ã€float16ã€int32ã€‚æ ¼å¼ï¼š[ND, NHWC]ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.6335</span><span class="p">,</span> <span class="mf">4.3713</span><span class="p">,</span> <span class="mf">2.4440</span><span class="p">,</span> <span class="mf">2.0081</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.3273</span><span class="p">,</span> <span class="mf">6.3089</span><span class="p">,</span> <span class="mf">3.9601</span><span class="p">,</span> <span class="mf">3.2410</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_lstm">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_lstm</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_lstm" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_lstm(x, weight, bias, seqMask, h, c, has_biases, num_layers, dropout, train, bidirectional, batch_first, flag_seq, direction)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—DynamicRNNã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>weight (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_ZN_LSTMã€‚</p>
<p>bias (Tensor) - 1Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šNDã€‚</p>
<p>seqMask (Tensor) - å¼ é‡ã€‚ä»…æ”¯æŒä¸ºFRACTAL_NZæ ¼å¼çš„float16å’ŒNDæ ¼å¼çš„int32ç±»å‹ã€‚</p>
<p>h (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>c (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>has_biases (Bool) - å¦‚æœå€¼ä¸ºTrue,åˆ™å­˜åœ¨åå·®ã€‚</p>
<p>num_layers (Int) - å¾ªç¯å±‚æ•°ï¼Œç›®å‰åªæ”¯æŒå•å±‚ã€‚</p>
<p>dropout (Float) - å¦‚æœå€¼ä¸ºéé›¶ï¼Œåˆ™åœ¨é™¤æœ€åä¸€å±‚å¤–çš„æ¯ä¸ªLSTMå±‚çš„è¾“å‡ºä¸Šå¼•å…¥ä¸€ä¸ªdropoutå±‚ï¼Œä¸¢å¼ƒæ¦‚ç‡ç­‰äºdropoutå‚æ•°å€¼ã€‚ç›®å‰ä¸æ”¯æŒã€‚</p>
<p>train (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue) - æ ‡è¯†è®­ç»ƒæ˜¯å¦åœ¨opè¿›è¡Œçš„boolå‚æ•°ã€‚</p>
<p>bidirectional (Bool) - å¦‚æœå€¼ä¸ºTrueï¼ŒLSTMä¸ºåŒå‘ã€‚å½“å‰ä¸æ”¯æŒã€‚</p>
<p>batch_first (Bool) - å¦‚æœå€¼ä¸ºTrueï¼Œåˆ™è¾“å…¥å’Œè¾“å‡ºå¼ é‡å°†è¡¨ç¤ºä¸º(batch, seq, feature)ã€‚å½“å‰ä¸æ”¯æŒã€‚</p>
<p>flag_seq (Bool) - å¦‚æœå€¼ä¸ºTrueï¼Œè¾“å…¥ä¸ºPackSequnceã€‚å½“å‰ä¸æ”¯æŒã€‚</p>
<p>direction (Bool) - å¦‚æœå€¼ä¸ºTrueï¼Œåˆ™æ–¹å‘ä¸ºâ€œREDIRECTIONALâ€ï¼Œå¦åˆ™ä¸ºâ€œUNIDIRECTIONALâ€ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>y (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>output_h (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>output_c (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>i (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>j (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>f (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>o (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<p>tanhct (Tensor) - 4Då¼ é‡ã€‚æ•°æ®ç±»å‹ï¼šfloat16, float32ï¼›æ ¼å¼ï¼šFRACTAL_NZã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_masked_fill_range">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_masked_fill_range</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_masked_fill_range" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_masked_fill_range(self, start, end, value, axis=-1) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>åŒè½´ä¸Šè¢«range.boxeså±è”½(masked)çš„å¡«å……å¼ é‡ã€‚è‡ªå®šä¹‰å±è”½å¡«å……èŒƒå›´ç®—å­ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - shapeä¸º1D (D,)ã€2D (N,D)æˆ–3D (N,D)çš„float32/float16/int32/int8 NDå¼ é‡ã€‚</p>
<p>start (Tensor) - å±è”½å¡«å……å¼€å§‹ä½ç½®ã€‚shapeä¸º(num,N)çš„int32 3Då¼ é‡ã€‚</p>
<p>end (Tensor) - å±è”½å¡«å……ç»“æŸä½ç½®ã€‚shapeä¸º(num,N)çš„int32 3Då¼ é‡ã€‚</p>
<p>value (Tensor) - å±è”½å¡«å……å€¼ã€‚shapeä¸º(num,)çš„float32/float16/int32/int8 2Då¼ é‡ã€‚</p>
<p>axis (Intï¼Œé»˜è®¤å€¼ä¸º-1) - å¸¦æœ‰int32å±è”½å¡«å……çš„è½´ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9419</span><span class="p">,</span> <span class="mf">0.4919</span><span class="p">,</span> <span class="mf">0.2874</span><span class="p">,</span> <span class="mf">0.6560</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6691</span><span class="p">,</span> <span class="mf">0.6668</span><span class="p">,</span> <span class="mf">0.0330</span><span class="p">,</span> <span class="mf">0.1006</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3888</span><span class="p">,</span> <span class="mf">0.7011</span><span class="p">,</span> <span class="mf">0.7141</span><span class="p">,</span> <span class="mf">0.7878</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0366</span><span class="p">,</span> <span class="mf">0.9738</span><span class="p">,</span> <span class="mf">0.4689</span><span class="p">,</span> <span class="mf">0.0979</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_masked_fill_range</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.4919</span><span class="p">,</span> <span class="mf">0.2874</span><span class="p">,</span> <span class="mf">0.6560</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6691</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.0330</span><span class="p">,</span> <span class="mf">0.1006</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3888</span><span class="p">,</span> <span class="mf">0.7011</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.7878</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0366</span><span class="p">,</span> <span class="mf">0.9738</span><span class="p">,</span> <span class="mf">0.4689</span><span class="p">,</span> <span class="mf">0.0979</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_max">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_max</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_max" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_max(self, dim, keepdim=False) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä½¿ç”¨dimå¯¹æœ€å¤§ç»“æœè¿›è¡Œè®¡æ•°ã€‚ç±»ä¼¼äºtorch.max, ä¼˜åŒ–NPUè®¾å¤‡å®ç°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>dim (Int) - å¾…é™ä½ç»´åº¦ã€‚</p>
<p>keepdim (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - è¾“å‡ºå¼ é‡æ˜¯å¦ä¿ç•™dimã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>values (Tensor) - è¾“å…¥å¼ é‡ä¸­çš„æœ€å¤§å€¼ã€‚</p>
<p>indices (Tensor) - è¾“å…¥å¼ é‡ä¸­æœ€å¤§å€¼çš„indexã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">1.8135</span><span class="p">,</span>  <span class="mf">0.2078</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.6678</span><span class="p">,</span>  <span class="mf">0.7846</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.6458</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0923</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.2124</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9112</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="o">-</span><span class="mf">0.5800</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4979</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2580</span><span class="p">,</span>  <span class="mf">1.1335</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.6669</span><span class="p">,</span>  <span class="mf">0.1876</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.1160</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1061</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.6678</span><span class="p">,</span>  <span class="mf">0.7846</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6458</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0923</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.2580</span><span class="p">,</span>  <span class="mf">1.1335</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6669</span><span class="p">,</span>  <span class="mf">0.1876</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_min">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_min</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_min" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_min(self, dim, keepdim=False) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä½¿ç”¨dimå¯¹æœ€å°ç»“æœè¿›è¡Œè®¡æ•°ã€‚ç±»ä¼¼äºtorch.min, ä¼˜åŒ–NPUè®¾å¤‡å®ç°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>dim (Int) - å¾…é™ä½ç»´åº¦ã€‚</p>
<p>keepdim (Bool) - è¾“å‡ºå¼ é‡æ˜¯å¦ä¿ç•™dimã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>values (Tensor) - è¾“å…¥å¼ é‡ä¸­çš„æœ€å°å€¼ã€‚</p>
<p>indices (Tensor) - è¾“å…¥å¼ é‡ä¸­æœ€å°å€¼çš„indexã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">0.9909</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2369</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.9569</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6223</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.1157</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3147</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.7761</span><span class="p">,</span>  <span class="mf">0.1344</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span> <span class="mf">1.6292</span><span class="p">,</span>  <span class="mf">0.5953</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6940</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6367</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.2335</span><span class="p">,</span>  <span class="mf">0.2131</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0748</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7046</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_min</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.9909</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6223</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.7761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3147</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.6940</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6367</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.2335</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7046</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_mish">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_mish</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_mish" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>æŒ‰å…ƒç´ è®¡ç®—selfçš„åŒæ›²æ­£åˆ‡ã€‚</p>
<p><strong>å‚æ•°è§£é‡Š</strong>ï¼š</p>
<p>self (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ã€float32ã€‚</p>
<p>çº¦æŸæ¡ä»¶ï¼š</p>
<p>æ— </p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_mish</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_multi_head_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_multi_head_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_multi_head_attention" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor query_bias, Tensor key_bia, Tensor value_bias, Tensor out_proj_bias, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å®ç°Transformeræ¨¡å—ä¸­çš„MultiHeadAttentionè®¡ç®—é€»è¾‘ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>query: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>key: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>value: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>query_weight: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>key_weight: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>value_weight: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>attn_mask: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>out_proj_weight: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>query_bias: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>key_bias: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>value_bias: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>out_proj _bias: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>dropout_mask_input: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>attn_head_numï¼š Attention Head numbers, Intå‹</p>
<p>attn_dim_per_headï¼šAttention dim of a Head , Intå‹</p>
<p>src_lenï¼šsource length, Intå‹</p>
<p>tgt_lenï¼štarget length, Intå‹</p>
<p>keep_probï¼šdropout keep probability, Floatå‹</p>
<p>softmax_use_floatï¼šSoftMax Use Float32 to keep precision, Boolå‹</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>y: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>dropout_mask: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>query_res: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>key_res: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>value_res: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>attn_scores: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>attn_res: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p>context: Tensorç±»å‹ï¼Œä»…æ”¯æŒfloat16</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>Attr attn_head_numï¼šéœ€16å¯¹é½</p>
<p>Attr attn_dim_per_headï¼šéœ€16å¯¹é½</p>
<p>Attr src_lenï¼šéœ€16å¯¹é½</p>
<p>tgt_lenï¼šéœ€16å¯¹é½</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_head_num</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_dim_per_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src_len</span> <span class="o">=</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tgt_len</span> <span class="o">=</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax_use_float</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight_col</span> <span class="o">=</span> <span class="n">attn_head_num</span> <span class="o">*</span> <span class="n">attn_dim_per_head</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_proj_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">attn_head_num</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_proj_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_mask_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_result</span><span class="p">,</span> <span class="n">npu_dropout_mask</span><span class="p">,</span> <span class="n">npu_query_res</span><span class="p">,</span> <span class="n">npu_key_res</span><span class="p">,</span> <span class="n">npu_value_res</span><span class="p">,</span> <span class="n">npu_attn_scores</span><span class="p">,</span> <span class="n">npu_attn_res</span><span class="p">,</span> <span class="n">npu_context</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_multi_head_attention</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">query_weight</span><span class="p">,</span> <span class="n">key_weight</span><span class="p">,</span> <span class="n">value_weight</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">query_bias</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">,</span> <span class="n">value_bias</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">,</span>  <span class="n">dropout_mask_input</span><span class="p">,</span> <span class="n">attn_head_num</span><span class="p">,</span> <span class="n">attn_dim_per_head</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="p">,</span> <span class="n">softmax_use_float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">npu_result</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">623.5000</span><span class="p">,</span>   <span class="mf">75.5000</span><span class="p">,</span>  <span class="mf">307.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">25.3125</span><span class="p">,</span> <span class="o">-</span><span class="mf">418.7500</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">35.9688</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">254.2500</span><span class="p">,</span> <span class="o">-</span><span class="mf">165.6250</span><span class="p">,</span>  <span class="mf">176.2500</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">87.3750</span><span class="p">,</span>   <span class="mf">78.0000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">65.2500</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">233.2500</span><span class="p">,</span>  <span class="mf">207.3750</span><span class="p">,</span>  <span class="mf">324.7500</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">38.6250</span><span class="p">,</span> <span class="o">-</span><span class="mf">264.2500</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">153.7500</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">110.2500</span><span class="p">,</span>  <span class="o">-</span><span class="mf">92.5000</span><span class="p">,</span>  <span class="o">-</span><span class="mf">74.0625</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="o">-</span><span class="mf">68.0625</span><span class="p">,</span>  <span class="mf">195.6250</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">157.6250</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">300.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">184.6250</span><span class="p">,</span>   <span class="o">-</span><span class="mf">6.0039</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="o">-</span><span class="mf">15.7969</span><span class="p">,</span> <span class="o">-</span><span class="mf">299.0000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">93.1875</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>  <span class="o">-</span><span class="mf">2.5996</span><span class="p">,</span>   <span class="mf">36.8750</span><span class="p">,</span>  <span class="mf">100.0625</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">112.7500</span><span class="p">,</span>  <span class="mf">202.0000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">166.3750</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_nms_rotated">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_nms_rotated</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_nms_rotated" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_nms_rotated(dets, scores, iou_threshold, scores_threshold=0, max_output_size=-1, mode=0) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æŒ‰åˆ†æ•°é™åºé€‰æ‹©æ—‹è½¬æ ‡æ³¨æ¡†çš„å­é›†ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>dets (Tensor) - shapeä¸º[num_boxes, 5]çš„2Dæµ®ç‚¹å¼ é‡</p>
<p>scores (Tensor) - shapeä¸º[num_boxes]çš„1Dæµ®ç‚¹å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæ¡†(æ¯è¡Œæ¡†)å¯¹åº”çš„ä¸€ä¸ªåˆ†æ•°ã€‚</p>
<p>iou_threshold (Float) - è¡¨ç¤ºæ¡†ä¸IoUé‡å ä¸Šé™é˜ˆå€¼çš„æ ‡é‡ã€‚</p>
<p>scores_threshold (Floatï¼Œé»˜è®¤å€¼ä¸º0) - è¡¨ç¤ºå†³å®šä½•æ—¶åˆ é™¤æ¡†çš„åˆ†æ•°é˜ˆå€¼çš„æ ‡é‡ã€‚</p>
<p>max_output_size (Intï¼Œé»˜è®¤å€¼ä¸º-1) - æ ‡é‡æ•´æ•°å¼ é‡ï¼Œè¡¨ç¤ºéæœ€å¤§æŠ‘åˆ¶ä¸‹è¦é€‰æ‹©çš„æœ€å¤§æ¡†æ•°ã€‚ä¸º-1æ—¶å³ä¸æ–½åŠ ä»»ä½•çº¦æŸã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - æŒ‡å®šdetså¸ƒå±€ç±»å‹ã€‚å¦‚æœmodeè®¾ç½®ä¸º0ï¼Œåˆ™detsçš„è¾“å…¥å€¼ä¸ºxã€yã€wã€hå’Œè§’åº¦ã€‚å¦‚æœmodeè®¾ç½®ä¸º1ï¼Œåˆ™detsçš„è¾“å…¥å€¼ä¸ºx1ã€y1ã€x2ã€y2å’Œè§’åº¦ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>selected_index (Tensor) - shapeä¸º[M]çš„1Dæ•´æ•°å¼ é‡ï¼Œè¡¨ç¤ºä»detså¼ é‡ä¸­é€‰å®šçš„indexï¼Œå…¶ä¸­M &lt;= max_output_sizeã€‚</p>
<p>selected_num (Tensor) - 0Dæ•´æ•°å¼ é‡ï¼Œè¡¨ç¤ºselected_indicesä¸­æœ‰æ•ˆå…ƒç´ çš„æ•°é‡ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>ç›®å‰ä¸æ”¯æŒmode=1çš„åœºæ™¯ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dets</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dets</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_nms_rotated</span><span class="p">(</span><span class="n">dets</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">76</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">55</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">79</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">88</span><span class="p">,</span> <span class="mi">83</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">64</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">86</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">71</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span> <span class="mi">52</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2tensor</span><span class="p">([</span><span class="mi">62</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_nms_v4">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_nms_v4</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_nms_v4" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_nms_v4(boxes, scores, max_output_size, iou_threshold, scores_threshold, pad_to_max_output_size=False) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æŒ‰åˆ†æ•°é™åºé€‰æ‹©æ ‡æ³¨æ¡†çš„å­é›†ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>boxes (Tensor) - shapeä¸º[num_boxes, 4]çš„2Dæµ®ç‚¹å¼ é‡ã€‚</p>
<p>scores (Tensor) - shapeä¸º[num_boxes]çš„1Dæµ®ç‚¹å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæ¡†(æ¯è¡Œæ¡†)å¯¹åº”çš„ä¸€ä¸ªåˆ†æ•°ã€‚</p>
<p>max_output_size (Scalar) - è¡¨ç¤ºnon-max suppressionä¸‹è¦é€‰æ‹©çš„æœ€å¤§æ¡†æ•°çš„æ ‡é‡ã€‚</p>
<p>iou_threshold (Tensor) - 0Dæµ®ç‚¹å¼ é‡ï¼Œè¡¨ç¤ºæ¡†ä¸IoUé‡å ä¸Šé™çš„é˜ˆå€¼ã€‚</p>
<p>scores_threshold (Tensor) - 0Dæµ®ç‚¹å¼ é‡ï¼Œè¡¨ç¤ºå†³å®šä½•æ—¶åˆ é™¤æ¡†çš„åˆ†æ•°é˜ˆå€¼ã€‚</p>
<p>pad_to_max_output_size (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - å¦‚æœä¸ºTrueï¼Œåˆ™è¾“å‡ºçš„selected_indiceså°†å¡«å……ä¸ºmax_output_sizeé•¿åº¦ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>selected_indices (Tensor) - shapeä¸º[M]çš„1Dæ•´æ•°å¼ é‡ï¼Œè¡¨ç¤ºä»boxeså¼ é‡ä¸­é€‰å®šçš„indexï¼Œå…¶ä¸­M &lt;= max_output_sizeã€‚</p>
<p>valid_outputs (Tensor) - 0Dæ•´æ•°å¼ é‡ï¼Œè¡¨ç¤ºselected_indicesä¸­æœ‰æ•ˆå…ƒç´ çš„æ•°é‡ï¼Œæœ‰æ•ˆå…ƒç´ é¦–å…ˆå‘ˆç°ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_output_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iou_threshold</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores_threshold</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_nms_v4</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">max_output_size</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">scores_threshold</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">57</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">94</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">54</span><span class="p">,</span> <span class="mi">92</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">tensor</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_nms_with_mask">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_nms_with_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_nms_with_mask" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_nms_with_mask(input, iou_threshold) -&gt; (Tensor, Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ç”Ÿæˆå€¼0æˆ–1ï¼Œç”¨äºnmsç®—å­ç¡®å®šæœ‰æ•ˆä½ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - è¾“å…¥å¼ é‡</p>
<p>iou_threshold (Scalar) - é˜ˆå€¼ã€‚å¦‚æœè¶…è¿‡æ­¤é˜ˆå€¼ï¼Œåˆ™å€¼ä¸º1ï¼Œå¦åˆ™å€¼ä¸º0ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>selected_boxes (Tensor) - shapeä¸º[N,5]çš„2Då¼ é‡ï¼Œè¡¨ç¤ºfiltered boxï¼ŒåŒ…æ‹¬proposal boxå’Œç›¸åº”çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚</p>
<p>selected_idx (Tensor) - shapeä¸º[N]çš„1Då¼ é‡ï¼Œè¡¨ç¤ºè¾“å…¥å»ºè®®æ¡†çš„indexã€‚</p>
<p>selected_mask (Tensor) - shapeä¸º[N]çš„1Då¼ é‡ï¼Œåˆ¤æ–­è¾“å‡ºå»ºè®®æ¡†æ˜¯å¦æœ‰æ•ˆã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>è¾“å…¥box_scoresçš„2nd-dimå¿…é¡»ç­‰äº8ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iou_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">output3</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_nms_with_mask</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">2.0000</span><span class="p">,</span> <span class="mf">3.0000</span><span class="p">,</span> <span class="mf">0.6001</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.0000</span><span class="p">,</span> <span class="mf">7.0000</span><span class="p">,</span> <span class="mf">8.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">0.3999</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span>      <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_normalize_batch">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_normalize_batch</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_normalize_batch" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_normalize_batch(self, seq_len, normalize_type=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ‰§è¡Œæ‰¹é‡å½’ä¸€åŒ–ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ”¯æŒfloat32æ•°æ®ç±»å‹ï¼Œshapeä¸º(n, c, d)ã€‚</p>
<p>seq_len (Tensor) - æ”¯æŒInt32æ•°æ®ç±»å‹ï¼Œshapeä¸º(n, )ï¼Œ è¡¨ç¤ºæ¯æ‰¹æ¬¡æ ‡å‡†åŒ–æ•°æ®é‡ ã€‚</p>
<p>normalize_type (Intï¼Œé»˜è®¤å€¼ä¸º0) - æ”¯æŒ &quot;per_feature&quot;æˆ–&quot;all_features&quot;ã€‚å€¼ä¸º0è¡¨ç¤º &quot;per_feature&quot;ï¼Œå€¼ä¸º1è¡¨ç¤º&quot;all_features&quot;ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seqlen</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_normalize_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.1496</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4812</span><span class="p">,</span>  <span class="mf">1.7611</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5187</span><span class="p">,</span>  <span class="mf">0.7571</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1445</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4393</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7051</span><span class="p">,</span>  <span class="mf">1.0474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2646</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1582</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.1477</span><span class="p">,</span>  <span class="mf">0.9179</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0656</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8692</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7437</span><span class="p">,</span>  <span class="mf">2.8621</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.6880</span><span class="p">,</span>  <span class="mf">0.1337</span><span class="p">,</span>  <span class="mf">1.3623</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8081</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2291</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9410</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3070</span><span class="p">,</span>  <span class="mf">0.5489</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4858</span><span class="p">,</span>  <span class="mf">0.6300</span><span class="p">,</span>  <span class="mf">0.6428</span><span class="p">,</span>  <span class="mf">0.0433</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.5387</span><span class="p">,</span>  <span class="mf">0.8204</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1401</span><span class="p">,</span>  <span class="mf">0.8584</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3686</span><span class="p">,</span>  <span class="mf">0.8444</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_one_hot">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_one_hot</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_one_hot" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_one_hot(input, num_classes=-1, depth=1, on_value=1, off_value=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è¿”å›ä¸€ä¸ªone-hotå¼ é‡ã€‚inputä¸­indexè¡¨ç¤ºçš„ä½ç½®é‡‡ç”¨on_valueå€¼ï¼Œè€Œå…¶ä»–æ‰€æœ‰ä½ç½®é‡‡ç”¨off_valueçš„å€¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - ä»»ä½•shapeçš„classå€¼ã€‚</p>
<p>num_classes (Intï¼Œé»˜è®¤å€¼ä¸º-1) - å¾…å¡«å……çš„è½´ã€‚</p>
<p>depth (Intï¼Œé»˜è®¤å€¼ä¸º1) - one_hotç»´åº¦çš„æ·±åº¦ã€‚</p>
<p>on_value (Scalarï¼Œé»˜è®¤å€¼ä¸º1) - å½“indices[j] == iæ—¶è¾“å‡ºä¸­çš„å¡«å……å€¼ã€‚</p>
<p>off_value (Scalarï¼Œé»˜è®¤å€¼ä¸º0) - å½“indices[j] != iæ—¶è¾“å‡ºä¸­çš„å¡«å……å€¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_one_hot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">btensor</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_pad">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_pad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_pad" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_pad(input, paddings) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å¡«å……å¼ é‡ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>input (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>paddings (ListInt) - æ•°æ®ç±»å‹ï¼šint32ã€int64ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paddings</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ps_roi_pooling">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ps_roi_pooling</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ps_roi_pooling" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ps_roi_pooling(x, rois, spatial_scale, group_size, output_dim) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ‰§è¡ŒPosition Sensitive ROI Poolingã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x (Tensor) - æè¿°ç‰¹å¾å›¾çš„NC1HWC0å¼ é‡ã€‚ç»´åº¦C1å¿…é¡»ç­‰äº(int(output_dim+15)/C0)) group_sizeã€‚</p>
<p>rois (Tensor) - shapeä¸º[batch, 5, rois_num]çš„å¼ é‡ï¼Œç”¨äºæè¿°ROIã€‚æ¯ä¸ªROIç”±äº”ä¸ªå…ƒç´ ç»„æˆï¼šâ€œbatch_idâ€ã€â€œx1â€ã€â€œy1â€ã€â€œx2â€å’Œâ€œy2â€ï¼Œå…¶ä¸­â€œbatch_idâ€è¡¨ç¤ºè¾“å…¥ç‰¹å¾å›¾çš„indexï¼Œâ€œx1â€ã€â€œy1â€ã€â€œx2â€ï¼Œå’Œâ€œy2â€å¿…é¡»å¤§äºæˆ–ç­‰äºâ€œ0.0â€ã€‚</p>
<p>spatial_scale (Float32) - å°†è¾“å…¥åæ ‡æ˜ å°„åˆ°ROIåæ ‡çš„ç¼©æ”¾ç³»æ•°ã€‚</p>
<p>group_size (Int32) - æŒ‡å®šç”¨äºç¼–ç position-sensitiveè¯„åˆ†å›¾çš„ç»„æ•°ã€‚è¯¥å€¼å¿…é¡»åœ¨(0,128)èŒƒå›´å†…ã€‚</p>
<p>output_dim (Int32) - æŒ‡å®šè¾“å‡ºé€šé“æ•°ã€‚å¿…é¡»å¤§äº0ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">roi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">4</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mi">5</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">6</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">7</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">8</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span> <span class="mi">9</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">10</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">11</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">12</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">13</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">14</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">15</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">16</span><span class="p">]]]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ps_roi_pooling</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outtensor</span><span class="p">([[[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ptiou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ptiou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ptiou" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ptiou(bboxes, gtboxes, mode=0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ ¹æ®ground-truthå’Œé¢„æµ‹åŒºåŸŸè®¡ç®—äº¤å¹¶æ¯”(IoU)æˆ–å‰æ™¯äº¤å‰æ¯”(IoF)ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>bboxes (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>gtboxes (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - 0ä¸ºIoUæ¨¡å¼ï¼Œ1ä¸ºIoFæ¨¡å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">42</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gtboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_iou</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">gtboxes</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4985</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.9961</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_random_choice_with_mask">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_random_choice_with_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_random_choice_with_mask" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_random_choice_with_mask(x, count=256, seed=0, seed2=0) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ··æ´—éé›¶å…ƒç´ çš„indexã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>count (Intï¼Œé»˜è®¤å€¼ä¸º256) - è¾“å‡ºè®¡æ•°ã€‚å¦‚æœå€¼ä¸º0ï¼Œåˆ™è¾“å‡ºæ‰€æœ‰éé›¶å…ƒç´ ã€‚</p>
<p>seed (Intï¼Œé»˜è®¤å€¼ä¸º0) - æ•°æ®ç±»å‹ï¼šint32ï¼Œint64ã€‚</p>
<p>seed2 (Intï¼Œé»˜è®¤å€¼ä¸º2) - æ•°æ®ç±»å‹ï¼šint32ï¼Œint64ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>y (Tensor) - 2Då¼ é‡, éé›¶å…ƒç´ çš„indexã€‚</p>
<p>mask (Tensor) - 1Då¼ é‡, ç¡®å®šå¯¹åº”indexæ˜¯å¦æœ‰æ•ˆã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_random_choice_with_mask</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">resulttensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_reshape">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_reshape</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_reshape" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_reshape(self, shape, bool can_refresh=False) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>reshapeå¼ é‡ã€‚ä»…æ›´æ”¹å¼ é‡shapeï¼Œå…¶æ•°æ®ä¸å˜ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>shape (ListInt) - å®šä¹‰è¾“å‡ºå¼ é‡çš„shapeã€‚</p>
<p>can_refresh (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - æ˜¯å¦å°±åœ°åˆ·æ–°reshapeã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>è¯¥è¿ç®—ç¬¦ä¸èƒ½è¢«aclopExecute APIç›´æ¥è°ƒç”¨ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.6657</span><span class="p">,</span> <span class="mf">0.9857</span><span class="p">,</span> <span class="mf">0.7614</span><span class="p">,</span> <span class="mf">0.4368</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3761</span><span class="p">,</span> <span class="mf">0.4397</span><span class="p">,</span> <span class="mf">0.8609</span><span class="p">,</span> <span class="mf">0.5544</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.7002</span><span class="p">,</span> <span class="mf">0.3063</span><span class="p">,</span> <span class="mf">0.9279</span><span class="p">,</span> <span class="mf">0.5085</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1009</span><span class="p">,</span> <span class="mf">0.7133</span><span class="p">,</span> <span class="mf">0.8118</span><span class="p">,</span> <span class="mf">0.6193</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rms_norm">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rms_norm</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rms_norm" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rms_norm(Tensor self, Tensor gamma, float epsilon=1e-06) -&gt; (Tensor, Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>RmsNormç®—å­æ˜¯å¤§æ¨¡å‹å¸¸ç”¨çš„å½’ä¸€åŒ–æ“ä½œï¼Œç›¸æ¯”LayerNormç®—å­ï¼Œå…¶å»æ‰äº†å‡å»å‡å€¼çš„éƒ¨åˆ†ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>selfï¼šTensorç±»å‹ï¼Œæ”¯æŒfloat16ã€bfloat16ã€float32ï¼Œè¾“å…¥shapeæ”¯æŒ2-8ç»´ã€‚</p>
<p>gammaï¼šTensorç±»å‹ï¼Œæ•°æ®ç±»å‹éœ€è¦å’Œselfä¿æŒä¸€è‡´ï¼Œè¾“å…¥shapeæ”¯æŒ2-8ç»´ï¼Œé€šå¸¸ä¸ºselfçš„æœ€åä¸€ç»´ã€‚</p>
<p>epsilonï¼šfloatæ•°æ®ç±»å‹ï¼Œç”¨äºé˜²æ­¢é™¤0é”™è¯¯ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong></p>
<p>å…±ä¸¤ä¸ªè¾“å‡ºï¼Œæ ¼å¼ä¸ºï¼š (Tensor, Tensor)</p>
<p>ç¬¬1ä¸ªè¾“å‡ºä¸ºTensorï¼Œè®¡ç®—å…¬å¼çš„æœ€ç»ˆè¾“å‡ºyï¼›</p>
<p>ç¬¬2ä¸ªè¾“å‡ºä¸ºTensorï¼Œrms_normçš„reverse rmsä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>è¾“å…¥æ•°æ®ç±»å‹ä»…æ”¯æŒfloat16ã€bfloat16å’Œfloat32ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; x = torch.randn(24, 1, 128).bfloat16().npu()
&gt;&gt;&gt; w = torch.randn(128).bfloat16().npu()
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; out1 = torch.npu_rms_norm(x, w, epsilon=1e-5)[0]
&gt;&gt;&gt; print(out1)
&gt;&gt;&gt; tensor([[[-0.1123,  0.3398,  0.0986,  ..., -2.1250, -0.8477, -0.3418]],
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; [[-0.0591,  0.3184, -0.5000,  ...,  1.0312, -1.1719, -0.1621]],
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; [[-0.1445,  0.3828, -0.3438,  ..., -0.9102, -0.5703,  0.0073]],
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; ...,
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; [[-0.1631, -0.3477,  0.4297,  ...,  0.9219,  0.1621,  0.3125]],
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; [[-0.1387,  0.0815,  0.0967,  ...,  1.7109,  0.1455, -0.1406]],
&gt;&gt;&gt; â€‹
&gt;&gt;&gt; [[ 0.0698,  1.3438, -0.0127,  ..., -2.2656, -0.4473,  0.3281]]],
&gt;&gt;&gt; device=&#39;npu:0&#39;, dtype=torch.bfloat16)
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_roi_align">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_roi_align</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_roi_align" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_roi_align(features, rois, spatial_scale, pooled_height, pooled_width, sample_num, roi_end_mode) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä»ç‰¹å¾å›¾ä¸­è·å–ROIç‰¹å¾çŸ©é˜µã€‚è‡ªå®šä¹‰FasterRcnnç®—å­ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>features (Tensor) - 5HDå¼ é‡</p>
<p>rois (Tensor) - ROIä½ç½®ï¼Œshapeä¸º(N, 5)çš„2Då¼ é‡ã€‚â€œNâ€è¡¨ç¤ºROIçš„æ•°é‡ï¼Œâ€œ5â€è¡¨ç¤ºROIæ‰€åœ¨å›¾åƒçš„indexï¼Œåˆ†åˆ«ä¸ºâ€œx0â€ã€â€œy0â€ã€â€œx1â€å’Œâ€œy1â€ã€‚</p>
<p>spatial_scale (Float32) - æŒ‡å®šâ€œfeaturesâ€ä¸åŸå§‹å›¾åƒçš„ç¼©æ”¾æ¯”ç‡ã€‚</p>
<p>pooled_height (Int32) - æŒ‡å®šHç»´åº¦ã€‚</p>
<p>pooled_width (Int32) - æŒ‡å®šWç»´åº¦ã€‚</p>
<p>sample_num (Int32ï¼Œé»˜è®¤å€¼ä¸º2) - æŒ‡å®šæ¯æ¬¡è¾“å‡ºçš„æ°´å¹³å’Œå‚ç›´é‡‡æ ·é¢‘ç‡ã€‚è‹¥æ­¤å±æ€§è®¾ç½®ä¸º0ï¼Œåˆ™é‡‡æ ·é¢‘ç‡ç­‰äºâ€œroisâ€çš„å‘ä¸Šå–æ•´å€¼(ä¸€ä¸ªæµ®ç‚¹æ•°)ã€‚</p>
<p>roi_end_mode (Int32ï¼Œé»˜è®¤å€¼ä¸º1)</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">]]]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rois</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_roi_align</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rois</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">4.5000</span><span class="p">,</span>  <span class="mf">6.5000</span><span class="p">,</span>  <span class="mf">8.5000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">16.5000</span><span class="p">,</span> <span class="mf">18.5000</span><span class="p">,</span> <span class="mf">20.5000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">28.5000</span><span class="p">,</span> <span class="mf">30.5000</span><span class="p">,</span> <span class="mf">32.5000</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotary_mul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotary_mul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotary_mul" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotary_mul(Tensor x, Tensor r1, Tensor r2): -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å®ç°RotaryEmbeddingæ—‹è½¬ä½ç½®ç¼–ç ã€‚æ”¯æŒFakeTensoræ¨¡å¼ã€‚</p>
<p>x1, x2 = torch.chunk(x, 2, -1)</p>
<p>x_new = torch.cat((-x2, x1), dim=-1)</p>
<p>output = r1 * x + r2 * x_new</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>Tensor xï¼š4ç»´å¼ é‡ï¼Œshapeä¸º(B, N, S, D)ã€‚</p>
<p>Tensor r1ï¼š4ç»´å¼ é‡cosè§’åº¦ï¼Œshapeä¸º(X, X, X, D)ï¼Œæ”¯æŒé™¤æœ€åä¸€è½´å¤–ä»»æ„è½´å¹¿æ’­ã€‚</p>
<p>Tensor r2ï¼š4ç»´å¼ é‡sinè§’åº¦ï¼Œshapeä¸º(X, X, X, D)ï¼Œ æ”¯æŒé™¤æœ€åä¸€è½´å¤–ä»»æ„è½´å¹¿æ’­ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>xï¼Œr1ï¼Œr2çš„æœ€åä¸€ç»´å¿…é¡»æ˜¯64çš„å€æ•°ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.8594</span><span class="p">,</span> <span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.9075</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.2126</span><span class="p">,</span> <span class="mf">0.6520</span><span class="p">,</span> <span class="mf">0.2206</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5515</span><span class="p">,</span> <span class="mf">0.3353</span><span class="p">,</span> <span class="mf">0.6568</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3686</span><span class="p">,</span> <span class="mf">0.1457</span><span class="p">,</span> <span class="mf">0.8528</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0504</span><span class="p">,</span> <span class="mf">0.2687</span><span class="p">,</span> <span class="mf">0.4036</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3032</span><span class="p">,</span> <span class="mf">0.8262</span><span class="p">,</span> <span class="mf">0.6302</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0537</span><span class="p">,</span> <span class="mf">0.5141</span><span class="p">,</span> <span class="mf">0.7016</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4948</span><span class="p">,</span> <span class="mf">0.9778</span><span class="p">,</span> <span class="mf">0.8535</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3602</span><span class="p">,</span> <span class="mf">0.7874</span><span class="p">,</span> <span class="mf">0.9913</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1474</span><span class="p">,</span> <span class="mf">0.3422</span><span class="p">,</span> <span class="mf">0.6830</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.4641</span><span class="p">,</span> <span class="mf">0.6254</span><span class="p">,</span> <span class="mf">0.7415</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1834</span><span class="p">,</span> <span class="mf">0.1067</span><span class="p">,</span> <span class="mf">0.7171</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.8084</span><span class="p">,</span> <span class="mf">0.7570</span><span class="p">,</span> <span class="mf">0.4728</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4603</span><span class="p">,</span> <span class="mf">0.4991</span><span class="p">,</span> <span class="mf">0.1723</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0483</span><span class="p">,</span> <span class="mf">0.6931</span><span class="p">,</span> <span class="mf">0.0935</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.7522</span><span class="p">,</span> <span class="mf">0.0054</span><span class="p">,</span> <span class="mf">0.1736</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6196</span><span class="p">,</span> <span class="mf">0.1028</span><span class="p">,</span> <span class="mf">0.7076</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.2745</span><span class="p">,</span> <span class="mf">0.9943</span><span class="p">,</span> <span class="mf">0.6971</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3267</span><span class="p">,</span> <span class="mf">0.3748</span><span class="p">,</span> <span class="mf">0.1232</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0507</span><span class="p">,</span> <span class="mf">0.4302</span><span class="p">,</span> <span class="mf">0.6249</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.2783</span><span class="p">,</span> <span class="mf">0.8262</span><span class="p">,</span> <span class="mf">0.6014</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.8040</span><span class="p">,</span> <span class="mf">0.7986</span><span class="p">,</span> <span class="mf">0.2831</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6035</span><span class="p">,</span> <span class="mf">0.2955</span><span class="p">,</span> <span class="mf">0.7711</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.7464</span><span class="p">,</span> <span class="mf">0.3739</span><span class="p">,</span> <span class="mf">0.6637</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6282</span><span class="p">,</span> <span class="mf">0.7243</span><span class="p">,</span> <span class="mf">0.5445</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3755</span><span class="p">,</span> <span class="mf">0.0533</span><span class="p">,</span> <span class="mf">0.9468</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5179</span><span class="p">,</span> <span class="mf">0.3967</span><span class="p">,</span> <span class="mf">0.6558</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0267</span><span class="p">,</span> <span class="mf">0.5549</span><span class="p">,</span> <span class="mf">0.9707</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4388</span><span class="p">,</span> <span class="mf">0.7458</span><span class="p">,</span> <span class="mf">0.2065</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.6080</span><span class="p">,</span> <span class="mf">0.4242</span><span class="p">,</span> <span class="mf">0.8879</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.3428</span><span class="p">,</span> <span class="mf">0.6976</span><span class="p">,</span> <span class="mf">0.0970</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9552</span><span class="p">,</span> <span class="mf">0.3663</span><span class="p">,</span> <span class="mf">0.2139</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.2019</span><span class="p">,</span> <span class="mf">0.2452</span><span class="p">,</span> <span class="mf">0.1142</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3651</span><span class="p">,</span> <span class="mf">0.6993</span><span class="p">,</span> <span class="mf">0.5257</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.9636</span><span class="p">,</span> <span class="mf">0.1691</span><span class="p">,</span> <span class="mf">0.4807</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9137</span><span class="p">,</span> <span class="mf">0.3510</span><span class="p">,</span> <span class="mf">0.0905</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0177</span><span class="p">,</span> <span class="mf">0.9496</span><span class="p">,</span> <span class="mf">0.1560</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.7437</span><span class="p">,</span> <span class="mf">0.9043</span><span class="p">,</span> <span class="mf">0.0131</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.9699</span><span class="p">,</span> <span class="mf">0.5352</span><span class="p">,</span> <span class="mf">0.9763</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1850</span><span class="p">,</span> <span class="mf">0.2056</span><span class="p">,</span> <span class="mf">0.0368</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">r1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.8433</span><span class="p">,</span> <span class="mf">0.5262</span><span class="p">,</span> <span class="mf">0.2608</span><span class="p">,</span> <span class="mf">0.8501</span><span class="p">,</span> <span class="mf">0.7187</span><span class="p">,</span> <span class="mf">0.6944</span><span class="p">,</span> <span class="mf">0.0193</span><span class="p">,</span> <span class="mf">0.1507</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0450</span><span class="p">,</span> <span class="mf">0.2257</span><span class="p">,</span> <span class="mf">0.4679</span><span class="p">,</span> <span class="mf">0.8309</span><span class="p">,</span> <span class="mf">0.4740</span><span class="p">,</span> <span class="mf">0.8715</span><span class="p">,</span> <span class="mf">0.7443</span><span class="p">,</span> <span class="mf">0.3354</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.5533</span><span class="p">,</span> <span class="mf">0.9151</span><span class="p">,</span> <span class="mf">0.4215</span><span class="p">,</span> <span class="mf">0.4631</span><span class="p">,</span> <span class="mf">0.9076</span><span class="p">,</span> <span class="mf">0.3093</span><span class="p">,</span> <span class="mf">0.0270</span><span class="p">,</span> <span class="mf">0.7681</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.1800</span><span class="p">,</span> <span class="mf">0.0847</span><span class="p">,</span> <span class="mf">0.6965</span><span class="p">,</span> <span class="mf">0.2059</span><span class="p">,</span> <span class="mf">0.8806</span><span class="p">,</span> <span class="mf">0.3987</span><span class="p">,</span> <span class="mf">0.8446</span><span class="p">,</span> <span class="mf">0.6225</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.1375</span><span class="p">,</span> <span class="mf">0.8765</span><span class="p">,</span> <span class="mf">0.5965</span><span class="p">,</span> <span class="mf">0.3092</span><span class="p">,</span> <span class="mf">0.0193</span><span class="p">,</span> <span class="mf">0.9220</span><span class="p">,</span> <span class="mf">0.4997</span><span class="p">,</span> <span class="mf">0.8170</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.8575</span><span class="p">,</span> <span class="mf">0.5525</span><span class="p">,</span> <span class="mf">0.8528</span><span class="p">,</span> <span class="mf">0.7262</span><span class="p">,</span> <span class="mf">0.4026</span><span class="p">,</span> <span class="mf">0.5704</span><span class="p">,</span> <span class="mf">0.0390</span><span class="p">,</span> <span class="mf">0.9240</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9780</span><span class="p">,</span> <span class="mf">0.3927</span><span class="p">,</span> <span class="mf">0.7343</span><span class="p">,</span> <span class="mf">0.3922</span><span class="p">,</span> <span class="mf">0.5004</span><span class="p">,</span> <span class="mf">0.8561</span><span class="p">,</span> <span class="mf">0.6021</span><span class="p">,</span> <span class="mf">0.6530</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.6565</span><span class="p">,</span> <span class="mf">0.9988</span><span class="p">,</span> <span class="mf">0.4238</span><span class="p">,</span> <span class="mf">0.0092</span><span class="p">,</span> <span class="mf">0.5131</span><span class="p">,</span> <span class="mf">0.5257</span><span class="p">,</span> <span class="mf">0.1649</span><span class="p">,</span> <span class="mf">0.0272</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9103</span><span class="p">,</span> <span class="mf">0.2476</span><span class="p">,</span> <span class="mf">0.7573</span><span class="p">,</span> <span class="mf">0.8500</span><span class="p">,</span> <span class="mf">0.9348</span><span class="p">,</span> <span class="mf">0.4306</span><span class="p">,</span> <span class="mf">0.3612</span><span class="p">,</span> <span class="mf">0.5378</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.7141</span><span class="p">,</span> <span class="mf">0.3559</span><span class="p">,</span> <span class="mf">0.6620</span><span class="p">,</span> <span class="mf">0.3335</span><span class="p">,</span> <span class="mf">0.4000</span><span class="p">,</span> <span class="mf">0.2479</span><span class="p">,</span> <span class="mf">0.3490</span><span class="p">,</span> <span class="mf">0.7000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.5321</span><span class="p">,</span> <span class="mf">0.3485</span><span class="p">,</span> <span class="mf">0.9162</span><span class="p">,</span> <span class="mf">0.9207</span><span class="p">,</span> <span class="mf">0.3262</span><span class="p">,</span> <span class="mf">0.7929</span><span class="p">,</span> <span class="mf">0.1258</span><span class="p">,</span> <span class="mf">0.6689</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.1023</span><span class="p">,</span> <span class="mf">0.1938</span><span class="p">,</span> <span class="mf">0.3887</span><span class="p">,</span> <span class="mf">0.6893</span><span class="p">,</span> <span class="mf">0.0849</span><span class="p">,</span> <span class="mf">0.3700</span><span class="p">,</span> <span class="mf">0.5747</span><span class="p">,</span> <span class="mf">0.9674</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.4520</span><span class="p">,</span> <span class="mf">0.5313</span><span class="p">,</span> <span class="mf">0.0377</span><span class="p">,</span> <span class="mf">0.1202</span><span class="p">,</span> <span class="mf">0.9326</span><span class="p">,</span> <span class="mf">0.0442</span><span class="p">,</span> <span class="mf">0.4651</span><span class="p">,</span> <span class="mf">0.7036</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3994</span><span class="p">,</span> <span class="mf">0.9332</span><span class="p">,</span> <span class="mf">0.5104</span><span class="p">,</span> <span class="mf">0.0930</span><span class="p">,</span> <span class="mf">0.4481</span><span class="p">,</span> <span class="mf">0.8753</span><span class="p">,</span> <span class="mf">0.5597</span><span class="p">,</span> <span class="mf">0.6068</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9895</span><span class="p">,</span> <span class="mf">0.5833</span><span class="p">,</span> <span class="mf">0.6771</span><span class="p">,</span> <span class="mf">0.4255</span><span class="p">,</span> <span class="mf">0.4513</span><span class="p">,</span> <span class="mf">0.6330</span><span class="p">,</span> <span class="mf">0.9070</span><span class="p">,</span> <span class="mf">0.3103</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0609</span><span class="p">,</span> <span class="mf">0.8202</span><span class="p">,</span> <span class="mf">0.6031</span><span class="p">,</span> <span class="mf">0.3628</span><span class="p">,</span> <span class="mf">0.1118</span><span class="p">,</span> <span class="mf">0.2747</span><span class="p">,</span> <span class="mf">0.4521</span><span class="p">,</span> <span class="mf">0.8347</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.6759</span><span class="p">,</span> <span class="mf">0.8744</span><span class="p">,</span> <span class="mf">0.3595</span><span class="p">,</span> <span class="mf">0.2361</span><span class="p">,</span> <span class="mf">0.4899</span><span class="p">,</span> <span class="mf">0.3769</span><span class="p">,</span> <span class="mf">0.6809</span><span class="p">,</span> <span class="mf">0.0101</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0730</span><span class="p">,</span> <span class="mf">0.0576</span><span class="p">,</span> <span class="mf">0.5242</span><span class="p">,</span> <span class="mf">0.5510</span><span class="p">,</span> <span class="mf">0.9780</span><span class="p">,</span> <span class="mf">0.4704</span><span class="p">,</span> <span class="mf">0.9607</span><span class="p">,</span> <span class="mf">0.1699</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3613</span><span class="p">,</span> <span class="mf">0.6096</span><span class="p">,</span> <span class="mf">0.0246</span><span class="p">,</span> <span class="mf">0.6088</span><span class="p">,</span> <span class="mf">0.4984</span><span class="p">,</span> <span class="mf">0.9788</span><span class="p">,</span> <span class="mf">0.2026</span><span class="p">,</span> <span class="mf">0.1484</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3086</span><span class="p">,</span> <span class="mf">0.9697</span><span class="p">,</span> <span class="mf">0.8166</span><span class="p">,</span> <span class="mf">0.9566</span><span class="p">,</span> <span class="mf">0.9874</span><span class="p">,</span> <span class="mf">0.4547</span><span class="p">,</span> <span class="mf">0.5250</span><span class="p">,</span> <span class="mf">0.2041</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.7784</span><span class="p">,</span> <span class="mf">0.4269</span><span class="p">,</span> <span class="mf">0.0110</span><span class="p">,</span> <span class="mf">0.6878</span><span class="p">,</span> <span class="mf">0.6575</span><span class="p">,</span> <span class="mf">0.3382</span><span class="p">,</span> <span class="mf">0.1889</span><span class="p">,</span> <span class="mf">0.8344</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9608</span><span class="p">,</span> <span class="mf">0.6153</span><span class="p">,</span> <span class="mf">0.4812</span><span class="p">,</span> <span class="mf">0.0547</span><span class="p">,</span> <span class="mf">0.2978</span><span class="p">,</span> <span class="mf">0.3610</span><span class="p">,</span> <span class="mf">0.5285</span><span class="p">,</span> <span class="mf">0.6162</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.2123</span><span class="p">,</span> <span class="mf">0.1364</span><span class="p">,</span> <span class="mf">0.6027</span><span class="p">,</span> <span class="mf">0.7450</span><span class="p">,</span> <span class="mf">0.2485</span><span class="p">,</span> <span class="mf">0.2149</span><span class="p">,</span> <span class="mf">0.7849</span><span class="p">,</span> <span class="mf">0.8886</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0514</span><span class="p">,</span> <span class="mf">0.9511</span><span class="p">,</span> <span class="mf">0.4865</span><span class="p">,</span> <span class="mf">0.8380</span><span class="p">,</span> <span class="mf">0.6947</span><span class="p">,</span> <span class="mf">0.2378</span><span class="p">,</span> <span class="mf">0.5839</span><span class="p">,</span> <span class="mf">0.8434</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0871</span><span class="p">,</span> <span class="mf">0.4179</span><span class="p">,</span> <span class="mf">0.1669</span><span class="p">,</span> <span class="mf">0.8703</span><span class="p">,</span> <span class="mf">0.1946</span><span class="p">,</span> <span class="mf">0.0302</span><span class="p">,</span> <span class="mf">0.9516</span><span class="p">,</span> <span class="mf">0.1208</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.5780</span><span class="p">,</span> <span class="mf">0.6859</span><span class="p">,</span> <span class="mf">0.2405</span><span class="p">,</span> <span class="mf">0.5083</span><span class="p">,</span> <span class="mf">0.3872</span><span class="p">,</span> <span class="mf">0.7649</span><span class="p">,</span> <span class="mf">0.1329</span><span class="p">,</span> <span class="mf">0.0252</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.2404</span><span class="p">,</span> <span class="mf">0.5456</span><span class="p">,</span> <span class="mf">0.7009</span><span class="p">,</span> <span class="mf">0.6524</span><span class="p">,</span> <span class="mf">0.7623</span><span class="p">,</span> <span class="mf">0.5965</span><span class="p">,</span> <span class="mf">0.0437</span><span class="p">,</span> <span class="mf">0.4080</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.8390</span><span class="p">,</span> <span class="mf">0.4172</span><span class="p">,</span> <span class="mf">0.4781</span><span class="p">,</span> <span class="mf">0.2405</span><span class="p">,</span> <span class="mf">0.1502</span><span class="p">,</span> <span class="mf">0.2020</span><span class="p">,</span> <span class="mf">0.4192</span><span class="p">,</span> <span class="mf">0.8185</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0899</span><span class="p">,</span> <span class="mf">0.1961</span><span class="p">,</span> <span class="mf">0.7368</span><span class="p">,</span> <span class="mf">0.4798</span><span class="p">,</span> <span class="mf">0.4303</span><span class="p">,</span> <span class="mf">0.9281</span><span class="p">,</span> <span class="mf">0.5410</span><span class="p">,</span> <span class="mf">0.0620</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.8945</span><span class="p">,</span> <span class="mf">0.3589</span><span class="p">,</span> <span class="mf">0.5637</span><span class="p">,</span> <span class="mf">0.4875</span><span class="p">,</span> <span class="mf">0.1523</span><span class="p">,</span> <span class="mf">0.9478</span><span class="p">,</span> <span class="mf">0.9040</span><span class="p">,</span> <span class="mf">0.3410</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3591</span><span class="p">,</span> <span class="mf">0.2702</span><span class="p">,</span> <span class="mf">0.5949</span><span class="p">,</span> <span class="mf">0.3337</span><span class="p">,</span> <span class="mf">0.3578</span><span class="p">,</span> <span class="mf">0.8890</span><span class="p">,</span> <span class="mf">0.6608</span><span class="p">,</span> <span class="mf">0.6578</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.4953</span><span class="p">,</span> <span class="mf">0.7975</span><span class="p">,</span> <span class="mf">0.2891</span><span class="p">,</span> <span class="mf">0.9552</span><span class="p">,</span> <span class="mf">0.0092</span><span class="p">,</span> <span class="mf">0.1293</span><span class="p">,</span> <span class="mf">0.2362</span><span class="p">,</span> <span class="mf">0.7821</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">r2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">6.4270e-01</span><span class="p">,</span> <span class="mf">1.3050e-01</span><span class="p">,</span> <span class="mf">9.6509e-01</span><span class="p">,</span> <span class="mf">1.4090e-01</span><span class="p">,</span> <span class="mf">1.8660e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.7512e-01</span><span class="p">,</span> <span class="mf">3.8567e-01</span><span class="p">,</span> <span class="mf">4.1776e-01</span><span class="p">,</span> <span class="mf">9.7718e-01</span><span class="p">,</span> <span class="mf">5.6305e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.3091e-01</span><span class="p">,</span> <span class="mf">4.6385e-01</span><span class="p">,</span> <span class="mf">1.8142e-01</span><span class="p">,</span> <span class="mf">3.7779e-01</span><span class="p">,</span> <span class="mf">3.8012e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.1807e-01</span><span class="p">,</span> <span class="mf">3.3292e-01</span><span class="p">,</span> <span class="mf">5.8488e-01</span><span class="p">,</span> <span class="mf">5.8188e-01</span><span class="p">,</span> <span class="mf">5.7776e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.1828e-01</span><span class="p">,</span> <span class="mf">9.6087e-01</span><span class="p">,</span> <span class="mf">7.2219e-01</span><span class="p">,</span> <span class="mf">8.5045e-02</span><span class="p">,</span> <span class="mf">3.6623e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3758e-01</span><span class="p">,</span> <span class="mf">7.9666e-01</span><span class="p">,</span> <span class="mf">6.9932e-01</span><span class="p">,</span> <span class="mf">9.9202e-01</span><span class="p">,</span> <span class="mf">2.5493e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.3017e-01</span><span class="p">,</span> <span class="mf">7.9396e-01</span><span class="p">,</span> <span class="mf">5.0109e-01</span><span class="p">,</span> <span class="mf">6.5580e-01</span><span class="p">,</span> <span class="mf">3.2200e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8023e-01</span><span class="p">,</span> <span class="mf">4.4098e-01</span><span class="p">,</span> <span class="mf">1.0576e-01</span><span class="p">,</span> <span class="mf">8.0548e-01</span><span class="p">,</span> <span class="mf">2.2453e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4705e-01</span><span class="p">,</span> <span class="mf">8.7682e-02</span><span class="p">,</span> <span class="mf">4.7264e-01</span><span class="p">,</span> <span class="mf">8.9034e-02</span><span class="p">,</span> <span class="mf">8.5720e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.7576e-01</span><span class="p">,</span> <span class="mf">2.8438e-01</span><span class="p">,</span> <span class="mf">8.6523e-01</span><span class="p">,</span> <span class="mf">8.1707e-02</span><span class="p">,</span> <span class="mf">3.0075e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.9069e-01</span><span class="p">,</span> <span class="mf">9.7404e-01</span><span class="p">,</span> <span class="mf">9.3865e-01</span><span class="p">,</span> <span class="mf">5.7160e-01</span><span class="p">,</span> <span class="mf">1.6332e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.3868e-01</span><span class="p">,</span> <span class="mf">5.8658e-01</span><span class="p">,</span> <span class="mf">5.3993e-01</span><span class="p">,</span> <span class="mf">3.8271e-02</span><span class="p">,</span> <span class="mf">9.9662e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2892e-01</span><span class="p">,</span> <span class="mf">7.8558e-01</span><span class="p">,</span> <span class="mf">9.4502e-01</span><span class="p">,</span> <span class="mf">9.7633e-01</span><span class="p">,</span> <span class="mf">1.7877e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6446e-02</span><span class="p">,</span> <span class="mf">2.3411e-01</span><span class="p">,</span> <span class="mf">6.7531e-01</span><span class="p">,</span> <span class="mf">1.5023e-01</span><span class="p">,</span> <span class="mf">4.4280e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4457e-01</span><span class="p">,</span> <span class="mf">3.6683e-01</span><span class="p">,</span> <span class="mf">4.3424e-01</span><span class="p">,</span> <span class="mf">7.4145e-01</span><span class="p">,</span> <span class="mf">8.2433e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.8660e-01</span><span class="p">,</span> <span class="mf">6.7477e-01</span><span class="p">,</span> <span class="mf">5.5000e-02</span><span class="p">,</span> <span class="mf">5.1344e-01</span><span class="p">,</span> <span class="mf">9.3115e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.8280e-01</span><span class="p">,</span> <span class="mf">9.2177e-01</span><span class="p">,</span> <span class="mf">4.5470e-01</span><span class="p">,</span> <span class="mf">2.5540e-01</span><span class="p">,</span> <span class="mf">4.6632e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.3960e-01</span><span class="p">,</span> <span class="mf">4.4320e-01</span><span class="p">,</span> <span class="mf">1.0808e-01</span><span class="p">,</span> <span class="mf">7.5544e-01</span><span class="p">,</span> <span class="mf">4.6372e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4322e-01</span><span class="p">,</span> <span class="mf">1.9141e-01</span><span class="p">,</span> <span class="mf">5.5918e-02</span><span class="p">,</span> <span class="mf">7.0804e-01</span><span class="p">,</span> <span class="mf">1.8789e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.4276e-01</span><span class="p">,</span> <span class="mf">9.1742e-01</span><span class="p">,</span> <span class="mf">9.1980e-01</span><span class="p">,</span> <span class="mf">6.2728e-01</span><span class="p">,</span> <span class="mf">4.1787e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.9545e-01</span><span class="p">,</span> <span class="mf">9.0569e-01</span><span class="p">,</span> <span class="mf">7.9123e-01</span><span class="p">,</span> <span class="mf">9.7596e-01</span><span class="p">,</span> <span class="mf">7.2507e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.3772e-01</span><span class="p">,</span> <span class="mf">8.2560e-01</span><span class="p">,</span> <span class="mf">5.9359e-01</span><span class="p">,</span> <span class="mf">7.1134e-01</span><span class="p">,</span> <span class="mf">5.1029e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1601e-01</span><span class="p">,</span> <span class="mf">2.9094e-01</span><span class="p">,</span> <span class="mf">3.4174e-01</span><span class="p">,</span> <span class="mf">9.0532e-01</span><span class="p">,</span> <span class="mf">5.0960e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.4441e-01</span><span class="p">,</span> <span class="mf">7.0498e-01</span><span class="p">,</span> <span class="mf">4.2729e-01</span><span class="p">,</span> <span class="mf">7.6714e-01</span><span class="p">,</span> <span class="mf">6.3755e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.4604e-01</span><span class="p">,</span> <span class="mf">5.9109e-01</span><span class="p">,</span> <span class="mf">7.9137e-01</span><span class="p">,</span> <span class="mf">7.5149e-01</span><span class="p">,</span> <span class="mf">2.2092e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.5235e-01</span><span class="p">,</span> <span class="mf">3.6915e-01</span><span class="p">,</span> <span class="mf">6.4961e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">8.7862e-01</span><span class="p">,</span> <span class="mf">1.1325e-01</span><span class="p">,</span> <span class="mf">2.4575e-01</span><span class="p">,</span> <span class="mf">9.7429e-01</span><span class="p">,</span> <span class="mf">1.9362e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2297e-01</span><span class="p">,</span> <span class="mf">3.5184e-02</span><span class="p">,</span> <span class="mf">5.2755e-01</span><span class="p">,</span> <span class="mf">7.6429e-01</span><span class="p">,</span> <span class="mf">2.4700e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.2860e-01</span><span class="p">,</span> <span class="mf">2.4555e-01</span><span class="p">,</span> <span class="mf">4.4557e-01</span><span class="p">,</span> <span class="mf">7.0955e-03</span><span class="p">,</span> <span class="mf">2.0326e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.6354e-02</span><span class="p">,</span> <span class="mf">3.5959e-01</span><span class="p">,</span> <span class="mf">3.4059e-01</span><span class="p">,</span> <span class="mf">8.6852e-01</span><span class="p">,</span> <span class="mf">1.3858e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.8500e-01</span><span class="p">,</span> <span class="mf">1.3601e-01</span><span class="p">,</span> <span class="mf">7.3152e-01</span><span class="p">,</span> <span class="mf">8.3474e-01</span><span class="p">,</span> <span class="mf">2.7017e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.8078e-01</span><span class="p">,</span> <span class="mf">6.1084e-01</span><span class="p">,</span> <span class="mf">1.6540e-01</span><span class="p">,</span> <span class="mf">4.3081e-01</span><span class="p">,</span> <span class="mf">8.5738e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.1890e-01</span><span class="p">,</span> <span class="mf">6.6872e-01</span><span class="p">,</span> <span class="mf">3.1698e-01</span><span class="p">,</span> <span class="mf">4.2576e-02</span><span class="p">,</span> <span class="mf">1.5236e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.0526e-01</span><span class="p">,</span> <span class="mf">1.9493e-01</span><span class="p">,</span> <span class="mf">6.6122e-03</span><span class="p">,</span> <span class="mf">1.8332e-01</span><span class="p">,</span> <span class="mf">5.6981e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.4090e-01</span><span class="p">,</span> <span class="mf">6.0783e-01</span><span class="p">,</span> <span class="mf">5.8742e-01</span><span class="p">,</span> <span class="mf">9.1761e-04</span><span class="p">,</span> <span class="mf">2.0904e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.6419e-01</span><span class="p">,</span> <span class="mf">9.9559e-01</span><span class="p">,</span> <span class="mf">5.8233e-01</span><span class="p">,</span> <span class="mf">6.8562e-01</span><span class="p">,</span> <span class="mf">8.6456e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.9931e-01</span><span class="p">,</span> <span class="mf">3.5637e-01</span><span class="p">,</span> <span class="mf">2.4642e-01</span><span class="p">,</span> <span class="mf">2.3428e-02</span><span class="p">,</span> <span class="mf">6.9037e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.7560e-01</span><span class="p">,</span> <span class="mf">1.8703e-01</span><span class="p">,</span> <span class="mf">3.5244e-01</span><span class="p">,</span> <span class="mf">6.3031e-01</span><span class="p">,</span> <span class="mf">1.8450e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.2194e-01</span><span class="p">,</span> <span class="mf">9.3016e-02</span><span class="p">,</span> <span class="mf">9.0488e-01</span><span class="p">,</span> <span class="mf">2.4294e-02</span><span class="p">,</span> <span class="mf">5.1122e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.0793e-01</span><span class="p">,</span> <span class="mf">7.9585e-01</span><span class="p">,</span> <span class="mf">7.9594e-02</span><span class="p">,</span> <span class="mf">5.2137e-01</span><span class="p">,</span> <span class="mf">9.8359e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.5022e-01</span><span class="p">,</span> <span class="mf">4.1925e-01</span><span class="p">,</span> <span class="mf">3.3284e-01</span><span class="p">,</span> <span class="mf">4.7939e-01</span><span class="p">,</span> <span class="mf">9.9081e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3931e-01</span><span class="p">,</span> <span class="mf">2.6461e-01</span><span class="p">,</span> <span class="mf">5.3063e-01</span><span class="p">,</span> <span class="mf">1.0328e-01</span><span class="p">,</span> <span class="mf">8.0720e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.9480e-01</span><span class="p">,</span> <span class="mf">3.0833e-01</span><span class="p">,</span> <span class="mf">5.6780e-01</span><span class="p">,</span> <span class="mf">3.9551e-01</span><span class="p">,</span> <span class="mf">6.7176e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.8049e-01</span><span class="p">,</span> <span class="mf">1.5653e-01</span><span class="p">,</span> <span class="mf">1.7595e-02</span><span class="p">,</span> <span class="mf">6.6493e-02</span><span class="p">,</span> <span class="mf">5.1989e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2691e-01</span><span class="p">,</span> <span class="mf">7.3295e-01</span><span class="p">,</span> <span class="mf">5.7169e-01</span><span class="p">,</span> <span class="mf">4.9911e-01</span><span class="p">,</span> <span class="mf">1.0260e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.2307e-01</span><span class="p">,</span> <span class="mf">7.4247e-01</span><span class="p">,</span> <span class="mf">1.1682e-01</span><span class="p">,</span> <span class="mf">5.8123e-01</span><span class="p">,</span> <span class="mf">7.3496e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.4274e-02</span><span class="p">,</span> <span class="mf">2.4704e-01</span><span class="p">,</span> <span class="mf">6.0424e-02</span><span class="p">,</span> <span class="mf">2.6161e-01</span><span class="p">,</span> <span class="mf">7.7966e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.1244e-01</span><span class="p">,</span> <span class="mf">2.2077e-01</span><span class="p">,</span> <span class="mf">5.0723e-01</span><span class="p">,</span> <span class="mf">9.6665e-01</span><span class="p">,</span> <span class="mf">6.0933e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.1332e-01</span><span class="p">,</span> <span class="mf">3.0749e-01</span><span class="p">,</span> <span class="mf">2.1297e-02</span><span class="p">,</span> <span class="mf">3.6734e-01</span><span class="p">,</span> <span class="mf">9.2542e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3554e-01</span><span class="p">,</span> <span class="mf">9.7240e-01</span><span class="p">,</span> <span class="mf">4.4344e-01</span><span class="p">,</span> <span class="mf">4.2534e-01</span><span class="p">,</span> <span class="mf">4.6205e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1811e-01</span><span class="p">,</span> <span class="mf">5.8800e-01</span><span class="p">,</span> <span class="mf">5.4673e-01</span><span class="p">,</span> <span class="mf">1.2535e-01</span><span class="p">,</span> <span class="mf">2.9959e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.4890e-01</span><span class="p">,</span> <span class="mf">2.7185e-01</span><span class="p">,</span> <span class="mf">5.0243e-01</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotary_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">0.1142</span><span class="p">,</span>  <span class="mf">0.1891</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4689</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5704</span><span class="p">,</span>  <span class="mf">0.5375</span><span class="p">,</span>  <span class="mf">0.6079</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2264</span><span class="p">,</span>  <span class="mf">0.1155</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7678</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9857</span><span class="p">,</span>  <span class="mf">0.3382</span><span class="p">,</span>  <span class="mf">0.9441</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1902</span><span class="p">,</span>  <span class="mf">0.1329</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3613</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9793</span><span class="p">,</span>  <span class="mf">0.5628</span><span class="p">,</span>  <span class="mf">0.8669</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.3349</span><span class="p">,</span>  <span class="mf">0.1532</span><span class="p">,</span>  <span class="mf">0.1124</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.3125</span><span class="p">,</span>  <span class="mf">0.6741</span><span class="p">,</span>  <span class="mf">1.1248</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.0473</span><span class="p">,</span>  <span class="mf">0.2978</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6940</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2753</span><span class="p">,</span>  <span class="mf">0.2604</span><span class="p">,</span>  <span class="mf">1.0379</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.0136</span><span class="p">,</span>  <span class="mf">0.4723</span><span class="p">,</span>  <span class="mf">0.1371</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1081</span><span class="p">,</span>  <span class="mf">0.2462</span><span class="p">,</span>  <span class="mf">0.6316</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0769</span><span class="p">,</span>  <span class="mf">0.6558</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0734</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2714</span><span class="p">,</span>  <span class="mf">0.2221</span><span class="p">,</span>  <span class="mf">0.2195</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.3755</span><span class="p">,</span>  <span class="mf">0.5364</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1131</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.3105</span><span class="p">,</span>  <span class="mf">0.1225</span><span class="p">,</span>  <span class="mf">0.6166</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3535</span><span class="p">,</span>  <span class="mf">0.0164</span><span class="p">,</span>  <span class="mf">0.0095</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1361</span><span class="p">,</span>  <span class="mf">0.2570</span><span class="p">,</span>  <span class="mf">0.5811</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.2992</span><span class="p">,</span>  <span class="mf">0.2981</span><span class="p">,</span>  <span class="mf">0.0242</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2881</span><span class="p">,</span>  <span class="mf">0.2367</span><span class="p">,</span>  <span class="mf">0.9582</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span> <span class="mf">0.1699</span><span class="p">,</span>  <span class="mf">0.3589</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7443</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4751</span><span class="p">,</span>  <span class="mf">0.7291</span><span class="p">,</span>  <span class="mf">0.2717</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3657</span><span class="p">,</span>  <span class="mf">0.0397</span><span class="p">,</span>  <span class="mf">0.1818</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9113</span><span class="p">,</span>  <span class="mf">0.4130</span><span class="p">,</span>  <span class="mf">0.8279</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.0657</span><span class="p">,</span>  <span class="mf">0.2528</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6658</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.8184</span><span class="p">,</span>  <span class="mf">0.2057</span><span class="p">,</span>  <span class="mf">1.2864</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1058</span><span class="p">,</span>  <span class="mf">0.1859</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0998</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.5590</span><span class="p">,</span>  <span class="mf">1.0525</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2651</span><span class="p">,</span>  <span class="mf">0.3719</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8170</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2789</span><span class="p">,</span>  <span class="mf">0.3916</span><span class="p">,</span>  <span class="mf">1.0407</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.5998</span><span class="p">,</span>  <span class="mf">0.5740</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0154</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1746</span><span class="p">,</span>  <span class="mf">0.1982</span><span class="p">,</span>  <span class="mf">0.6338</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0766</span><span class="p">,</span>  <span class="mf">0.1790</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1490</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4387</span><span class="p">,</span>  <span class="mf">0.2592</span><span class="p">,</span>  <span class="mf">0.4924</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.4765</span><span class="p">,</span>  <span class="mf">0.0485</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0226</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2219</span><span class="p">,</span>  <span class="mf">0.3445</span><span class="p">,</span>  <span class="mf">0.2265</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1006</span><span class="p">,</span>  <span class="mf">0.8073</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1540</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1045</span><span class="p">,</span>  <span class="mf">0.2633</span><span class="p">,</span>  <span class="mf">0.2194</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0157</span><span class="p">,</span>  <span class="mf">0.3997</span><span class="p">,</span>  <span class="mf">0.3131</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0538</span><span class="p">,</span>  <span class="mf">0.0647</span><span class="p">,</span>  <span class="mf">0.4821</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_box_decode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_box_decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_box_decode" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_box_decode(anchor_boxes, deltas, weight) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ—‹è½¬æ ‡æ³¨æ¡†ç¼–ç ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>anchor_box (Tensor) - shapeä¸º(B,5,N)çš„3Dè¾“å…¥å¼ é‡ï¼Œè¡¨ç¤ºé”šç‚¹æ¡†ã€‚â€œBâ€è¡¨ç¤ºæ‰¹å¤„ç†å¤§å°æ•°é‡ï¼Œâ€œNâ€è¡¨ç¤ºæ ‡æ³¨æ¡†æ•°é‡ï¼Œå€¼â€œ5â€è¡¨ç¤ºâ€œx0â€ã€â€œx1â€ã€â€œy0â€ã€â€œy1â€å’Œâ€œangleâ€ã€‚</p>
<p>deltas (Tensor) - shapeä¸º(B,5,N)æ•°æ®ç±»å‹ä¸ºfloat32 (float16)çš„3Då¼ é‡ã€‚</p>
<p>weight (Tensorï¼Œé»˜è®¤å€¼ä¸º[1.0, 1.0, 1.0, 1.0, 1.0]) - â€œx0â€ã€â€œx1â€ã€â€œy0â€ã€â€œy1â€å’Œâ€œangleâ€çš„æµ®ç‚¹åˆ—è¡¨ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">4.137</span><span class="p">],[</span><span class="mf">33.72</span><span class="p">],[</span><span class="mf">29.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">54.06</span><span class="p">],</span> <span class="p">[</span><span class="mf">41.28</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deltas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0244</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.992</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2109</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.315</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">37.25</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_box_decode</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span>  <span class="mf">1.7861</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">10.5781</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">33.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">17.2969</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">88.4375</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_box_encode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_box_encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_box_encode" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_box_encode(anchor_box, gt_bboxes, weight) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ—‹è½¬æ ‡æ³¨æ¡†ç¼–ç ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>anchor_box (Tensor) - shapeä¸º(B,5,N)çš„3Dè¾“å…¥å¼ é‡ï¼Œè¡¨ç¤ºé”šç‚¹æ¡†ã€‚â€œBâ€è¡¨ç¤ºæ‰¹å¤„ç†å¤§å°æ•°é‡ï¼Œâ€œNâ€è¡¨ç¤ºæ ‡æ³¨æ¡†æ•°é‡ï¼Œå€¼â€œ5â€è¡¨ç¤ºâ€œx0â€ã€â€œx1â€ã€â€œy0â€ã€â€œy1â€å’Œâ€œangleâ€ã€‚</p>
<p>gt_bboxes (Tensor) - shapeä¸º(B,5,N)æ•°æ®ç±»å‹ä¸ºfloat32 (float16)çš„3Då¼ é‡ã€‚</p>
<p>weight (Tensorï¼Œé»˜è®¤å€¼ä¸º[1.0, 1.0, 1.0, 1.0, 1.0]) - â€œx0â€ã€â€œx1â€ã€â€œy0â€ã€â€œy1â€å’Œâ€œangleâ€çš„æµ®ç‚¹åˆ—è¡¨ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">30.69</span><span class="p">],</span> <span class="p">[</span><span class="mf">32.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">45.94</span><span class="p">],</span> <span class="p">[</span><span class="mf">59.88</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">44.53</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">30.44</span><span class="p">],</span> <span class="p">[</span><span class="mf">18.72</span><span class="p">],</span> <span class="p">[</span><span class="mf">33.22</span><span class="p">],</span> <span class="p">[</span><span class="mf">45.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">8.5</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_box_encode</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_bboxes</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.4253</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.5166</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.7021</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.0162</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1328</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_iou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_iou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_iou" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_iou(self, query_boxes, trans=False, mode=0, is_cross=True,v_threshold=0.0, e_threshold=0.0) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—æ—‹è½¬æ¡†çš„IoUã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ¢¯åº¦å¢é‡æ•°æ®ï¼Œshapeä¸º(B, 5, N)æ•°æ®ç±»å‹ä¸ºfloat32çš„3Då¼ é‡ã€‚</p>
<p>query_boxes (Tensor) - æ ‡æ³¨æ¡†ï¼Œshapeä¸º(B, 5, K) æ•°æ®ç±»å‹ä¸ºfloat32çš„3Då¼ é‡ã€‚</p>
<p>trans (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - å€¼ä¸ºTrueè¡¨ç¤ºâ€œxyxytâ€ï¼Œå€¼ä¸ºFalseè¡¨ç¤ºâ€œxywhtâ€ã€‚</p>
<p>is_cross (Boolï¼Œé»˜è®¤å€¼ä¸ºTrue) - å€¼ä¸ºTrueæ—¶è¡¨ç¤ºäº¤å‰è®¡ç®—ï¼Œä¸ºFalseæ—¶è¡¨ç¤ºä¸€å¯¹ä¸€è®¡ç®—ã€‚</p>
<p>mode (Intï¼Œé»˜è®¤å€¼ä¸º0) - è®¡ç®—æ¨¡å¼ï¼Œå–å€¼ä¸º0æˆ–1ã€‚0è¡¨ç¤ºIoUï¼Œ1è¡¨ç¤ºIoFã€‚</p>
<p>v_threshold (Floatï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸º0.0) - provide condition relaxation for intersection calculation.</p>
<p>e_threshold (Floatï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸º0.0) - provide condition relaxation for intersection calculation.</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_iou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">is_cross</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">3.3325e-01</span><span class="p">,</span> <span class="mf">1.0162e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.0162e-01</span><span class="p">,</span> <span class="mf">1.0000e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000e+00</span><span class="p">,</span> <span class="mf">0.0000e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000e+00</span><span class="p">,</span> <span class="mf">5.9605e-08</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_overlaps">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_overlaps</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_overlaps" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_overlaps(self, query_boxes, trans=False) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—æ—‹è½¬æ¡†çš„é‡å é¢ç§¯ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) -æ¢¯åº¦å¢é‡æ•°æ®ï¼Œshapeä¸º(B, 5, N)æ•°æ®ç±»å‹ä¸ºfloat32çš„3Då¼ é‡ã€‚</p>
<p>query_boxes (Tensor) - æ ‡æ³¨æ¡†ï¼Œshapeä¸º(B, 5, K) æ•°æ®ç±»å‹ä¸ºfloat32çš„3Då¼ é‡ã€‚</p>
<p>trans (Boolï¼Œé»˜è®¤å€¼ä¸ºFalse) - å€¼ä¸ºTrueè¡¨ç¤ºâ€œxyxytâ€ï¼Œå€¼ä¸ºFalseè¡¨ç¤ºâ€œxywhtâ€ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_overlaps</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1562</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1562</span><span class="p">,</span> <span class="mf">0.3713</span><span class="p">,</span> <span class="mf">0.0611</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0611</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scaled_masked_softmax">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scaled_masked_softmax</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scaled_masked_softmax" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_scaled_masked_softmax(x, mask, scale=1.0, fixed_triu_mask=False) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—è¾“å…¥å¼ é‡xç¼©æ”¾å¹¶æŒ‰ç…§maské®è”½åçš„Softmaxç»“æœã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x(Tensor)- è¾“å…¥çš„logitsã€‚æ”¯æŒæ•°æ®ç±»å‹ï¼šfloat16ã€float32ã€bfloat16ã€‚æ”¯æŒæ ¼å¼ï¼š[NDï¼ŒFRACTAL_NZ]ã€‚</p>
<p>mask(Tensor)- è¾“å…¥çš„æ©ç ã€‚æ”¯æŒæ•°æ®ç±»å‹ï¼šboolã€‚æ”¯æŒæ ¼å¼ï¼š[NDï¼ŒFRACTAL_NZ]ã€‚</p>
<p>scale(floatï¼Œé»˜è®¤å€¼ä¸º1.0)- xçš„ç¼©æ”¾ç³»æ•°ã€‚</p>
<p>fixed_triu_mask(boolï¼Œé»˜è®¤å€¼ä¸ºFalse)- æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ä¸Šä¸‰è§’boolæ©ç ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>å½“å‰è¾“å…¥xçš„shapeï¼Œåªæ”¯æŒè½¬ä¸º[NCHW]æ ¼å¼åï¼ŒHå’ŒWè½´é•¿åº¦å¤§äºç­‰äº32ã€å°äºç­‰äº4096ã€ä¸”èƒ½è¢«32æ•´é™¤çš„åœºæ™¯ã€‚</p>
<p>è¾“å…¥maskçš„shapeï¼Œå¿…é¡»èƒ½è¢«broadcastæˆxçš„shapeã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">2048</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fixed_triu_mask</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scaled_masked_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">fixed_triu_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scatter">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scatter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scatter" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_scatter(self, indices, updates, dim) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä½¿ç”¨dimå¯¹scatterç»“æœè¿›è¡Œè®¡æ•°ã€‚ç±»ä¼¼äºtorch.scatterï¼Œä¼˜åŒ–NPUè®¾å¤‡å®ç°ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>indices (Tensor) - å¾…scatterçš„å…ƒç´ indexï¼Œå¯ä»¥ä¸ºç©ºï¼Œä¹Ÿå¯ä»¥ä¸srcæœ‰ç›¸åŒçš„ç»´æ•°ã€‚å½“ä¸ºç©ºæ—¶ï¼Œæ“ä½œè¿”å›â€œself unchangedâ€ã€‚</p>
<p>updates (Tensor) - å¾…scatterçš„æºå…ƒç´ ã€‚</p>
<p>dim (Int) - è¦è¿›è¡Œindexçš„è½´ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.6279</span><span class="p">,</span> <span class="mf">0.1226</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9041</span><span class="p">,</span> <span class="mf">1.0980</span><span class="p">]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.6279</span><span class="p">,</span> <span class="mf">0.1226</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.9041</span><span class="p">,</span> <span class="mf">1.0980</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1993</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5247</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1993</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5247</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.1993</span><span class="p">,</span>  <span class="mf">0.1226</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.9041</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5247</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_sign_bits_pack">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_sign_bits_pack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_sign_bits_pack" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_sign_bits_pack(Tensor self, int size) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å°†floatç±»å‹1ä½Adamæ‰“åŒ…ä¸ºuint8ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x(Tensor) - 1D floatå¼ é‡ã€‚</p>
<p>size(Int) - reshapeæ—¶è¾“å‡ºå¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>Sizeå¯è¢«floatæ‰“åŒ…çš„è¾“å‡ºæ•´é™¤ã€‚å¦‚æœxçš„sizeå¯è¢«8æ•´é™¤ï¼Œåˆ™è¾“å‡ºçš„sizeä¸º(size of x)/8ï¼›å¦åˆ™ï¼Œè¾“å‡ºçš„sizeä¸º(size of x // 8) + 1ã€‚å°†åœ¨å°ç«¯ä½ç½®æ·»åŠ -1æµ®ç‚¹å€¼ä»¥å¡«å……å¯æ•´é™¤æ€§ã€‚Atlas è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒfloat32å’Œfloat16ç±»å‹è¾“å…¥ã€‚Atlas æ¨ç†ç³»åˆ—äº§å“(Ascend 310På¤„ç†å™¨)æ”¯æŒfloat32å’Œfloat16ç±»å‹è¾“å…¥ã€‚Atlas 200/300/500 æ¨ç†äº§å“ä»…æ”¯æŒfloat16ç±»å‹è¾“å…¥ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">sign_bits_pack</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">159</span><span class="p">],[</span><span class="mi">15</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">binary</span> <span class="n">form</span> <span class="n">of</span> <span class="mi">159</span> <span class="ow">is</span> <span class="n">ob10011111</span><span class="p">,</span> <span class="n">corresponds</span> <span class="n">to</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span> <span class="n">respectively</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_sign_bits_unpack">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_sign_bits_unpack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_sign_bits_unpack" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_sign_bits_unpack(x, dtype, size) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>å°†uint8ç±»å‹1ä½Adamæ‹†åŒ…ä¸ºfloatã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x(Tensor) - 1D uint8å¼ é‡ã€‚</p>
<p>dtype(torch.dtype) - å€¼ä¸º1è®¾ç½®è¾“å‡ºç±»å‹ä¸ºfloat16ï¼Œå€¼ä¸º0è®¾ç½®è¾“å‡ºç±»å‹ä¸ºfloat32ã€‚</p>
<p>size(Int) - reshapeæ—¶è¾“å‡ºå¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>Sizeå¯è¢«uint8sæ‹†åŒ…çš„è¾“å‡ºæ•´é™¤ã€‚è¾“å‡ºå¤§å°ä¸º(size of x) * 8ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">159</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_sign_bits_unpack</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">binary</span> <span class="n">form</span> <span class="n">of</span> <span class="mi">159</span> <span class="ow">is</span> <span class="n">ob00001111</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_silu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_silu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_silu" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_silu(self) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—selfçš„Swishã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - æ•°æ®ç±»å‹ï¼šfloat16ã€float32</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_silu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4397</span><span class="p">,</span> <span class="mf">0.7178</span><span class="p">,</span> <span class="mf">0.5190</span><span class="p">,</span> <span class="mf">0.2654</span><span class="p">,</span> <span class="mf">0.2230</span><span class="p">,</span> <span class="mf">0.2674</span><span class="p">,</span> <span class="mf">0.6051</span><span class="p">,</span> <span class="mf">0.3522</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4679</span><span class="p">,</span> <span class="mf">0.1764</span><span class="p">,</span> <span class="mf">0.6650</span><span class="p">,</span> <span class="mf">0.3175</span><span class="p">,</span> <span class="mf">0.0530</span><span class="p">,</span> <span class="mf">0.4787</span><span class="p">,</span> <span class="mf">0.5621</span><span class="p">,</span> <span class="mf">0.4026</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_slice">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_slice</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_slice" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_slice(self, offsets, size) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>ä»å¼ é‡ä¸­æå–åˆ‡ç‰‡ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>offsets (ListInt) - æ•°æ®ç±»å‹ï¼šint32ï¼Œint64ã€‚</p>
<p>size (ListInt) - æ•°æ®ç±»å‹ï¼šint32ï¼Œint64ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span> <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_slice</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_softmax_cross_entropy_with_logits">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_softmax_cross_entropy_with_logits</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_softmax_cross_entropy_with_logits" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_softmax_cross_entropy_with_logits(features, labels) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è®¡ç®—softmaxçš„äº¤å‰ç†µcostã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>features (Tensor) - å¼ é‡ï¼Œä¸€ä¸ªâ€œbatch_size * num_classesâ€çŸ©é˜µã€‚</p>
<p>labels (Tensor) - ä¸â€œfeaturesâ€åŒç±»å‹çš„å¼ é‡ã€‚ä¸€ä¸ªâ€œbatch_size * num_classesâ€çŸ©é˜µã€‚</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_sort_v2">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_sort_v2</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_sort_v2" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_sort_v2(self, dim=-1, descending=False, out=None) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ²¿ç»™å®šç»´åº¦ï¼ŒæŒ‰æ— indexå€¼å¯¹è¾“å…¥å¼ é‡å…ƒç´ è¿›è¡Œå‡åºæ’åºã€‚è‹¥dimæœªè®¾ç½®ï¼Œåˆ™é€‰æ‹©è¾“å…¥çš„æœ€åä¸€ä¸ªç»´åº¦ã€‚å¦‚æœdescendingä¸ºTrueï¼Œåˆ™å…ƒç´ å°†æŒ‰å€¼é™åºæ’åºã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>dim (Int, å¯é€‰,é»˜è®¤å€¼ä¸º-1) - è¿›è¡Œæ’åºçš„ç»´åº¦ã€‚</p>
<p>descending (Bool, å¯é€‰ï¼Œé»˜è®¤å€¼ä¸ºNone) - æ’åºé¡ºåºæ§åˆ¶(å‡åºæˆ–é™åº)ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong></p>
<p>ç›®å‰ä»…æ”¯æŒè¾“å…¥çš„æœ€åä¸€ä¸ªç»´åº¦(dim=-1)ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="mf">1.7790</span><span class="p">,</span>  <span class="mf">0.5031</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7217</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1685</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0486</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2938</span><span class="p">,</span>  <span class="mf">1.3241</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.1880</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7447</span><span class="p">,</span>  <span class="mf">1.3976</span><span class="p">,</span>  <span class="mf">0.7380</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sorted_x</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_sort_v2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sorted_x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.7217</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="mf">0.5029</span><span class="p">,</span>  <span class="mf">1.7793</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2937</span><span class="p">,</span>  <span class="mf">1.1689</span><span class="p">,</span>  <span class="mf">1.3242</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.7441</span><span class="p">,</span>  <span class="mf">0.1880</span><span class="p">,</span>  <span class="mf">0.7378</span><span class="p">,</span>  <span class="mf">1.3975</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_stride_add">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_stride_add</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_stride_add" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_stride_add(x1, x2, offset1, offset2, c1_len) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>æ·»åŠ ä¸¤ä¸ªå¼ é‡çš„partial valuesï¼Œæ ¼å¼ä¸ºNC1HWC0ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>x1 (Tensor) - 5HDå¼ é‡ã€‚</p>
<p>x2 (Tensor) - ä¸â€œx1â€ç±»å‹ç›¸åŒshapeç›¸åŒ(C1å€¼é™¤å¤–)çš„å¼ é‡ã€‚</p>
<p>offset1 (Scalar) - â€œx1â€ä¸­C1çš„offset valueã€‚</p>
<p>offset2 (Scalar) - â€œx2â€ä¸­C1çš„offset valueã€‚</p>
<p>c1_len (Scalar) - â€œyâ€çš„C1 lenã€‚è¯¥å€¼å¿…é¡»å°äºâ€œx1â€å’Œâ€œx2â€ä¸­C1ä¸offsetçš„å·®å€¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[[</span><span class="mf">1.</span><span class="p">]]]]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_stride_add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">btensor</span><span class="p">([[[[[</span><span class="mf">2.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_transpose">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_transpose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_transpose" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_transpose(self, perm, require_contiguous=True) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è¿”å›åŸå§‹å¼ é‡è§†å›¾ï¼Œå…¶ç»´åº¦å·²permuteï¼Œç»“æœè¿ç»­ã€‚æ”¯æŒFakeTensoræ¨¡å¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>perm (ListInt) - å¯¹åº”ç»´åº¦æ’åˆ—ã€‚</p>
<p>require_contiguous(Boolï¼Œé»˜è®¤å€¼ä¸ºTrue) - ç”¨æˆ·æ˜¯å¦æ˜¾å¼æŒ‡å®šnpu_contiguousç®—å­é€‚é…éœ€è¦å¯¹è¾“å…¥Tensoråšè½¬è¿ç»­ã€‚é»˜è®¤ä¸ºFalseï¼Œä½æ€§èƒ½æ¨¡å¼ã€‚ç”¨æˆ·æ˜ç¡®çŸ¥é“è¾“å…¥Tensorä¸ºè¿ç»­Tensoræˆ–è½¬ç½®Tensoræ—¶ï¼Œæ‰èƒ½è®¾ç½®ä¸ºTrueä½¿ç”¨é«˜æ€§èƒ½æ¨¡å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_yolo_boxes_encode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_yolo_boxes_encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_yolo_boxes_encode" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.npu_transpose(self, perm, require_contiguous=True) -&gt; Tensor</p>
<p><strong>åŠŸèƒ½æè¿°</strong></p>
<p>è¿”å›åŸå§‹å¼ é‡è§†å›¾ï¼Œå…¶ç»´åº¦å·²permuteï¼Œç»“æœè¿ç»­ã€‚æ”¯æŒFakeTensoræ¨¡å¼ã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong></p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>perm (ListInt) - å¯¹åº”ç»´åº¦æ’åˆ—ã€‚</p>
<p>require_contiguous(Boolï¼Œé»˜è®¤å€¼ä¸ºTrue) - ç”¨æˆ·æ˜¯å¦æ˜¾å¼æŒ‡å®šnpu_contiguousç®—å­é€‚é…éœ€è¦å¯¹è¾“å…¥Tensoråšè½¬è¿ç»­ã€‚é»˜è®¤ä¸ºFalseï¼Œä½æ€§èƒ½æ¨¡å¼ã€‚ç”¨æˆ·æ˜ç¡®çŸ¥é“è¾“å…¥Tensorä¸ºè¿ç»­Tensoræˆ–è½¬ç½®Tensoræ—¶ï¼Œæ‰èƒ½è®¾ç½®ä¸ºTrueä½¿ç”¨é«˜æ€§èƒ½æ¨¡å¼ã€‚</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.one_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">one_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.one_" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p>torch_npu.one_(self) -&gt; Tensor</p>
<p>ç”¨1å¡«å……selfå¼ é‡ã€‚</p>
<p><strong>å‚æ•°è§£é‡Š</strong>ï¼š</p>
<p>self (Tensor) - è¾“å…¥å¼ é‡ã€‚</p>
<p>çº¦æŸæ¡ä»¶ï¼š</p>
<p>æ— </p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xtensor</span><span class="p">([[</span><span class="mf">0.6072</span><span class="p">,</span> <span class="mf">0.9726</span><span class="p">,</span> <span class="mf">0.3475</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3717</span><span class="p">,</span> <span class="mf">0.6135</span><span class="p">,</span> <span class="mf">0.6788</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">one_</span><span class="p">()</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_swiglu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_swiglu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_swiglu" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>æ¥å£åŸå‹</strong>ï¼š</p>
<p>torch_npu.npu_swiglu(Tensor self, int dim=-1) -&gt; (Tensor)</p>
<p><strong>åŠŸèƒ½æè¿°</strong>ï¼š</p>
<p>æä¾›swigluçš„æ¿€æ´»å‡½æ•°ã€‚</p>
<p>å…¬å¼å¦‚ä¸‹ï¼š</p>
<p>outputs = swiglu(x, dim = -1) = swish(A) * B = A * sigmoid(A) * B</p>
<p>â€œxâ€æ˜¯è¾“å…¥Tensorã€‚</p>
<p>â€œdimâ€æ˜¯åˆ‡åˆ†ç»´åº¦ï¼Œé»˜è®¤ä¸º-1ã€‚</p>
<p>â€œAâ€å’Œâ€œBâ€æ˜¯xæ²¿dimç»´åº¦åˆ‡åˆ†çš„Tensorã€‚</p>
<p><strong>å‚æ•°è¯´æ˜</strong>ï¼š</p>
<p>â€œxâ€ï¼šTensorç±»å‹ï¼Œshapeæ”¯æŒ1-8ç»´ï¼Œdtypeæ”¯æŒFP32ã€FP16æˆ–BF16ç±»å‹ã€‚</p>
<p>â€œdimâ€ï¼šIntç±»å‹ï¼Œé»˜è®¤ä¸º-1ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>ï¼š</p>
<p>è¾“å‡ºä¸ºTensorï¼Œè®¡ç®—å…¬å¼çš„æœ€ç»ˆè¾“å‡ºoutputsã€‚</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_swiglu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_trans_quant_param">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_trans_quant_param</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_trans_quant_param" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å®Œæˆé‡åŒ–è®¡ç®—å‚æ•°scaleæ•°æ®ç±»å‹çš„è½¬æ¢</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_trans_quant_param(Tensor scale, Tensor? offset=None) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>scale(è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæ˜¯1ç»´(tï¼Œ)æˆ–è€…2ç»´(1, n)ã€‚å…¶ä¸­t=1æˆ–n, å…¶ä¸­nä¸x2çš„nä¸€è‡´ã€‚</p>
<p>offset( è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚æ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæ˜¯1ç»´(tï¼Œ)ï¼Œæˆ–è€…2ç»´(1, n)ã€‚å…¶ä¸­t=1æˆ–n, å…¶ä¸­nä¸x2çš„nä¸€è‡´ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>ä¸€ä¸ªTensorç±»å‹çš„è¾“å‡ºï¼Œä»£è¡¨npu_trans_quant_paramçš„è®¡ç®—ç»“æœã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>1.ä¼ å…¥çš„scaleï¼Œoutä¸èƒ½æ˜¯ç©ºã€‚</p>
<p>2.scaleã€offsetã€outçš„æ•°æ®ç±»å‹å’Œæ•°æ®æ ¼å¼éœ€è¦åœ¨æ”¯æŒçš„èŒƒå›´ä¹‹å†…ã€‚</p>
<p>3.scaleã€offsetçš„shapeéœ€è¦ä¸º1ç»´(t,)æˆ–è€…2ç»´(1, n)ã€‚å…¶ä¸­t = 1æˆ–nï¼Œå…¶ä¸­nä¸x2çš„nä¸€è‡´ã€‚</p>
<p>4.å½“scaleçš„shapeä¸ºä¸¤ç»´(1, n)æ—¶ï¼Œscaleå’Œoffsetçš„shapeéœ€è¦ä¿æŒä¸€è‡´ï¼Œä¸”è¾“å‡ºshapeä¹Ÿä¸º(1, n)ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; å•ç®—å­è°ƒç”¨ï¼š
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import logging
&gt;&gt;&gt; import os
&gt;&gt;&gt;
&gt;&gt;&gt; scale = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; offset = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; npu_out = torch_npu.npu_trans_quant_param(scale.npu(), offset.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; å›¾æ¨¡å¼ï¼š
&gt;&gt;&gt; è¯´æ˜ï¼šå›¾æ¨¡å¼ä¸‹ï¼Œnpu_trans_quant_paramè®¡ç®—å‡ºæ¥çš„ç»“æœtensorä¸ºuint64æ•°æ®ç±»å‹ã€‚ç”±äºtorchä¸æ”¯æŒè¯¥æ•°æ®ç±»å‹ï¼Œéœ€è¦æ­é…å…¶ä»–æ¥å£ä½¿ç”¨ï¼Œå¦‚ä¸‹é¢ç¤ºä¾‹ä»£ç ä¸­çš„npu_quant_matmulã€‚
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias):
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale, offset)
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2, scale_1, offset=offset, bias=bias)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (15, 1, 512), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (15, 512, 128), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randn(1, dtype=torch.float32)
&gt;&gt;&gt; offset = torch.randn(1, dtype=torch.float32)
&gt;&gt;&gt; bias = torch.randint(-1,1, (15, 1, 128), dtype=torch.int32)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), offset.npu(), bias.npu())
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_quant_matmul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_quant_matmul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_quant_matmul" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å®Œæˆé‡åŒ–çš„çŸ©é˜µä¹˜è®¡ç®—ï¼Œæœ€å°æ”¯æŒè¾“å…¥ç»´åº¦ä¸º2ç»´ï¼Œæœ€å¤§æ”¯æŒè¾“å…¥ç»´åº¦ä¸º6ç»´ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_quant_matmul(Tensor x1, Tensor x2, Tensor scale, <a href="#id5"><span class="problematic" id="id6">*</span></a>ï¼ŒTensor? offset=None, Tensor? pertoken_scale=None, Tensor? bias=None, ScalarType? output_dtype=None) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>x1(è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT8ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæœ€å°‘æ˜¯2ç»´ï¼Œæœ€å¤šæ˜¯6ç»´ã€‚</p>
<p>x2(è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT8ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæœ€å°‘æ˜¯2ç»´ï¼Œæœ€å¤šæ˜¯6ç»´ã€‚</p>
<p>scale(è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32, INT64, BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæ˜¯1ç»´(tï¼Œ)ï¼Œt = 1æˆ–nï¼Œå…¶ä¸­nä¸x2çš„nä¸€è‡´ã€‚å¦‚éœ€ä¼ å…¥INT64æ•°æ®ç±»å‹çš„scale,  éœ€è¦æå‰è°ƒç”¨torch_npu.npu_trans_quant_paramæ¥å£æ¥è·å–INT64æ•°æ®ç±»å‹çš„scaleã€‚</p>
<p>offset( è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚æ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæ˜¯1ç»´(tï¼Œ)ï¼Œt = 1æˆ–nï¼Œå…¶ä¸­nä¸x2çš„nä¸€è‡´ã€‚</p>
<p>pertoken_scale(è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚æ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæ˜¯1ç»´(mï¼Œ)ï¼Œå…¶ä¸­mä¸x1çš„mä¸€è‡´ã€‚310På½“å‰ä¸æ”¯æŒpertoken_scaleã€‚</p>
<p>bias( è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚æ•°æ®ç±»å‹æ”¯æŒINT32ï¼ŒBFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œshapeæ”¯æŒ1ç»´(nï¼Œ)æˆ–3ç»´(batch,1,n)ï¼Œnä¸x2çš„nä¸€è‡´ã€‚bias 3ç»´(batch,1,n)åªå‡ºç°åœ¨outä¸º3ç»´çš„åœºæ™¯ä¸‹ï¼ŒåŒæ—¶batchå€¼éœ€è¦ç­‰äºx1, x2 boardcaståæ¨å¯¼å‡ºçš„batchå€¼ã€‚</p>
<p>output_dtype( è®¡ç®—è¾“å…¥)ï¼šDeviceä¾§çš„ScalarTypeï¼Œå¯é€‰å‚æ•°ã€‚è¡¨ç¤ºè¾“å‡ºTensorçš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒè¾“å…¥torch.int8ï¼Œtorch.float16, torch.bfloat16ã€‚é»˜è®¤å€¼ä¸ºNoneï¼Œä»£è¡¨è¾“å‡ºTensoræ•°æ®ç±»å‹ä¸ºINT8ã€‚310Påªæ”¯æŒoutput_dtypeä¸ºtorch.int8(å«None, ä¸‹åŒ)å’Œtorch.float16ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>ä¸€ä¸ªTensorç±»å‹çš„è¾“å‡ºï¼Œä»£è¡¨é‡åŒ–matmulçš„è®¡ç®—ç»“æœã€‚å¦‚æœoutput_dtypeä¸ºtorch.float16ï¼Œè¾“å‡ºçš„æ•°æ®ç±»å‹ä¸ºFLOAT16ï¼›å¦‚æœoutput_dtypeä¸ºtorch.bfloat16ï¼Œè¾“å‡ºçš„æ•°æ®ç±»å‹ä¸ºBFLOAT16ï¼›å¦‚æœoutput_dtypeä¸ºtorch.int8æˆ–è€…Noneï¼Œè¾“å‡ºçš„æ•°æ®ç±»å‹ä¸ºINT8ï¼›å¦‚æœoutput_dtypeéä»¥ä¸Šæ•°æ®ç±»å‹ï¼Œè¿”å›é”™è¯¯ç ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>ä¼ å…¥çš„x1ã€x2ã€scaleä¸èƒ½æ˜¯ç©ºã€‚</p>
<p>x1ã€x2ã€biasã€scaleã€offsetã€pertoken_scaleã€output_dtypeçš„æ•°æ®ç±»å‹å’Œæ•°æ®æ ¼å¼éœ€è¦åœ¨æ”¯æŒçš„èŒƒå›´ä¹‹å†…ã€‚</p>
<p>x1ã€x2çš„shapeéœ€è¦åœ¨2-6ç»´èŒƒå›´ã€‚</p>
<p>scale, offsetçš„shapeéœ€è¦ä¸º1ç»´(tï¼Œ)ï¼Œt = 1æˆ–nï¼Œnä¸x2çš„nä¸€è‡´ã€‚</p>
<p>pertoken_scaleçš„shapeéœ€è¦ä¸º1ç»´(m, )ï¼Œmä¸x1çš„mä¸€è‡´ï¼Œ310På½“å‰ä¸æ”¯æŒpertoken_scaleã€‚</p>
<p>biasçš„shapeæ”¯æŒ1ç»´(nï¼Œ)æˆ–3ç»´(batch,1,n)ï¼Œnä¸x2çš„nä¸€è‡´, batchå€¼éœ€è¦ç­‰äºx1, x2 boardcaståæ¨å¯¼å‡ºçš„batchå€¼ã€‚</p>
<p>biasçš„shapeåœ¨out æ˜¯2,4,5,6ç»´æƒ…å†µä¸‹éœ€è¦ä¸º1ç»´ï¼Œåœ¨out æ˜¯3ç»´æƒ…å†µä¸‹å¯ä»¥ä¸º1ç»´æˆ–3ç»´ã€‚</p>
<p>output_dtypeä¸ºtorch.bfloat16æ—¶ï¼Œscaleéœ€è¦ä¸ºBFLOAT16æ•°æ®ç±»å‹çš„Tensorã€‚output_dtypeä¸ºtorch.float16æˆ–torch.int8æ—¶ï¼Œscaleåœ¨pertoken_scaleä¸ºç©ºæ—¶å¯ä¸ºFLOAT32æˆ–INT64æ•°æ®ç±»å‹çš„Tensorã€‚output_dtypeä¸ºtorch.float16æ—¶ï¼Œscaleåœ¨pertoken_scaleä¸ä¸ºç©ºæ—¶å¿…é¡»ä¸ºfloat32ã€‚</p>
<p>biasä¸ºBFLOAT16æ•°æ®ç±»å‹æ—¶ï¼Œoutput_dtypeéœ€è¦ä¸ºtorch.bfloat16ã€‚</p>
<p>ç›®å‰è¾“å‡ºINT8/FLOAT16ä¸”æ— pertoken_scaleæƒ…å†µä¸‹ï¼Œå›¾æ¨¡å¼ä¸æ”¯æŒscaleç›´æ¥ä¼ å…¥FLOAT32æ•°æ®ç±»å‹ã€‚</p>
<p>pertoken_scaleä»…æ”¯æŒfloat32ï¼Œç›®å‰ä»…åœ¨è¾“å‡ºfloat16å’Œbfloat16åœºæ™¯ä¸‹å¯ä¸ä¸ºç©ºã€‚</p>
<p>offsetä¸ä¸ºç©ºæ—¶ï¼Œoutput_dtypeä»…æ”¯æŒint8ã€‚</p>
<p>x1ä¸x2æœ€åä¸€ç»´çš„shapeå¤§å°ä¸èƒ½è¶…è¿‡65535</p>
<p>310På’ŒAtlas A2èŠ¯ç‰‡ä¸‹ï¼Œéœ€è¦è°ƒç”¨npu_format_castå®Œæˆè¾“å…¥x2(weight)é«˜æ€§èƒ½æ•°æ®æ’å¸ƒåŠŸèƒ½ã€‚310Péœ€è¦å°†x2è½¬ç½®åè°ƒç”¨npu_format_castï¼ŒAtlas A2éœ€è¦å°†x2éè½¬ç½®åè°ƒç”¨npu_format_castã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 1.å•ç®—å­è°ƒç”¨ï¼š
&gt;&gt;&gt; åœ¨å•ç®—å­æ¨¡å¼ä¸‹ä¸æ”¯æŒä½¿èƒ½é«˜å¸¦å®½çš„x2æ•°æ®æ’å¸ƒï¼Œå¦‚æœæƒ³è¿½æ±‚æè‡´æ€§èƒ½ï¼Œè¯·ä½¿ç”¨å›¾æ¨¡å¼
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import logging
&gt;&gt;&gt; import os
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x1 = torch.randint(-5, 5, (1, 256, 768), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-5, 5, (31, 768, 16), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; offset = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; bias = torch.randint(-5, 5, (31, 1, 16), dtype=torch.int32)
&gt;&gt;&gt; # Method 1: You can directly call npu_quant_matmul
&gt;&gt;&gt; npu_out = torch_npu.npu_quant_matmul(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), offset=offset.npu(), bias=bias.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; # Method 2: You can first call npu_trans_quant_param to convert scale and offset from float32 to int64 when output dtype is torch.int8 or torch.float16
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale.npu(), offset.npu())
&gt;&gt;&gt; npu_out = torch_npu.npu_quant_matmul(cpu_x1.npu(), cpu_x2.npu(), scale_1, bias=bias.npu())
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; 2.å›¾æ¨¡å¼(è¾“å‡ºint8/fp16ä¸”æ— pertokenæƒ…å†µä¸‹ï¼Œå¿…é¡»å…ˆè°ƒç”¨npu_trans_quant_param):
&gt;&gt;&gt; 2.1 é€šç”¨
&gt;&gt;&gt; 2.1.1 ç¤ºä¾‹ä¸€ï¼šè¾“å‡ºfloat16
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; # &quot;ENABLE_ACLNN&quot;æ˜¯å¦ä½¿èƒ½èµ°aclnnï¼Œtrue: å›è°ƒèµ°aclnnï¼Œfalse: åœ¨çº¿ç¼–è¯‘
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2, scale, offset=offset, bias=bias, output_dtype=torch.float16)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (15, 1, 512), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (15, 512, 128), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randn(1, dtype=torch.float32)
&gt;&gt;&gt; # pertoken_scaleä¸ºç©ºæ—¶ï¼Œè¾“å‡ºfp16å¿…é¡»å…ˆè°ƒç”¨npu_trans_quant_param, å°†scale(offset)ä»floatè½¬ä¸ºint64.
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale.npu(), None)
&gt;&gt;&gt; bias = torch.randint(-1, 1, (15, 1, 128), dtype=torch.int32)
&gt;&gt;&gt; # dynamic=True: åŠ¨æ€å›¾æ¨¡å¼ï¼Œdynamic=False: é™æ€å›¾æ¨¡å¼
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale_1, None, bias.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; 2.1.2 ç¤ºä¾‹2ï¼šè¾“å‡ºbfloat16
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias, pertoken_scale):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2.t(), scale, offset=offset, bias=bias, pertoken_scale=pertoken_scale, output_dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; m = 15
&gt;&gt;&gt; k = 11264
&gt;&gt;&gt; n = 6912
&gt;&gt;&gt; bias_flag = True
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (m, k), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (n, k), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randint(-1, 1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; pertoken_scale = torch.randint(-1, 1, (m,), dtype=torch.float32)
&gt;&gt;&gt;
&gt;&gt;&gt; bias = torch.randint(-1, 1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; if bias_flag:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), None, None, pertoken_scale.npu())
&gt;&gt;&gt; else:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), None, bias.npu(), pertoken_scale.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; 2.2.1 310P å°†x2è½¬ç½®(batch,n,k)åformat
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2.transpose(2,1), scale, offset=offset, bias=bias)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (15, 1, 512), dtype=torch.int8).npu()
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (15, 512, 128), dtype=torch.int8).npu()
&gt;&gt;&gt; # Process x2 into a high-bandwidth format(29) offline to improve performance, please ensure that the input is continuous with (batch,n,k) layout
&gt;&gt;&gt; cpu_x2_t_29 = torch_npu.npu_format_cast(cpu_x2.transpose(2,1).contiguous(), 29)
&gt;&gt;&gt; scale = torch.randn(1, dtype=torch.float32).npu()
&gt;&gt;&gt; offset = torch.randn(1, dtype=torch.float32).npu()
&gt;&gt;&gt; bias = torch.randint(-1,1, (128,), dtype=torch.int32).npu()
&gt;&gt;&gt; # Process scale from float32 to int64 offline to improve performance
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale, offset)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=False)
&gt;&gt;&gt; npu_out = model(cpu_x1, cpu_x2_t_29, scale_1, offset, bias)
&gt;&gt;&gt;
&gt;&gt;&gt; 2.2.2 Atlas A2å°†éè½¬ç½®(batch,k,n)åè½¬format
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias, pertoken_scale):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2, scale, offset=offset, bias=bias, pertoken_scale=pertoken_scale,output_dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; m = 15
&gt;&gt;&gt; k = 11264
&gt;&gt;&gt; n = 6912
&gt;&gt;&gt; bias_flag = True
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (m, k), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (n, k), dtype=torch.int8)
&gt;&gt;&gt; # Process x2 into a high-bandwidth format(29) offline to improve performance, please ensure that the input is continuous with (batch,k,n) layout
&gt;&gt;&gt; x2_notranspose_29 = torch_npu.npu_format_cast(cpu_x2.npu().transpose(1,0).contiguous(), 29)
&gt;&gt;&gt; scale = torch.randint(-1, 1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; pertoken_scale = torch.randint(-1, 1, (m,), dtype=torch.float32)
&gt;&gt;&gt;
&gt;&gt;&gt; bias = torch.randint(-1,1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; if bias_flag:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), x2_notranspose_29, scale.npu(), None, None, pertoken_scale.npu())
&gt;&gt;&gt; else:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), x2_notranspose_29, scale.npu(), None, bias.npu(), pertoken_scale.npu())
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_weight_quant_batchmatmul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_weight_quant_batchmatmul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_weight_quant_batchmatmul" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>è¯¥æ¥å£ç”¨äºå®ç°çŸ©é˜µä¹˜è®¡ç®—ä¸­çš„weightè¾“å…¥å’Œè¾“å‡ºçš„é‡åŒ–æ“ä½œï¼Œæ”¯æŒpertensorï¼Œperchannelï¼Œpergroupå¤šåœºæ™¯é‡åŒ–(310På½“å‰ä»…æ”¯æŒperchannel)ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_weight_quant_batchmatmul(Tensor x, Tensor weight, Tensor antiquant_scale, Tensor? antiquant_offset=None, Tensor? quant_scale=None, Tensor? quant_offset=None, Tensor? bias=None, int antiquant_group_size=0) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>x : Deviceä¾§Tensorç±»å‹ï¼Œå³çŸ©é˜µä¹˜ä¸­çš„xã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16/BFLOAT16ï¼Œ æ”¯æŒéè¿ç»­çš„Tensorï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(M,K) ï¼›310Pä¸Šæ•°æ®ç±»å‹ä»…æ”¯æŒFLOAT16ï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸º2-6ç»´ï¼Œæ”¯æŒbatchè½´ä½†ä¸æ”¯æŒbroadcastã€‚</p>
<p>weightï¼šDeviceä¾§Tensorç±»å‹ï¼Œå³çŸ©é˜µä¹˜ä¸­çš„weightã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒINT8ï¼Œ æ”¯æŒéè¿ç»­çš„Tensorï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(K,N)ï¼›310Pä¸Šæ•°æ®ç±»å‹ä»…æ”¯æŒFLOAT16ï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸º2-6ç»´ï¼Œæ”¯æŒbatchè½´ä½†ä¸æ”¯æŒbroadcastï¼Œç»´åº¦éœ€ä¸xä¿æŒä¸€è‡´ã€‚</p>
<p>antiquantscaleï¼šDeviceä¾§Tensorç±»å‹ï¼Œåé‡åŒ–çš„scaleï¼Œç”¨äºweightçŸ©é˜µåé‡åŒ– ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16/BFLOAT16ï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(1, N)æˆ– ä¸€ç»´(N, )ã€(1, )ï¼›310Pä¸Šæ•°æ®ç±»å‹ä»…æ”¯æŒFLOAT16ã€‚</p>
<p>antiquantoffsetï¼šDeviceä¾§Tensorç±»å‹ï¼Œåé‡åŒ–çš„offsetï¼Œç”¨äºweightçŸ©é˜µåé‡åŒ– ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16/BFLOAT16ï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(1, N)æˆ– ä¸€ç»´(N, )ã€(1, )ï¼›310Pä¸Šæ•°æ®ç±»å‹ä»…æ”¯æŒFLOAT16ã€‚</p>
<p>quantscaleï¼šDeviceä¾§Tensorç±»å‹ï¼Œé‡åŒ–çš„scaleï¼Œç”¨äºè¾“å‡ºçŸ©é˜µçš„é‡åŒ– ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32/INT64ï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(1, N) æˆ– ä¸€ç»´(N, )ã€(1, )ï¼›310Pæš‚æœªä½¿ç”¨æ­¤å‚æ•°ã€‚</p>
<p>quantoffset: Deviceä¾§Tensorç±»å‹ï¼Œé‡åŒ–çš„offsetï¼Œç”¨äºè¾“å‡ºçŸ©é˜µçš„é‡åŒ– ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(1, N) æˆ– ä¸€ç»´(N, )ã€(1, )ï¼›310Pæš‚æœªä½¿ç”¨æ­¤å‚æ•°ã€‚</p>
<p>biasï¼šDeviceä¾§Tensorç±»å‹ï¼Œ å³çŸ©é˜µä¹˜ä¸­çš„biasï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16/FLOAT32ï¼Œ æ”¯æŒéè¿ç»­çš„Tensorï¼Œæ”¯æŒè¾“å…¥ç»´åº¦ä¸ºä¸¤ç»´(1, N) æˆ– ä¸€ç»´(N, )ã€(1, )ã€‚</p>
<p>antiquant_group_sizeï¼šintç±»å‹ï¼Œ ç”¨äºæ§åˆ¶pergroupåœºæ™¯ä¸‹çš„groupå¤§å°ï¼Œå½“å‰é»˜è®¤ä¸º0ï¼Œé¢„ç•™å‚æ•°ï¼Œæš‚æœªä½¿ç”¨ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>è¾“å‡ºä¸ºTensorç±»å‹ï¼Œä»£è¡¨è®¡ç®—ç»“æœã€‚å½“è¾“å…¥å­˜åœ¨quantscaleæ—¶è¾“å‡ºæ•°æ®ç±»å‹ä¸ºINT8ï¼Œå½“è¾“å…¥ä¸å­˜quant_sclaeæ—¶è¾“å‡ºæ•°æ®ç±»å‹å’Œè¾“å…¥xä¸€è‡´ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>xå’Œweightå¿…é¡»ä¸º(M,K)å’Œ(K,N)æ ¼å¼ï¼ŒMã€Kã€Nçš„èŒƒå›´ä¸º[1, 65535]ï¼›310Pæ— æ­¤çº¦æŸã€‚</p>
<p>ä¸æ”¯æŒç©ºTensorè¾“å…¥ã€‚</p>
<p>antiquantscaleå’Œantiquantoffsetçš„è¾“å…¥shapeè¦ä¿æŒä¸€è‡´ã€‚</p>
<p>quantscaleå’Œquantoffsetçš„è¾“å…¥shapeè¦ä¿æŒä¸€è‡´ã€‚</p>
<p>quantoffsetä¸èƒ½ç‹¬ç«‹äºquantscaleå­˜åœ¨ã€‚</p>
<p>å½“xè¾“å…¥ç±»å‹ä¸ºBFLOAT16ç±»å‹æ—¶å€™ï¼Œbiasçš„è¾“å…¥ç±»å‹ä¸ºFLOAT32ï¼›å½“xè¾“å…¥ç±»å‹ä¸ºFLOAT16ç±»å‹æ—¶å€™ï¼Œbiasçš„è¾“å…¥ç±»å‹ä¸ºFLOAT16ã€‚</p>
<p>å¦‚éœ€ä¼ å…¥INT64æ•°æ®ç±»å‹çš„quantscale,  éœ€è¦æå‰è°ƒç”¨torch_npu.npu_trans_quant_paramæ¥å£å°†æ•°æ®ç±»å‹ä¸ºFLOAT32çš„quantscaleå’Œquantoffsetè½¬æ¢ä¸ºæ•°æ®ç±»å‹ä¸ºINT64çš„quantscaleè¾“å…¥ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.0</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11.0</p>
<p>æ”¯æŒçš„èŠ¯ç‰‡å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; å•ç®—å­æ¨¡å¼ï¼š
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x = torch.randn((8192, 320),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_weight = torch.randn((320, 256),device=&#39;npu&#39;,dtype=torch.int8)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; npu_out = torch_npu.npu_weight_quant_batchmatmul(cpu_x.npu(), cpu_weight.npu(), cpu_antiquantscale.npu(), cpu_antiquantoffset.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; å›¾æ¨¡å¼ï¼š
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import  torchair as tng
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; config.debug.graph_dump.type = &quot;pbtxt&quot;
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x = torch.randn((8192, 320),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_weight = torch.randn((320, 256),device=&#39;npu&#39;,dtype=torch.int8)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt;
&gt;&gt;&gt; def forward(self, x, weight, antiquant_scale, antiquant_offset, quant_scale,quant_offset, bias, antiquant_group_size):
&gt;&gt;&gt; return torch_npu.npu_weight_quant_batchmatmul(x, weight, antiquant_scale, antiquant_offset, quant_scale ,quant_offset, bias, antiquant_group_size)
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)npu_out = model(cpu_x.npu(), cpu_weight.npu(), cpu_antiquantscale.npu(), cpu_antiquantoffset.npu(), None, None, None, 0)
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_convert_weight_to_int4pack">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_convert_weight_to_int4pack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_convert_weight_to_int4pack" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>è¯¥æ¥å£å°†int32çš„è¾“å…¥tensoræ‰“åŒ…ä¸ºint4å­˜æ”¾ï¼Œæ¯8ä¸ªint4æ•°æ®é€šè¿‡ä¸€ä¸ªint32æ•°æ®æ‰¿è½½ï¼Œå¹¶è¿›è¡Œäº¤å æ’æ”¾ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_convert_weight_to_int4pack(Tensor weight, int inner_k_tiles=0) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>weight : Deviceä¾§Tensorç±»å‹ï¼Œè¾“å…¥çš„weightã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ•°æ®ç±»å‹æ”¯æŒINT32ï¼Œ ä¸æ”¯æŒéè¿ç»­çš„Tensorã€‚ç»´åº¦æ”¯æŒ2ç»´ï¼Œshapeæ”¯æŒï¼ˆk, nï¼‰, (n, k)ã€‚æœ€åä¸€ç»´åº¦éœ€è¦8ä¸ªå…ƒç´ å¯¹é½ã€‚</p>
<p>inner_k_tilesï¼šintç±»å‹ï¼Œç”¨äºåˆ¶å®šå†…éƒ¨æ‰“åŒ…æ ¼å¼ä¸­ï¼Œå¤šå°‘ä¸ªK-tilesè¢«æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œé»˜è®¤å€¼ä¸º0. é¢„ç•™å‚æ•°ï¼Œæš‚æœªä½¿ç”¨ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>è¾“å‡ºä¸ºTensorç±»å‹ï¼Œä»£è¡¨int4æ‰“åŒ…åçš„è¾“å‡ºã€‚æ•°æ®ç±»å‹ä¸ºINT32ï¼Œshapeä¸ºï¼ˆk, n/8ï¼‰, (n, k/8), æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>è¾“å…¥weightä¸­çš„å…ƒç´ çš„å€¼éœ€è¦åœ¨int4çš„è¡¨ç¤ºèŒƒå›´å†…ï¼Œå³[-8, 7]ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.3.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.2</p>
<p>PyTorch 1.11</p>
<p>æ”¯æŒçš„èŠ¯ç‰‡å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; å•ç®—å­æ¨¡å¼ï¼š
&gt;&gt;&gt;
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt;
&gt;&gt;&gt; m = 128
&gt;&gt;&gt; k = 64
&gt;&gt;&gt; n = 32
&gt;&gt;&gt; trans_weight = False
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x = torch.randn((m, k), dtype=torch.float16)
&gt;&gt;&gt; if trans_weight:
&gt;&gt;&gt; cpu_weight = torch.randint(low=-8, high=8, size=(n, k), dtype=torch.int32)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((n, 1), dtype=torch.float16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((n, 1), dtype=torch.float16)
&gt;&gt;&gt; else:
&gt;&gt;&gt; cpu_weight = torch.randint(low=-8, high=8, size=(k, n), dtype=torch.int32)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((1, n), dtype=torch.float16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((1, n), dtype=torch.float16)
&gt;&gt;&gt;
&gt;&gt;&gt; weight_int4 = torch_npu.npu_convert_weight_to_int4pack(cpu_weight.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; if trans_weight:
&gt;&gt;&gt; cpu_weight = cpu_weight.transpose(-1, -2)
&gt;&gt;&gt; weight_int4 = weight_int4.transpose(-1, -2)
&gt;&gt;&gt; cpu_antiquantscale = cpu_antiquantscale.transpose(-1, -2)
&gt;&gt;&gt; cpu_antiquantoffset = cpu_antiquantoffset.transpose(-1, -2)
&gt;&gt;&gt;
&gt;&gt;&gt; npu_out = torch_npu.npu_weight_quant_batchmatmul(cpu_x.npu(), weight_int4.npu(), cpu_antiquantscale.npu(), cpu_antiquantoffset.npu())
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_grouped_matmul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_grouped_matmul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_grouped_matmul" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>GroupedMatmulç®—å­å¯ä»¥å®ç°åˆ†ç»„çŸ©é˜µä¹˜è®¡ç®—ï¼Œæ¯ç»„çŸ©é˜µä¹˜çš„ç»´åº¦å¤§å°å¯ä»¥ä¸åŒï¼Œæ˜¯ä¸€ç§çµæ´»çš„æ”¯æŒæ–¹å¼ã€‚å…¶ä¸»è¦è¾“å…¥ä¸è¾“å‡ºå‡ä¸ºTensorListï¼Œå…¶ä¸­è¾“å…¥æ•°æ®xä¸è¾“å‡ºç»“æœyå‡æ”¯æŒåˆ‡åˆ†åŠä¸åˆ‡åˆ†çš„æ¨¡å¼ï¼Œæ ¹æ®å‚æ•°split_itemæ¥ç¡®å®šxä¸yæ˜¯å¦éœ€è¦åˆ‡åˆ†ï¼Œåœ¨xéœ€è¦åˆ‡åˆ†çš„æƒ…å†µä¸‹ä½¿ç”¨å‚æ•°group_listæ¥æè¿°å¯¹xçš„mè½´è¿›è¡Œåˆ‡åˆ†çš„æ–¹å¼ã€‚</p>
<p>æ ¹æ®è¾“å…¥xã€è¾“å…¥weightä¸è¾“å‡ºyçš„Tensoræ•°é‡ä¸åŒï¼Œå¯ä»¥æ”¯æŒå¦‚ä¸‹4ç§åœºæ™¯ï¼š</p>
<p>xã€weightã€yéƒ½ä¸ºå¤šTensorï¼Œå³æ¯ç»„çš„æ•°æ®å¯¹åº”çš„Tensoræ˜¯ç‹¬ç«‹çš„ã€‚</p>
<p>xä¸ºå•Tensorï¼Œweight/yä¸ºå¤šTensorï¼Œæ­¤æ—¶éœ€è¦é€šè¿‡å¯é€‰å‚æ•°group_listè¯´æ˜xåœ¨è¡Œä¸Šçš„åˆ†ç»„æƒ…å†µï¼Œå¦‚group_list[0]=10è¯´æ˜xçš„å‰10è¡Œå‚ä¸ç¬¬ä¸€ç»„çŸ©é˜µä¹˜è®¡ç®—ã€‚</p>
<p>xã€weightä¸ºå¤šTensorï¼Œyä¸ºå•Tensorï¼Œæ­¤æ—¶æ¯ç»„çŸ©é˜µä¹˜çš„ç»“æœæ”¾åœ¨åŒä¸€ä¸ªTensorä¸­è¿ç»­å­˜æ”¾ã€‚</p>
<p>xã€yä¸ºå•Tensorï¼Œweightä¸ºå¤šTensorï¼Œå±äºå‰ä¸¤ç§æƒ…å†µçš„ç»„åˆã€‚</p>
<p>è®¡ç®—å…¬å¼ä¸ºï¼š</p>
<p>éé‡åŒ–åœºæ™¯ï¼š</p>
<p>y_i = x_i * weight_i + bias_i</p>
<p>é‡åŒ–åœºæ™¯ï¼š</p>
<p>y_i = (x_i * weight_i + bias_i) * scale_i + offset_i</p>
<p>åé‡åŒ–åœºæ™¯ï¼š</p>
<p>y_i = (x_i * weight_i + bias_i) * scale_i</p>
<p>ä¼ªé‡åŒ–åœºæ™¯ï¼š</p>
<p>y_i = x_i * (weight_i + antiquant_offset_i) * antiquant_scale_i + bias_i</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>PyTorch 2.1åŠæ›´é«˜çš„ç‰ˆæœ¬ä¸­ï¼š</p>
<p>npu_grouped_matmul(Tensor[] x, Tensor[] weight, <a href="#id7"><span class="problematic" id="id8">*</span></a>, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, int[]? group_list=None, int? split_item=0, ScalarType? output_dtype=None) -&gt; Tensor[]</p>
<p>PyTorch 1.11ä¸2.0ç‰ˆæœ¬ï¼š</p>
<p>npu_grouped_matmul(Tensor[] x, Tensor[] weight, <a href="#id9"><span class="problematic" id="id10">*</span></a>, Tensor[] bias, Tensor[] scale, Tensor[] offset, Tensor[] antiquant_scale, Tensor[] antiquant_offset, int[]? group_list=None, int? split_item=0, ScalarType? output_dtype=None) -&gt; Tensor[]</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<ul class="simple">
<li><p>xï¼šå¿…é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œå³è¾“å…¥å‚æ•°ä¸­çš„xï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ã€INT8ï¼›æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒçš„æœ€å¤§é•¿åº¦ä¸º128ä¸ªï¼Œå…¶ä¸­æ¯ä¸ªTensoråœ¨split_item=0çš„æ¨¡å¼ä¸‹æ”¯æŒè¾“å…¥2è‡³6ç»´ï¼Œå…¶ä½™æ¨¡å¼ä¸‹æ”¯æŒè¾“å…¥ä¸º2ç»´ã€‚</p></li>
<li><p>weightï¼šå¿…é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œå³è¾“å…¥å‚æ•°ä¸­matmulçš„weightè¾“å…¥ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ã€INT8ï¼›æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒçš„æœ€å¤§é•¿åº¦ä¸º128ä¸ªï¼Œå…¶ä¸­æ¯ä¸ªTensoræ”¯æŒè¾“å…¥ä¸º2ç»´ã€‚</p></li>
<li><p>biasï¼šåœ¨PyTorch 1.11ä¸2.0ç‰ˆæœ¬ä¸­æ˜¯å¿…é€‰å‚æ•°ï¼Œåœ¨PyTorch 2.1ä¸æ›´é«˜çš„ç‰ˆæœ¬ä¸­æ˜¯å¯é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œå³è¾“å…¥å‚æ•°ä¸­matmulçš„biasè¾“å…¥ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€FLOAT32ã€INT32ï¼›æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒçš„æœ€å¤§é•¿åº¦ä¸º128ä¸ªï¼Œå…¶ä¸­æ¯ä¸ªTensoræ”¯æŒè¾“å…¥ä¸º1ç»´ã€‚</p></li>
<li><p>scaleï¼šå¯é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œä»£è¡¨é‡åŒ–å‚æ•°ä¸­çš„ç¼©æ”¾å› å­ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œé•¿åº¦ä¸weightç›¸åŒã€‚</p></li>
<li><p>offsetï¼šå¯é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œä»£è¡¨é‡åŒ–å‚æ•°ä¸­çš„åç§»é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œé•¿åº¦ä¸weightç›¸åŒã€‚</p></li>
<li><p>antiquant_scaleï¼šå¯é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œä»£è¡¨ä¼ªé‡åŒ–å‚æ•°ä¸­çš„ç¼©æ”¾å› å­ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œé•¿åº¦ä¸weightç›¸åŒã€‚</p></li>
<li><p>antiquant_offsetï¼šå¯é€‰å‚æ•°ï¼ŒDeviceä¾§çš„TensorListï¼Œä»£è¡¨ä¼ªé‡åŒ–å‚æ•°ä¸­çš„åç§»é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œé•¿åº¦ä¸weightç›¸åŒã€‚</p></li>
</ul>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>Deviceä¾§çš„TensorListç±»å‹è¾“å‡ºï¼Œä»£è¡¨GroupedMatmulçš„è®¡ç®—ç»“æœï¼Œå½“split_itemå–0æˆ–1æ—¶ï¼Œå…¶Tensorä¸ªæ•°ä¸weightç›¸åŒï¼Œå½“split_itemå–2æˆ–3æ—¶ï¼Œå…¶Tensorä¸ªæ•°ä¸º1ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<ol class="arabic simple">
<li><p>è‹¥xä¸ºå¤šTensorï¼Œgroup_listå¯ä»¥ä¸ºç©ºï¼›å½“xä¸ºå•Tensorï¼Œgroup_listçš„é•¿åº¦ä¸weightçš„Tensorä¸ªæ•°ç›¸åŒã€‚</p></li>
<li><p>è‹¥biasä¸ä¸ºç©ºï¼Œå…¶Tensoræ•°é‡é¡»ä¸weightä¿æŒä¸€è‡´ã€‚</p></li>
<li><p>è®°ä¸€ä¸ªmatmulè®¡ç®—æ¶‰åŠçš„xã€weightä¸yçš„ç»´åº¦åˆ†åˆ«ä¸º(mÃ—k)ã€(kÃ—n)å’Œ(mÃ—n)ï¼Œåˆ™æ¯ä¸€ä¸ªmatmulçš„è¾“å…¥ä¸è¾“å‡ºé¡»æ»¡è¶³[m, k]å’Œ[k, n]çš„kç»´åº¦ç›¸ç­‰å…³ç³»ã€‚</p></li>
<li><p>éé‡åŒ–åœºæ™¯æ”¯æŒçš„è¾“å…¥ç±»å‹ä¸ºï¼š</p></li>
</ol>
<ul class="simple">
<li><p>xä¸ºFLOAT16ã€weightä¸ºFLOAT16ã€biasä¸ºFLOAT16ã€scaleä¸ºç©ºã€offsetä¸ºç©ºã€antiquant_scaleä¸ºç©ºã€antiquant_offsetä¸ºç©ºã€output_dtypeä¸ºFLOAT16ï¼›</p></li>
<li><p>xä¸ºBFLOAT16ã€weightä¸ºBFLOAT16ã€biasä¸ºFLOAT32ã€scaleä¸ºç©ºã€offsetä¸ºç©ºã€antiquant_scaleä¸ºç©ºã€antiquant_offsetä¸ºç©ºã€output_dtypeä¸ºBFLOAT16ï¼›</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>é‡åŒ–åœºæ™¯æ”¯æŒçš„è¾“å…¥ç±»å‹ä¸ºï¼šxä¸ºINT8ã€weightä¸ºINT8ã€biasä¸ºINT32ã€scaleä¸ºUINT64ã€offsetä¸ºç©ºã€antiquant_scaleä¸ºç©ºã€antiquant_offsetä¸ºç©ºã€output_dtypeä¸ºINT8ï¼›</p></li>
<li><p>ä¼ªé‡åŒ–åœºæ™¯æ”¯æŒçš„è¾“å…¥ç±»å‹ä¸ºï¼š</p></li>
</ol>
<ul class="simple">
<li><p>xä¸ºFLOAT16ã€weightä¸ºINT8ã€biasä¸ºFLOAT16ã€scaleä¸ºç©ºï¼Œoffsetä¸ºç©ºï¼Œantiquant_scaleä¸ºFLOAT16ã€antiquant_offsetä¸ºFLOAT16ã€output_dtypeä¸ºFLOAT16ï¼›</p></li>
<li><p>xä¸ºBFLOAT16ã€weightä¸ºINT8ã€biasä¸ºFLOAT32ã€scaleä¸ºç©ºï¼Œoffsetä¸ºç©ºï¼Œantiquant_scaleä¸ºBFLOAT16ã€antiquant_offsetä¸ºBFLOAT16ã€output_dtypeä¸ºBFLOAT16ï¼›</p></li>
</ul>
<ol class="arabic simple" start="7">
<li><p>å¯¹äºå®é™…æ— biasçš„åœºæ™¯ï¼Œåœ¨PyTorch 1.11ä¸2.0ç‰ˆæœ¬ä¸­ï¼Œé¡»æ‰‹åŠ¨æŒ‡å®šâ€œbias=[]â€ï¼›åœ¨PyTorch 2.1åŠæ›´é«˜çš„ç‰ˆæœ¬ä¸­ï¼Œå¯ä»¥ç›´æ¥ä¸æŒ‡å®šbiaså‚æ•°ã€‚scaleã€offsetã€antiquantScaleã€antiquantOffsetå››ä¸ªå‚æ•°åœ¨ä¸åŒPyTorchç‰ˆæœ¬ä¸­çš„çº¦æŸä¸biasç›¸åŒã€‚</p></li>
</ol>
<p>output_dtypeçš„æ•°æ®ç±»å‹å½“å‰åªæ”¯æŒNoneï¼Œæˆ–è€…ä¸è¾“å…¥xçš„æ•°æ®ç±»å‹ç›¸åŒã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p>Atlas A3 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># å•ç®—å­è°ƒç”¨æ¨¡å¼ï¼ŒTorch1.11ã€Torch2.0ç‰ˆæœ¬</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">bias1</span><span class="p">,</span> <span class="n">bias2</span><span class="p">,</span> <span class="n">bias3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_list</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split_item</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grouped_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">[],</span> <span class="n">offset</span><span class="o">=</span><span class="p">[],</span> <span class="n">antiquant_scale</span><span class="o">=</span><span class="p">[],</span> <span class="n">antiquant_offset</span><span class="o">=</span><span class="p">[],</span> <span class="n">group_list</span><span class="o">=</span><span class="n">group_list</span><span class="p">,</span> <span class="n">split_item</span><span class="o">=</span><span class="n">split_item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># å•ç®—å­è°ƒç”¨æ¨¡å¼ï¼ŒTorch2.1åŠæ›´é«˜çš„ç‰ˆæœ¬</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">bias1</span><span class="p">,</span> <span class="n">bias2</span><span class="p">,</span> <span class="n">bias3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_list</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split_item</span> <span class="o">=</span> <span class="mi">0</span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grouped_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">group_list</span><span class="o">=</span><span class="n">group_list</span><span class="p">,</span> <span class="n">split_item</span><span class="o">=</span><span class="n">split_item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># å›¾æ¨¡å¼è°ƒç”¨ï¼ŒTorch2.1åŠæ›´é«˜çš„ç‰ˆæœ¬</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">GMMModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grouped_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GMMModel</span><span class="p">()</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">custom_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_quant_scatter">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_quant_scatter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_quant_scatter" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å…ˆå°†updatesè¿›è¡Œé‡åŒ–ï¼Œç„¶åå°†updatesä¸­çš„å€¼æŒ‰æŒ‡å®šçš„è½´axiså’Œç´¢å¼•indicesæ›´æ–°selfä¸­çš„å€¼ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°è¾“å‡ºtensorï¼Œselfæœ¬èº«çš„æ•°æ®ä¸å˜ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_quant_scatter(Tensor self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>selfï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæºæ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>indicesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œç´¢å¼•å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>updatesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæ›´æ–°æ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒBFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>quant_scalesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œé‡åŒ–ç¼©æ”¾å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒBFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>quant_zero_pointsï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¯é€‰è¾“å…¥ï¼Œé‡åŒ–åç§»å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒBFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>axisï¼šHostä¾§çš„intç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œupdatesä¸Šç”¨æ¥æ›´æ–°çš„è½´ã€‚</p>
<p>quant_axisï¼šHostä¾§çš„intç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œupdatesä¸Šç”¨æ¥é‡åŒ–çš„è½´ã€‚</p>
<p>reduceï¼šHostä¾§çš„strç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œè¡¨ç¤ºæ•°æ®æ“ä½œæ–¹å¼ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>ä¸€ä¸ªTensorç±»å‹çš„è¾“å‡ºï¼Œä»£è¡¨selfè¢«æ›´æ–°åçš„ç»“æœã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>selfçš„ç»´æ•°åªèƒ½æ˜¯3~8ç»´ã€‚</p>
<p>indicesçš„ç»´æ•°åªèƒ½æ˜¯1ç»´æˆ–è€…2ç»´ï¼›å¦‚æœæ˜¯2ç»´ï¼Œå…¶ç¬¬2ç»´çš„å¤§å°å¿…é¡»æ˜¯2ï¼›ä¸æ”¯æŒç´¢å¼•è¶Šç•Œï¼Œç´¢å¼•è¶Šç•Œä¸æ ¡éªŒï¼›indicesæ˜ å°„çš„selfæ•°æ®æ®µä¸èƒ½é‡åˆï¼Œè‹¥é‡åˆåˆ™ä¼šå› ä¸ºå¤šæ ¸å¹¶å‘åŸå› å¯¼è‡´å¤šæ¬¡æ‰§è¡Œç»“æœä¸ä¸€æ ·ã€‚</p>
<p>updatesçš„ç»´æ•°éœ€è¦ä¸selfçš„ç»´æ•°ä¸€æ ·ï¼›å…¶ç¬¬1ç»´çš„å¤§å°ç­‰äºindicesçš„ç¬¬1ç»´çš„å¤§å°ï¼Œä¸”ä¸å¤§äºselfçš„ç¬¬1ç»´çš„å¤§å°ï¼›å…¶axisè½´çš„å¤§å°ä¸å¤§äºselfçš„axisè½´çš„å¤§å°ï¼›å…¶ä½™ç»´åº¦çš„å¤§å°è¦è·Ÿselfå¯¹åº”ç»´åº¦çš„å¤§å°ç›¸ç­‰ï¼›å…¶æœ€åä¸€ç»´çš„å¤§å°å¿…é¡»32Bå¯¹é½ã€‚</p>
<p>quant_scalesçš„å…ƒç´ ä¸ªæ•°éœ€è¦ç­‰äºupdatesåœ¨quant_axisè½´çš„å¤§å°ã€‚</p>
<p>quant_zero_pointsçš„å…ƒç´ ä¸ªæ•°éœ€è¦ç­‰äºupdatesåœ¨quant_axisè½´çš„å¤§å°ã€‚</p>
<p>axisä¸èƒ½ä¸ºupdatesçš„ç¬¬1ç»´æˆ–æœ€å1ç»´ã€‚</p>
<p>quant_axisåªèƒ½ä¸ºupdatesçš„æœ€å1ç»´ã€‚</p>
<p>reduceå½“å‰åªæ”¯æŒâ€˜updateâ€™ï¼Œå³æ›´æ–°æ“ä½œã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_scales</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_zero_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_zero_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_zero_points</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reduce</span> <span class="o">=</span> <span class="s2">&quot;update&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_quant_scatter</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">quant_scales</span><span class="p">,</span> <span class="n">quant_zero_points</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">quant_axis</span><span class="o">=</span><span class="n">quant_axis</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_quant_scatter_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_quant_scatter_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_quant_scatter_" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><a href="#id11"><span class="problematic" id="id12">**</span></a>åŠŸèƒ½æè¿°**å…ˆå°†:</p>
<p>updatesè¿›è¡Œé‡åŒ–ï¼Œç„¶åå°†updatesä¸­çš„å€¼æŒ‰æŒ‡å®šçš„è½´axiså’Œç´¢å¼•indicesæ›´æ–°selfä¸­çš„å€¼ï¼Œselfä¸­çš„æ•°æ®è¢«æ”¹å˜ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_quant_scatter_(Tensor(a!) self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -&gt; Tensor(a!)</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>selfï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæºæ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>indicesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œç´¢å¼•å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>updatesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæ›´æ–°æ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒBFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>quant_scalesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œé‡åŒ–ç¼©æ”¾å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒBFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>quant_zero_pointsï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¯é€‰è¾“å…¥ï¼Œé‡åŒ–åç§»å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒBFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚</p>
<p>axisï¼šHostä¾§çš„intç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œupdatesä¸Šç”¨æ¥æ›´æ–°çš„è½´ã€‚</p>
<p>quant_axisï¼šHostä¾§çš„intç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œupdatesä¸Šç”¨æ¥é‡åŒ–çš„è½´ã€‚</p>
<p>reduceï¼šHostä¾§çš„strç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œè¡¨ç¤ºæ•°æ®æ“ä½œæ–¹å¼ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>è¿”å›è¢«æ›´æ–°åçš„selfã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>selfçš„ç»´æ•°åªèƒ½æ˜¯3~8ç»´ã€‚</p>
<p>indicesçš„ç»´æ•°åªèƒ½æ˜¯1ç»´æˆ–è€…2ç»´ï¼›å¦‚æœæ˜¯2ç»´ï¼Œå…¶ç¬¬2ç»´çš„å¤§å°å¿…é¡»æ˜¯2ï¼›ä¸æ”¯æŒç´¢å¼•è¶Šç•Œï¼Œç´¢å¼•è¶Šç•Œä¸æ ¡éªŒï¼›indicesæ˜ å°„çš„selfæ•°æ®æ®µä¸èƒ½é‡åˆï¼Œè‹¥é‡åˆåˆ™ä¼šå› ä¸ºå¤šæ ¸å¹¶å‘åŸå› å¯¼è‡´å¤šæ¬¡æ‰§è¡Œç»“æœä¸ä¸€æ ·ã€‚</p>
<p>updatesçš„ç»´æ•°éœ€è¦ä¸selfçš„ç»´æ•°ä¸€æ ·ï¼›å…¶ç¬¬1ç»´çš„å¤§å°ç­‰äºindicesçš„ç¬¬1ç»´çš„å¤§å°ï¼Œä¸”ä¸å¤§äºselfçš„ç¬¬1ç»´çš„å¤§å°ï¼›å…¶axisè½´çš„å¤§å°ä¸å¤§äºselfçš„axisè½´çš„å¤§å°ï¼›å…¶ä½™ç»´åº¦çš„å¤§å°è¦è·Ÿselfå¯¹åº”ç»´åº¦çš„å¤§å°ç›¸ç­‰ï¼›å…¶æœ€åä¸€ç»´çš„å¤§å°å¿…é¡»32Bå¯¹é½ã€‚</p>
<p>quant_scalesçš„å…ƒç´ ä¸ªæ•°éœ€è¦ç­‰äºupdatesåœ¨quant_axisè½´çš„å¤§å°ã€‚</p>
<p>quant_zero_pointsçš„å…ƒç´ ä¸ªæ•°éœ€è¦ç­‰äºupdatesåœ¨quant_axisè½´çš„å¤§å°ã€‚</p>
<p>axisä¸èƒ½ä¸ºupdatesçš„ç¬¬1ç»´æˆ–æœ€å1ç»´ã€‚</p>
<p>quant_axisåªèƒ½ä¸ºupdatesçš„æœ€å1ç»´ã€‚</p>
<p>reduceå½“å‰åªæ”¯æŒâ€˜updateâ€™ï¼Œå³æ›´æ–°æ“ä½œã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_scales</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_zero_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_zero_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_zero_points</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reduce</span> <span class="o">=</span> <span class="s2">&quot;update&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_quant_scatter_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">quant_scales</span><span class="p">,</span> <span class="n">quant_zero_points</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">quant_axis</span><span class="o">=</span><span class="n">quant_axis</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scatter_nd_update">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scatter_nd_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scatter_nd_update" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å°†updatesä¸­çš„å€¼æŒ‰æŒ‡å®šçš„ç´¢å¼•indicesæ›´æ–°selfä¸­çš„å€¼ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°è¾“å‡ºtensorï¼Œselfæœ¬èº«çš„æ•°æ®ä¸å˜ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_scatter_nd_update(Tensor self, Tensor indices, Tensor updates) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>selfï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæºæ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€FLOAT16ã€BOOLã€BFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ã€INT64(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®ç±»å‹éœ€è¦ä¸updatesä¸€è‡´ï¼Œç»´æ•°åªèƒ½æ˜¯1~8ç»´ã€‚</p>
<p>indicesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œç´¢å¼•å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT32ã€INT64ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œindicesä¸­çš„ç´¢å¼•æ•°æ®ä¸æ”¯æŒè¶Šç•Œã€‚</p>
<p>updatesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæ›´æ–°æ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€FLOAT16ã€BOOLã€BFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ã€INT64(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®ç±»å‹éœ€è¦ä¸selfä¸€è‡´ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>ä¸€ä¸ªTensorç±»å‹çš„è¾“å‡ºï¼Œä»£è¡¨selfè¢«æ›´æ–°åçš„ç»“æœã€‚</p>
<p><a href="#id13"><span class="problematic" id="id14">**</span></a>çº¦æŸè¯´æ˜**indicesè‡³å°‘æ˜¯2ç»´ï¼Œå…¶æœ€å1ç»´çš„å¤§å°ä¸èƒ½è¶…è¿‡selfçš„ç»´åº¦å¤§å°ã€‚</p>
<p>å‡è®¾indicesæœ€å1ç»´çš„å¤§å°æ˜¯aï¼Œåˆ™updatesçš„shapeç­‰äºindicesé™¤æœ€å1ç»´å¤–çš„shapeåŠ ä¸Šselfé™¤å‰aç»´å¤–çš„shapeã€‚ä¸¾ä¾‹ï¼šselfçš„shapeæ˜¯(4, 5, 6)ï¼Œindicesçš„shapeæ˜¯(3, 2)ï¼Œåˆ™updatesçš„shapeå¿…é¡»æ˜¯(3, 6)ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scatter_nd_update</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scatter_nd_update_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scatter_nd_update_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scatter_nd_update_" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å°†updatesä¸­çš„å€¼æŒ‰æŒ‡å®šçš„ç´¢å¼•indicesæ›´æ–°selfä¸­çš„å€¼ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°è¾“å‡ºtensorï¼Œselfä¸­çš„æ•°æ®è¢«æ”¹å˜ã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_scatter_nd_update_(Tensor(a!) self, Tensor indices, Tensor updates) -&gt; Tensor(a!)</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>selfï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæºæ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€FLOAT16ã€BOOLã€BFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ã€INT64(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®ç±»å‹éœ€è¦ä¸updatesä¸€è‡´ï¼Œç»´æ•°åªèƒ½æ˜¯1~8ç»´ã€‚</p>
<p>indicesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œç´¢å¼•å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT32ã€INT64ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œindicesä¸­çš„ç´¢å¼•æ•°æ®ä¸æ”¯æŒè¶Šç•Œã€‚</p>
<p>updatesï¼šDeviceä¾§çš„Tensorç±»å‹ï¼Œå¿…é€‰è¾“å…¥ï¼Œæ›´æ–°æ•°æ®å¼ é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€FLOAT16ã€BOOLã€BFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ã€INT64(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œæ•°æ®ç±»å‹éœ€è¦ä¸selfä¸€è‡´ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>è¿”å›è¢«æ›´æ–°åçš„selfã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>indicesè‡³å°‘æ˜¯2ç»´ï¼Œå…¶æœ€å1ç»´çš„å¤§å°ä¸èƒ½è¶…è¿‡selfçš„ç»´åº¦å¤§å°ã€‚</p>
<p>å‡è®¾indicesæœ€å1ç»´çš„å¤§å°æ˜¯aï¼Œåˆ™updatesçš„shapeç­‰äºindicesé™¤æœ€å1ç»´å¤–çš„shapeåŠ ä¸Šselfé™¤å‰aç»´å¤–çš„shapeã€‚ä¸¾ä¾‹ï¼šselfçš„shapeæ˜¯(4, 5, 6)ï¼Œindicesçš„shapeæ˜¯(3, 2)ï¼Œåˆ™updatesçš„shapeå¿…é¡»æ˜¯(3, 6)ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scatter_nd_update_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_anti_quant">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_anti_quant</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_anti_quant" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å°†INT4æˆ–è€…INT8æ•°æ®åé‡åŒ–ä¸ºFP16æˆ–è€…BF16ï¼Œå…¶ä¸­è¾“å…¥æ˜¯INT4ç±»å‹æ—¶ï¼Œå°†æ¯8ä¸ªæ•°æ®çœ‹ä½œæ˜¯ä¸€ä¸ªINT32æ•°æ®ã€‚</p>
<p>è®¡ç®—å…¬å¼ä¸ºï¼š</p>
<p>anti_quant(x)=float16((x+offset)*scale)</p>
<p>anti_quant(x)=bfloat16((x+offset)*scale)</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_anti_quant(Tensor x, Tensor scale, <a href="#id15"><span class="problematic" id="id16">*</span></a>, Tensor? offset=None, ScalarType? dst_dtype=None, ScalarType? src_dtype=None) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>xï¼šTensorç±»å‹ï¼Œå³è¾“å…¥å‚æ•°ä¸­çš„xã€‚æ•°æ®ç±»å‹æ”¯æŒINT8ã€INT32(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œå…¶ä¸­INT32ç±»å‹æ•°æ®çš„æ¯ä¸ªå€¼æ˜¯ç”±8ä¸ªINT4æ•°å€¼æ‹¼æˆçš„ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorã€‚è¾“å…¥æœ€å¤§æ”¯æŒ8ç»´ã€‚</p>
<p>scaleï¼šTensorç±»å‹ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€BFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œä»…æ”¯æŒ1ç»´Tensorã€‚</p>
<p>offsetï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ã€BFLOAT16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œä¸”æ•°æ®ç±»å‹å¿…é¡»ä¸scaleçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒéè¿ç»­çš„Tensorï¼Œä»…æ”¯æŒ1ç»´Tensorï¼Œä¸”shapeå¿…é¡»ä¸scaleçš„shapeå¤§å°ä¸€è‡´ã€‚</p>
<p>dst_dtypeï¼šScalarTypeç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œè¾“å…¥å€¼å…è®¸ä¸ºtorch.float16ã€torch.bfloat16(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ï¼Œé»˜è®¤å€¼ä¸ºtorch.float16ã€‚</p>
<p>src_dtypeï¼šScalarTypeç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œè¾“å…¥å€¼å…è®¸ä¸ºtorch.quint4x2(ä»…Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ)ã€torch.int8ï¼Œé»˜è®¤å€¼ä¸ºtorch.int8ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>ä¸€ä¸ªTensorç±»å‹çš„è¾“å‡ºï¼Œä»£è¡¨antiquantçš„è®¡ç®—ç»“æœã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>xã€scaleè¿™ä¸¤ä¸ªè¾“å…¥ä¸­ä¸èƒ½å«æœ‰ç©ºæŒ‡é’ˆã€‚</p>
<p>å¦‚æœè¾“å…¥scaleçš„shapeå€¼ä¸ä¸º1ï¼Œåˆ™è¾“å…¥xçš„æœ€åä¸€ç»´shapeå€¼å¿…é¡»ä¸scaleçš„shapeä¸€è‡´ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.4</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2è®­ç»ƒç³»åˆ—äº§å“</p>
<p>Atlas æ¨ç†ç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#å•ç®—å­è°ƒç”¨æ¨¡å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_anti_quant</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#torch apiå…¥å›¾æ¨¡å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s1">&#39;pbtxt&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">offset</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_anti_quant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">cpu_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">offset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_mm_all_reduce_base">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_mm_all_reduce_base</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_mm_all_reduce_base" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>TPåˆ‡åˆ†åœºæ™¯ä¸‹, å®ç°mmå’Œall_reduceçš„èåˆ, èåˆç®—å­å†…éƒ¨å®ç°è®¡ç®—å’Œé€šä¿¡æµæ°´å¹¶è¡Œã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_mm_all_reduce_base(Tensor x1, Tensor x2, str hcom, <a href="#id17"><span class="problematic" id="id18">*</span></a>, str reduce_op='sum', Tensor? bias=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? x3=None, Tensor? dequant_scale=None, Tensor? pertoken_scale=None, int comm_turn=0, int antiquant_group_size=0) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>x1: Deviceä¾§çš„Tensorç±»å‹, æ”¯æŒfloat16ã€bfloat16ã€int8, æ”¯æŒND, è¾“å…¥shapeæ”¯æŒ2ç»´æˆ–è€…3ç»´ã€‚</p>
<p>x2: Deviceä¾§çš„Tensorç±»å‹, æ”¯æŒfloat16ã€bfloat16ã€int8, æ”¯æŒND, éé‡åŒ–åœºæ™¯, æ•°æ®ç±»å‹éœ€è¦å’Œx1ä¿æŒä¸€è‡´, è¾“å…¥shapeç»´åº¦ç¬¬0ç»´å’Œx1çš„æœ€åä¸€ç»´ä¿æŒä¸€è‡´ã€‚</p>
<p>hcom: Hostä¾§çš„Stringç±»å‹, é€šä¿¡åŸŸhandleå, é€šè¿‡get_hccl_comm_nameæ¥å£è·å–ã€‚</p>
<p><a href="#id19"><span class="problematic" id="id20">*</span></a>: ä»£è¡¨å…¶ä¹‹å‰çš„å˜é‡æ˜¯ä½ç½®ç›¸å…³, æŒ‰ç…§é¡ºåºè¾“å…¥, å¿…é€‰; ä¹‹åçš„å˜é‡æ˜¯é”®å€¼å¯¹èµ‹å€¼çš„, ä½ç½®æ— å…³, å¯é€‰(ä¸è¾“å…¥ä¼šä½¿ç”¨é»˜è®¤å€¼)ã€‚</p>
<p>reduce_op: Hostä¾§çš„Stringç±»å‹, reduceæ“ä½œç±»å‹, å½“å‰ç‰ˆæœ¬ä»…æ”¯æŒ'sum', é»˜è®¤å€¼: 'sum'ã€‚</p>
<p>bias: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, æ”¯æŒint32ã€float16ã€bfloat16, æ”¯æŒNDæ ¼å¼ã€‚biaså½“å‰ä»…æ”¯æŒä¸€ç»´, ä¸”ç»´åº¦å¤§å°ä¸output/x2çš„æœ€åä¸€ç»´å¤§å°ç›¸åŒã€‚</p>
<p>antiquant_scale: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, ä¼ªé‡åŒ–åœºæ™¯å¯¹x2è¿›è¡Œå»é‡åŒ–çš„ç³»æ•°, æ”¯æŒfloat16ã€bfloat16, æ”¯æŒNDæ ¼å¼ã€‚ä¼ªé‡åŒ–åœºæ™¯æ•°æ®ç±»å‹éœ€è¦å’Œx1ä¿æŒä¸€è‡´ã€‚antiquant_scaleå½“å‰per-tensoråœºæ™¯shapeä¸º[1],per-channelåœºæ™¯æ”¯æŒshapeä¸º[1,n]æˆ–è€…[n]ã€‚å…¶ä¸­nä¸ºx2æœ€åä¸€ç»´çš„å¤§å°ã€‚per-groupåœºæ™¯æ”¯æŒshapeä¸º[ceil(k, antiquant_group_size), n](å…·ä½“è®¡ç®—é€»è¾‘è§**çº¦æŸè¯´æ˜**)ã€‚å…¶ä¸­kä¸ºx2ç¬¬ä¸€ç»´çš„å¤§å°, nä¸ºx2æœ€åä¸€ç»´çš„å¤§å°, antiquant_group_sizeä¸ºä¼ªé‡åŒ–åœºæ™¯å¯¹è¾“å…¥x2è¿›è¡Œåé‡åŒ–è®¡ç®—çš„groupSizeè¾“å…¥ã€‚</p>
<p>antiquant_offset: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, ä¼ªé‡åŒ–åœºæ™¯å¯¹x2è¿›è¡Œå»é‡åŒ–çš„ç³»æ•°, æ”¯æŒfloat16ã€bfloat16, æ”¯æŒNDæ ¼å¼ã€‚æ•°æ®ç±»å‹éœ€è¦å’Œantiquant_scaleä¿æŒä¸€è‡´ã€‚shapeä¸antiquant_scaleä¿æŒä¸€è‡´ã€‚</p>
<p>x3: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, matmulè®¡ç®—åçš„åç§»ã€‚æ”¯æŒfloat16ã€bfloat16ã€‚æ”¯æŒNDæ ¼å¼ã€‚æ•°æ®ç±»å‹éœ€è¦å’Œè¾“å‡ºoutputä¿æŒä¸€è‡´ã€‚shapeä¸outputçš„shapeç›¸åŒã€‚ä¼ªé‡åŒ–åœºæ™¯æš‚ä¸æ”¯æŒå¤„ç†x3ã€‚</p>
<p>dequant_scale: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, matmulè®¡ç®—åçš„å»é‡åŒ–ç³»æ•°ã€‚æ”¯æŒint64ã€uint64ã€bfloat16ã€float32, æ”¯æŒNDæ ¼å¼ã€‚shapeåœ¨per-tensoråœºæ™¯ä¸º[1], per-channelåœºæ™¯ä¸º[n]/[1,n], å…¶ä¸­nä¸ºx2æœ€åä¸€ç»´çš„å¤§å°ã€‚</p>
<p>pertoken_scale: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, matmulè®¡ç®—åçš„å»é‡åŒ–ç³»æ•°ã€‚æ”¯æŒfloat32, æ”¯æŒNDæ ¼å¼ã€‚x1ä¸º[b, s, k]æ—¶shapeä¸º[b*s]ï¼Œx1ä¸º[m, k]æ—¶shapeä¸º[m]ã€‚</p>
<p>comm_quant_scale_1: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, alltoallé€šä¿¡å‰åçš„é‡åŒ–ã€å»é‡åŒ–ç³»æ•°ã€‚æ”¯æŒfloat16ã€bfloat16, æ”¯æŒNDæ ¼å¼ã€‚per-channelåœºæ™¯ï¼Œx2ä¸º[k, n]æ—¶shapeä¸º[1, n]æˆ–[n]ï¼Œç”¨æˆ·éœ€ä¿è¯æ¯å¼ å¡ä¸Šæ•°æ®ä¿æŒä¸€è‡´ä¸”æ­£ç¡®ã€‚</p>
<p>comm_quant_scale_2: Deviceä¾§çš„Tensorç±»å‹, å¯é€‰è¾“å…¥, allgatheré€šä¿¡å‰åçš„é‡åŒ–ã€å»é‡åŒ–ç³»æ•°ã€‚æ”¯æŒfloat16ã€bfloat16, æ”¯æŒNDæ ¼å¼ã€‚per-channelåœºæ™¯ï¼Œx2ä¸º[k, n]æ—¶shapeä¸º[1, n]æˆ–[n]ï¼Œç”¨æˆ·éœ€ä¿è¯æ¯å¼ å¡ä¸Šæ•°æ®ä¿æŒä¸€è‡´ä¸”æ­£ç¡®ã€‚</p>
<p>comm_turn: Hostä¾§çš„intç±»å‹, è¡¨ç¤ºranké—´é€šä¿¡åˆ‡åˆ†ç²’åº¦, é»˜è®¤å€¼: 0, è¡¨ç¤ºé»˜è®¤çš„åˆ‡åˆ†æ–¹å¼ã€‚å½“å‰ç‰ˆæœ¬ä»…æ”¯æŒè¾“å…¥0ã€‚</p>
<p>antiquant_group_size: Hostä¾§çš„intç±»å‹, è¡¨ç¤ºä¼ªé‡åŒ–pre-groupç®—æ³•æ¨¡å¼ä¸‹, å¯¹è¾“å…¥x2è¿›è¡Œåé‡åŒ–è®¡ç®—çš„groupSizeè¾“å…¥, æè¿°ä¸€ç»„åé‡åŒ–å‚æ•°å¯¹åº”çš„å¾…åé‡åŒ–æ•°æ®é‡åœ¨kè½´æ–¹å‘çš„å¤§å°ã€‚å½“ä¼ªé‡åŒ–ç®—æ³•æ¨¡å¼ä¸ä¸ºpre_groupæ—¶ä¼ å…¥0; å½“ä¼ªé‡åŒ–ç®—æ³•æ¨¡å¼ä¸ºpre_groupæ—¶ä¼ å…¥å€¼çš„èŒƒå›´ä¸º[32, min(k-1, INT_NAX)]ä¸”å€¼è¦æ±‚æ˜¯32çš„å€æ•°, å…¶ä¸­kä¸ºx2ç¬¬ä¸€ç»´çš„å¤§å°ã€‚é»˜è®¤å€¼: 0, ä¸º0åˆ™è¡¨ç¤ºéper-groupåœºæ™¯ã€‚</p>
<p><a href="#id21"><span class="problematic" id="id22">**</span></a>è¾“å‡ºè¯´æ˜**Tensorç±»å‹, æ•°æ®ç±»å‹éé‡åŒ–åœºæ™¯ä»¥åŠä¼ªé‡åŒ–åœºæ™¯ä¸x1ä¿æŒä¸€è‡´, å…¨é‡åŒ–åœºæ™¯ä¸ºfloat16æˆ–è€…bfloat16ã€‚shapeç¬¬0ç»´åº¦å’Œx1çš„0ç»´ä¿æŒä¸€è‡´, è‹¥x1ä¸º2ç»´, shapeç¬¬1ç»´åº¦å’Œx2çš„1ç»´ä¿æŒä¸€è‡´, è‹¥x1ä¸º3ç»´, shapeç¬¬1ç»´åº¦å’Œx1çš„1ç»´ä¿æŒä¸€è‡´, shapeç¬¬2ç»´åº¦å’Œx2çš„1ç»´ä¿æŒä¸€è‡´ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>è¾“å…¥x1å¯ä¸º2ç»´æˆ–è€…3ç»´ã€x2å¿…é¡»æ˜¯2ç»´, åˆ†åˆ«ä¸º(s, m, k)/(m, k), (k, n), kè½´æ»¡è¶³mmç®—å­å…¥å‚è¦æ±‚, kè½´ç›¸ç­‰ã€‚biaså½“å‰ä»…æ”¯æŒä¸€ç»´, ä¸”ç»´åº¦å¤§å°ä¸outputçš„æœ€åä¸€ç»´å¤§å°ç›¸åŒã€‚x3çš„shapeä¸outputçš„shapeç›¸åŒã€‚antiquant_scaleå½“å‰per-tensoråœºæ™¯shapeä¸º[1],per-channelåœºæ™¯æ”¯æŒshapeä¸º[1,n]æˆ–è€…[n], per-groupåœºæ™¯æ”¯æŒshapeä¸º(ceil(k, antiquant_group_size), n)ã€‚antiquant_offsetçš„shapeä¸antiquant_scaleä¸€è‡´ã€‚dequant_scaleçš„shapeåœ¨per-tensoråœºæ™¯ä¸º[1], per-channelåœºæ™¯ä¸º[n]/[1,n]ã€‚</p>
<p>[ceil(k, antiquant_group_size), n]ä¸­çš„ceil(k, antiquant_group_size)è®¡ç®—é€»è¾‘ä¸º: (k + antiquant_group_size - 1) / antiquant_group_size, å¹¶å¯¹è®¡ç®—ç»“æœå–æ•´æ•°éƒ¨åˆ†ã€‚</p>
<p>x1ã€x2ä¸èƒ½ä¸ºç©ºtensorã€‚</p>
<p>éé‡åŒ–åœºæ™¯, x1ã€x2ã€biasã€x3ã€outputçš„æ•°æ®ç±»å‹ä¿æŒä¸€è‡´, å¯ä¸ºfloat16æˆ–è€…bfloat16, antiquant_scaleã€antiquant_offsetã€dequant_scaleä¸ºNoneã€‚</p>
<p>ä¼ªé‡åŒ–åœºæ™¯, x1ã€biasã€x3ã€antiquant_scaleã€antiquant_offset, outputçš„æ•°æ®ç±»å‹ä¿æŒä¸€è‡´, å¯ä¸ºfloat16æˆ–è€…bfloat16, x2çš„æ•°æ®ç±»å‹ä¸ºint8, dequant_scaleä¸ºNoneã€‚</p>
<p>å…¨é‡åŒ–åœºæ™¯, x1ã€x2çš„æ•°æ®ç±»å‹ä¸ºint8, dequant_scaleçš„æ•°æ®ç±»å‹ä¸ºint64ã€uint64ã€float32æˆ–è€…bfloat16ã€‚dequant_scaleç±»å‹ä¸ºint64ã€uint64ã€float32(ä»…pertokenåœºæ™¯)æ—¶, outputæ•°æ®ç±»å‹ä¸ºfloat16; dequant_scaleç±»å‹ä¸ºbfloat16æ—¶, outputæ•°æ®ç±»å‹ä¸ºbfloat16; biasæ•°æ®ç±»å‹ä¸ºint32; antiquant_scaleã€antiquant_offsetä¸ºNoneã€‚ä»…è¾“å‡ºä¸ºBF16æ—¶, æ”¯æŒä¼ å…¥x3ã€‚å¦å¤–, è‹¥dequant_scaleéœ€è¦ä»¥int64ç±»å‹ä¼ å…¥, åœ¨è°ƒç”¨torch_npu.npu_mm_all_reduce_base()å‰, éœ€é€šè¿‡torch_npu.npu_trans_quant_param()æ¥å£å¯¹dequant_scaleè¿›è¡Œå¤„ç†(å¤„ç†æ–¹æ³•è§å¯¹åº”çš„æ¥å£ä½¿ç”¨è¯´æ˜)ã€‚</p>
<p>antiquant_group_sizeä¸­kå€¼çš„èŒƒå›´ä¸matmulä¸€è‡´, ä¸º[1,65535], INT_MAXå¤§äº(k-1)ã€‚</p>
<p>x1ä¸æ”¯æŒè¾“å…¥è½¬ç½®åçš„tensor, x2è½¬ç½®åè¾“å…¥, éœ€è¦æ»¡è¶³shapeçš„ç¬¬ä¸€ç»´å¤§å°ä¸x1çš„æœ€åä¸€ç»´ç›¸åŒ, æ»¡è¶³matmulçš„è®¡ç®—æ¡ä»¶ã€‚</p>
<p>Atlas 300I Duo æ¨ç†ç³»åˆ—äº§å“åªæ”¯æŒ2å¡ã€‚</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“æ”¯æŒ2ã€4ã€8å¡ã€‚</p>
<p>å¢é‡åœºæ™¯ä¸ä½¿èƒ½MC2, å…¨é‡åœºæ™¯ä½¿èƒ½MC2ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p>Atlas 300I Duo æ¨ç†ç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">run_mm_all_reduce_base</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">master_ip</span><span class="p">,</span> <span class="n">master_port</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_method</span> <span class="o">=</span> <span class="s1">&#39;tcp://&#39;</span> <span class="o">+</span> <span class="n">master_ip</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span> <span class="o">+</span> <span class="n">master_port</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;hccl&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.distributed_c10d</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_default_group</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;</span> <span class="s1">&#39;2.0.1&#39;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hcom_info</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">_get_backend</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">get_hccl_comm_name</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hcom_info</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">get_hccl_comm_name</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">x1_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">x2_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_mm_all_reduce_base</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">hcom_info</span><span class="p">,</span> <span class="n">reduce_op</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output: &quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">worksize</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">master_ip</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">master_port</span> <span class="o">=</span> <span class="s1">&#39;50001&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run_mm_all_reduce_base</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">worksize</span><span class="p">,</span> <span class="n">master_ip</span><span class="p">,</span> <span class="n">master_port</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">worksize</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ffn">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ffn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ffn" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>ç®—å­åŠŸèƒ½ï¼šè¯¥FFNç®—å­æä¾›MoeFFNå’ŒFFNçš„è®¡ç®—åŠŸèƒ½ã€‚åœ¨æ²¡æœ‰ä¸“å®¶åˆ†ç»„ï¼ˆexpert_tokensä¸ºç©ºï¼‰æ—¶æ˜¯FFNï¼Œæœ‰ä¸“å®¶åˆ†ç»„æ—¶æ˜¯MoeFFNã€‚</p>
<p>è®¡ç®—å…¬å¼ä¸ºï¼š</p>
<p>out = activation(xW1+b1)W2+b2</p>
<p>è¯´æ˜ï¼šæ¿€æ´»å±‚ä¸ºgeglu/swiglu/regluæ—¶ï¼Œæ€§èƒ½ä½¿èƒ½éœ€è¦æ»¡è¶³é—¨æ§›è¦æ±‚ï¼Œå³æ•´ç½‘ä¸­FFNç»“æ„æ‰€å¯¹åº”çš„å°ç®—å­ä¸­vectorè€—æ—¶30usä¸”å æ¯”10%ä»¥ä¸Šçš„ç”¨ä¾‹æ–¹å¯å°è¯•FFNèåˆç®—å­ï¼›æˆ–åœ¨ä¸çŸ¥é“å°ç®—å­æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°è¯•ä½¿èƒ½FFNï¼Œè‹¥æ€§èƒ½åŠ£åŒ–åˆ™ä¸ä½¿èƒ½FFNã€‚</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>npu_ffn(Tensor x, Tensor weight1, Tensor weight2, str activation, <a href="#id23"><span class="problematic" id="id24">*</span></a>, int[]? expert_tokens=None, int[]? expert_tokens_index=None, Tensor? bias1=None, Tensor? bias2=None, Tensor? scale=None, Tensor? offset=None, Tensor? deq_scale1=None, Tensor? deq_scale2=None, Tensor? antiquant_scale1=None, Tensor? antiquant_scale2=None, Tensor? antiquant_offset1=None, Tensor? antiquant_offset2=None, int? inner_precise=None, ScalarType? output_dtype=None) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>xï¼šTensorç±»å‹ï¼Œå³è¾“å…¥å‚æ•°ä¸­çš„xã€‚å…¬å¼ä¸­çš„è¾“å…¥xï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ã€INT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œæ”¯æŒè¾“å…¥çš„ç»´åº¦æœ€å°‘æ˜¯2ç»´[M, K1]ï¼Œæœ€å¤šæ˜¯8ç»´ã€‚</p>
<p>weight1ï¼šTensorç±»å‹ï¼Œä¸“å®¶çš„æƒé‡æ•°æ®ï¼Œå…¬å¼ä¸­çš„W1ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ã€INT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, K1, N1]/[K1, N1]ã€‚</p>
<p>weight2ï¼šTensorç±»å‹ï¼Œä¸“å®¶çš„æƒé‡æ•°æ®ï¼Œå…¬å¼ä¸­çš„W2ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ã€INT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, K2, N2]/[K2, N2]ã€‚</p>
<p>è¯´æ˜ï¼š Mè¡¨ç¤ºtokenä¸ªæ•°ï¼Œå¯¹åº”transformä¸­çš„BS(Bï¼ˆBatchï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬æ‰¹é‡å¤§å°ã€Sï¼ˆSeq-Lengthï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬åºåˆ—é•¿åº¦)ï¼›K1è¡¨ç¤ºç¬¬ä¸€ç»„matmulçš„è¾“å…¥é€šé“æ•°ï¼Œå¯¹åº”transformä¸­çš„H(Head-Sizeï¼‰è¡¨ç¤ºéšè—å±‚çš„å¤§å°)ï¼›N1è¡¨ç¤ºç¬¬ä¸€ç»„matmulçš„è¾“å‡ºé€šé“æ•°ï¼›K2è¡¨ç¤ºç¬¬äºŒç»„matmulçš„è¾“å…¥é€šé“æ•°ï¼›N2è¡¨ç¤ºç¬¬äºŒç»„matmulçš„è¾“å‡ºé€šé“æ•°ï¼Œå¯¹åº”transformä¸­çš„Hï¼›Eè¡¨ç¤ºæœ‰ä¸“å®¶åœºæ™¯çš„ä¸“å®¶æ•°ã€‚</p>
<p>expert_tokensï¼šListç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚ä»£è¡¨å„ä¸“å®¶çš„tokenæ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒINTï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè‹¥ä¸ä¸ºç©ºæ—¶å¯æ”¯æŒçš„æœ€å¤§é•¿åº¦ä¸º256ä¸ªã€‚</p>
<p>expert_tokens_indexï¼šListç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚ä»£è¡¨å„ä¸“å®¶è®¡ç®—tokençš„ç´¢å¼•å€¼ï¼Œæ•°æ®ç±»å‹æ”¯æŒINTï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè‹¥ä¸ä¸ºç©ºæ—¶å¯æ”¯æŒçš„æœ€å¤§é•¿åº¦ä¸º256ä¸ªã€‚</p>
<p>bias1ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚æƒé‡æ•°æ®ä¿®æ­£å€¼ï¼Œå…¬å¼ä¸­çš„b1ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€FLOAT32ã€INT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N1]/[N1]ã€‚</p>
<p>bias2ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ã€‚æƒé‡æ•°æ®ä¿®æ­£å€¼ï¼Œå…¬å¼ä¸­çš„b2ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€FLOAT32ã€INT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N2]/[N2]ã€‚</p>
<p>activationï¼šstringç±»å‹ï¼Œä»£è¡¨ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œå³è¾“å…¥å‚æ•°ä¸­çš„activationã€‚å½“å‰ä»…æ”¯æŒfastgelu/gelu/relu/silu/geglu/swiglu/regluã€‚</p>
<p>scaleï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œé‡åŒ–å‚æ•°ï¼Œé‡åŒ–ç¼©æ”¾ç³»æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œper-tensorä¸‹è¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶å‡ä¸ºä¸€ç»´å‘é‡ï¼Œè¾“å…¥å…ƒç´ ä¸ªæ•°åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E]/[1]ï¼›per-channelä¸‹è¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶ä¸ºäºŒç»´å‘é‡/ä¸€ç»´å‘é‡ï¼Œè¾“å…¥å…ƒç´ ä¸ªæ•°åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N1]/[N1]ã€‚</p>
<p>offsetï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œé‡åŒ–å‚æ•°ï¼Œé‡åŒ–åç§»é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT32ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œä¸€ç»´å‘é‡ï¼Œè¾“å…¥å…ƒç´ ä¸ªæ•°åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E]/[1]ã€‚</p>
<p>deq_scale1ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œé‡åŒ–å‚æ•°ï¼Œç¬¬ä¸€ç»„matmulçš„åé‡åŒ–ç¼©æ”¾ç³»æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€FLOAT32ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N1]/[N1]ã€‚</p>
<p>deq_scale2ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œé‡åŒ–å‚æ•°ï¼Œç¬¬äºŒç»„matmulçš„åé‡åŒ–ç¼©æ”¾ç³»æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ã€FLOAT32ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N2]/[N2]ã€‚</p>
<p>antiquant_scale1ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œä¼ªé‡åŒ–å‚æ•°ï¼Œç¬¬ä¸€ç»„matmulçš„ç¼©æ”¾ç³»æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œper-channelä¸‹è¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N1]/[N1]ã€‚</p>
<p>antiquant_scale2ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œä¼ªé‡åŒ–å‚æ•°ï¼Œç¬¬äºŒç»„matmulçš„ç¼©æ”¾ç³»æ•°ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œper-channelä¸‹è¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N2]/[N2]ã€‚</p>
<p>antiquant_offset1ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œä¼ªé‡åŒ–å‚æ•°ï¼Œç¬¬ä¸€ç»„matmulçš„åç§»é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œper-channelä¸‹è¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N1]/[N1]ã€‚</p>
<p>antiquant_offset2ï¼šTensorç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œä¼ªé‡åŒ–å‚æ•°ï¼Œç¬¬äºŒç»„matmulçš„åç§»é‡ï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œper-channelä¸‹è¾“å…¥åœ¨æœ‰/æ— ä¸“å®¶æ—¶åˆ†åˆ«ä¸º[E, N2]/[N2]ã€‚</p>
<p>inner_preciseï¼šintç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œè¡¨ç¤ºé«˜ç²¾åº¦æˆ–è€…é«˜æ€§èƒ½é€‰æ‹©ã€‚æ•°æ®ç±»å‹æ”¯æŒï¼šINT64ã€‚è¯¥å‚æ•°ä»…å¯¹FLOAT16ç”Ÿæ•ˆï¼ŒBFLOAT16å’ŒINT8ä¸åŒºåˆ†é«˜ç²¾åº¦å’Œé«˜æ€§èƒ½ã€‚</p>
<p>innerPreciseä¸º0æ—¶ï¼Œä»£è¡¨å¼€å¯é«˜ç²¾åº¦æ¨¡å¼ï¼Œç®—å­å†…éƒ¨é‡‡ç”¨FLOAT32æ•°æ®ç±»å‹è®¡ç®—ã€‚</p>
<p>innerPreciseä¸º1æ—¶ï¼Œä»£è¡¨é«˜æ€§èƒ½æ¨¡å¼ã€‚</p>
<p>output_dtypeï¼š ScalarTypeç±»å‹ï¼Œå¯é€‰å‚æ•°ï¼Œè¯¥å‚æ•°åªåœ¨é‡åŒ–åœºæ™¯ç”Ÿæ•ˆï¼Œå…¶ä»–åœºæ™¯ä¸ç”Ÿæ•ˆã€‚è¡¨ç¤ºè¾“å‡ºTensorçš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒè¾“å…¥float16, bfloat16ã€‚é»˜è®¤å€¼ä¸ºNoneï¼Œä»£è¡¨è¾“å‡ºTensoræ•°æ®ç±»å‹ä¸ºfloat16ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>ä¸€ä¸ªTensorç±»å‹çš„è¾“å‡ºï¼Œå…¬å¼ä¸­çš„è¾“å‡ºyï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDï¼Œè¾“å‡ºç»´åº¦ä¸xä¸€è‡´ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>æœ‰ä¸“å®¶æ—¶ï¼Œä¸“å®¶æ•°æ®çš„æ€»æ•°éœ€è¦ä¸xçš„Mä¿æŒä¸€è‡´ã€‚</p>
<p>æ¿€æ´»å±‚ä¸ºgeglu/swiglu/regluæ—¶ï¼Œä»…æ”¯æŒæ— ä¸“å®¶åˆ†ç»„æ—¶çš„FLOAT16é«˜æ€§èƒ½åœºæ™¯ï¼ˆFLOAT16åœºæ™¯æŒ‡ç±»å‹ä¸ºTensorçš„å¿…é€‰å‚æ•°æ•°æ®ç±»å‹éƒ½ä¸ºFLOAT16çš„åœºæ™¯ï¼‰ï¼Œä¸”N1=2*K2ã€‚</p>
<p>æ¿€æ´»å±‚ä¸ºgelu/fastgelu/relu/siluæ—¶ï¼Œæ”¯æŒæœ‰ä¸“å®¶æˆ–æ— ä¸“å®¶åˆ†ç»„çš„FLOAT16é«˜ç²¾åº¦åŠé«˜æ€§èƒ½åœºæ™¯ï¼ŒBFLOAT16åœºæ™¯ï¼Œé‡åŒ–åœºæ™¯åŠä¼ªé‡åŒ–åœºæ™¯ï¼Œä¸”N1=K2ã€‚</p>
<p>éé‡åŒ–åœºæ™¯ä¸èƒ½è¾“å…¥é‡åŒ–å‚æ•°å’Œä¼ªé‡åŒ–å‚æ•°ï¼Œé‡åŒ–åœºæ™¯ä¸èƒ½è¾“å…¥ä¼ªé‡åŒ–å‚æ•°ï¼Œä¼ªé‡åŒ–åœºæ™¯ä¸èƒ½è¾“å…¥é‡åŒ–å‚æ•°ã€‚</p>
<p>é‡åŒ–åœºæ™¯å‚æ•°ç±»å‹ï¼šxä¸ºINT8ã€weightä¸ºINT8ã€biasä¸ºINT32ã€scaleä¸ºFLOAT32ã€offsetä¸ºFLOAT32ï¼Œå…¶ä½™å‚æ•°ç±»å‹æ ¹æ®yä¸åŒåˆ†ä¸¤ç§æƒ…å†µï¼š</p>
<p>yä¸ºFLOAT16ï¼ŒdeqScaleæ”¯æŒæ•°æ®ç±»å‹ï¼šUINT64ã€INT64ã€FLOAT32ã€‚</p>
<p>yä¸ºBFLOAT16ï¼ŒdeqScaleæ”¯æŒæ•°æ®ç±»å‹ï¼šBFLOAT16ã€‚</p>
<p>è¦æ±‚deqScale1ä¸deqScale2çš„æ•°æ®ç±»å‹ä¿æŒä¸€è‡´ã€‚</p>
<p>é‡åŒ–åœºæ™¯æ”¯æŒscaleçš„per-channelæ¨¡å¼å‚æ•°ç±»å‹ï¼šxä¸ºINT8ã€weightä¸ºINT8ã€biasä¸ºINT32ã€scaleä¸ºFLOAT32ã€offsetä¸ºFLOAT32ï¼Œå…¶ä½™å‚æ•°ç±»å‹æ ¹æ®yä¸åŒåˆ†ä¸¤ç§æƒ…å†µï¼š</p>
<p>yä¸ºFLOAT16ï¼ŒdeqScaleæ”¯æŒæ•°æ®ç±»å‹ï¼šUINT64ã€INT64ã€‚</p>
<p>yä¸ºBFLOAT16ï¼ŒdeqScaleæ”¯æŒæ•°æ®ç±»å‹ï¼šBFLOAT16ã€‚</p>
<p>è¦æ±‚deqScale1ä¸deqScale2çš„æ•°æ®ç±»å‹ä¿æŒä¸€è‡´ã€‚</p>
<p>ä¼ªé‡åŒ–åœºæ™¯æ”¯æŒä¸¤ç§ä¸åŒå‚æ•°ç±»å‹ï¼š</p>
<p>yä¸ºFLOAT16ã€xä¸ºFLOAT16ã€biasä¸ºFLOAT16ï¼Œantiquant_scaleä¸ºFLOAT16ã€antiquant_offsetä¸ºFLOAT16ï¼Œweightæ”¯æŒæ•°æ®ç±»å‹INT8ã€‚</p>
<p>yä¸ºBFLOAT16ã€xä¸ºBFLOAT16ã€biasä¸ºFLOAT32ï¼Œantiquant_scaleä¸ºBFLOAT16ã€antiquant_offsetä¸ºBFLOAT16ï¼Œweightæ”¯æŒæ•°æ®ç±»å‹INT8ã€‚innerPreciseå‚æ•°åœ¨BFLOAT16éé‡åŒ–åœºæ™¯ï¼Œåªèƒ½é…ç½®ä¸º0ï¼›FLOAT16éé‡åŒ–åœºæ™¯ï¼Œå¯ä»¥é…ç½®ä¸º0æˆ–è€…1ï¼›é‡åŒ–æˆ–è€…ä¼ªé‡åŒ–åœºæ™¯ï¼Œ0å’Œ1éƒ½å¯é…ç½®ï¼Œä½†æ˜¯é…ç½®åä¸ç”Ÿæ•ˆã€‚</p>
<p>expert_tokenså’Œexpert_tokens_indexä¸å¯ä»¥åŒæ—¶ä¼ ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2è®­ç»ƒç³»åˆ—äº§å“/Atlas 800I A2æ¨ç†äº§å“ä¸­çš„æ¨ç†äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#å•ç®—å­è°ƒç”¨æ¨¡å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1280</span><span class="p">,</span> <span class="mi">10240</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">1280</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;fastgelu&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ffn</span><span class="p">(</span><span class="n">cpu_x</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight1</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight2</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">activation</span><span class="p">,</span> <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#torch api å…¥å›¾æ¨¡å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ENABLE_ACLNN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">expert</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ffn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>  <span class="n">expert_tokens</span><span class="o">=</span><span class="n">expert</span><span class="p">,</span> <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1954</span><span class="p">,</span> <span class="mi">2560</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2560</span><span class="p">,</span> <span class="mi">5120</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5120</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;fastgelu&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expert</span> <span class="o">=</span> <span class="p">[</span><span class="mi">227</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">126</span><span class="p">,</span> <span class="mi">178</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">182</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">217</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="mi">243</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">cpu_model</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">cpu_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">cpu_x</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight1</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight2</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">activation</span><span class="p">,</span> <span class="n">expert</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_incre_flash_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_incre_flash_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_incre_flash_attention" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å¢é‡FAå®ç°, å®ç°å¯¹åº”å…¬å¼:</p>
<p>atten_out = softmax(scale*(query*key)+atten_mask)*value</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_incre_flash_attention(Tensor query, Tensor key, Tensor value, <a href="#id25"><span class="problematic" id="id26">*</span></a>, Tensor? padding_mask=None, Tensor? atten_mask=None, symint[]? actual_seq_lengths=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? block_table=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, str input_layout=&quot;BSH&quot;, int num_key_value_heads=0, int block_size=0 , int inner_precise=1) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>query: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,1,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,1,D), å¯¹åº”çš„input_layoutæ˜¯BNSD, å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>key: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å¯¹åº”çš„input_layoutæ˜¯BNSD, å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>value: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å¯¹åº”çš„input_layoutæ˜¯BNSD, å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p><a href="#id27"><span class="problematic" id="id28">*</span></a>: ä»£è¡¨å…¶ä¹‹å‰çš„å˜é‡æ˜¯ä½ç½®ç›¸å…³, éœ€è¦æŒ‰ç…§é¡ºåºè¾“å…¥, å¿…é€‰; ä¹‹åçš„å˜é‡æ˜¯é”®å€¼å¯¹èµ‹å€¼çš„, ä½ç½®æ— å…³, å¯é€‰(ä¸è¾“å…¥ä¼šä½¿ç”¨é»˜è®¤å€¼)ã€‚</p>
<p>padding_mask: é¢„ç•™å‚æ•°, æš‚æœªä½¿ç”¨, é»˜è®¤å€¼ä¸ºNoneã€‚</p>
<p>atten_mask: å››ç»´Deviceä¾§çš„Input Tensor, shapeæ˜¯(B,1,1,S); å–å€¼ä¸º1ä»£è¡¨è¯¥ä½ä¸å‚ä¸è®¡ç®—(ä¸ç”Ÿæ•ˆ), ä¸º0ä»£è¡¨è¯¥ä½å‚ä¸è®¡ç®—, é»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨å‚ä¸è®¡ç®—; æ•°æ®ç±»å‹æ”¯æŒBOOLã€INT8ã€UINT8ï¼Œæ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>actual_seq_lengths: äºŒç»´Hostä¾§çš„Inputæ•°ç»„, å…¶shapeä¸º(B,1), å½¢å¦‚[1, 2, 3], ä»£è¡¨keyã€valueä¸­æœ‰æ•ˆçš„Såºåˆ—é•¿åº¦, é»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨æœ‰æ•ˆ, ç±»å‹ä¸ºList int; æ•°æ®ç±»å‹ä¸ºINT64, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>antiquantScale: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT16ã€BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºé‡åŒ–å› å­, æ”¯æŒper-channel(list)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>antiquantOffset: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT16ã€BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºé‡åŒ–åç§», æ”¯æŒper-channel(list), ç”±shapeå†³å®šã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>blocktable: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: INT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºPageAttentionä¸­KVå­˜å‚¨ä½¿ç”¨çš„blockæ˜ å°„è¡¨ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>dequantScale1: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºBMM1åé¢åé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒper-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>quantScale1: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºBMM2å‰é¢é‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒper-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>dequantScale2: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºBMM2åé¢åé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒper-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>quantScale2: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT32ã€BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºè¾“å‡ºé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒper-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>quantOffset2: Deviceä¾§çš„Input Tensor, æ•°æ®ç±»å‹æ”¯æŒ: FLOAT32ã€BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºè¾“å‡ºé‡åŒ–çš„é‡åŒ–åç§», æ”¯æŒper-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¸ä¼ æˆ–ä¼ å…¥Noneã€‚</p>
<p>kvPaddingSize: Deviceä¾§çš„aclTensor, æ•°æ®ç±»å‹æ”¯æŒ: INT64ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºkvå·¦paddingåœºæ™¯ä½¿èƒ½æ—¶, æœ€åä¸€ä¸ªæœ‰æ•ˆtokenåˆ°Sçš„è·ç¦»ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>num_heads: Hostä¾§çš„attribute, ä»£è¡¨queryçš„å¤´æ•°, å³queryçš„N, å…¶ä¹˜Dä¸ºH, é»˜è®¤å€¼ä¸º1; æ•°æ®ç±»å‹ä¸ºINTã€‚</p>
<p>scale_value: Hostä¾§çš„attribute, ä»£è¡¨ç¼©æ”¾ç³»æ•°, ç”¨æ¥çº¦æŸæ¢¯åº¦, å…¶é»˜è®¤å€¼ä¸º1.0, å…¸å‹å€¼ä¸º; æ•°æ®ç±»å‹ä¸ºFLOAT32ã€‚</p>
<p>input_layout: Hostä¾§çš„attribute, ä»£è¡¨queryã€keyã€valueçš„å¸ƒå±€, æ ¹æ®è¾“å…¥çš„queryã€keyã€valueçš„shapeç¡®å®š, ä¸‰ç»´Tensoræ˜¯BSH, å››ç»´Tensoræ˜¯BNSD, é»˜è®¤å€¼ä¸ºBSH, ä¸æ”¯æŒå…¶ä»–å€¼; æ•°æ®ç±»å‹ä¸ºstringã€‚</p>
<p>num_key_value_heads: Hostä¾§çš„attribute, ä»£è¡¨keyã€valueçš„å¤´æ•°, é»˜è®¤å€¼ä¸º0, è¡¨ç¤ºä¸queryçš„å¤´æ•°ç›¸åŒ, å¦åˆ™è¡¨ç¤ºkeyã€valueçš„å¤´æ•°, éœ€è¦èƒ½è¢«queryçš„å¤´æ•°(num_heads)æ•´é™¤; æ•°æ®ç±»å‹ä¸ºINT64ã€‚</p>
<p>blockSize (int64_t, è®¡ç®—è¾“å…¥): Hostä¾§çš„int64_t, PageAttentionä¸­KVå­˜å‚¨æ¯ä¸ªblockä¸­æœ€å¤§çš„tokenä¸ªæ•°, é»˜è®¤ä¸º0, æ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p>innerPrecise (int64_t, è®¡ç®—è¾“å…¥): Hostä¾§çš„int64_t, ä»£è¡¨é«˜ç²¾åº¦/é«˜æ€§èƒ½é€‰æ‹©, é»˜è®¤å€¼ä¸º1(é«˜æ€§èƒ½),  æ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</p>
<p><strong>è¾“å‡ºè¯´æ˜</strong>:</p>
<p>å…±ä¸€ä¸ªè¾“å‡º, ä¸ºè®¡ç®—çš„æœ€ç»ˆç»“æœatten_out, ç±»å‹ä¸ºTensor, shapeä¸queryä¿æŒä¸€è‡´ã€‚</p>
<p>éé‡åŒ–åœºæ™¯ä¸‹, è¾“å‡ºæ•°æ®ç±»å‹ä¸queryçš„æ•°æ®ç±»å‹ä¿æŒä¸€è‡´ã€‚</p>
<p>é‡åŒ–åœºæ™¯ä¸‹, è‹¥ä¼ å…¥quantScale2, åˆ™è¾“å‡ºæ•°æ®ç±»å‹ä¸ºint8; è‹¥ä¸ä¼ å…¥quantScale2, ä¸”queryã€keyã€valueç±»å‹ä¸ºint8, åˆ™è¾“å‡ºæ•°æ®ç±»å‹ä¸ºFLOAT16ã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>queryã€keyã€valueçš„ç»´åº¦å¿…é¡»ä¿æŒä¸€è‡´, keyã€valueçš„shapeå¿…é¡»ä¿æŒä¸€è‡´ã€‚</p>
<p>num_headsçš„å€¼è¦ç­‰äºqueryçš„Nã€‚</p>
<p>input_layoutçš„å€¼ä¸queryçš„shapeç›¸å…³, ä¸‰ç»´æ˜¯BSH, å››ç»´æ˜¯BNSDã€‚</p>
<p>num_key_value_headsçš„å€¼è¦ç­‰äºkeyã€valueçš„N, éœ€è¦èƒ½è¢«queryçš„å¤´æ•°(num_heads)æ•´é™¤ã€‚</p>
<p>Dä¸€èˆ¬å–å€¼128ã€256ç­‰å…¸å‹å€¼, Dçš„é™åˆ¶ä¸º16k, å¤§äº16kä¼šæŠ¥é”™æ‹¦æˆªã€‚</p>
<p>page attentionçš„ä½¿èƒ½å¿…è¦æ¡ä»¶æ˜¯blocktableå­˜åœ¨ä¸”æœ‰æ•ˆ, åŒæ—¶keyã€valueæ˜¯æŒ‰ç…§blocktableä¸­çš„ç´¢å¼•åœ¨ä¸€ç‰‡è¿ç»­å†…å­˜ä¸­æ’å¸ƒ, æ”¯æŒkeyã€value dtypeä¸ºFLOAT16/BFLOAT16ã€‚</p>
<p>page attentionçš„ä½¿èƒ½åœºæ™¯ä¸‹, blockSizeæ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„å‚æ•°, è¯¥å‚æ•°çš„å–å€¼ä¼šå½±å“page attentionçš„æ€§èƒ½, æ¨èä½¿ç”¨128, æˆ–è€…æ»¡è¶³32byteå¯¹é½ã€‚é€šå¸¸æƒ…å†µä¸‹, page attentionå¯ä»¥æé«˜ååé‡, ä½†ä¼šå¸¦æ¥æ€§èƒ½ä¸Šçš„ä¸‹é™ã€‚</p>
<p>blockTableå½“å‰æ”¯æŒçš„maxBlockNumPerSeqæœ€å¤§ä¸º16k, è¶…è¿‡16kä¼šè¢«æ‹¦æˆªæŠ¥é”™; å¦‚æœé‡åˆ°Sè¶…å¤§å¯¼è‡´maxBlockNumPerSeqè¶…è¿‡16k, å¯ä»¥è°ƒå¤§blockSizeè§£å†³ã€‚</p>
<p>dequantScale1ã€quantScale1ã€dequantScale2ä¸ºä¸€ç»„å‚æ•°, éœ€è¦åŒæ—¶ä¼ å…¥, ä¸”ä¼ å…¥è¯¥ç»„å‚æ•°åä¼šæŒ‰ç…§é‡åŒ–åœºæ™¯å¤„ç†, éœ€è¦queryã€keyã€valueçš„æ•°æ®ç±»å‹ä¸ºint8, å¦åˆ™ä¼šæŠ¥é”™ã€‚</p>
<p>quantScale2ã€quantOffset2ä¸ºä¸€ç»„å‚æ•°, å…¶ä¸­quantOffset2å¯é€‰, ä¼ å…¥è¯¥ç»„å‚æ•°åç®—å­è¾“å‡ºæ•°æ®ç±»å‹ä¼šæ¨å¯¼ä¸ºint8, è‹¥ä¸æœŸæœ›int8è¾“å‡º, è¯·å‹¿ä¼ å…¥è¯¥ç»„å‚æ•°ã€‚</p>
<p>kvå·¦paddingåœºæ™¯kvCacheçš„æ¬è¿èµ·ç‚¹è®¡ç®—å…¬å¼ä¸º: Smax - kvPaddingSize - actualSeqLengthsã€‚kvCacheçš„æ¬è¿ç»ˆç‚¹è®¡ç®—å…¬å¼ä¸º: Smax - kvPaddingSizeã€‚å…¶ä¸­kvCacheçš„æ¬è¿èµ·ç‚¹æˆ–ç»ˆç‚¹å°äº0æ—¶, è¿”å›æ•°æ®ç»“æœä¸ºå…¨0ã€‚</p>
<p>kvå·¦paddingåœºæ™¯kvPaddingSizeå°äº0æ—¶å°†è¢«ç½®ä¸º0ã€‚</p>
<p>kvå·¦paddingåœºæ™¯ä½¿èƒ½éœ€è¦åŒæ—¶å­˜åœ¨actualSeqLengthså‚æ•°, å¦åˆ™é»˜è®¤ä¸ºkvå³paddingåœºæ™¯ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.1</p>
<p>æ”¯æŒçš„å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># å•ç®—å­è°ƒç”¨æ–¹å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ç”Ÿæˆéšæœºæ•°æ®, å¹¶å‘é€åˆ°npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># è°ƒç”¨IFAç®—å­</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_incre_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ‰§è¡Œä¸Šè¿°ä»£ç çš„è¾“å‡ºç±»ä¼¼å¦‚ä¸‹</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.3091</span><span class="p">,</span>  <span class="mf">0.0651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3525</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8252</span><span class="p">,</span>  <span class="mf">0.4084</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2754</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># å…¥å›¾æ–¹å¼</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCHDYNAMO_VERBOSE</span><span class="o">=</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCH_LOGS</span><span class="o">=</span><span class="s2">&quot;+dynamo&quot;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ”¯æŒå…¥å›¾çš„æ‰“å°å®</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">aoe_config</span><span class="o">.</span><span class="n">aoe_mode</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.library</span><span class="w"> </span><span class="kn">import</span> <span class="n">Library</span><span class="p">,</span> <span class="n">impl</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ•°æ®ç”Ÿæˆ</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale_value</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_incre_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale_value</span><span class="p">,</span> <span class="n">atten_mask</span><span class="o">=</span><span class="n">atten</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">MetaInfershape</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single_op</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_incre_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale_value</span><span class="p">,</span> <span class="n">atten_mask</span><span class="o">=</span><span class="n">atten</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;single op output with mask:&quot;</span><span class="p">,</span> <span class="n">single_op</span><span class="p">,</span> <span class="n">single_op</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;graph output with mask:&quot;</span><span class="p">,</span> <span class="n">graph_output</span><span class="p">,</span> <span class="n">graph_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MetaInfershape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ‰§è¡Œä¸Šè¿°ä»£ç çš„è¾“å‡ºç±»ä¼¼å¦‚ä¸‹</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single</span> <span class="n">op</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.2488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6572</span><span class="p">,</span>  <span class="mf">1.0928</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1694</span><span class="p">,</span>  <span class="mf">0.1142</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2266</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.9595</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6602</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.7959</span><span class="p">,</span>  <span class="mf">1.7920</span><span class="p">,</span>  <span class="mf">0.0783</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5120</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.2488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6572</span><span class="p">,</span>  <span class="mf">1.0928</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1694</span><span class="p">,</span>  <span class="mf">0.1142</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2266</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.9595</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6602</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.7959</span><span class="p">,</span>  <span class="mf">1.7920</span><span class="p">,</span>  <span class="mf">0.0783</span><span class="p">]]],</span>       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5120</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_prompt_flash_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_prompt_flash_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_prompt_flash_attention" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>å…¨é‡FAå®ç°, å®ç°å¯¹åº”å…¬å¼:</p>
<p>atten_out = softmax(scale*(query*key)+atten_mask)*value</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_prompt_flash_attention(Tensor query, Tensor key, Tensor value, <a href="#id29"><span class="problematic" id="id30">*</span></a>, Tensor? pse_shiftpadding_mask=None, Tensor? atten_mask=None, int[]? actual_seq_lengths=None, Tensor? deq_scale1=None, Tensor? quant_scale1=None, Tensor? deq_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, int pre_tokens=2147473647, int next_tokens=0, str input_layout=&quot;BSH&quot;, int num_key_value_heads=0, int[]? actual_seq_lengths_kv=None, int sparse_mode=0) -&gt; Tensor</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>Query: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>Key: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>Value: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p><a href="#id31"><span class="problematic" id="id32">*</span></a>: ä»£è¡¨å…¶ä¹‹å‰çš„å˜é‡æ˜¯ä½ç½®ç›¸å…³, éœ€è¦æŒ‰ç…§é¡ºåºè¾“å…¥, å¿…é€‰; ä¹‹åçš„å˜é‡æ˜¯é”®å€¼å¯¹èµ‹å€¼çš„, ä½ç½®æ— å…³, å¯é€‰(ä¸è¾“å…¥ä¼šä½¿ç”¨é»˜è®¤å€¼)ã€‚</p>
<p>pse_shift: å››ç»´Deviceä¾§çš„Input Tensor, shapeæ˜¯(B,N,Query S, Key S)æˆ–è€…(1,N,Query S, Key S), é»˜è®¤å€¼ä¸ºNoneã€‚æ•°æ®ç±»å‹æ”¯æŒFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>padding_mask: é¢„ç•™å‚æ•°, æš‚æœªä½¿ç”¨, é»˜è®¤å€¼ä¸ºNoneã€‚</p>
<p>atten_mask: å››ç»´Deviceä¾§çš„Input Tensor, shapeæ˜¯(B,1,Query S, Key S), å–å€¼ä¸º1ä»£è¡¨è¯¥ä½ä¸å‚ä¸è®¡ç®—(ä¸ç”Ÿæ•ˆ), ä¸º0ä»£è¡¨è¯¥ä½å‚ä¸è®¡ç®—, é»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨å‚ä¸è®¡ç®—; æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BOOL, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>actual_seq_lengths: äºŒç»´Hostä¾§çš„Inputæ•°ç»„, å…¶shapeä¸º(B,1), å½¢å¦‚[1, 2, 3], ä»£è¡¨æ¯ä¸ªbatchä¸­ queryçš„æœ‰æ•ˆåºåˆ—é•¿åº¦, é»˜è®¤å€¼ä¸ºé»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨æœ‰æ•ˆ</p>
<p>deqScale1: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: UINT64ã€FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºç¬¬1æ¬¡Matmulè®¡ç®—ååé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>quantScale1: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: FLOATã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºç¬¬2æ¬¡Matmulè®¡ç®—å‰é‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>deqScale2: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: UINT64ã€FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºç¬¬2æ¬¡Matmulè®¡ç®—åé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>quantScale2: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: FLOATã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºè¾“å‡ºé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>quantOffset2: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: FLOATã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºè¾“å‡ºé‡åŒ–çš„é‡åŒ–åç§», æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>num_heads: Hostä¾§çš„attribute, queryçš„å¤´æ•°, å³BNSDä¸­çš„N, å…¶ä¹˜ä»¥Dä¸ºH, é»˜è®¤å€¼ä¸º1; æ•°æ®ç±»å‹ä¸ºINTã€‚</p>
<p>scale_value: Hostä¾§çš„attribute, ç¼©æ”¾ç³»æ•°, ç”¨æ¥çº¦æŸæ¢¯åº¦, å…¶é»˜è®¤å€¼ä¸º1.0, å…¸å‹å€¼ä¸º1/sqrt(D); æ•°æ®ç±»å‹ä¸ºFLOAT32ã€‚</p>
<p>pre_tokens: Hostä¾§çš„attribute, ç”¨äºæŒ‡å®šå‚ä¸è®¡ç®—çš„æœ‰æ•ˆæ•°æ®å—, å…¶é»˜è®¤å€¼ä¸º2147473647</p>
<p>next_tokens: Hostä¾§çš„attribute, ç”¨äºæŒ‡å®šå‚ä¸è®¡ç®—çš„æœ‰æ•ˆæ•°æ®å—, å…¶é»˜è®¤å€¼ä¸º0</p>
<p>input_layout: Hostä¾§çš„attribute, ä»£è¡¨queryã€keyã€valueçš„å¸ƒå±€, æ ¹æ®è¾“å…¥çš„Queryã€Keyã€Valueçš„shapeç¡®å®š, ä¸‰ç»´å¼ é‡æ˜¯BSH, å››ç»´å¼ é‡æ˜¯BNSD, é»˜è®¤å€¼ä¸ºBSH; æ•°æ®ç±»å‹ä¸ºstringã€‚</p>
<p>num_key_value_heads: Hostä¾§çš„attribute, kvçš„å¤´æ•°, é»˜è®¤å€¼ä¸º0, è¡¨ç¤ºä¸qçš„å¤´æ•°ç›¸åŒ; å¦åˆ™è¡¨ç¤ºkvçš„å¤´æ•°, éœ€è¦èƒ½è¢«qçš„å¤´æ•°(num_heads)æ•´é™¤; æ•°æ®ç±»å‹ä¸ºINT64ã€‚</p>
<p>actual_seq_lengths_kv: Hostä¾§çš„attribute, æ¯ä¸ªbatchä¸­keyå’Œvalueçš„ Sçš„æœ‰æ•ˆé•¿åº¦, å…¶shapeä¸º(B,1), ä»£è¡¨kvä¸­æœ‰æ•ˆçš„åºåˆ—é•¿åº¦, é»˜è®¤å€¼ä¸ºé»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨æœ‰æ•ˆ; æ•°æ®ç±»å‹ä¸ºINT64ã€‚</p>
<p>sparse_mode: Hostä¾§çš„attribute, é’ˆå¯¹noMaskã€leftUpCasualã€rightDownCasualã€bandå››ç±»sparseåœºæ™¯, æ–°å¢å¯é€‰å±æ€§sparse_mode(UINT64, æšä¸¾), å¯¹åº”æšä¸¾å€¼åˆ†åˆ«ä¸º0ã€2ã€3ã€4ã€‚</p>
<p><a href="#id33"><span class="problematic" id="id34">**</span></a>è¾“å‡ºè¯´æ˜**å…±ä¸€ä¸ªè¾“å‡º, ä¸ºè®¡ç®—çš„æœ€ç»ˆç»“æœatten_out, ç±»å‹ä¸ºTensor, shapeä¸queryä¿æŒä¸€è‡´ã€‚Â·</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>queryã€keyã€valueçš„ç»´åº¦å¿…é¡»ä¿æŒä¸€è‡´, keyã€valueçš„shapeå¿…é¡»ä¿æŒä¸€è‡´ã€‚</p>
<p>pse_shiftä½¿èƒ½æ—¶, ç›®å‰åªæ”¯æŒqueryä¸ºFLOAT16ç±»å‹, ä¸”pse_shiftä¹Ÿä¸ºFLOAT16ç±»å‹ã€‚</p>
<p>num_headsçš„å€¼è¦ç­‰äºqueryçš„Nã€‚</p>
<p>input_layoutçš„å€¼ä¸queryçš„shapeç›¸å…³, ä¸‰ç»´æ˜¯â€œBSHâ€, å››ç»´æ˜¯â€œBNSDâ€ã€‚</p>
<p>num_key_value_headsçš„å€¼è¦ç­‰äºkeyã€valueçš„N, éœ€è¦èƒ½è¢«queryçš„å¤´æ•°(num_heads)æ•´é™¤ã€‚</p>
<p>Dä¸€èˆ¬å–å€¼128ã€256ç­‰å…¸å‹å€¼, Dçš„é™åˆ¶ä¸º16k, å¤§äº16kä¼šæŠ¥é”™æ‹¦æˆªã€‚</p>
<p>int8é‡åŒ–ç›¸å…³å…¥å‚æ•°é‡ä¸è¾“å…¥ã€è¾“å‡ºæ•°æ®æ ¼å¼çš„ç»¼åˆé™åˆ¶:</p>
<p>è¾“å…¥ä¸ºINT8, è¾“å‡ºä¸ºINT8çš„åœºæ™¯: å…¥å‚deqScale1ã€quantScale1ã€deqScale2ã€quantScale2ã€quantOffset2éœ€è¦åŒæ—¶å­˜åœ¨ã€‚</p>
<p>è¾“å…¥ä¸ºINT8, è¾“å‡ºä¸ºFLOAT16çš„åœºæ™¯: å…¥å‚deqScale1ã€quantScale1ã€deqScale2éœ€è¦åŒæ—¶å­˜åœ¨, è‹¥å­˜åœ¨å…¥å‚quantOffset2 æˆ–quantScale2(å³ä¸ä¸ºnullptr), åˆ™æŠ¥é”™å¹¶è¿”å›ã€‚</p>
<p>è¾“å…¥ä¸ºFLOAT16, è¾“å‡ºä¸ºINT8çš„åœºæ™¯: å…¥å‚quantOffset2 æˆ– quantScale2éœ€è¦åŒæ—¶å­˜åœ¨, è‹¥å­˜åœ¨å…¥å‚deqScale1 æˆ– quantScale1 æˆ– deqScale2(å³ä¸ä¸ºnullptr), åˆ™æŠ¥é”™å¹¶è¿”å›ã€‚</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.1</p>
<p>æ”¯æŒçš„èŠ¯ç‰‡å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># å•ç®—å­è°ƒç”¨æ–¹å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ç”Ÿæˆéšæœºæ•°æ®, å¹¶å‘é€åˆ°npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># è°ƒç”¨PFAç®—å­</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_prompt_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ‰§è¡Œä¸Šè¿°ä»£ç çš„è¾“å‡ºç±»ä¼¼å¦‚ä¸‹</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># å…¥å›¾æ–¹å¼</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCHDYNAMO_VERBOSE</span><span class="o">=</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCH_LOGS</span><span class="o">=</span><span class="s2">&quot;+dynamo&quot;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ”¯æŒå…¥å›¾çš„æ‰“å°å®</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">aoe_config</span><span class="o">.</span><span class="n">aoe_mode</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.library</span><span class="w"> </span><span class="kn">import</span> <span class="n">Library</span><span class="p">,</span> <span class="n">impl</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ•°æ®ç”Ÿæˆ</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_prompt_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">MetaInfershape</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single_op</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_prompt_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;single op output with mask:&quot;</span><span class="p">,</span> <span class="n">single_op</span><span class="p">,</span> <span class="n">single_op</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;graph output with mask:&quot;</span><span class="p">,</span> <span class="n">graph_output</span><span class="p">,</span> <span class="n">graph_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MetaInfershape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ‰§è¡Œä¸Šè¿°ä»£ç çš„è¾“å‡ºç±»ä¼¼å¦‚ä¸‹</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single</span> <span class="n">op</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>        <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_fused_infer_attention_score">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_fused_infer_attention_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_fused_infer_attention_score" title="Link to this definition">ïƒ</a></dt>
<dd></dd></dl>

<p><strong>åŠŸèƒ½æè¿°</strong>:</p>
<p>ç®—å­åŠŸèƒ½ï¼šé€‚é…å¢é‡&amp;å…¨é‡æ¨ç†åœºæ™¯çš„FlashAttentionç®—å­ï¼Œæ—¢å¯ä»¥æ”¯æŒå…¨é‡è®¡ç®—åœºæ™¯ï¼ˆPromptFlashAttentionï¼‰ï¼Œä¹Ÿå¯æ”¯æŒå¢é‡è®¡ç®—åœºæ™¯ï¼ˆIncreFlashAttentionï¼‰ã€‚å½“QueryçŸ©é˜µçš„Sä¸º1ï¼Œè¿›å…¥IncreFlashAttentionåˆ†æ”¯ï¼Œå…¶ä½™åœºæ™¯è¿›å…¥PromptFlashAttentionåˆ†æ”¯ã€‚</p>
<p>è®¡ç®—å…¬å¼ï¼šatten_out = softmax(scale*(query*key)+atten_mask)*value</p>
<p><strong>æ¥å£åŸå‹</strong>:</p>
<p>torch_npu.npu_fused_infer_attention_score(Tensor query, Tensor key, Tensor value, <a href="#id35"><span class="problematic" id="id36">*</span></a>, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout=&quot;BSH&quot;, int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -&gt; (Tensor, Tensor)</p>
<p><strong>å‚æ•°è¯´æ˜</strong>:</p>
<p>Query: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>Key: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>Value: ä¸‰ç»´æˆ–è€…å››ç»´Deviceä¾§çš„Input Tensor; ä¸‰ç»´: shapeæ˜¯(B,S,H), å¯¹åº”çš„input_layoutæ˜¯BSH; å››ç»´: shapeæ˜¯(B,N,S,D), å…¶ä¸­N*D=H, æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p><a href="#id37"><span class="problematic" id="id38">*</span></a>: ä»£è¡¨å…¶ä¹‹å‰çš„å˜é‡æ˜¯ä½ç½®ç›¸å…³, éœ€è¦æŒ‰ç…§é¡ºåºè¾“å…¥, å¿…é€‰; ä¹‹åçš„å˜é‡æ˜¯é”®å€¼å¯¹èµ‹å€¼çš„, ä½ç½®æ— å…³, å¯é€‰(ä¸è¾“å…¥ä¼šä½¿ç”¨é»˜è®¤å€¼)ã€‚</p>
<p>pse_shift: å››ç»´Deviceä¾§çš„Input Tensor, shapeæ˜¯(B,N,Query S, Key S)æˆ–è€…(1,N,Query S, Key S), é»˜è®¤å€¼ä¸ºNoneã€‚æ•°æ®ç±»å‹æ”¯æŒFLOAT16, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>atten_mask: å››ç»´Deviceä¾§çš„Input Tensor, shapeæ˜¯(B,1,Query S, Key S), å–å€¼ä¸º1ä»£è¡¨è¯¥ä½ä¸å‚ä¸è®¡ç®—(ä¸ç”Ÿæ•ˆ), ä¸º0ä»£è¡¨è¯¥ä½å‚ä¸è®¡ç®—, é»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨å‚ä¸è®¡ç®—; æ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BOOL, æ•°æ®æ ¼å¼æ”¯æŒNDã€‚</p>
<p>actual_seq_lengths: äºŒç»´Hostä¾§çš„Inputæ•°ç»„, å…¶shapeä¸º(B,1), å½¢å¦‚[1, 2, 3], ä»£è¡¨æ¯ä¸ªbatchä¸­ queryçš„æœ‰æ•ˆåºåˆ—é•¿åº¦, é»˜è®¤å€¼ä¸ºé»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨æœ‰æ•ˆã€‚</p>
<p>actual_seq_lengths_kv: Hostä¾§çš„attribute, æ¯ä¸ªbatchä¸­keyå’Œvalueçš„ Sçš„æœ‰æ•ˆé•¿åº¦, å…¶shapeä¸º(B,1), ä»£è¡¨kvä¸­æœ‰æ•ˆçš„åºåˆ—é•¿åº¦, é»˜è®¤å€¼ä¸ºé»˜è®¤å€¼ä¸ºNone, å³å…¨éƒ¨æœ‰æ•ˆ; æ•°æ®ç±»å‹ä¸ºINT64ã€‚</p>
<p>antiquantScaleï¼šDeviceä¾§çš„aclTensorï¼Œæ•°æ®ç±»å‹æ”¯æŒï¼šFLOAT32ã€FLOAT16ã€BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼ˆå‚è€ƒï¼‰ï¼Œè¡¨ç¤ºåé‡åŒ–å› å­ï¼Œæ”¯æŒper-tensorï¼Œper-channelï¼ŒQ_Sä¸º1æ—¶åªæ”¯æŒper-channelï¼Œç»¼åˆçº¦æŸè¯·è§çº¦æŸä¸é™åˆ¶ã€‚</p>
<p>antiquantOffsetï¼šDeviceä¾§çš„aclTensorï¼Œæ•°æ®ç±»å‹æ”¯æŒï¼šFLOAT32ã€FLOAT16ã€BFLOAT16ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼ˆå‚è€ƒï¼‰ï¼Œè¡¨ç¤ºåé‡åŒ–åç§»ï¼Œæ”¯æŒper-tensorï¼Œper-channelï¼ŒQ_Sä¸º1æ—¶åªæ”¯æŒper-channelï¼Œç»¼åˆçº¦æŸè¯·è§çº¦æŸä¸é™åˆ¶ã€‚</p>
<p>dequant_scale1: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: UINT64ã€FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºç¬¬1æ¬¡Matmulè®¡ç®—ååé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>quantScale1: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: FLOATã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºç¬¬2æ¬¡Matmulè®¡ç®—å‰é‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>dequant_scale2: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: UINT64ã€FLOAT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºç¬¬2æ¬¡Matmulè®¡ç®—åé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>quantScale2: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: FLOATã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºè¾“å‡ºé‡åŒ–çš„é‡åŒ–å› å­, æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>quantOffset2: Deviceä¾§çš„Input Tensor, å…¶shapeä¸º(1), æ•°æ®ç±»å‹æ”¯æŒ: FLOATã€‚æ•°æ®æ ¼å¼æ”¯æŒND, è¡¨ç¤ºè¾“å‡ºé‡åŒ–çš„é‡åŒ–åç§», æ”¯æŒpre-tensor(scalar)ã€‚ å¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½æ—¶å¯ä¼ å…¥nullptrã€‚</p>
<p>blocktableï¼šDeviceä¾§çš„aclTensorï¼Œæ•°æ®ç±»å‹æ”¯æŒï¼šINT32ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDï¼ˆå‚è€ƒï¼‰ã€‚è¡¨ç¤ºPageAttentionä¸­KVå­˜å‚¨ä½¿ç”¨çš„blockæ˜ å°„è¡¨ï¼Œå¦‚ä¸ä½¿ç”¨è¯¥åŠŸèƒ½å¯ä¼ å…¥nullptrï¼›Q_Så¤§äºç­‰äº2æ—¶è¯¥å‚æ•°æ— æ•ˆã€‚</p>
<p>queryPaddingSizeï¼šQueryä¸­æ¯ä¸ªbatchçš„æ•°æ®æ˜¯å¦å³å¯¹é½ï¼Œä¸”å³å¯¹é½çš„ä¸ªæ•°æ˜¯å¤šå°‘ã€‚ä»…æ”¯æŒQ_Sç­‰äº1ï¼›Q_Så¤§äºç­‰äº2æ—¶è¯¥å‚æ•°æ— æ•ˆã€‚ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å¯ä¼ å…¥é»˜è®¤å€¼nullptrã€‚</p>
<p>kvPaddingSizeï¼škey/valueä¸­æ¯ä¸ªbatchçš„æ•°æ®æ˜¯å¦å³å¯¹é½ï¼Œä¸”å³å¯¹é½çš„ä¸ªæ•°æ˜¯å¤šå°‘ã€‚ä»…æ”¯æŒQ_Sç­‰äº1ï¼›Q_Så¤§äºç­‰äº2æ—¶è¯¥å‚æ•°æ— æ•ˆã€‚ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å¯ä¼ å…¥é»˜è®¤å€¼nullptrã€‚</p>
<p>num_heads: Hostä¾§çš„attribute, queryçš„å¤´æ•°, å³BNSDä¸­çš„N, å…¶ä¹˜ä»¥Dä¸ºH, é»˜è®¤å€¼ä¸º1; æ•°æ®ç±»å‹ä¸ºINTã€‚</p>
<p>scale: Hostä¾§çš„attribute, ç¼©æ”¾ç³»æ•°, ç”¨æ¥çº¦æŸæ¢¯åº¦, å…¶é»˜è®¤å€¼ä¸º1.0, å…¸å‹å€¼ä¸º1/sqrt(D); æ•°æ®ç±»å‹ä¸ºFLOAT32ã€‚</p>
<p>pre_tokens: Hostä¾§çš„attribute, ç”¨äºæŒ‡å®šå‚ä¸è®¡ç®—çš„æœ‰æ•ˆæ•°æ®å—, å…¶é»˜è®¤å€¼ä¸º2147473647</p>
<p>next_tokens: Hostä¾§çš„attribute, ç”¨äºæŒ‡å®šå‚ä¸è®¡ç®—çš„æœ‰æ•ˆæ•°æ®å—, å…¶é»˜è®¤å€¼ä¸º0</p>
<p>input_layout: Hostä¾§çš„attribute, ä»£è¡¨queryã€keyã€valueçš„å¸ƒå±€, æ ¹æ®è¾“å…¥çš„Queryã€Keyã€Valueçš„shapeç¡®å®š, ä¸‰ç»´å¼ é‡æ˜¯BSH, å››ç»´å¼ é‡æ˜¯BNSD, é»˜è®¤å€¼ä¸ºBSH; æ•°æ®ç±»å‹ä¸ºstringã€‚</p>
<p>num_key_value_heads: Hostä¾§çš„attribute, kvçš„å¤´æ•°, é»˜è®¤å€¼ä¸º0, è¡¨ç¤ºä¸qçš„å¤´æ•°ç›¸åŒ; å¦åˆ™è¡¨ç¤ºkvçš„å¤´æ•°, éœ€è¦èƒ½è¢«qçš„å¤´æ•°(num_heads)æ•´é™¤; æ•°æ®ç±»å‹ä¸ºINT64ã€‚</p>
<p>sparse_mode: Hostä¾§çš„attribute, é’ˆå¯¹noMaskã€leftUpCasualã€rightDownCasualã€bandå››ç±»sparseåœºæ™¯, æ–°å¢å¯é€‰å±æ€§sparse_mode(UINT64, æšä¸¾), å¯¹åº”æšä¸¾å€¼åˆ†åˆ«ä¸º0ã€2ã€3ã€4ã€‚</p>
<p>innerPreciseï¼šHostä¾§çš„intï¼Œä¸€å…±4ç§æ¨¡å¼ï¼š0ã€1ã€2ã€3ã€‚ä¸€å…±ä¸¤ä½bitä½ï¼Œç¬¬0ä½ï¼ˆbit0ï¼‰è¡¨ç¤ºé«˜ç²¾åº¦æˆ–è€…é«˜æ€§èƒ½é€‰æ‹©ï¼Œç¬¬1ä½ï¼ˆbit1ï¼‰è¡¨ç¤ºæ˜¯å¦åšè¡Œæ— æ•ˆä¿®æ­£ã€‚æ•°æ®ç±»å‹æ”¯æŒï¼šINT64ã€‚Q_Sä¸º1æ—¶è¯¥å‚æ•°æ— æ•ˆã€‚ç»¼åˆçº¦æŸè¯·è§çº¦æŸä¸é™åˆ¶ã€‚</p>
<p>innerPreciseä¸º0æ—¶ï¼Œä»£è¡¨å¼€å¯é«˜ç²¾åº¦æ¨¡å¼ï¼Œä¸”ä¸åšè¡Œæ— æ•ˆä¿®æ­£ã€‚</p>
<p>innerPreciseä¸º1æ—¶ï¼Œä»£è¡¨é«˜æ€§èƒ½æ¨¡å¼ï¼Œä¸”ä¸åšè¡Œæ— æ•ˆä¿®æ­£ã€‚</p>
<p>innerPreciseä¸º2æ—¶ï¼Œä»£è¡¨å¼€å¯é«˜ç²¾åº¦æ¨¡å¼ï¼Œä¸”åšè¡Œæ— æ•ˆä¿®æ­£ã€‚</p>
<p>innerPreciseä¸º3æ—¶ï¼Œä»£è¡¨é«˜æ€§èƒ½æ¨¡å¼ï¼Œä¸”åšè¡Œæ— æ•ˆä¿®æ­£ã€‚</p>
<p>softmaxLseFlagï¼šæ˜¯å¦è¾“å‡ºsoftmax_lseï¼Œæ”¯æŒSè½´å¤–åˆ‡ï¼ˆå¢åŠ è¾“å‡ºï¼‰ã€‚é¢„ç•™å‚æ•°ï¼Œæš‚ä¸æ”¯æŒã€‚ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å¯ä¼ å…¥é»˜è®¤å€¼falseã€‚</p>
<p>blockSizeï¼šHostä¾§çš„int64_tï¼ŒPageAttentionä¸­KVå­˜å‚¨æ¯ä¸ªblockä¸­æœ€å¤§çš„tokenä¸ªæ•°ï¼Œé»˜è®¤ä¸º0ï¼Œæ•°æ®ç±»å‹æ”¯æŒINT64ï¼ŒQ_Så¤§äºç­‰äº2æ—¶è¯¥å‚æ•°æ— æ•ˆã€‚</p>
<p>antiquantModeï¼šä¼ªé‡åŒ–çš„æ–¹å¼ï¼Œåˆ†ä¸ºperchannelï¼ˆperchannelåŒ…å«pertensorï¼‰å’Œpertokenã€‚ä»…æ”¯æŒQ_Sç­‰äº1ï¼›Q_Så¤§äºç­‰äº2æ—¶è¯¥å‚æ•°æ— æ•ˆã€‚ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å¯ä¼ å…¥é»˜è®¤å€¼nullptrã€‚</p>
<p>attentionOutï¼ˆaclTensor*ï¼Œè®¡ç®—è¾“å‡ºï¼‰ï¼šDeviceä¾§çš„aclTensorï¼Œå…¬å¼ä¸­çš„è¾“å‡ºï¼Œæ•°æ®ç±»å‹æ”¯æŒFLOAT16ã€BFLOAT16ã€INT8ã€‚æ•°æ®æ ¼å¼æ”¯æŒNDã€‚é™åˆ¶ï¼šè¯¥å…¥å‚çš„shapeéœ€è¦ä¸å…¥å‚queryçš„shapeä¿æŒä¸€è‡´ã€‚</p>
<p>softmaxLseï¼ˆaclTensor*ï¼Œè®¡ç®—è¾“å‡ºï¼‰ï¼šflashdecodeç®—æ³•å¯¹queryä¹˜keyçš„ç»“æœå…ˆå–expå†å–sumï¼Œæœ€åå–logå¾—åˆ°çš„ç»“æœã€‚é¢„ç•™å‚æ•°ï¼Œæš‚ä¸æ”¯æŒã€‚ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å¯ä¼ å…¥é»˜è®¤å€¼nullptrã€‚</p>
<p><a href="#id39"><span class="problematic" id="id40">**</span></a>è¾“å‡ºè¯´æ˜**å…±ä¸¤ä¸ªè¾“å‡º, atten_outä¸ºè®¡ç®—çš„æœ€ç»ˆç»“æœ, ç±»å‹ä¸ºTensor, shapeä¸queryä¿æŒä¸€è‡´ã€‚softmaxLseå½“å‰é¢„ç•™ï¼Œæš‚ä¸æ”¯æŒã€‚</p>
<p><strong>çº¦æŸè¯´æ˜</strong>:</p>
<p>å½“Q_Sç­‰äº1æ—¶ï¼šè¯·å‚è€ƒIncre_Flash_Attentioné™åˆ¶</p>
<p>å½“Q_Så¤§äº1æ—¶ï¼šè¯·å‚è€ƒPrompt_Flash_Attentioné™åˆ¶</p>
<p>æ”¯æŒçš„PyTorchç‰ˆæœ¬:</p>
<p>PyTorch 2.1</p>
<p>æ”¯æŒçš„èŠ¯ç‰‡å‹å·:</p>
<p>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</p>
<p><strong>ç¤ºä¾‹</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># å•ç®—å­è°ƒç”¨æ–¹å¼</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ç”Ÿæˆéšæœºæ•°æ®, å¹¶å‘é€åˆ°npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># è°ƒç”¨FIAç®—å­</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_infer_attention_score</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ‰§è¡Œä¸Šè¿°ä»£ç çš„è¾“å‡ºç±»ä¼¼å¦‚ä¸‹</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># å…¥å›¾æ–¹å¼</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCHDYNAMO_VERBOSE</span><span class="o">=</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCH_LOGS</span><span class="o">=</span><span class="s2">&quot;+dynamo&quot;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ”¯æŒå…¥å›¾çš„æ‰“å°å®</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">aoe_config</span><span class="o">.</span><span class="n">aoe_mode</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.library</span><span class="w"> </span><span class="kn">import</span> <span class="n">Library</span><span class="p">,</span> <span class="n">impl</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ•°æ®ç”Ÿæˆ</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_infer_attention_score</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">MetaInfershape</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single_op</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_infer_attention_score</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;single op output with mask:&quot;</span><span class="p">,</span> <span class="n">single_op</span><span class="p">,</span> <span class="n">single_op</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;graph output with mask:&quot;</span><span class="p">,</span> <span class="n">graph_output</span><span class="p">,</span> <span class="n">graph_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MetaInfershape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># æ‰§è¡Œä¸Šè¿°ä»£ç çš„è¾“å‡ºç±»ä¼¼å¦‚ä¸‹</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single</span> <span class="n">op</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>        <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="é¡µè„š">
        <a href="examples.html" class="btn btn-neutral float-left" title="åŠŸèƒ½æ ·ä¾‹" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> ä¸Šä¸€é¡µ</a>
        <a href="faq.html" class="btn btn-neutral float-right" title="FAQ" accesskey="n" rel="next">ä¸‹ä¸€é¡µ <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; ç‰ˆæƒæ‰€æœ‰ 2024, Ascendã€‚</p>
  </div>

  åˆ©ç”¨ <a href="https://www.sphinx-doc.org/">Sphinx</a> æ„å»ºï¼Œä½¿ç”¨çš„ 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">ä¸»é¢˜</a>
    ç”± <a href="https://readthedocs.org">Read the Docs</a> å¼€å‘.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>